[
  {
    "objectID": "posts/2025-01-08_using-targets/index.html",
    "href": "posts/2025-01-08_using-targets/index.html",
    "title": "Three short stories about targets",
    "section": "",
    "text": "About 18 months ago I wrote a post about balrogs and makefiles. The post was long, strange, but also cathartic. It had bothered me for years that I didn’t really understand make as well as I wanted to, and it was really helpful to write up some notes about it as a way of teaching myself how to use it more effectively than I had done in the past.1 Buried at the very end of the post is an almost-apologetic reference to the targets R package by Will Landau, which provides a similar toolkit designed to work cleanly for R users.\nEven then I knew that I was going to need to learn how to use targets, but somehow I never quite got around to it… life gets in the way, I suppose. I have been preoccupied by other tasks, sadly, and it has taken me until now to (a) sit down and read through the targets user manual, and (b) come up with some fun side projects that would give me the opportunity to try it out.\nThankfully, I recently managed to set some time aside to teach myself how to use the package, using three toy projects that I bundled together into a small github repository. Realising that I would inevitably forget what I’ve learned if I didn’t write up some notes on the projects, I decided to write them up as a blog post.\nSo, here goes. This is a tale of three targets…\nlibrary(targets)\nlibrary(ggplot2)\nlibrary(legendry)\nlibrary(tidyselect)\nlibrary(dplyr)\nlibrary(fs)"
  },
  {
    "objectID": "posts/2025-01-08_using-targets/index.html#story-1-an-analysis-pipeline",
    "href": "posts/2025-01-08_using-targets/index.html#story-1-an-analysis-pipeline",
    "title": "Three short stories about targets",
    "section": "Story 1: An analysis pipeline",
    "text": "Story 1: An analysis pipeline\nThe first pipeline I built using targets was based the fun Tidy Tuesday visualisations that I described in my last post. In that post, I talked about two images I made using a data set about Dungeons & Dragons spells. When I wrote that post, I didn’t talk about targets at all, and none of the code presented in that blog post is written in a “targets-friendly” way. However, although that form of the code was the simplest way to write it up, it’s not how I originally wrote it. It was originally written as a targets pipeline.\nAs it happens, I have a slightly-modified copy of the project in the spells directory within this blog post, so that I can play with the project within this post. Yay! But this in turn brings us to the first point to make about targets: it’s a project-oriented tool, in the sense that each project corresponds to a single folder (and its sub-folders), and not surprisingly it’s easiest to use from within the project. To make my life easier I’ll sometimes change that directory so that when I’m discussing a specific targets project, the code will execute from the root directory of that project. At the R console we would use setwd() to do so, but that’s not the best approach within a knitr-based tool (quarto, rmarkdown, etc), and you will get warning messages if you try to do it that way. The preferred method for changing directories with an R markdown or quarto document is to set the root.dir knitr option. Because I’ll be doing that several times in this post, I’ll write a little convenience function to take care of this whenever I need to switch to a new project:\n\nset_knitr_dir &lt;- function(dir, post = \"2025-01-08_using-targets\") {\n  knitr::opts_knit$set(root.dir = here::here(\"posts\", post, dir))\n}\nset_knitr_dir(\"spells\")\n\nNext, let’s take a look at the structure of this project. At the moment this is a clean project (i.e., no code has been run yet), so it contains only the source files. There are only three:\n\ndir_tree()\n\n.\n├── _targets.R\n├── analysis.R\n└── spells.csv\n\n\nHere’s what each file does:\n\nThe spells.csv file is the data set I wish to analyse\nThe analysis.R script defines a collection of functions which, when called, will perform the required analyses and generate the outputs\nThe _targets.R script is (unsurprisingly) the build script\n\nIn a moment I’ll talk about the _targets.R script, but first I’ll quickly describe what the analysis itself does.\n\nWhat does the analysis do?\nIf you read the schools of magic post that I wrote earlier this year, you’ve already seen the code underpinning the analysis. The only difference between the version in the previous post and the version I’ve used here is that the analysis.R script wraps each step of the analysis into a function. This is pretty crucial to constructing a targets pipeline, actually, and the targets user manual has a whole section discussing function-oriented workflow.\n\nFunctions are the building blocks of most computer code. They make code easier to think about, and they break down complicated ideas into small manageable pieces. Out of context, you can develop and test a function in isolation without mentally juggling the rest of the project. In the context of the whole workflow, functions are convenient shorthand to make your work easier to read.\n\nOne thing that continually surprises me about analysis scripts that I encounter in the wild is that analysts don’t write functions enough. It’s easily the most common trap I see people falling into, to be honest, and it leads to some very dangerous and hard-to-isolate bugs because the dependencies between different parts of the code become hard to see as the script gets longer. So I am a big fan of the design feature in targets that pushes the user to break a big analysis into a smaller number of functions that perform specific tasks. For example, once the spells data set has been loaded, the pipeline that constructs the “spell dice” plot is encapsulated by two key functions:\n\ndice_data() takes the spells data as input, and performs all the data wrangling steps required to construct a tidied version of the data that is suitable for visualisation\ndice_plot() takes the tidied dice data as input, specifies the “spell dice” plot, and writes it to an output file\n\n\n\n\n\n\ndice_pic.png\n\n\nThe “schools of magic” plot is slightly more elaborate, and uses three functions:\n\nscholastic_data() takes the spells data as input, and performs the data wrangling steps required to create tidy data suitable for constructing the heatmap\nscholastic_clusters() takes this tidy data as input, and the performs additional steps required to construct the hierarchical clustering used to draw dendrograms alongside the heatmap\nscholastic_plot() takes the data set and the clustering as input, and uses them to build the “schools of magic” plot that is written to an output file\n\n\n\n\n\n\nscholastic_pic.png\n\n\nThe actual code for these functions isn’t very important for the purposes of understanding the targets pipelinem, and in any case I’ve described the plots in detail before. But for what it’s worth, the exact code is included below the fold here:\n\n\n\nClick to show/hide the analysis.R code\n\nanalysis.R\n\n# spell dice plot ---------------------------------------------------------\n\ndice_data &lt;- function(spells) {\n  dice_dat &lt;- spells |&gt;\n    select(name, level, description) |&gt;\n    mutate(\n      dice_txt = str_extract_all(description, \"\\\\b\\\\d+d\\\\d+\\\\b\"),\n      dice_txt = purrr::map(dice_txt, unique)\n    ) |&gt;\n    unnest_longer(\n      col = \"dice_txt\",\n      values_to = \"dice_txt\",\n      indices_to = \"position\"\n    ) |&gt;\n    mutate(\n      dice_num = dice_txt |&gt; str_extract(\"\\\\d+(?=d)\") |&gt; as.numeric(),\n      dice_die = dice_txt |&gt; str_extract(\"(?&lt;=d)\\\\d+\") |&gt; as.numeric(),\n      dice_val = dice_num * (dice_die + 1)/2,\n      dice_txt = factor(dice_txt) |&gt; fct_reorder(dice_val)\n    )\n  return(dice_dat)\n}\n\ndice_plot &lt;- function(dice_dat) {\n  \n  palette &lt;- hcl.colors(n = 10, palette = \"PuOr\")\n  \n  labs &lt;- dice_dat |&gt;\n    summarise(\n      dice_txt = first(dice_txt),\n      count = n(),\n      .by = dice_txt\n    )\n  \n  pic &lt;- ggplot(\n    data = dice_dat,\n    mapping = aes(\n      x = dice_txt,\n      fill = factor(level)\n    )\n  ) +\n    geom_bar(color = \"#222\") +\n    geom_label_repel(\n      data = labs,\n      mapping = aes(\n        x = dice_txt,\n        y = count,\n        label = dice_txt\n      ),\n      size = 3,\n      direction = \"y\",\n      seed = 1,\n      nudge_y = 4,\n      color = \"#ccc\",\n      fill = \"#222\",\n      arrow = NULL,\n      inherit.aes = FALSE\n    ) +\n    scale_fill_manual(\n      name = \"Spell level\",\n      values = palette\n    ) +\n    scale_x_discrete(\n      name = \"Increasing average outcome \\u27a1\",\n      breaks = NULL,\n      expand = expansion(.05)\n    ) +\n    scale_y_continuous(name = NULL) +\n    labs(title = \"Dice rolls described in D&D spell descriptions\") +\n    theme_void() +\n    theme(\n      plot.background = element_rect(fill = \"#222\"),\n      text = element_text(color = \"#ccc\"),\n      axis.text = element_text(color = \"#ccc\"),\n      axis.title = element_text(color = \"#ccc\"),\n      plot.margin = unit(c(1, 1, 1, 1), units = \"cm\"),\n      legend.position = \"inside\",\n      legend.position.inside = c(.3, .825),\n      legend.direction = \"horizontal\",\n      legend.title.position = \"top\",\n      legend.byrow = TRUE\n    )\n  \n  ggsave(\n    filename = \"dice_pic.png\",\n    plot = pic,\n    width = 2000,\n    height = 1000,\n    units = \"px\",\n    dpi = 150\n  )\n  \n  return(\"dice_pic.png\")\n}\n\n\n# schools of magic plot ---------------------------------------------------\n\n# constructs the data frame used by geom_tile() later\nscholastic_data &lt;- function(spells) {\n  spells |&gt;\n    select(name, school, bard:wizard) |&gt;\n    pivot_longer(\n      cols = bard:wizard,\n      names_to = \"class\",\n      values_to = \"castable\"\n    ) |&gt;\n    summarise(\n      count = sum(castable),\n      .by = c(\"school\", \"class\")\n    ) |&gt;\n    mutate(\n      school = str_to_title(school),\n      class  = str_to_title(class)\n    )\n}\n\n# hierarchical clustering for the schools and classes\nscholastic_clusters &lt;- function(dat) {\n  \n  # matrix of counts for each school/class combination\n  mat &lt;- dat |&gt;\n    pivot_wider(\n      names_from = \"school\",\n      values_from = \"count\"\n    ) |&gt;\n    as.data.frame()\n  rownames(mat) &lt;- mat$class\n  mat$class &lt;- NULL\n  as.matrix(mat)\n  \n  # each school is a distribution over classes,\n  # each class is a distribution over schools\n  class_distribution  &lt;- mat / replicate(ncol(mat), rowSums(mat))\n  school_distribution &lt;- t(mat) / (replicate(nrow(mat), colSums(mat)))\n  \n  # pairwise distances\n  class_dissimilarity  &lt;- dist(class_distribution)\n  school_dissimilarity &lt;- dist(school_distribution)\n  \n  # hierarchical clustering\n  clusters &lt;- list(\n    class = hclust(class_dissimilarity, method = \"average\"),\n    school = hclust(school_dissimilarity, method = \"average\")\n  )\n  \n  return(clusters)\n}\n\nscholastic_plot &lt;- function(dat, clusters) {\n  \n  pic &lt;- ggplot(dat, aes(school, class, fill = count)) +\n    geom_tile() +\n    scale_x_dendro(\n      clust = clusters$school,\n      guide = guide_axis_dendro(n.dodge = 2),\n      expand = expansion(0, 0),\n      position = \"top\"\n    ) +\n    scale_y_dendro(\n      clust = clusters$class,\n      expand = expansion(0, 0)\n    ) +\n    scale_fill_distiller(palette = \"RdPu\") +\n    labs(\n      x = \"The Schools of Magic\",\n      y = \"The Classes of Character\",\n      fill = \"Number of Learnable Spells\"\n    ) +\n    coord_equal() +\n    theme(\n      plot.background = element_rect(fill = \"#222\", color = \"#222\"),\n      plot.margin = unit(c(2, 2, 2, 2), units = \"cm\"),\n      text = element_text(color = \"#ccc\"),\n      axis.text = element_text(color = \"#ccc\"),\n      axis.title = element_text(color = \"#ccc\"),\n      axis.ticks = element_line(color = \"#ccc\"),\n      legend.position = \"bottom\",\n      legend.background = element_rect(fill = \"#222\", color = \"#222\")\n    )\n  \n  ggsave(\n    filename = \"scholastic_pic.png\",\n    plot = pic,\n    width = 1000,\n    height = 1000,\n    units = \"px\",\n    dpi = 150\n  )\n  \n  return(\"scholastic_pic.png\")\n}\n\n\n\n\n\nDefining the pipeline\nNow that you’ve read the verbal description of what each function does, it’s intuitively pretty clear how the analysis pipeline is supposed to work. Roughly speaking, you’d expect the analysis to be executed using a script like this:\n\n\n\nrun_analysis.R\n\n# load packages\nlibrary(tibble)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(forcats)\nlibrary(ggrepel)\nlibrary(legendry)\n\n# read analysis script\nsource(\"analysis.R\")\n\n# setup\ninput  &lt;- \"spells.csv\"\nspells &lt;- read_csv(input, show_col_types = FALSE)\n\n# make the spell dice plot & write to output\ndice_dat &lt;- dice_data(spells)\ndice_pic &lt;- dice_plot(dice_dat)\n\n# make the schools of magic plot & write to output\nscholastic_dat  &lt;- scholastic_data(spells)\nscholastic_clus &lt;- scholastic_clusters(scholastic_dat)\nscholastic_pic  &lt;- scholastic_plot(scholastic_dat, scholastic_clus)\n\n\nSo, how does it work with targets? Well, if we take a look at the _targets.R script, we can see it looks suspiciously similar to the code above:\n\n\n\n_targets.R\n\nlibrary(targets)\n\n# specify required packages\ntar_option_set(packages = c(\n  \"tibble\", \"readr\", \"ggplot2\", \"dplyr\", \"stringr\", \n  \"tidyr\", \"forcats\", \"ggrepel\", \"legendry\"\n))\n\n# read analysis script\ntar_source(\"analysis.R\")\n\nlist(\n  # setup\n  tar_target(input, \"spells.csv\", format = \"file\"),\n  tar_target(spells, read_csv(input, show_col_types = FALSE)),\n  \n  # dice plot\n  tar_target(dice_dat, dice_data(spells)),\n  tar_target(dice_pic, dice_plot(dice_dat)),\n  \n  # scholastic plot\n  tar_target(scholastic_dat, scholastic_data(spells)),\n  tar_target(scholastic_clus, scholastic_clusters(scholastic_dat)),\n  tar_target(\n    scholastic_pic,\n    scholastic_plot(scholastic_dat, scholastic_clus)\n  )\n)\n\n\nIn this pipeline, the set up involves two steps:\n\nI’ve used tar_option_set() to declare the required R packages, thereby making those packages available to the pipeline\nI’ve used tar_source() to read the analysis script, thereby making the functions in that file accessible to the pipeline\n\nHaving taken care of the preliminaries, the pipeline is specified via a list of targets, each of which is defined by a call to tar_target(). Each target has a name, and is associated with a command that is to be executed. If I’d named the arguments on line 18 in my _targets.R script, the code would look like this:\n\ntar_target(\n  name = dice_dat, \n  command = dice_data(spells)\n)\n\nNotice the similarity to line 20 of the run_analysis.R script:\n\ndice_dat &lt;- dice_data(spells)\n\nIn essence, that’s what I’m doing with the call to tar_target(). I’m specifying that the command set_output_dir() is to be executed, and the results should be stored as the variable output. However, instead of immediately executing this code in the current R environment, what tar_target() does is create the infrastructure so that this command can be incorporated into the pipeline when it actually gets built. With one exception, this is the recipe I followed for constructing all the targeta in my _targets.R script.\nThe one exception to this pattern occurs on line 14 of _targets.R, where my target is defined by this call to tar_target():\n\ntar_target(\n  name = input, \n  command = \"spells.csv\", \n  format = \"file\"\n)\n\nIn one sense, this target is pretty much the same as the others: it defines a variable called input using the “command” \"spells.csv\", and so in that respect it’s much the same as line 16 of the run_analysis.R script. However, notice that I’ve also specified that format = \"file\". This tells targets that \"spells.csv\" isn’t just a string, it’s also the name of a file that needs to be tracked. By declaring it as a file target, I’m ensuring that if the spells.csv file gets altered in some way, this target and any target that depends on it will need to be rebuilt.\nThe tar_visnetwork() function provides a handy way of visualising the structure of a pipeline as a little HTML widget. To keep things as simple as possible, at least for the moment, I’ll set targets_only = TRUE so that the graph ignores the functions defined in analysis.R, and only focuses on the targets themselves:2\n\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\nAll these targets are shown in blue, indicating that they are “outdated”. That’s to be expected at this point, of course: I haven’t actually run anything yet! I’ll get to that momentarily, but before I do I’ll call tar_outdated() to confirm the obvious:\n\ntar_outdated()\n\n[1] \"scholastic_clus\" \"scholastic_dat\"  \"spells\"          \"scholastic_pic\" \n[5] \"dice_pic\"        \"input\"           \"dice_dat\"       \n\n\nThese are the targets that need to be (re)run.\n\n\nRunning the pipeline\nOkay, it is now time to run the pipeline. We can do this by calling tar_make(). Here’s what happens when we do that:\n\ntar_make()\n\n▶ dispatched target input\n● completed target input [0.285 seconds, 302.514 kilobytes]\n▶ dispatched target spells\n● completed target spells [0.115 seconds, 73.966 kilobytes]\n▶ dispatched target scholastic_dat\n● completed target scholastic_dat [0.016 seconds, 401 bytes]\n▶ dispatched target dice_dat\n● completed target dice_dat [0.028 seconds, 33.486 kilobytes]\n▶ dispatched target scholastic_clus\n● completed target scholastic_clus [0.016 seconds, 634 bytes]\n▶ dispatched target dice_pic\n● completed target dice_pic [0.95 seconds, 65 bytes]\n▶ dispatched target scholastic_pic\n● completed target scholastic_pic [0.221 seconds, 71 bytes]\n▶ ended pipeline [1.727 seconds]\n\n\nThe output here is pretty descriptive, but it’s still worth expanding on it a little. For each target, there are two lines of output that look like this:\n▶ dispatched target TARGETNAME\n● completed target TARGETNAME [blah seconds, blah kilobytes]\nWhat these two lines mean is that, under the hood, targets “dispatches” the task to a separate R session using the callr package, and the code is executed there. The target is deemed “completed” once that R session finishes running the code and returns the output. This approach has two advantages over simply running the code at the console. First, it is more reproducible, because each target is run in a clean R session. Second, this design makes it waaaaaaaay easier to parallelise the execution. More on that later.\nIn any case, now that we’ve finished running the pipeline, let’s take a look at the state of our “spells” project folder:\n\ndir_tree(recurse = FALSE)\n\n.\n├── _targets\n├── _targets.R\n├── analysis.R\n├── dice_pic.png\n├── scholastic_pic.png\n└── spells.csv\n\n\nThere are three new things here. First, as expected, we have two image files dice_pic.png and scholastic_pic.png. These are the outputs produced by our analysis script. Yay, it worked! There’s also a _targets folder: this is the place where the targets package stashes all its metadata and stores copies of the built targets. I’ll talk more about the contents of this folder later, but just to give a sense of it now, here’s a sneak peek at what is stored in that folder:\n\ndir_tree(\"_targets\")\n\n_targets\n├── meta\n│   ├── meta\n│   ├── process\n│   └── progress\n├── objects\n│   ├── dice_dat\n│   ├── dice_pic\n│   ├── scholastic_clus\n│   ├── scholastic_dat\n│   ├── scholastic_pic\n│   └── spells\n└── user\n\n\n\n\nWhen the project changes\nNow that we’ve run the pipeline, let’s take another look at the network. The structure of it hasn’t changed, but all the targets are showing as “up to date”:\n\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\nIf I call tar_make() again, essentially nothing happens. All the targets are up to date, so everything is skipped:\n\ntar_make()\n\n✔ skipped target input\n✔ skipped target spells\n✔ skipped target scholastic_dat\n✔ skipped target dice_dat\n✔ skipped target scholastic_clus\n✔ skipped target dice_pic\n✔ skipped target scholastic_pic\n✔ skipped pipeline [0.074 seconds]\n\n\nNice!\nActually, you know what? Now feels like a good time to show the full network, including all the functions that contribute to the pipeline…\n\ntar_visnetwork()\n\n\n\n\n\nNow let’s suppose that, for whatever reason, I tinker with the plotting code for the “schools of magic” image. Perhaps I want to use a different colour scheme or something. So I go back into my code and change the code for the scholastic_plot() function, but only that function &lt;edits the file behind the scenes…&gt;. Let’s see what this does to the pipeline visualisation:\n\ntar_visnetwork()\n\n\n\n\n\nNot only does targets detect that the function has been altered, it also recognises that the scholastic_pic target (and only that target) is now outdated:\n\ntar_outdated()\n\n[1] \"scholastic_pic\"\n\n\nTaking this a little further, suppose I also decide to tinker with the dice_data() function &lt;edits the file again…&gt;. Perhaps I’ve decided that actually I would like the plot to count every instance of a dice roll description in each spell, not merely the unique instances as the previous version did. Let’s see what our network visualisation looks like now:\n\ntar_visnetwork()\n\n\n\n\n\nThat makes sense too: the dice_data() function affects the dice_dat target, but that also has implications for the dice_pic target because it depends on dice_dat. So now our list of outdated targets looks like this:\n\ntar_outdated()\n\n[1] \"scholastic_pic\" \"dice_pic\"       \"dice_dat\"      \n\n\nWhen we re-run the pipeline this time, those three targets (and only those three targets) are rebuilt. The others are skipped:\n\ntar_make()\n\n✔ skipped target input\n✔ skipped target spells\n✔ skipped target scholastic_dat\n▶ dispatched target dice_dat\n● completed target dice_dat [0.034 seconds, 35.004 kilobytes]\n✔ skipped target scholastic_clus\n▶ dispatched target dice_pic\n● completed target dice_pic [1.007 seconds, 65 bytes]\n▶ dispatched target scholastic_pic\n● completed target scholastic_pic [0.236 seconds, 71 bytes]\n▶ ended pipeline [1.642 seconds]\n\n\nThis, as you might imagine, is extremely useful in situations where you have a project that involves hundreds of analyses and figures that take a really long time to execute if you re-run everything from the beginning… but all you actually want to do is change the fontsize on figure 312.\n\n\n\n\n\ndice_pic_2.png\n\n\n\n\n\n\nscholastic_pic_2.png\n\n\n\n\n\nEpilogue\nFor a first attempt at using targets, I’m not unhappy with this. It did what I needed it to do, and I was able to understand the basic structure of the package.\nBut there are some limitations. One thing that really bothers me is the way I handled the ggplot code. My thinking at the time was based on the thinking that real life analysis pipelines often have some very slow ggplot2 code, but the slow part is not the construction of the object itself, but rather the build, render, and draw stages. It’s not much of an issue in this specific example because everything is runs fast, but you can see why I had this worry by looking at what benchplot() has to say about the plots I created here:\n\ntar_load(c(scholastic_dat, scholastic_clus))\n\nbenchplot(\n  ggplot(scholastic_dat, aes(school, class, fill = count)) +\n    geom_tile() +\n    scale_x_dendro(\n      clust = scholastic_clus$school,\n      guide = guide_axis_dendro(n.dodge = 2),\n      expand = expansion(0, 0),\n      position = \"top\"\n    ) +\n    scale_y_dendro(\n      clust = scholastic_clus$class,\n      expand = expansion(0, 0)\n    ) +\n    coord_equal()\n)\n\n\n\n\n\n\n\n\n       step user.self sys.self elapsed\n1 construct     0.020    0.000   0.021\n2     build     0.038    0.002   0.040\n3    render     0.048    0.000   0.045\n4      draw     0.030    0.000   0.031\n5     TOTAL     0.136    0.002   0.137\n\n\nEven in this example, where there isn’t very much that needs to be drawn to the graphics device, constructing the plot isn’t the step that takes the most time. So it makes very little sense to treat the plot specification (i.e., the gg plot object) as the terminal target of a plotting pipeline, because 90% of the compute time takes place after the gg object is specified (this is even more obvious when you have a scatterplot that needs to draw millions of dots to the canvas).\nHaving been burned by this in the past, I made the decision that my plotting target would encapsulate all stages in the plot rendering process. Only once the image has been written to a file would my plot target be deemed complete. As far as it goes, this is very sensible reasoning, but in retrospect I think it might have made a lot more sense to split the plotting target into stages. Saving the gg object as an intermediate target (and possibly other stages of the plot construction too) might have been sensible. It might seem like I’m being weirdly nitpicky, but my motivation here is very practical: I have a couple of projects where rerunning the analysis very time-consuming, and the biggest bottleneck (by far) is rendering and drawing some very unpleasant gg objects. I’ve been using a variety of tricks to work around this issue, none of which have been satisfying.3 Targets offers a much cleaner solution to my problem, but it’s clear to me just from this toy exercise that I will need to be careful about how I set up targets for these analysis pipelines.\nStill, it seems very clear to me that the problem can be solved with targets. My toy example is a kind of worst case solution… if all else fails, I can define the output image itself as the the target to be built. From that perspective, mission accomplished babes.\n\n\n\n\nAn even more lightweight blog"
  },
  {
    "objectID": "posts/2025-01-08_using-targets/index.html#story-2-building-a-blog",
    "href": "posts/2025-01-08_using-targets/index.html#story-2-building-a-blog",
    "title": "Three short stories about targets",
    "section": "Story 2: Building a blog",
    "text": "Story 2: Building a blog\n\nset_knitr_dir(\"liteblog\")\n\nOkay, so the first project went better than I’d hoped. Shall we try something different then? This time around I decided I’d be a little more ambitious, and attempted to write a lightweight blogging tool using litedown as a drop-in replacement for R markdown or quarto, and using targets to manage the build process for the site as a whole. I had two reasons for picking this as my second attempt. First, personal preference: I seem to have developed a habit for rolling my own half-arsed blogging tools. A very long time ago I wrote a bad blogging package called caladown that I would not recommend anyone use ever, and about a about a year ago I repeated the foolishness over my summer break when I got bored and built a very silly blog based on knitr and eleventy, for literally no reason. Neither exercise was particularly useful, but both were fun.\nThe second reason for choosing this project is a little more serious. Unlike a simple analysis pipeline, there are a lot of details about the blog you don’t know in advance. You don’t know what the files will be called, you don’t know how many of them there will be, and so on. On top of that, even if you are able to write a target for “build a blog post”, you will need to build dozens or hundreds of tiny variations of the same target. There is no way in hell anyone wants to hand-code a targets pipeline for this situation: instead, we’ll need a mechanism for defining and building targets on the fly, rather than trying to specify all the targets in advance.\nIn other words, we’ll need to think about dynamic branching.4\n\nDesigning the blog\nJust like last time, before we talk about the targets pipeline, it’s helpful to understand the rest of the project. After a bit of thought, I decided that a “liteblog” project would have a structure that looks something like this:\n\ndir_tree()\n\n.\n├── _liteblog-footer.html\n├── _liteblog-header.html\n├── _liteblog.R\n├── _liteblog.css\n├── _targets.R\n└── source\n    ├── 404.rmd\n    ├── _001_hello-cruel-world.rmd\n    ├── _002_blog-object.rmd\n    └── index.rmd\n\n\nThis is what a clean project looks like before the blog has been built. Inside the source folder there are four R markdown documents, corresponding to the four HTML documents that will be created when the site is built. Specifically:5\n\nindex.rmd becomes the blog homepage index.html\n404.rmd becomes the 404 page 404.html\n_001_hello-cruel-world.rmd becomes /001/hello-cruel-world/index.html\n_002_blog-object.rmd becomes /002/blog-object/index.html\n\nThe other files are used to define blog structure, style, and build process:\n\n_liteblog-header.html provides HTML for the blog navbar\n_liteblog-footer.html provides HTML for the blog footer\n_liteblog.css provides the visual styling\n_liteblog.R supplies an R6 class called Liteblog used to configure the blog\n_targets.R defines the build process\n\nThe header, footer, and css files aren’t very important for our purposes, and neither is the content of the R markdown files. All the heavy lifting is done by _liteblog.R and _targets.R. I’ll talk about each of these in turn.\n\n\nThe Liteblog class\nFor reasons that I have already forgotten, I made a decision to implement the blogging tool as an R6 class. This blog post isn’t the place to talk about how R6 works, but conveniently Hadley already did this better than I could so now I don’t have to. Even more conveniently, it doesn’t matter too much for this post if you don’t already know R6. But for those who do, here’s the code:\n\n\n\n_liteblog.R\n\nLiteblog &lt;- R6::R6Class(\n  classname = \"Liteblog\",\n  public = list(\n    \n    initialize = function(root = \".\",\n                          source = \"source\",\n                          output = \"site\",\n                          url = NULL) {\n      self$root &lt;- root\n      self$source &lt;- source\n      self$output &lt;- output\n      self$url &lt;- url\n    },\n    \n    root = NULL,\n    source = NULL,\n    output = NULL,\n    url = NULL,\n    pattern = \"[.][rR]?md$\",\n    \n    find_posts = function() {\n      files &lt;- fs::dir_ls(\n        path = fs::path(self$root, self$source),\n        recurse = TRUE,\n        regexp = self$pattern,\n        type = \"file\"\n      )\n      unname(unclass(files))\n    },\n    \n    find_static = function() {\n      files &lt;- fs::dir_ls(\n        path = fs::path(self$root, self$source),\n        recurse = TRUE,\n        regexp = self$pattern,\n        invert = TRUE,\n        all = TRUE,\n        type = \"file\"\n      )\n      unname(unclass(files))\n    },\n    \n    fuse_post = function(file, ...) {\n      output_path &lt;- litedown::fuse(file)\n      output_file &lt;- fs::path_file(output_path)\n      if (stringr::str_detect(output_file, \"^_\")) {\n        destination &lt;- output_file |&gt;\n          stringr::str_replace_all(\"_\", \"/\") |&gt;\n          stringr::str_replace(\"\\\\.html$\", \"/index.html\") |&gt;\n          stringr::str_replace(\"^\", paste0(self$output, \"/\"))\n      } else {\n        destination &lt;- paste0(self$output, \"/\", output_file)\n      }\n      destination &lt;- fs::path(self$root, destination)\n      fs::dir_create(fs::path_dir(destination))\n      fs::file_move(output_path, destination)\n    },\n    \n    copy_static = function(file) {\n      destination &lt;- file |&gt;\n        stringr::str_replace(\n          pattern = paste0(\"/\", self$source, \"/\"),\n          replacement = paste0(\"/\", self$output, \"/\")\n        )\n      fs::dir_create(fs::path_dir(destination))\n      fs::file_copy(\n        path = file,\n        new_path = destination,\n        overwrite = TRUE\n      )\n    }\n  )\n)\n\n\nIt’s… honestly not that great.6 It is fit for purpose, so to speak, but if I were ever intending to use this in real life I’d want to give it a lot more love than I have to date. Nevertheless, just to give you a sense of how it works, I’ll create a new blog object by calling Liteblog$new():\n\nblog &lt;- Liteblog$new()\nblog\n\n&lt;Liteblog&gt;\n  Public:\n    clone: function (deep = FALSE) \n    copy_static: function (file) \n    find_posts: function () \n    find_static: function () \n    fuse_post: function (file, ...) \n    initialize: function (root = \".\", source = \"source\", output = \"site\", url = NULL) \n    output: site\n    pattern: [.][rR]?md$\n    root: .\n    source: source\n    url: NULL\n\n\nIgnoring the boring parts of this output, you can see that this blog object has four methods that will get used when the site gets built by our targets pipeline:\n\n$find_posts() detects R markdown documents in the source folder\n$find_static() detects other files in the source folder\n$fuse_post() converts an R markdown file to HTML in the output folder\n$copy_static() copies a file to the output folder\n\nThere’s not much to be gained by diving very deep into these functions, but as an example, here’s what you would do to discover all the R markdown posts in this project:\n\nblog$find_posts()\n\n[1] \"./source/404.rmd\"                   \n[2] \"./source/_001_hello-cruel-world.rmd\"\n[3] \"./source/_002_blog-object.rmd\"      \n[4] \"./source/index.rmd\"                 \n\n\nThis provides the core blogging toolkit that I’ll now use when writing my target pipeline. It’s extremely bare bones, and not very customisable, but of course my intention here isn’t to build a proper blogging platform. It’s just a toy that I can play with when building a more complex build pipeline. Speaking of which…\n\n\nThe build pipeline\nThe _targets.R file for this project is a little more complicated than the one I used for the D&D spells plots, and will take a bit of effort to unpack all of it. Let’s start by looking at the file as a whole:\n\n\n\n_targets.R\n\nlibrary(targets)\ntar_source(\"_liteblog.R\")\n\nlist(\n  \n  # define blog configuration\n  tar_target(\n    name = blog,\n    command = Liteblog$new(\n      root = rprojroot::find_root(\n        rprojroot::has_file(\"_liteblog.R\")\n      ),\n      source = \"source\",\n      output = \"site\",\n      url = \"liteblog.djnavarro.net\"\n    )\n  ),\n  \n  # track configuration files\n  tar_target(\n    name = blog_rds, \n    command = saveRDS(blog, file = \"_liteblog.rds\"), \n    format = \"file\"\n  ),\n  tar_target(blog_css, \"_liteblog.css\", format = \"file\"),\n  tar_target(blog_hdr, \"_liteblog-header.html\", format = \"file\"),\n  tar_target(blog_ftr, \"_liteblog-footer.html\", format = \"file\"),\n  \n  # detect file paths (always run)\n  tar_target(\n    name = post_paths, \n    command = blog$find_posts(), \n    cue = tar_cue(\"always\")\n  ),\n  tar_target(\n    name = static_paths, \n    command = blog$find_static(), \n    cue = tar_cue(\"always\")\n  ),\n  \n  # specify file targets\n  tar_target(\n    name = post_files, \n    command = post_paths, \n    pattern = map(post_paths), \n    format = \"file\"\n  ),\n  tar_target(\n    name = static_files, \n    command = static_paths, \n    pattern = map(static_paths), \n    format = \"file\"\n  ),\n  \n  # fuse targets depend on blog configuration files\n  # copy targets don't need dependencies\n  tar_target(\n    name = post_fuse,\n    command = blog$fuse_post(\n      post_files,\n      blog_css,\n      blog_hdr,\n      blog_ftr\n    ),\n    pattern = map(post_files)\n  ),\n  tar_target(\n    name = static_copy, \n    command = blog$copy_static(static_files), \n    pattern = map(static_files)\n  )\n)\n\n\nOkay, yes, there’s a lot to unpack here as the young people7 say. Not only is the code considerably longer than it was last time, I’m also making use of several targets features that weren’t present last time. Rather than try to explain it all at once, I’ll start by calling tar_visnetwork() again, focusing only on the targets that form the core of my build pipeline:\n\ntar_visnetwork(\n  allow = any_of(c(\n    \"static_paths\", \"post_paths\", \"post_files\", \n    \"static_files\", \"post_fuse\", \"static_copy\"\n  ))\n)\n\n\n\n\n\nLet’s look at the top path first. It comprises the following steps:\n\nFind the R markdown documents using blog$find_posts(). This creates a single build target, a character vector post_paths.\nDeclare that every R markdown document should be a tracked file. This is the role of the post_files target, but it’s a target comprised of several branches, and those branches are defined using a pattern. More on this later.\nFor each tracked R markdown document, use blog$post_fuse() to build the post and move the resulting HTML file to the correct location. This is the role of post_fuse, and again this is a pattern target comprised of many branches.\n\nThe second path in the pipeline is analogous, but simpler:\n\nThe static_paths target tracks the paths to static files\nThe static_files target tracks the content of the static files\nThe static_copy target copies the static files to the output folder\n\nThese two paths form the core of the build process, but it’s not the whole story. So let’s zoom out now and take a look at the complete network:\n\ntar_visnetwork()\n\n\n\n\n\nIn this version, you can see that most of the build pipeline depends on the blog object: that makes sense because this is the R6 object that defines methods like $find_posts() and $fuse_post(), and it’s these functions that are called when the blog is built. Similarly, since blog is itself an instance of the Liteblog class, the class itself is an upstream dependency of blog.\nThe other four targets are less interesting. Three of them (blog_css, blog_hdr, and blog_ftr) are used to keep track of the style files (i.e. _liteblog.css, _liteblog-header.html, and _liteblog-footer.html), so that if any of those files are modified the it will trigger a rebuild of any target that uses those files. The fourth one, blog_rds, is a bit of an odd one and I’m not sure I like it: this target saves the blog object to an rds file. The reason for this is that it makes this object easier to access from within an R markdown post, by loading the file. This in turn makes it easier to construct the post listing on the home page. But I keep thinking there must be a better way to do this. Eh, whatever, it’s not a serious project.\n\n\nBuilding the blog\nAt this point, you probably (hopefully!) have a general sense of what the build process looks like for this blog. I haven’t explained all the details yet, but I hope the gist is clear already. So let’s ignore those tiresome details for a little longer and call tar_make() to build the blog. The output is long, but there’s not much in there that we didn’t already see earlier with the D&D spells project:\n\ntar_make()\n\n▶ dispatched target blog\n● completed target blog [0.003 seconds, 9.905 kilobytes]\n▶ dispatched target blog_css\n● completed target blog_css [0 seconds, 3.759 kilobytes]\n▶ dispatched target blog_ftr\n● completed target blog_ftr [0 seconds, 115 bytes]\n▶ dispatched target blog_hdr\n● completed target blog_hdr [0.001 seconds, 271 bytes]\n▶ dispatched target static_paths\n● completed target static_paths [0.035 seconds, 140 bytes]\n▶ dispatched target post_paths\n● completed target post_paths [0 seconds, 186 bytes]\n▶ dispatched target blog_rds\n● completed target blog_rds [0.003 seconds, 0 bytes]\n▶ dispatched branch static_files_1725c3c24144eb91\n● completed branch static_files_1725c3c24144eb91 [0 seconds, 0 bytes]\n● completed pattern static_files \n▶ dispatched branch post_files_9bbb5ae3d1d3f95b\n● completed branch post_files_9bbb5ae3d1d3f95b [0 seconds, 320 bytes]\n▶ dispatched branch post_files_0a8225f176a1d698\n● completed branch post_files_0a8225f176a1d698 [0.001 seconds, 4.667 kilobytes]\n▶ dispatched branch post_files_2ed928ff2eafb31e\n● completed branch post_files_2ed928ff2eafb31e [0 seconds, 2.451 kilobytes]\n▶ dispatched branch post_files_b12b144c84b5b3a0\n● completed branch post_files_b12b144c84b5b3a0 [0 seconds, 1.777 kilobytes]\n● completed pattern post_files \n▶ dispatched branch static_copy_dfa803e4dbdaa48f\n● completed branch static_copy_dfa803e4dbdaa48f [0.017 seconds, 173 bytes]\n● completed pattern static_copy \n▶ dispatched branch post_fuse_c2dd6a62cc9d9565\n● completed branch post_fuse_c2dd6a62cc9d9565 [0.67 seconds, 171 bytes]\n▶ dispatched branch post_fuse_bb77d7e34f5b9d8d\n● completed branch post_fuse_bb77d7e34f5b9d8d [0.358 seconds, 187 bytes]\n▶ dispatched branch post_fuse_bb123c5d7a5d991e\n● completed branch post_fuse_bb123c5d7a5d991e [0.101 seconds, 182 bytes]\n▶ dispatched branch post_fuse_b2591b053d5431f7\n● completed branch post_fuse_b2591b053d5431f7 [0.051 seconds, 173 bytes]\n● completed pattern post_fuse \n▶ ended pipeline [1.442 seconds]\n\n\nSetting aside obvious questions like “what’s going on with all those weirdly named ‘branch’ targets like post_files_bfe2f528519c0ff0???”, the gist of this is pretty clear: everything has run smoothly and the blog has been built. In fact, you can see a live version of the blog, built from the source as it exists up to this point: it’s here). The project folder now looks like this:\n\ndir_tree()\n\n.\n├── _liteblog-footer.html\n├── _liteblog-header.html\n├── _liteblog.R\n├── _liteblog.css\n├── _liteblog.rds\n├── _targets\n│   ├── meta\n│   │   ├── meta\n│   │   ├── process\n│   │   └── progress\n│   ├── objects\n│   │   ├── blog\n│   │   ├── post_fuse_b2591b053d5431f7\n│   │   ├── post_fuse_bb123c5d7a5d991e\n│   │   ├── post_fuse_bb77d7e34f5b9d8d\n│   │   ├── post_fuse_c2dd6a62cc9d9565\n│   │   ├── post_paths\n│   │   ├── static_copy_dfa803e4dbdaa48f\n│   │   └── static_paths\n│   └── user\n├── _targets.R\n├── site\n│   ├── 001\n│   │   └── hello-cruel-world\n│   │       └── index.html\n│   ├── 002\n│   │   └── blog-object\n│   │       └── index.html\n│   ├── 404.html\n│   └── index.html\n└── source\n    ├── 404.rmd\n    ├── _001_hello-cruel-world.rmd\n    ├── _002_blog-object.rmd\n    └── index.rmd\n\n\nThe key thing to notice here is that two new folders have been created: _targets stores metadata and tracked objects for the pipeline, and site contains the built website. Yay! It works!\nSo now let’s start digging into the details…\n\n\n\n\n\nthe blog homepage\n\n\n\n\nAlways-run targets\nOne of the problems I ran into pretty early in this project was how to monitor the source directory properly. It’s easy enough to scan a directory to find the files that live there: the $find_posts() and $find_static() methods for Liteblog objects do exactly this. But consider a target like this:\n\ntar_target(post_paths, blog$find_posts())\n\nThis target stores a list of all R markdown posts, but it has only one dependency: the blog object itself. It is not dependent on the state of the source folder. It can’t be: a folder in the file system isn’t an R object. So, if the state of the folder changes (e.g., a new blog post is added), targets would have no way to know that the post_paths target needs to be updated.\nA simple solution is to declare that the post_paths target must always be run whenever tar_make() is called. We can do this by using tar_cue(), and that’s why the actual target is defined like this:\n\ntar_target(\n  name = post_paths, \n  command = blog$find_posts(), \n  cue = tar_cue(\"always\")\n)\n\nA similar approach is taken when defining the static_paths target. These two targets are always executed at build time. What this means, in practice, is that even though I have just run tar_make() and haven’t changed any of the files since then, the network visualisation appears like this:\n\ntar_visnetwork()\n\n\n\n\n\nAs expected, static_paths and post_paths both show up as outdated targets, and so does everything downstream of those two. Again, this makes sense: until those two targets are re-run, the targets package has no way to know if the downstream targets will also need to be rebuilt. Consequently, calling tar_outdated() gives us this:\n\ntar_outdated()\n\n[1] \"static_paths\" \"post_fuse\"    \"static_files\" \"post_files\"  \n[5] \"static_copy\"  \"post_paths\"  \n\n\nHowever, the fact that the downstream targets are flagged as outdated doesn’t mean these targets will need to be executed again. To see this, let’s call tar_make() again:\n\ntar_make()\n\n✔ skipped target blog\n✔ skipped target blog_css\n✔ skipped target blog_ftr\n✔ skipped target blog_hdr\n▶ dispatched target static_paths\n● completed target static_paths [0.033 seconds, 140 bytes]\n▶ dispatched target post_paths\n● completed target post_paths [0 seconds, 186 bytes]\n✔ skipped target blog_rds\n✔ skipped branch static_files_1725c3c24144eb91\n✔ skipped pattern static_files\n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_b12b144c84b5b3a0\n✔ skipped pattern post_files\n✔ skipped branch static_copy_dfa803e4dbdaa48f\n✔ skipped pattern static_copy\n✔ skipped branch post_fuse_c2dd6a62cc9d9565\n✔ skipped branch post_fuse_bb77d7e34f5b9d8d\n✔ skipped branch post_fuse_bb123c5d7a5d991e\n✔ skipped branch post_fuse_b2591b053d5431f7\n✔ skipped pattern post_fuse\n▶ ended pipeline [0.121 seconds]\n\n\nAs expected, the static_paths and post_paths targets are both re-run, as instructed by tar_cue(\"always\"). However, once those targets are completed, the targets package is smart enough to detect that nothing has actually changed. The static_paths and post_paths objects are exactly the same as they were previously, so there is no need to execute any of the downstream targets. In other words, at build time the blog will always check to see if the content in the source folder has changed, but if nothing has changed the fuse and copy targets are skipped.\n\n\nDynamic branching\nThe second design feature that caused me some grief was how to write a _targets.R file that would build every R markdown post in the source folder, even though I wouldn’t know in advance how many of those documents exist or what they would be called. As described in the last section, one part of the solution was to define the post_paths target, which would always be executed at build time, and would store the names of all the R markdown documents. I’ll use tar_read() to pull these paths from targets storage:8\n\ntar_read(post_paths)\n\n[1] \"/home/danielle/GitHub/djnavarro/blog/posts/2025-01-08_using-targets/liteblog/source/404.rmd\"                   \n[2] \"/home/danielle/GitHub/djnavarro/blog/posts/2025-01-08_using-targets/liteblog/source/_001_hello-cruel-world.rmd\"\n[3] \"/home/danielle/GitHub/djnavarro/blog/posts/2025-01-08_using-targets/liteblog/source/_002_blog-object.rmd\"      \n[4] \"/home/danielle/GitHub/djnavarro/blog/posts/2025-01-08_using-targets/liteblog/source/index.rmd\"                 \n\n\nEach of these corresponds to a file that needs to be tracked, in much the same way I treated spells.csv as a tracked file in the D&D plots project. There are multiple ways you can do this in targets, but after a few different tries I decided to use dynamic branching, a trick in which a single target is split at build time into multiple distinct branches. For the liteblog project, I defined post_file like this:\n\ntar_target(\n  name = post_files, \n  command = post_paths, \n  pattern = map(post_paths), \n  format = \"file\"\n)\n\nBy including the argument pattern = map(post_paths), I am telling targets to create a separate branch for post_files for every element in the post_paths vector. Specifically, since I’ve also set format = \"file\", I’m instructing each of these branches to track the state of one of these files. If that file changes, the branch that tracks the file will become outdated and need to be rebuilt. To get a little more information about the branches, I can call tar_branches():\n\ntar_branches(post_files)\n\n# A tibble: 4 × 2\n  post_files                  post_paths                 \n  &lt;chr&gt;                       &lt;chr&gt;                      \n1 post_files_9bbb5ae3d1d3f95b post_paths_b1b0e0a7d31afe4b\n2 post_files_0a8225f176a1d698 post_paths_3444a4f5b2ff26a6\n3 post_files_2ed928ff2eafb31e post_paths_b75d461c06347275\n4 post_files_b12b144c84b5b3a0 post_paths_a853bb7ccac490b8\n\n\nEach branch corresponds to a single row in this table: the value of the post_files column records the name assigned by targets to that branch, and the value of the post_paths column assigns a name to the corresponding element of the post_paths target over which we have branched. These names aren’t particularly user-friendly, to be sure, but nevertheless my problem is solved.\nTo see how this dynamic branching plays out in practice, let’s see what happens when I modify the content of the blog in different ways.\n\n\nEditing an existing page\nLet’s modify the source for the 404 page. I’ll change it so that the title of the page now reads “404 PAGE NOT FOUND” rather than simply “404” as it had previously. Okay… … yep, done. I’ve edited the file now.\nHow does this edit get picked up in the targets build process? This time around, I’ll build it in stages so that the output is a little easier to read. We can do this by asking tar_make() to build a specific target, rather than build the entire project. First, let’s build the post_files target again:\n\ntar_make(post_files)\n\n✔ skipped target blog\n▶ dispatched target post_paths\n● completed target post_paths [0.035 seconds, 186 bytes]\n▶ dispatched branch post_files_9bbb5ae3d1d3f95b\n● completed branch post_files_9bbb5ae3d1d3f95b [0 seconds, 335 bytes]\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_b12b144c84b5b3a0\n● completed pattern post_files \n▶ ended pipeline [0.112 seconds]\n\n\nAs we hoped, we see that one (and only one) of the post_files branches has been rebuilt, specifically the one corresponding to the 404.rmd file that I just edited. So far so good. Next, let’s see what happens when post_fuse is rebuilt:\n\ntar_make(post_fuse)\n\n✔ skipped target blog\n✔ skipped target blog_css\n✔ skipped target blog_ftr\n✔ skipped target blog_hdr\n▶ dispatched target post_paths\n● completed target post_paths [0.036 seconds, 186 bytes]\n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_b12b144c84b5b3a0\n✔ skipped pattern post_files\n▶ dispatched branch post_fuse_c2dd6a62cc9d9565\n● completed branch post_fuse_c2dd6a62cc9d9565 [0.49 seconds, 171 bytes]\n✔ skipped branch post_fuse_bb77d7e34f5b9d8d\n✔ skipped branch post_fuse_bb123c5d7a5d991e\n✔ skipped branch post_fuse_b2591b053d5431f7\n● completed pattern post_fuse \n▶ ended pipeline [0.619 seconds]\n\n\nAgain, notice all the skipped targets. The only targets that are dispatched are post_paths (because we instructed that one to always run), and one branch of post_fuse. In other words, the only thing that happens here is that the outdated post gets rebuilt.\nYay!\n\n\nAdding new pages\nAs the next check that the blog functionality is working as expected, let’s have a look at what happens when… … two new blog posts are added to the source folder, along with a new static file:\n\n\nsource\n├── 404.rmd\n├── _001_hello-cruel-world.rmd\n├── _002_blog-object.rmd\n├── _003_schools-of-magic.rmd\n├── _004_spell-dice.rmd\n├── data\n│   └── spells.csv\n└── index.rmd\n\n\nAgain, I’ll do it in stages so that the output is a little easier to read:\n\ntar_make(post_files)\n\n✔ skipped target blog\n▶ dispatched target post_paths\n● completed target post_paths [0.034 seconds, 218 bytes]\n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n▶ dispatched branch post_files_454f9b18b99302f6\n● completed branch post_files_454f9b18b99302f6 [0 seconds, 11.136 kilobytes]\n▶ dispatched branch post_files_b77b5ec03a0b7e01\n● completed branch post_files_b77b5ec03a0b7e01 [0 seconds, 3.631 kilobytes]\n✔ skipped branch post_files_b12b144c84b5b3a0\n● completed pattern post_files \n▶ ended pipeline [0.116 seconds]\n\n\nThe four existing R markdown files are skipped, as they are already being tracked and none of them have changed. The two new R markdown files are added, and we can see that the corresponding post_files branches have both been updated. We are now tracking these files.\nNext, we build the pages:\n\ntar_make(post_fuse)\n\n✔ skipped target blog\n✔ skipped target blog_css\n✔ skipped target blog_ftr\n✔ skipped target blog_hdr\n▶ dispatched target post_paths\n● completed target post_paths [0.037 seconds, 218 bytes]\n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_454f9b18b99302f6\n✔ skipped branch post_files_b77b5ec03a0b7e01\n✔ skipped branch post_files_b12b144c84b5b3a0\n✔ skipped pattern post_files\n✔ skipped branch post_fuse_c2dd6a62cc9d9565\n✔ skipped branch post_fuse_bb77d7e34f5b9d8d\n✔ skipped branch post_fuse_bb123c5d7a5d991e\n▶ dispatched branch post_fuse_f59600e87d9d07da\n● completed branch post_fuse_f59600e87d9d07da [1.441 seconds, 185 bytes]\n▶ dispatched branch post_fuse_a98becd61e7088dd\n● completed branch post_fuse_a98becd61e7088dd [1.108 seconds, 183 bytes]\n✔ skipped branch post_fuse_b2591b053d5431f7\n● completed pattern post_fuse \n▶ ended pipeline [2.688 seconds]\n\n\nAgain, works as expected. Most targets get skipped, and only the two new posts are built to HTML pages.\nFinally, because we’ve added a new static file, I’ll now rebuild everything to ensure that this file gets copied over to the site correctly:\n\ntar_make()\n\n✔ skipped target blog\n✔ skipped target blog_css\n✔ skipped target blog_ftr\n✔ skipped target blog_hdr\n▶ dispatched target static_paths\n● completed target static_paths [0.034 seconds, 159 bytes]\n▶ dispatched target post_paths\n● completed target post_paths [0 seconds, 218 bytes]\n✔ skipped target blog_rds\n✔ skipped branch static_files_1725c3c24144eb91\n▶ dispatched branch static_files_cfb2777954f617d0\n● completed branch static_files_cfb2777954f617d0 [0 seconds, 302.514 kilobytes]\n● completed pattern static_files \n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_454f9b18b99302f6\n✔ skipped branch post_files_b77b5ec03a0b7e01\n✔ skipped branch post_files_b12b144c84b5b3a0\n✔ skipped pattern post_files\n✔ skipped branch static_copy_dfa803e4dbdaa48f\n▶ dispatched branch static_copy_501faf107279df5b\n● completed branch static_copy_501faf107279df5b [0.015 seconds, 173 bytes]\n● completed pattern static_copy \n✔ skipped branch post_fuse_c2dd6a62cc9d9565\n✔ skipped branch post_fuse_bb77d7e34f5b9d8d\n✔ skipped branch post_fuse_bb123c5d7a5d991e\n✔ skipped branch post_fuse_f59600e87d9d07da\n✔ skipped branch post_fuse_a98becd61e7088dd\n✔ skipped branch post_fuse_b2591b053d5431f7\n✔ skipped pattern post_fuse\n▶ ended pipeline [0.15 seconds]\n\n\nYou can browse the built website, as it appears at this stage in the process, at the cached location here.\n\n\nEditing the style files\nThere’s one last thing I want to check. Let’s suppose I created a light-themed version of the blog by editing _liteblog.css. Or, to be more accurate, let’s suppose I made a quick and dirty edit to the CSS file that almost creates a light theme but actually leaves a lot of weirdness unfixed because the author was too lazy to do it properly and frankly isn’t very strong at CSS.\nLet’s see what this does to the network visualisation:\n\ntar_visnetwork()\n\n\n\n\n\nTargets has detected the edit, but notice that there is a dependency between blog_css and fuse_post. This dependency is added because for this blog the CSS is embedded9 within the HTML file at build time: as a consequence, editing the CSS means that all pages will need to be rebuilt. And indeed…\n\ntar_make()\n\n✔ skipped target blog\n▶ dispatched target blog_css\n● completed target blog_css [0.001 seconds, 3.759 kilobytes]\n✔ skipped target blog_ftr\n✔ skipped target blog_hdr\n▶ dispatched target static_paths\n● completed target static_paths [0.036 seconds, 159 bytes]\n▶ dispatched target post_paths\n● completed target post_paths [0.001 seconds, 218 bytes]\n✔ skipped target blog_rds\n✔ skipped branch static_files_1725c3c24144eb91\n✔ skipped branch static_files_cfb2777954f617d0\n✔ skipped pattern static_files\n✔ skipped branch post_files_9bbb5ae3d1d3f95b\n✔ skipped branch post_files_0a8225f176a1d698\n✔ skipped branch post_files_2ed928ff2eafb31e\n✔ skipped branch post_files_454f9b18b99302f6\n✔ skipped branch post_files_b77b5ec03a0b7e01\n✔ skipped branch post_files_b12b144c84b5b3a0\n✔ skipped pattern post_files\n✔ skipped branch static_copy_dfa803e4dbdaa48f\n✔ skipped branch static_copy_501faf107279df5b\n✔ skipped pattern static_copy\n▶ dispatched branch post_fuse_c2dd6a62cc9d9565\n● completed branch post_fuse_c2dd6a62cc9d9565 [0.567 seconds, 171 bytes]\n▶ dispatched branch post_fuse_bb77d7e34f5b9d8d\n● completed branch post_fuse_bb77d7e34f5b9d8d [0.385 seconds, 187 bytes]\n▶ dispatched branch post_fuse_bb123c5d7a5d991e\n● completed branch post_fuse_bb123c5d7a5d991e [0.152 seconds, 182 bytes]\n▶ dispatched branch post_fuse_f59600e87d9d07da\n● completed branch post_fuse_f59600e87d9d07da [0.707 seconds, 185 bytes]\n▶ dispatched branch post_fuse_a98becd61e7088dd\n● completed branch post_fuse_a98becd61e7088dd [1.099 seconds, 183 bytes]\n▶ dispatched branch post_fuse_b2591b053d5431f7\n● completed branch post_fuse_b2591b053d5431f7 [0.051 seconds, 173 bytes]\n● completed pattern post_fuse \n▶ ended pipeline [3.114 seconds]\n\n\n…that’s exactly what happens. Again, you can browse the built website, horrible CSS failures and all, at the cached location here.\n\n\n\n\n\nlight-themed blog homepage\n\n\n\n\nEpilogue\nThat… went about as well as could be expected, I suppose? This “liteblog” project was never intended to be a serious effort at writing a blogging tool, but it does actually work, and maybe one day I’ll use it for something. Probably not though. Having gotten as far as I have into building this, I can see a lot of design limitations that would cause difficulties if I ever tried to use it for real.\nSo what was the point? Well, I now have a basic understanding of litedown, and I have a better grasp of dynamic branching in targets. That’s a big win as far as I’m concerned.\n\n\n\n\nMultiple threads in parallel"
  },
  {
    "objectID": "posts/2025-01-08_using-targets/index.html#story-3-parallel-computing",
    "href": "posts/2025-01-08_using-targets/index.html#story-3-parallel-computing",
    "title": "Three short stories about targets",
    "section": "Story 3: Parallel computing",
    "text": "Story 3: Parallel computing\nFor my third foray into targets – and the last one for this blog post – I wanted to take a look at how a targets pipeline can be distributed across multiple parallel threads. Happily, parallel computing is supported out of the box in targets, using the crew package to distribute the targets across multiple workers. This ended up being the smallest of my three projects, but there are a couple slightly different variations on how I looked at this, and I’ll go through them each in turn.\n\nMinimal version\n\nset_knitr_dir(\"threading1\")\n\nAs a very minimal implementation, consider this pipeline:\n\n\n\n_targets.R\n\nlibrary(targets)\nlibrary(crew)\n\ntar_option_set(controller = crew_controller_local(workers = 3))\n\nlist(\n  tar_target(wait1, Sys.sleep(1)),\n  tar_target(wait2, Sys.sleep(2)),\n  tar_target(wait3, Sys.sleep(3)),\n  tar_target(wait4, Sys.sleep(4))\n)\n\n\nThere are four targets here, and all they do is pause execution. If these were run serially, you would expect this to take about 10 seconds to complete. But that’s not what happens because I’m using crew_controller_local() to define a controller that will split the processing across three parallel workers. Here’s what actually happens:\n\ntar_make()\n\n▶ dispatched target wait1\n▶ dispatched target wait2\n▶ dispatched target wait3\n● completed target wait1 [1.015 seconds, 44 bytes]\n▶ dispatched target wait4\n● completed target wait2 [2.015 seconds, 44 bytes]\n● completed target wait3 [3.013 seconds, 44 bytes]\n● completed target wait4 [4.005 seconds, 44 bytes]\n▶ ended pipeline [6.94 seconds]\n\n\nUpon starting the job, the first three targets (wait1, wait2, and wait3) are dispatched to the three workers and they all start running concurrently. The fourth job (wait4) is placed on hold, and doesn’t start until the first of the three jobs finishes (wait1). Only then does the fourth job start. As the remaining jobs complete, the user is notified, and once they are all finalised targets reports that the pipeline is complete. You can get a high-level summary of the allocation of tasks across workers by calling tar_crew():\n\ntar_crew()\n\n# A tibble: 3 × 4\n  controller               worker seconds targets\n  &lt;chr&gt;                    &lt;chr&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 59acc4a1b3515135ec3c1478 1         5.11       2\n2 59acc4a1b3515135ec3c1478 2         2.10       1\n3 59acc4a1b3515135ec3c1478 3         3.10       1\n\n\nAs expected based on the intuitive description above, this output confirms that there’s one worker process that handled two of the targets, and one each handled by the other two.\n\n\nSlightly less minimal version\n\nset_knitr_dir(\"threading2\")\n\nThe previous example gives a general sense of how parallel execution works in a crew/targets pipeline, but – possibly because once upon a time I used callr to write my own R6 implementation of a multi-threaded queue,10 which piqued my curiosity about how these things play out – I found myself wanting a finer-grained description of what each of the workers is doing at each point in time.\nAs far as I can tell, neither targets nor crew provides an easy way to do this (edited to add: I later realised that tar_meta() provides a lot of what I need), so I wrote a slightly more elaborate version of the previous pipeline, in which the targets themselves keep track of the time at which execution starts and stops, as well as the pid of the R process in which they are being executed. In this version, there are three functions that do the work:\n\nstartup() is called to build the first target (start), and its primary job is to capture the system time at which execution of the first target begins\nsleeper() is used to build the wait1, wait2, wait3, and wait4 targets. These targets are analogous to the wait targets in the original version, but they also capture information about when execution of these targets started and stopped\ncollate() is called at the very end, and is used to construct the trace target. This target aggregates all the information from the other targets\n\nHere’s the entire code:\n\n\n\n_targets.R\n\nlibrary(targets)\nlibrary(crew)\n\ntar_option_set(controller = crew_controller_local(workers = 3))\n\nsleeper &lt;- function(duration, pipeline_start, name) {\n  sleep_start &lt;- Sys.time()\n  Sys.sleep(duration)\n  sleep_stop &lt;- Sys.time()\n  tibble::tibble(\n    name           = name,\n    pipeline_start = pipeline_start,\n    worker_pid     = Sys.getpid(),\n    begins_at      = difftime(sleep_start, pipeline_start),\n    finishes_at    = difftime(sleep_stop, pipeline_start)\n  )\n}\n\nstartup &lt;- function() {\n  tibble::tibble(\n    name = \"start\",\n    pipeline_start = Sys.time(),\n    worker_pid     = Sys.getpid(),\n    begins_at      = as.difftime(0, units = \"secs\"),\n    finishes_at    = difftime(Sys.time(), pipeline_start)\n  )\n}\n\ncollate &lt;- function(...) {\n  start &lt;- Sys.time()\n  na_difftime &lt;- as.difftime(NA_real_, units = \"secs\")\n  out &lt;- rbind(...)\n  pipeline_start &lt;- out$pipeline_start[1]\n  out$pipeline_start &lt;- NULL\n  out &lt;- rbind(\n    out,\n    tibble::tibble(\n      name         = \"trace\",\n      worker_pid   = Sys.getpid(),\n      begins_at    = difftime(start, pipeline_start),\n      finishes_at  = difftime(Sys.time(), pipeline_start)\n    )\n  )\n  out$duration    &lt;- out$finishes_at - out$begins_at\n  out$begins_at   &lt;- round(as.numeric(out$begins_at), digits = 3)\n  out$finishes_at &lt;- round(as.numeric(out$finishes_at), digits = 3)\n  out$duration    &lt;- round(as.numeric(out$duration), digits = 3)\n  out\n}\n\nlist(\n  tar_target(start, startup(), cue = tar_cue(\"always\")),\n  tar_target(wait1, sleeper(1, start$pipeline_start, \"wait1\")),\n  tar_target(wait2, sleeper(2, start$pipeline_start, \"wait2\")),\n  tar_target(wait3, sleeper(3, start$pipeline_start, \"wait3\")),\n  tar_target(wait4, sleeper(4, start$pipeline_start, \"wait4\")),\n  tar_target(trace, collate(start, wait1, wait2, wait3, wait4))\n)\n\n\nTo give you a sense of the dependencies, this is what the targets network looks like for this version of the pipeline. All four wait targets depend on the start target, and the trace target depends on everything:\n\ntar_visnetwork(targets_only = TRUE)\n\n\n\n\n\nIn hindsight, I realised that this could have been done more efficiently, but efficiency is not the primary goal here. I just want a pipeline in which all the targets report some detailed information about their execution. So let’s run it and see what we get:\n\ntar_make()\n\n▶ dispatched target start\n● completed target start [0.01 seconds, 274 bytes]\n▶ dispatched target wait1\n▶ dispatched target wait2\n▶ dispatched target wait3\n● completed target wait1 [1.003 seconds, 281 bytes]\n▶ dispatched target wait4\n● completed target wait2 [2.017 seconds, 284 bytes]\n● completed target wait3 [3.005 seconds, 283 bytes]\n● completed target wait4 [4.013 seconds, 283 bytes]\n▶ dispatched target trace\n● completed target trace [0.01 seconds, 355 bytes]\n▶ ended pipeline [7.104 seconds]\n\n\nOkay yes this makes sense. Every other target is dependent on start, and by design this target is always treated as outdated. So when the pipeline begins, the first thing that happens is that the start target is dispatched to a worker; the other two workers do nothing. Once start completes, all four wait targets are eligible for dispatch, but we only have three workers and so wait1, wait2, and wait4 are farmed out to workers while wait4 remains in the queue. When wait1 completes, one of the workers is freed up, thereby allowing wait4 to be dispatched. Once all four of the wait targets are completed, the final trace target is built.\nIf we call tar_crew(), we get the same high-level overview that we got last time:\n\ncrew  &lt;- tar_crew()\ncrew\n\n# A tibble: 3 × 4\n  controller               worker seconds targets\n  &lt;chr&gt;                    &lt;chr&gt;    &lt;dbl&gt;   &lt;int&gt;\n1 1185ab4d5ad0496d1d9d26dd 1         4.11       4\n2 1185ab4d5ad0496d1d9d26dd 2         4.09       1\n3 1185ab4d5ad0496d1d9d26dd 3         2.11       1\n\n\nHowever, this time around we also have access to the trace target that provides a more detailed summary of which R process excecuted each target, and at what time that execution started and stopped:\n\ntrace &lt;- tar_read(trace)\ntrace\n\n# A tibble: 6 × 5\n  name  worker_pid begins_at finishes_at duration\n  &lt;chr&gt;      &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;\n1 start    1202556     0            0       0    \n2 wait1    1202556     0.038        1.04    1.00 \n3 wait2    1202622     1.30         3.30    2.00 \n4 wait3    1202556     1.04         4.05    3.00 \n5 wait4    1202612     1.26         5.26    4.00 \n6 trace    1202556     5.28         5.29    0.002\n\n\nIn this data frame, the begins_at column records the amount of time that has passed between the time at which the pipeline started, and the time at which the current target begins execution. Similarly finishes_at records the time from pipeline start to the current target finishing. The duration column is the difference between the two. That being said, if you look at the code I used to calculate these, it’s clearly an approximation. But it will suffice.\nThe nice thing about the trace data is that the worker_pid column associates each target with a specific pid for the R process used to build that target. This is slightly finer-grained information than what we got by calling tar_crew(). For example, I now know that start and wait1 were both executed by the same R process (i.e., pid 1202556). Admittedly this is not much of a revelation: I could have guessed that just by looking at the logs when I called tar_make() and a few reasonable assumptions about how the scheduler11 works.\nOne minor irritation I have with the trace output is that it doesn’t directly allow me to match the worker_pid column against the worker column produced by tar_crew(). You can see the issue most cleanly if I aggregate the trace data frame by worker_pid like so:\n\ntrace_sum &lt;- trace |&gt; \n  select(worker_pid, duration) |&gt; \n  summarise(\n    seconds = sum(duration), \n    targets = n(), \n    .by = worker_pid\n  )\ntrace_sum\n\n# A tibble: 3 × 3\n  worker_pid seconds targets\n       &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;\n1    1202556    4.01       4\n2    1202622    2.00       1\n3    1202612    4.00       1\n\n\nLooking at this it’s immediately apparent that worker 1 must corresponded to the R process with pid 1202556, since trace and crew both agree that this is the only worker that executed four distinct targets. However, workers 2 and 3 both executed a single target, so we’ll have to resolve the ambiguity by looking at the seconds column. This is a little awkward because trace_sum$seconds will necessarily be slightly lower than crew$seconds because the time estimate in the trace data is constructed from within the R process that builds the target, but tar_crew() would (I assume) estimate the time from outside that process. The differences will be small, but noticeable. So I’ll use a rolling join:\n\nworker_lookup &lt;- crew |&gt; \n  left_join(\n    trace_sum, \n    by = join_by(targets, closest(x$seconds &gt; y$seconds))\n  ) |&gt; \n  select(worker, worker_pid)\nworker_lookup\n\n# A tibble: 3 × 2\n  worker worker_pid\n  &lt;chr&gt;       &lt;int&gt;\n1 1         1202556\n2 2         1202612\n3 3         1202622\n\n\nIt’s not ideal but it works: we now have a mapping between the numeric worker value returned by tar_crew() and the worker_pid value returned when Sys.getpid() is called from within the target function. Joining this gives us the following table:\n\ntarget_trace &lt;- trace |&gt; \n  left_join(worker_lookup, by = \"worker_pid\") |&gt; \n  mutate(id = row_number()) |&gt; \n  relocate(worker, .before = \"worker_pid\")\ntarget_trace\n\n# A tibble: 6 × 7\n  name  worker worker_pid begins_at finishes_at duration    id\n  &lt;chr&gt; &lt;chr&gt;       &lt;int&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1 start 1         1202556     0            0       0         1\n2 wait1 1         1202556     0.038        1.04    1.00      2\n3 wait2 3         1202622     1.30         3.30    2.00      3\n4 wait3 1         1202556     1.04         4.05    3.00      4\n5 wait4 2         1202612     1.26         5.26    4.00      5\n6 trace 1         1202556     5.28         5.29    0.002     6\n\n\nOr, to adopt a slightly nicer way of looking at it, we can draw a picture:\n\nggplot(target_trace, aes(begins_at, worker, color = name)) + \n  geom_segment(aes(xend = finishes_at, yend = worker)) +\n  geom_point() +\n  geom_point(aes(x = finishes_at)) +\n  theme_bw() + \n  scale_x_continuous(breaks = 0:6) +\n  labs(x = \"time\", y = \"worker\", color = \"target\")\n\n\n\n\n\n\n\n\nOkay yes, this now gives a detailed sense of which targets are dispatched to which workers, and at what time, but it does leave one thing missing: the total elapsed time shown by target_trace is about 5-6 seconds, but when I called tar_make() the total elapsed time was reported to be a little over 7 seconds. The discrepancy between the two is that the timing information recorded by my code doesn’t account for the time that the targets package spends setting up the pipeline, and recording metadata and target values in the _targets folder.\n\n\nEpilogue\nOne thing that bothered me a lot about the second version, when I implemented it, was this nagging intuition that most of what I was doing felt unnecessary. Certainly, there’s a limitation to the output of tar_crew() in the sense that it doesn’t tell you the worker to which each target was assigned, nor does it report the pid for each worker. That seems a little odd to me, but I am very willing to believe there’s a good reason for that.\nThe part that seemed utterly baffling to me was seeing that the on-screen output to tar_make() reports the execution time for each target, but (at the time) I couldn’t work out how to extract that information programmatically. It beggars belief to think that a package as sophisticated as targets wouldn’t record that information somewhere and…\n…yeah, of course it does. I was just looking in the wrong place. If you call tar_meta() it returns a tibble that stores a detailed listing of all targets – including the “secret”12 one that records the value of .Random.seed – that includes their execution time, any warnings or errors produced during their execution, and a great deal more besides:\n\ntar_meta()\n\n# A tibble: 10 × 18\n   name         type     data             command          depend                  seed path      time                size             bytes format repository iteration parent children  seconds warnings error\n   &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;                  &lt;int&gt; &lt;list&gt;    &lt;dttm&gt;              &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;  &lt;list&gt;      &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;\n 1 startup      function 050b477467191bc8 &lt;NA&gt;             &lt;NA&gt;                      NA &lt;chr [1]&gt; NA                  &lt;NA&gt;                NA &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;   &lt;chr [1]&gt;   NA    &lt;NA&gt;     &lt;NA&gt; \n 2 collate      function e1dc164dfb76a1e6 &lt;NA&gt;             &lt;NA&gt;                      NA &lt;chr [1]&gt; NA                  &lt;NA&gt;                NA &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;   &lt;chr [1]&gt;   NA    &lt;NA&gt;     &lt;NA&gt; \n 3 .Random.seed object   2972f998a0dffd17 &lt;NA&gt;             &lt;NA&gt;                      NA &lt;chr [1]&gt; NA                  &lt;NA&gt;                NA &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;   &lt;chr [1]&gt;   NA    &lt;NA&gt;     &lt;NA&gt; \n 4 sleeper      function 5ead800f7ec58068 &lt;NA&gt;             &lt;NA&gt;                      NA &lt;chr [1]&gt; NA                  &lt;NA&gt;                NA &lt;NA&gt;   &lt;NA&gt;       &lt;NA&gt;      &lt;NA&gt;   &lt;chr [1]&gt;   NA    &lt;NA&gt;     &lt;NA&gt; \n 5 start        stem     bc030b19dcc8d05c 2098fa32bc94d489 b151447ecf216509 -1013159115 &lt;chr [1]&gt; 2025-01-08 13:43:35 7588b18ac8276279   274 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    0.01 &lt;NA&gt;     &lt;NA&gt; \n 6 wait1        stem     7d70e7969b8b40fd 8900681d4f1181ad f0aa4f875b245efb  1208061404 &lt;chr [1]&gt; 2025-01-08 13:43:36 1945721079faa963   281 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    1.00 &lt;NA&gt;     &lt;NA&gt; \n 7 wait2        stem     2a9881811027da34 adfb6c7b6993720a f0aa4f875b245efb -2106940369 &lt;chr [1]&gt; 2025-01-08 13:43:38 38b61ce0f7e2cdb4   284 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    2.02 &lt;NA&gt;     &lt;NA&gt; \n 8 wait3        stem     8dee766f314b4ed0 71bbae8135d59048 f0aa4f875b245efb   216419040 &lt;chr [1]&gt; 2025-01-08 13:43:39 c93e8ca8fd5bc737   283 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    3.00 &lt;NA&gt;     &lt;NA&gt; \n 9 wait4        stem     b86f674901b70325 709dbf0fe45e36da f0aa4f875b245efb -1433985599 &lt;chr [1]&gt; 2025-01-08 13:43:40 c93e8ca8fd5bc737   283 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    4.01 &lt;NA&gt;     &lt;NA&gt; \n10 trace        stem     48ac84951180ac2c e0601f48904584d2 3642021a7a699dc5   -31475585 &lt;chr [1]&gt; 2025-01-08 13:43:40 cfeff01d1edd45fa   355 rds    local      vector    &lt;NA&gt;   &lt;chr [1]&gt;    0.01 &lt;NA&gt;     &lt;NA&gt; \n\n\nTrue, it doesn’t contain information about the system pid that executes the target, so I’d still have some work to do, but at least now I have the correct timing information for each target.\nOh well. I’ll know better next time, and anyway the entire point of this exercise was to teach myself how to use the package, so it’s all good really."
  },
  {
    "objectID": "posts/2025-01-08_using-targets/index.html#footnotes",
    "href": "posts/2025-01-08_using-targets/index.html#footnotes",
    "title": "Three short stories about targets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn all fairness, this is the primary purpose of this blog. Yes, it makes me very happy that other people find my posts useful and/or enjoyable, but that is actually a secondary goal. I write these posts for myself, because the act of writing is also an act of learning for me.↩︎\nYou can also call tar_manifest() to get a slightly more precise list of the targets and the commands with which they are associated, but I’m not going to bother with that here.↩︎\nI mean, if you’ve reached the point where you’re manually invalidating parts of the R markdown cache to spare yourself 15 minutes of compute time every time you want to tinker with a figure, girl he’s not the man for you and it’s time to look for better options.↩︎\nOkay fine, yes, if you’ve read the user manual you’d probably recognise that static branching would also work here if you were clever enough, and if I’m honest that was what I tried first. But then I realised that a blog is genuinely better suited to dynamic branching and the code was soooooo much simpler this way, so that’s what I ran with in the end.↩︎\nIn all honesty I don’t think this decision to use underscores to represent folder structure is a great design feature. Yes there was some logic for it at the time, but the underscore has an accepted meaning in the various R-based literate programming tools, and it is not good practice to violate user expectations the way I’m doing here. However, this is a silly side project that is not intended to be used for anything, so I feel no compunction at all about violating conventions.↩︎\nWhy is the url field never used? Should pattern really be called that? Shouldn’t some of these be private methods/fields? Blah blah blah. I threw this together quickly, it’s not meant to be taken seriously.↩︎\nWell, millenials. Not young people. The actual young people in my life keep talking about “rizz” and telling me what slaps, and sending memes I don’t understand. Frankly I need them to stop. My brain melted when my daughter tried to explain “skibidi” and it has never truly recovered.↩︎\nThese are absolute paths on my local filesystem, but in hindsight this seems like a bad choice: it would make more sense for them to be relative paths, taken from the blog root folder. Oh well.↩︎\nThis is the default behaviour for litedown, and while I believe you can change this, I’m still learning litedown and frankly I like this as a default, so I left it as is.↩︎\nI hasten to add that nobody should be using the queue package. Like most of my side-projects it was something I wrote just to prove to myself that I could do it, but it’s little more than a toy.↩︎\nScheduling in crew is handled with the mirai package, which looks amazing and is now on my to-do list to learn.↩︎\nNot very secret.↩︎"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html",
    "href": "posts/2024-12-19_art-from-code-2/index.html",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the second post in that series.\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#sampling-spatial-patterns",
    "href": "posts/2024-12-19_art-from-code-2/index.html#sampling-spatial-patterns",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Sampling spatial patterns",
    "text": "Sampling spatial patterns\nGenerative art relies on having access to a source of randomness, and using that randomness to construct patterned objects. In the last session I wrote a simple function to generate random palettes, for instance:\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nEvery time I call this function (assuming I don’t set the seed argument), R uses the pseudorandom number generator to select a palette of four colours:\n\nsample_canva()\nsample_canva()\nsample_canva()\n\n[1] \"#fcc875\" \"#baa896\" \"#e6ccb5\" \"#e38b75\"\n[1] \"#99d3df\" \"#88bbd6\" \"#cdcdcd\" \"#e9e9e9\"\n[1] \"#265c00\" \"#68a225\" \"#b3de81\" \"#fdffff\"\n\n\nEach of these colours is itself an object defined in a three dimensional space (the hex codes refer to co-ordinates in RGB space), and when I sample a palette of four colours what I’m really doing is constructing a random object with 12 components.\nWe can take this idea further. The sample_data() function I wrote in the last session creates random tibbles according to some simple rules, and those tibbles are structured objects too. Admittedly the structure to those objects isn’t very complicated, because there’s no pattern to numbers in the generated table, but there’s nothing stopping us from writing a function that randomly generates tabular data structures that have patterns in them, right? For instance, I could do this:\n\nsample_cross_matrix &lt;- function(n = 10, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  mat &lt;- matrix(data = 0, nrow = n, ncol = n)\n  mat[sample(n, 1), ] &lt;- 1\n  mat[, sample(n, 1)] &lt;- 1\n  return(mat)\n}\n\nsample_cross_matrix()\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    0    0    1    0    0    0    0    0    0     0\n [2,]    1    1    1    1    1    1    1    1    1     1\n [3,]    0    0    1    0    0    0    0    0    0     0\n [4,]    0    0    1    0    0    0    0    0    0     0\n [5,]    0    0    1    0    0    0    0    0    0     0\n [6,]    0    0    1    0    0    0    0    0    0     0\n [7,]    0    0    1    0    0    0    0    0    0     0\n [8,]    0    0    1    0    0    0    0    0    0     0\n [9,]    0    0    1    0    0    0    0    0    0     0\n[10,]    0    0    1    0    0    0    0    0    0     0\n\n\nAgain, this isn’t the most complicated example, but it’s an illustration of the idea that we can write functions to sample random spatial patterns:\nimage(sample_cross_matrix(n = 50), axes = FALSE, useRaster = TRUE)\nimage(sample_cross_matrix(n = 50), axes = FALSE, useRaster = TRUE)\nimage(sample_cross_matrix(n = 50), axes = FALSE, useRaster = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll concede that a generative art system that draws a cross at a random location in the image isn’t the most exciting or innovative thing I’ve ever written, but the core idea is clear I hope. And you would probably be unsurprised to learn that there are a number of more sophisticated tools you can use to generate random spatial patterns.\nIn this post I’ll introduce the ambient package, developed by Thomas Lin Pedersen, which supplies R bindings to a C++ library called FastNoise."
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#our-first-ambient-artwork",
    "href": "posts/2024-12-19_art-from-code-2/index.html#our-first-ambient-artwork",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Our first ambient artwork",
    "text": "Our first ambient artwork\nThe first step in creating art using the ambient package is to define the “canvas”, a spatial grid of x and y co-ordinates in which values will be stored. I find it helpful to imagine my canvas as the unit square: the smallest value is 0 and the largest value is 1. If I want an 800x800 grid, I use the seq() function to define a length-800 sequence of evenly-spaced numbers starting at 0 and ending at 1:\n\nx_coords &lt;- seq(from = 0, to = 1, length.out = 800)\ny_coords &lt;- seq(from = 0, to = 1, length.out = 800)\n\nThe canvas that I’ll paint on will be a data frame consisting of all possible combinations of x_coords and y_coords. In base R we can create an object like this using the expand.grid() function, and there’s a tidy equivalent called expand_grid() in the tidyr package. However, when working with the ambient package I prefer to use the long_grid() function that it supplies:\n\ncanvas &lt;- long_grid(x = x_coords, y = y_coords) \ncanvas\n\n# A tibble: 640,000 × 2\n       x       y\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1     0 0      \n 2     0 0.00125\n 3     0 0.00250\n 4     0 0.00375\n 5     0 0.00501\n 6     0 0.00626\n 7     0 0.00751\n 8     0 0.00876\n 9     0 0.0100 \n10     0 0.0113 \n# ℹ 639,990 more rows\n\n\nThe reason I use long_grid() rather than one of the more familiar versions is that under the hood Thomas has supplied some optimisations that make these objects more efficient for generative art purposes. Among other things, you can easily convert them to arrays, matrices, and raster objects that respect the implied spatial grid, which makes it a lot easier to render images from these objects. But let’s not dive too deep into the details right now!\nNow that we have a “canvas” we’ll want to add some “paint”, and to apply that paint we’ll need to select a “brush”. In this context, the brush is a spatial pattern generator of some kind. The ambient package includes many such generator functions. Some generate very regular patterns:\n\ngen_waves() generates smooth concentric wave-like patterns\ngen_spheres() generates concentric circles\ngen_checkerboard() generates grids of squares in a checkerboard pattern\n\nOthers generate very irregular patterns:\n\ngen_white() generates white noise: equal intensity at every spatial frequency\n\nThe most interesting generators tend to be those that create patterns that have some structure but are still quite unpredictable:\n\ngen_perlin() and gen_simplex() generate random “wavy” patterns\ngen_worley() generates random “cellular” patterns\n\nWe’ll see some examples of those later! For now let’s use gen_perlin() as our brush! Like all the pattern generators, the gen_perlin() function takes coordinate values as input. At a minimum it expects an x co-ordinate, but you can also supply y and z values if you like. You can also specify the frequency parameter, which sets the scale for the output: high frequency patterns will vary quickly as the co-ordinates change, low-frequency patterns will vary slowly. Compare these outputs for instance:\n\ngen_perlin(x = 1:5, y = 1, frequency = .001, seed = 1)\ngen_perlin(x = 1:5, y = 1, frequency = .5, seed = 1)\n\n[1] -0.001000010 -0.001000010 -0.001000009 -0.001000009 -0.001000007\n[1] -0.375  0.000  0.000 -0.250  0.000\n\n\nBoth versions show variability, but the scale is quite different! As an aside, notice that gen_perlin() also allows you to specify the seed used to generate the pattern, similar to the way I did earlier when writing sample_canva() to generate random palettes.\nNow that we have a sense for what the gen_perlin() function does let’s use it to add a new column to our canvas. I have dplyr loaded so I’ll use mutate() to do this:\n\ncanvas &lt;- canvas |&gt; \n  mutate(paint = gen_perlin(x, y, frequency = 10, seed = 1234))\n\ncanvas\n\n# A tibble: 640,000 × 3\n       x       y  paint\n   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1     0 0       0     \n 2     0 0.00125 0.0125\n 3     0 0.00250 0.0249\n 4     0 0.00375 0.0370\n 5     0 0.00501 0.0489\n 6     0 0.00626 0.0604\n 7     0 0.00751 0.0713\n 8     0 0.00876 0.0817\n 9     0 0.0100  0.0915\n10     0 0.0113  0.101 \n# ℹ 639,990 more rows\n\n\nNow that I have this column, I can use it to control the fill aesthetic in a ggplot. The co-ordinate values x and y specify a two dimensional grid, which means I can use geom_raster() here to create my artwork:\n\nart &lt;- ggplot(canvas, aes(x, y, fill = paint)) + \n  geom_raster(show.legend = FALSE) \n\nTo see what our Perlin art looks like, I’ll build the plot in three different ways. First poorly with no customisation of the ggplot theme and scales, then a little nicer by removing unneeded details, then finally with a little flair:\nart\nart + \n  theme_void() +\n  coord_equal()\nart + \n  theme_void() +\n  coord_equal() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_gradientn(colours = sample_canva())\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot bad. A little blurry looking, but it’s a nice place to start!\n\n\n\n\n\n\nExercise\n\n\n\nTry it yourself! In the materials there is a file called first-ambient-art.R that reproduces the code above. Try playing around with it to see what kind of output you can create by changing the values fed to gen_perlin(), or by trying other generator functions!"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#our-first-system",
    "href": "posts/2024-12-19_art-from-code-2/index.html#our-first-system",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Our first system",
    "text": "Our first system\nThe next step in the process is to start thinking about what aspects to the art should be variable, what aspects should be fixed, and use those insights to formalise this as a function. Some things that won’t change:\n\nThe art will always be a square grid rendered with geom_raster()\nThe spatial pattern will always come from one of the ambient::gen_*() functions\nWe’ll always remove extraneous elements from the art using theme_void() etc\n\nThese aspects to the art will be codified in the function body. There are some things that we might like to vary, however, and those will become arguments to the function:\n\nThe colour palette could vary from piece to piece\nThe underlying generator might be different in each piece\nThe spatial frequency could be different in each piece\nThe number of pixels in the grid could vary\nThe random number generator seed could vary\n\n\nmake_noise_art &lt;- function(\n    generator = gen_perlin, \n    frequency = 10, \n    seed = 1234,\n    pixels = 2000,\n    palette = c(\"#e5ddc8\", \"#01949a\", \"#004369\", \"#db1f48\"), \n    ...\n) {\n  \n  # define the grid\n  canvas &lt;- long_grid(\n    x = seq(from = 0, to = 1, length.out = pixels),\n    y = seq(from = 0, to = 1, length.out = pixels)\n  ) \n  \n  # use the generator to add paint\n  canvas &lt;- canvas |&gt;\n    mutate(\n      paint = generator(\n        x, y, \n        frequency = frequency, \n        seed = seed, \n        ...\n      )\n    )\n  \n  # use ggplot2 to draw the picture\n  art &lt;- canvas |&gt; \n    ggplot(aes(x, y, fill = paint)) + \n    geom_raster(show.legend = FALSE) +\n    theme_void() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_gradientn(colours = palette)\n  \n  return(art)\n}\n\nLet’s take a lot the effect of each of these arguments. Varying seed changes the spatial pattern depicted in each piece, but it doesn’t change it in any systematic way:\nmake_noise_art(seed = 1234)\nmake_noise_art(seed = 1001)\nmake_noise_art(seed = 9999)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn contrast, when I change the frequency argument I get systematic variation. The granularity of the spatial pattern changes in predictable ways as the frequency changes:\nmake_noise_art(frequency = 10)\nmake_noise_art(frequency = 20)\nmake_noise_art(frequency = 90)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s what happens when I vary palette. In the first example I’ve created a greyscale image by specifying a palette that runs from \"white\" to \"black\". The other two use palettes output by sample_canva():\nmake_noise_art(palette = c(\"white\", \"black\"))\nmake_noise_art(palette = sample_canva(seed = 123))\nmake_noise_art(palette = sample_canva(seed = 456))\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can vary the generator function. It probably will come as no surprise to discover that varying the generator has some wild effects. The output of a checkerboard pattern generator is fundamentally different to the output of a Worley noise generator, which in turn is very distinct from Perlin noise. As you become more familiar with using ambient you’ll start getting a sense of what each of these generators produce, and develop your own preferences for how to use them. For now, it’s enough to note that because the gen_*() functions all adopt (roughly) the same API, our make_noise_art() function works perfectly well when we swap out one for another:\nmake_noise_art(generator = gen_perlin)\nmake_noise_art(generator = gen_worley)\nmake_noise_art(generator = gen_waves) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nIn the materials there is a file called make-noise-art.R that includes the make_noise_art() function. Unlike the code I’ve shown here, the version in the script writes the output to a file (located at output/noise-art.png). Try playing around with the inputs to make_noise_art() to see what outputs you can create.\nAt the moment, the script is set up so that the output is always written to the same file, output/noise-art.png. When you create your own generative art systems you will want to ensure that each unique output is written to a file with a unique filename, and that this filename should (ideally!) allow you to work out what inputs were used to create the piece. How would you write code to do this?"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#why-dplyr-is-a-girls-best-friend",
    "href": "posts/2024-12-19_art-from-code-2/index.html#why-dplyr-is-a-girls-best-friend",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Why dplyr is a girls best friend",
    "text": "Why dplyr is a girls best friend\nAs you can see from the output we’ve created so far, spatial noise patterns can be quite pretty even without any special artistic intervention. Our make_noise_art() function isn’t complicated: it takes the output from a generator function like gen_perlin() and plots it as a raster object. It doesn’t manipulate or modify that output in any way. However, there’s nothing preventing us from doing precisely that if that’s what we want to do. To simplify the later code, let’s create a blank_canvas object that we can reuse as the starting point for our later pieces:\n\nblank_canvas &lt;- long_grid(\n  x = seq(from = 0, to = 1, length.out = 2000),\n  y = seq(from = 0, to = 1, length.out = 2000)\n) \n\nNow, let’s imagine that we’ve used some ambient magic to add a column called paint to our canvas. Here’s a plotting function that we can use that plots this as a raster object, just like we’ve been doing in the previous pieces (it optionally takes a palette too):\n\nplot_painted_canvas &lt;- function(canvas, palette = NULL) {\n  if(is.null(palette)) {\n    palette &lt;- c(\"#e5ddc8\",\"#01949a\",\"#004369\",\"#db1f48\")\n  }\n  canvas |&gt; \n    ggplot(aes(x, y, fill = paint)) + \n    geom_raster(show.legend = FALSE) +\n    theme_void() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_gradientn(colours = palette)\n}\n\nNow that we have these in place, we can recreate one of our earlier pieces by wrting it as a dplyr pipeline:\n\nblank_canvas |&gt;\n  mutate(paint = gen_perlin(x, y, frequency = 90, seed = 1234)) |&gt;\n  plot_painted_canvas()\n\n\n\n\n\n\n\n\nHowever, the mere fact that we can rewrite our art code like this opens up the possibility of using the dplyr data manipulation grammar in a more sophisticated way. Here’s an example that creates three different spatial patterns and then adds them together:\n\nblank_canvas |&gt; \n  mutate(\n    lf_noise = gen_simplex(x, y, frequency = 1, seed = 1234),\n    mf_noise = gen_simplex(x, y, frequency = 20, seed = 1234),\n    hf_noise = gen_simplex(x, y, frequency = 99, seed = 1234),\n    paint = lf_noise + mf_noise + hf_noise\n  ) |&gt;\n  plot_painted_canvas()\n\n\n\n\n\n\n\n\nRecall that our plot_painted_canvas() function uses the x and y columns to define the grid, and the paint column to define the to-be-plotted values. The lf_noise, mf_noise, and hf_noise columns are ignored. They’re intermediate steps, components that get mixed together when we define the paint column!\nIn the previous example I created the paint column by adding three columns together, but there is nothing preventing me from defining more elaborate mixing rules. In the example below, for example, I’ve generated a fourth spatially-varying pattern and used as a “gating” mechanism. So now what we have is a situation where the lf_noise, mf_noise, and hf_noise patterns are mixed together in a spatially inhomogeneous way that depends on the value of the gate column:\n\nblank_canvas |&gt; \n  mutate(\n    lf_noise = gen_simplex(x, y, frequency = 1),\n    mf_noise = gen_simplex(x, y, frequency = 20),\n    hf_noise = gen_simplex(x, y, frequency = 99),\n    gate = gen_spheres(x, y, frequency = 10) |&gt; normalise(),\n    paint = lf_noise +\n      (1 + mf_noise) * (gate &gt;= .1 & gate &lt; .6) +\n      (1 + hf_noise) * (gate &gt;= .05)\n  ) |&gt;\n  plot_painted_canvas(palette = sample_canva(seed = 2))\n\n\n\n\n\n\n\n\nThe normalise() function in this code is supplied by the ambient package and in this context all I’m doing with it is ensuring that the output of the gen_spheres() generator is rescaled to lie between 0 and 1.\nThe same basic idea can be used to produce some quite striking pieces when we apply a fancier generator to construct the spatial gate pattern:\n\nblank_canvas |&gt; \n  mutate(\n    lf_noise = gen_simplex(x, y, frequency = 1),\n    mf_noise = gen_simplex(x, y, frequency = 20),\n    hf_noise = gen_simplex(x, y, frequency = 99),\n    gate = gen_simplex(x, y, frequency = 10) |&gt; normalise(),\n    paint = lf_noise +\n      (2 + mf_noise) * (gate &gt;= .2 & gate &lt; .8) +\n      (2 + hf_noise) * (gate &gt;= .1)\n  ) |&gt;\n  plot_painted_canvas(palette = sample_canva(seed = 3))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nTry it yourself! In the materials there is a script called dplyr-ambient.R that defines blank_canvas and plot_painted_canvas() for you. At the bottom of the file there is space for you to add to the blank canvas. Try using dplyr and ambient together to create a spatial noise pattern of your own."
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#fractals",
    "href": "posts/2024-12-19_art-from-code-2/index.html#fractals",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Fractals",
    "text": "Fractals\nYou can also use the ambient package to create fractal patterns. The function that controls this is called fracture() and it’s easiest to demonstrate if we start with something simple. Suppose we have a “generator” function gen_sin() that generates sinusoidal patterns with a particular frequency. The code for this function is very simple:\n\ngen_sin &lt;- function(x, frequency, ...) {\n  sin(x * frequency)\n}\n\nTo create a fractal pattern based on this generator, we repeatedly apply this function to the input at different values of frequency. The outputs of repeated applications are combined together using a rule prescribed by a fractal function. This combination function doesn’t have to be very complicated: it might just be a linear combination! As an example, one of the fractal functions provided by ambient is fbm(), which stands for “fractional Brownian motion”. When this is used as the combination rule, the results are added together with increasing frequencies and decreasing strength. The code for fbm() is this:\n\nfbm &lt;- function(base, new, strength, ...) {\n  base + new * strength\n}\n\nA base pattern is added to a new pattern, weighted by some strength. That’s all it does!\nIf we wanted to create a fractal based on the gen_sin() generator, using fbm() as our fractal function, this is the code we would use:\n\nfracture(\n  x = 1:20, \n  noise = gen_sin, \n  fractal = fbm, \n  octaves = 8\n)\n\n [1]  1.24983550  0.80892271 -0.26356816 -0.20012820 -0.97755603 -0.80161946\n [7]  1.08394804  1.11211005 -0.24443826 -0.04492181 -0.97817310 -1.04255993\n[13]  1.07719639  0.86143412  0.21216704  0.24305786 -0.95445686 -1.32043009\n[19]  0.56698796  1.00122663\n\n\nIn this code, octaves = 8 specifies the number of times to apply the generator and fractal function. In essence it is the number of iterations over which we run the algorithm. It’s easiest to see what this looks like if we gradually increase the number of iterations and plot the results:\ndat &lt;- tibble(\n  x = seq(0, 10, length.out = 1000), \n  y1 = fracture(x = x, noise = gen_sin, fractal = fbm, octaves = 1),\n  y2 = fracture(x = x, noise = gen_sin, fractal = fbm, octaves = 2),\n  y8 = fracture(x = x, noise = gen_sin, fractal = fbm, octaves = 8),\n  y20 = fracture(x = x, noise = gen_sin, fractal = fbm, octaves = 20)\n) \n\nggplot(dat) + geom_path(aes(x, y1)) + ggtitle(\"One iteration\")\nggplot(dat) + geom_path(aes(x, y2)) + ggtitle(\"Two iterations\")\nggplot(dat) + geom_path(aes(x, y8)) + ggtitle(\"Eight iterations\")\nggplot(dat) + geom_path(aes(x, y20)) + ggtitle(\"Twenty iterations\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs the number of octaves increases the plots become more and more detailed. In this case we can’t visually discriminate between 8 and 20 octaves because the differences are too fine-grained to be visible. That’s quite typical because – unless you modify the gain and frequency functions used by fracture() – each successive iteration (or octave) will be calculated at double the frequency of the previous one (leading to finer-grained changes) and with half the strength (less weight is given to later octaves). You can modify this if you want to. For example:\ncustom_fracture &lt;- function(x, ...) {\n  fracture(\n    gain = function(strength) {strength * .8},\n    frequency = function(frequency) {frequency * 1.3},\n    noise = gen_sin, \n    fractal = fbm,\n    x = x,\n    ...\n  )\n}\n\ndat &lt;- tibble(\n  x = seq(0, 10, length.out = 1000), \n  y1 = custom_fracture(x, octaves = 1),\n  y2 = custom_fracture(x, octaves = 2),\n  y8 = custom_fracture(x, octaves = 8),\n  y20 = custom_fracture(x, octaves = 20)\n) \n\nggplot(dat) + geom_path(aes(x, y1)) + ggtitle(\"One iteration\")\nggplot(dat) + geom_path(aes(x, y2)) + ggtitle(\"Two iterations\")\nggplot(dat) + geom_path(aes(x, y8)) + ggtitle(\"Eight iterations\")\nggplot(dat) + geom_path(aes(x, y20)) + ggtitle(\"Twenty iterations\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHopefully you get the basic idea.\nIn any case, let’s take this same concept and start using it in conjunction with the spatial noise generators supplied by ambient. To keep things simple and avoid the need to write plotting code over and over, let’s define a fractal_art() function like this:\n\nfractal_art &lt;- function(fractal, generator, palette = NULL, ...) {\n  blank_canvas |&gt;\n    mutate(\n      paint = fracture(\n        noise = generator,\n        fractal = fractal,\n        x = x, \n        y = y, \n        ...\n      )\n    ) |&gt;\n    plot_painted_canvas(palette = palette)\n}\n\nHere’s what happens when we use gen_checkerboard() as our spatial pattern generator, and fbm() as our fractal function:\nfractal_art(fbm, gen_checkerboard, seed = 1, octaves = 1)\nfractal_art(fbm, gen_checkerboard, seed = 1, octaves = 2)\nfractal_art(fbm, gen_checkerboard, seed = 1, octaves = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt has the same “feel” as the sinusoidal fractals we were working with earlier: as we increase the number of octaves the output contains more copies of the “checker board” pattern, each one depicted on a smaller scale than the last one. The same idea applies to the gen_waves() generator:\nfractal_art(fbm, gen_waves, seed = 1, octaves = 1)\nfractal_art(fbm, gen_waves, seed = 1, octaves = 2)\nfractal_art(fbm, gen_waves, seed = 1, octaves = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy the time we reach 20 octaves, the image has a quite intricate pattern of concentric rings. It’s quite pretty, but gen_checkerboard() and gen_waves() are both very simple generator functions. What happens when our generator is a more elaborate multidimensional noise generator like gen_simplex()? Simplex noise looks like this before we apply any fractal function to it:\n\nblank_canvas |&gt;\n  mutate(paint = gen_simplex(x, y, seed = 2)) |&gt;\n  plot_painted_canvas()\n\n\n\n\n\n\n\n\nHere’s what happens when we combine gen_simplex() with the fbm() fractal function:\nfractal_art(fbm, gen_simplex, seed = 2, octaves = 1)\nfractal_art(fbm, gen_simplex, seed = 2, octaves = 2)\nfractal_art(fbm, gen_simplex, seed = 2, octaves = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe result, once we reach 20 octaves, is quite intricate.\nChanging the fractal function has a substantial effect on the output. So far all the fractals I’ve created have used fbm() as the fractal function, but there’s nothing stopping you from writing your own or using one of the other functions supplied by ambient. For example, the ridged() fractal function produces some very lovely patterns:\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 1)\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 2)\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s also possible to get nice effects by modifying the gain and frequency functions. For example, here’s an example where the strength of each successive iteration of ridged() noise diminishes to 80% of of the strength of the previous iteration:\ngf &lt;- function(x) x * .8\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 1, gain = gf)\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 2, gain = gf)\nfractal_art(ridged, gen_simplex, seed = 2, octaves = 20, gain = gf)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorley noise is an interesting case. The behaviour of gen_worley() is to carve the image up in to distinct cells, and colour each pixel in the image based on the cells they belong to. It’s closely related to Voronoi tesselation, a technique I’ll talk about in a later session. In the simplest case, all pixels in a particular cell are assigned the same colour. So a very basic Worley noise pattern might look like this:\nblank_canvas |&gt;\n  mutate(paint = gen_worley(x, y, seed = 6)) |&gt;\n  plot_painted_canvas()\n\n\n\n\n\n\n\n\n\n\n\nWhen we create fractals using this kind of generator, there’s a tendency to end up with “kaleidoscopic” looking patterns. Here’s an example using the billow() fractal function:\nfractal_art(billow, gen_worley, seed = 6, octaves = 1)\nfractal_art(billow, gen_worley, seed = 6, octaves = 3)\nfractal_art(billow, gen_worley, seed = 6, octaves = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nHowever, gen_worley() allows also allows you to colour the pixels in different ways. For example, if I set value = \"distance\", each pixel will be coloured as a function of how distant it is from the centroid of the cell it belongs to. A basic pattern looks like this:\nblank_canvas |&gt;\n  mutate(paint = gen_worley(x, y, seed = 6, value = \"distance\")) |&gt;\n  plot_painted_canvas()\n\n\n\n\n\n\n\n\n\n\n\nFractals created using this method look like this:\nfractal_art(billow, gen_worley, seed = 6, octaves = 1, value = \"distance\")\nfractal_art(billow, gen_worley, seed = 6, octaves = 3, value = \"distance\")\nfractal_art(billow, gen_worley, seed = 6, octaves = 8, value = \"distance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s nothing stopping you from writing your own generator function either. At the start of this section that’s exactly what I did in one dimension with gen_sin(). As a two dimensional example, let’s suppose I wanted to create a variation of Worley noise that mixes both the \"cell\" colouring and the \"distance\" colouring. Here’s a generator function that does precisely that:\ngen_scope &lt;- function(x, y, ...) {\n  worley_cell &lt;-gen_worley(x, y, value = \"cell\", ...)\n  worley_dist &lt;-gen_worley(x, y, value = \"distance\", ...)\n  return(normalise(worley_cell) + 5 * normalise(worley_dist))\n}\n\npal &lt;- sample_canva(seed = 2)\n\nblank_canvas |&gt;\n  mutate(paint = gen_scope(x, y, seed = 9)) |&gt;\n  plot_painted_canvas(palette = pal)\n\n\n\n\n\n\n\n\n\n\n\nI can now use my gen_scope() function as the generator for my fractal:\nfractal_art(billow, gen_scope, palette = pal, seed = 9, octaves = 1)\nfractal_art(billow, gen_scope, palette = pal, seed = 9, octaves = 2)\nfractal_art(billow, gen_scope, palette = pal, seed = 9, octaves = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nI can make the generator as elaborate as I like. The gen_gate() function below uses a “gating” mechanism just like the example I used earlier:\ngen_gate &lt;- function(x, y, frequency, ...) {\n  lf &lt;- gen_simplex(x, y, frequency = frequency, ...)\n  mf &lt;- gen_simplex(x, y, frequency = frequency * 20, ...)\n  hf &lt;- gen_simplex(x, y, frequency = frequency * 99, ...)\n  gate &lt;- gen_simplex(x, y, frequency = frequency * 10, ...) \n  gate &lt;- normalise(gate)\n  paint &lt;- lf + \n    (mf + 2) * (gate &gt;= .2 & gate &lt; .8) + \n    (hf + 2) * (gate &gt;= .1)\n  return(paint)\n}\n\npal &lt;- sample_canva(seed = 3)\n\nfractal_art(billow, gen_gate, palette = pal, seed = 9, octaves = 1)\nfractal_art(billow, gen_gate, palette = pal, seed = 9, octaves = 2)\nfractal_art(billow, gen_gate, palette = pal, seed = 9, octaves = 20)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nThe fractal_art.R script in the materials contains all the setup you need to play around with the fractal_art() function. Try using it to explore the possibilities! There are a lot of possibilities in fractals. Here are a few ideas to get you started:\n\nThe easiest way to play around with fractals is to modify the basic arguments. Try changing the generator, fractal, freq_init (i.e., frequency value for the first octave), octaves, seed, and palette to make a piece you really like.\nA fun inversion: use those same arguments to create something that you find incredibly ugly!\nThe fractal_art() function is written flexibly enough that you can pass your own gain functions and frequency functions. There’s an example of this above. Try writing your own functions to modify the gain and frequency rules that apply to the fractal.\nUsing the gen_scope() and gen_gate() examples to motivate you, write your own generator function. See what effect that has."
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#curl-of-a-spatial-noise-pattern",
    "href": "posts/2024-12-19_art-from-code-2/index.html#curl-of-a-spatial-noise-pattern",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Curl (of a spatial) noise (pattern)",
    "text": "Curl (of a spatial) noise (pattern)\nThe last topic to talk about in regards to the ambient package is curl noise. The concept comes from vector calculus, I’m afraid, but fortunately for us we don’t actually need to care. To quote from Wikipedia, the curl is\n\na vector operator that describes the infinitesimal circulation of a vector field in three-dimensional Euclidean space. The curl at a point in the field is represented by a vector whose length and direction denote the magnitude and axis of the maximum circulation. The curl of a field is formally defined as the circulation density at each point of the field.\n\nExciting stuff. But what does it mean? Well, let’s suppose I have a vector field and…\n… wait, what?\nOkay, let’s take a step back. Suppose I have a very small grid of points:\n\nsmol_grid &lt;- long_grid(x = 1:20, y = 1:20)\nggplot(smol_grid) +\n  geom_point(aes(x, y)) + \n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\nNow let’s compute the value of the simplex noise pattern at each of these points using gen_simplex(), and represent that as the size of the plot marker (because I can’t resist the urge to make something pretty), or more conventionally as a contour plot illustrating the “height” of the pattern at each point:\nsmol_simplex &lt;- smol_grid |&gt;\n  mutate(z = gen_simplex(x, y, seed = 1, frequency = .1)) \n\nsmol_simplex |&gt;\n  ggplot(aes(x, y, size = z)) +\n  geom_point(show.legend = FALSE) + \n  theme_void() + \n  coord_equal()\nsmol_simplex |&gt;\n  ggplot(aes(x, y, z = z)) +\n  geom_contour_filled(show.legend = FALSE, bins = 10) + \n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow imagine placing a ball at a point on this surface. Unless it’s at a completely flat spot, it will start rolling in a particular direction and at a particular speed. We can work out where it will start rolling by computing the slope of surface at each point. To do this, we’ll use a finite differencing approximation to calculate the partial derivatives. Or, to put it in less fancy terms, we’ll add a small diffrenece eps to the x-coordinate and compute the value of the simplex noise at the modified values. That gives us the slope in the x-direction. We do the same thing for the y-direction. Putting these two vectors together gives us the local slope.\n\neps &lt;- .001\nsmol_curl &lt;- smol_grid |&gt; mutate(\n  x_add = gen_simplex(x + eps, y, seed = 1, frequency = .1),\n  x_sub = gen_simplex(x - eps, y, seed = 1, frequency = .1),\n  y_add = gen_simplex(x, y + eps, seed = 1, frequency = .1),\n  y_sub = gen_simplex(x, y - eps, seed = 1, frequency = .1),\n  x_slope = (x_add - x_sub) / (2 * eps), \n  y_slope = (y_add - y_sub) / (2 * eps),\n  x_curl = -y_slope, \n  y_curl = x_slope\n)\n\nIf I wanted to plot how fast the simplex noise field was changing at each point on this grid, I’d just plot the x_slope and y_slope values. That would give me something like this:\n\nggplot(smol_curl) + \n  geom_segment(\n    mapping = aes(\n      x = x, \n      y = y, \n      xend = x + x_slope * 2, \n      yend = y + y_slope * 2\n    ), \n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) + \n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\nThis map of arrows (a.k.a. vector field) depicts the slope at each point on the simplex noise surface. It’s a measure of how fast a ball would start rolling if you placed it down at a particular spot.\nLet’s tweak the analogy slightly. Instead of a ball on a hill, imagine the arrows depict a current pushing a rough-edged disc around in a pool of water. When we place the disc in the water it will start moving because the current pushes it around, but because it’s rough-edged the friction of water flowing over it will make it start rotating. The curl of a field describes these rotational forces. In the same way that our simplex noise pattern implies a vector field of slope values, it also implies a vector field of curl values. For reasons that I’m sure a physicist can explain to me – that I’m certain will have something to do with a conservation law of some kind – x_curl = -y_slope and y_curl = x_slope.\nWhatever.\nAnyway.\nNow we have the curl of our simplex noise and we know vaguely what it means. More importantly, we can draw a pretty picture of the curl field:\n\nggplot(smol_curl) + \n  geom_segment(\n    mapping = aes(\n      x = x, \n      y = y, \n      xend = x + x_curl * 2, \n      yend = y + y_curl * 2\n    ), \n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) + \n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\nAs it turns out I actually didn’t need to bother with computing this manually, because ambient supplies a curl_noise() function that does the exact same computations for us. I pass it the x and y coordinates from my smol_grid, specify that the generator function is gen_simplex(), and pass the parameters of the noise function (e.g., its seed and frequency) as additional arguments:\n\ncurl &lt;- curl_noise(\n  generator = gen_simplex,\n  seed = 1,\n  frequency = .1,\n  x = smol_grid$x, \n  y = smol_grid$y\n)\n\nas_tibble(curl)\n\n# A tibble: 400 × 2\n         x        y\n     &lt;dbl&gt;    &lt;dbl&gt;\n 1  0.312   0.0597 \n 2  0.124   0.0560 \n 3 -0.0121  0.00537\n 4 -0.0647 -0.0316 \n 5 -0.0808 -0.0412 \n 6 -0.121  -0.0349 \n 7 -0.216  -0.0351 \n 8 -0.214  -0.0817 \n 9 -0.0387 -0.196  \n10  0.138  -0.311  \n# ℹ 390 more rows\n\n\nThis curl data frame contains x and y columns that specify the curl values at each point in the input. So now I can plot these curl values in the same “map of arrows” style, and unsurprisingly I obtain the same result as last time:\n\nsmol_grid |&gt;\n  mutate(\n    x2 = x + curl$x * 2,\n    y2 = y + curl$y * 2\n  ) |&gt; \n  ggplot() + \n  geom_segment(\n    mapping = aes(x, y, xend = x2, yend = y2),\n    arrow = arrow(length = unit(0.1, \"cm\"))\n  ) + \n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\nGenerative artists have a particular fondness for computing the curl of a noise field and using it for nefarious purposes. There are technical reasons for that, no doubt, but I’m lazy and I feel like I’ve spent too much of my life thinking about this already. So let’s skip the reasons this time and just start doing it. To make my life a little easier I’ll write an update_curl() function that takes a current_state data frame as input (which we assume contains variables x and y that define a grid), computes the curl at all points in this grid, and then returns a new set of x and y values that “add a little bit of curl” to those x and y values:\n\nupdate_curl &lt;- function(current_state, step_size = .0005, ...) {\n  curl &lt;- curl_noise(\n    x = current_state$x, \n    y = current_state$y,\n    ...\n  )\n  next_state &lt;- current_state |&gt;\n    mutate(\n      x = x + curl$x * step_size,\n      y = y + curl$y * step_size,\n      time = time + 1\n    )\n  return(next_state)\n}\n\nNext, let’s define an initial state. At “time” point 1 we have a set of co-ordinates laid out on a grid:\n\ncoords &lt;- seq(0, 1, length.out = 50)\ntime_1 &lt;- long_grid(x = coords, y = coords) |&gt; \n  mutate(id = row_number(), time = 1)\ntime_1\n\n# A tibble: 2,500 × 4\n       x      y    id  time\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     0 0          1     1\n 2     0 0.0204     2     1\n 3     0 0.0408     3     1\n 4     0 0.0612     4     1\n 5     0 0.0816     5     1\n 6     0 0.102      6     1\n 7     0 0.122      7     1\n 8     0 0.143      8     1\n 9     0 0.163      9     1\n10     0 0.184     10     1\n# ℹ 2,490 more rows\n\n\nNow we can use our update_curl() function to compute a new set of x and y values. We can do this multiple times if we like:\n\ntime_2 &lt;- time_1 |&gt;\n  update_curl(\n    generator = gen_simplex,\n    frequency = 10, \n    seed = 1234\n  )\n\ntime_3 &lt;- time_2 |&gt; \n  update_curl(\n    generator = gen_simplex,\n    frequency = 10, \n    seed = 1234\n  )\n\ntime_3\n\n# A tibble: 2,500 × 4\n          x      y    id  time\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1 -0.0264  0.0309     1     3\n 2  0.0226  0.0502     2     3\n 3  0.0417  0.0712     3     3\n 4  0.00234 0.0843     4     3\n 5 -0.0372  0.0573     5     3\n 6 -0.0253  0.0786     6     3\n 7 -0.0154  0.117      7     3\n 8 -0.00378 0.136      8     3\n 9  0.00913 0.159      9     3\n10 -0.0199  0.201     10     3\n# ℹ 2,490 more rows\n\n\nAt this point it’s important to notice something of particular relevance to generative art. Are there any physicists near you as you read this? Can you hear them sighing?\nGood.\nFrom a physics perspective I’ve done something quite peculiar in this code. I’ve updated the “position” of a set of points (or particles) by adding their rotation (i.e. curl) to their current “position”. I’m not really simulating real movement in physical space I’m plotting changes in rotational forces. Curl fields don’t plot real world movement, they’re an abstraction.\nWhich is fine. From an artistic point of view we care mostly about the fact that we can use this tool to make pretty things. So let’s visualise these “curl updates”:\ndat12 &lt;- bind_rows(time_1, time_2)\ndat123 &lt;- bind_rows(time_1, time_2, time_3)\n\ndat12 |&gt;\n  ggplot(aes(x, y, group = id)) + \n  geom_path() +\n  theme_void() + \n  coord_equal() \ndat123 |&gt;\n  ggplot(aes(x, y, group = id)) + \n  geom_path() +\n  theme_void() + \n  coord_equal() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis seems promising, right?"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#curl-of-a-fractal-pattern",
    "href": "posts/2024-12-19_art-from-code-2/index.html#curl-of-a-fractal-pattern",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Curl of a fractal pattern",
    "text": "Curl of a fractal pattern\nOne nice thing about curl_noise() is that it can be applied to any generator, including fracture(). Here’s the basic idea:\n\ncurl_data &lt;- function(\n    data, \n    iterations = 50,\n    step_size = .001,\n    ...\n) {\n  \n  update &lt;- function(current_state, iteration, ...) {\n    curl &lt;- curl_noise(\n      x = current_state$x, \n      y = current_state$y,\n      generator = fracture,\n      ...\n    )\n    next_state &lt;- current_state |&gt;\n      mutate(\n        x = x + curl$x * step_size,\n        y = y + curl$y * step_size,\n        time = time + 1\n      )\n    return(next_state)\n  }\n  \n  data |&gt; \n    mutate(id = row_number(), time = 1) |&gt;\n    accumulate(1:iterations, update, .init = _, ...) |&gt;\n    bind_rows()\n}\n\ncurl_art &lt;- function(...) {\n  curl_data(...) |&gt; \n    ggplot(aes(x, y, group = id)) + \n    geom_path() +\n    theme_void() + \n    coord_equal() \n}\n\nA grid of small fractal walks:\n\nsmol_grid |&gt;\n  mutate(x = normalise(x), y = normalise(y)) |&gt;\n  curl_art(noise = gen_simplex, fractal = fbm, octaves = 4, freq_init = .5)\n\n\n\n\n\n\n\n\nAn example where the initial points all lie on a circle:\ncircle &lt;- function(n = 100) {\n  tibble(\n    theta = 2 * pi * (1:n) / n, \n    x = cos(theta),\n    y = sin(theta)\n  )\n}\n\ncurl_circle &lt;- function(octaves) {\n  curl_art(\n    data = circle(500),\n    iterations = 100, \n    noise = gen_simplex,\n    fractal = fbm,\n    octaves = octaves, \n    seed = 1, \n    freq_init = 1,\n    frequency = ~ . * 1.2,\n    gain_init = 1,\n    gain = ~ . * .9,\n    step_size = .003\n  )\n}\n\ncurl_circle(octaves = 1)\ncurl_circle(octaves = 3)\ncurl_circle(octaves = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n\nA related example using polygons, heavily influenced by Thomas Lin Pedersen’s “genesis” system:\n\ncustom_curl_data &lt;- function(data) {\n  curl_data(\n    data = data,\n    iterations = 80, \n    octaves = 10,\n    fractal = ridged,\n    noise = gen_cubic,\n    freq_init = 1,\n    frequency = ~ . * 1.2,\n    gain_init = 1,\n    gain = ~ . * .9,\n    seed = 1\n  )\n}\n\ndat1 &lt;- circle(5000) |&gt; \n  custom_curl_data()\n\ndat2 &lt;- circle(5000) |&gt;\n  mutate(x = x * .99, y = y * .99) |&gt;\n  custom_curl_data()\n\nggplot(mapping = aes(x, y, group = time)) +\n  geom_polygon(data = dat1, fill = \"white\", alpha = .02) +\n  geom_polygon(data = dat2, fill = \"black\", alpha = .02) +\n  theme_void() + \n  coord_equal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\nThe curl-art-1.R and curl-art-2.R scripts contain code to generate the “small grid of fractal walks” image and the “genesis-inspired” image. In both cases the ouput is written to a 2000x2000 pixel png file, and the time taken to complete the task printed to the screen.\n\nRun both scripts, and compare the difference in rendering times.\nModify the “small grid” version so that it produces smoother looking results.\nExplore what you can do with the “genesis style”. It’s a powerful technique that can do a lot more than I’ve done in this code. By now you should have a good sense of what dials you can turn: the fractal, the generator, the parameters, the palette, etc. You can use dplyr to modify the data if you want to. Try to make something you really like!"
  },
  {
    "objectID": "posts/2024-12-19_art-from-code-2/index.html#materials",
    "href": "posts/2024-12-19_art-from-code-2/index.html#materials",
    "title": "Art from code II. Spatial tricks with ambient",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\ncurl-art-1.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\nlibrary(here)\nlibrary(tictoc)\n\ncurl_data &lt;- function(\n    data, \n    iterations = 50,\n    step_size = .001,\n    ...\n) {\n  \n  update &lt;- function(current_state, iteration, ...) {\n    curl &lt;- curl_noise(\n      x = current_state$x, \n      y = current_state$y,\n      generator = fracture,\n      ...\n    )\n    next_state &lt;- current_state |&gt;\n      mutate(\n        x = x + curl$x * step_size,\n        y = y + curl$y * step_size,\n        time = time + 1\n      )\n    return(next_state)\n  }\n  \n  data |&gt; \n    mutate(id = row_number(), time = 1) |&gt;\n    accumulate(1:iterations, update, .init = _, ...) |&gt;\n    bind_rows()\n}\n\ncurl_art &lt;- function(...) {\n  curl_data(...) |&gt; \n    ggplot(aes(x, y, group = id)) + \n    geom_path() +\n    theme_void() + \n    coord_equal() \n}\n\nsmol_grid &lt;- long_grid(x = 1:20, y = 1:20)\n\npic &lt;- smol_grid |&gt;\n  mutate(x = normalise(x), y = normalise(y)) |&gt;\n  curl_art(noise = gen_simplex, fractal = fbm, octaves = 4, freq_init = .5)\n\ntic()\nggsave(\n  filename = here(\"output\", \"curl-art-1.png\"), \n  plot = pic,\n  width = 2000,\n  height = 2000,\n  units = \"px\",\n  dpi = 300,\n  bg = \"white\"\n)\ntoc()\n\n\n\n\n\n\n\n\n\n\ncurl-art-2.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\nlibrary(here)\nlibrary(tictoc)\n\ncurl_data &lt;- function(\n    data, \n    iterations = 50,\n    step_size = .001,\n    ...\n) {\n  \n  update &lt;- function(current_state, iteration, ...) {\n    curl &lt;- curl_noise(\n      x = current_state$x, \n      y = current_state$y,\n      generator = fracture,\n      ...\n    )\n    next_state &lt;- current_state |&gt;\n      mutate(\n        x = x + curl$x * step_size,\n        y = y + curl$y * step_size,\n        time = time + 1\n      )\n    return(next_state)\n  }\n  \n  data |&gt; \n    mutate(id = row_number(), time = 1) |&gt;\n    accumulate(1:iterations, update, .init = _, ...) |&gt;\n    bind_rows()\n}\n\ncurl_art &lt;- function(...) {\n  curl_data(...) |&gt; \n    ggplot(aes(x, y, group = id)) + \n    geom_path() +\n    theme_void() + \n    coord_equal() \n}\n\ncustom_curl_data &lt;- function(data) {\n  curl_data(\n    data = data,\n    iterations = 80, \n    octaves = 10,\n    fractal = ridged,\n    noise = gen_cubic,\n    freq_init = 1,\n    frequency = ~ . * 1.2,\n    gain_init = 1,\n    gain = ~ . * .9,\n    seed = 1\n  )\n}\n\ncircle &lt;- function(n = 100) {\n  tibble(\n    theta = 2 * pi * (1:n) / n, \n    x = cos(theta),\n    y = sin(theta)\n  )\n}\n\ndat1 &lt;- circle(5000) |&gt; \n  custom_curl_data()\n\ndat2 &lt;- circle(5000) |&gt;\n  mutate(x = x * .99, y = y * .99) |&gt;\n  custom_curl_data()\n\npic &lt;- ggplot(mapping = aes(x, y, group = time)) +\n  geom_polygon(data = dat1, fill = \"#ffffff10\") +\n  geom_polygon(data = dat2, fill = \"#22222205\") +\n  theme_void() +\n  coord_equal()\n\ntic()\nggsave(\n  filename = here(\"output\", \"curl-art-2.png\"), \n  plot = pic,\n  width = 2000,\n  height = 2000,\n  units = \"px\",\n  dpi = 300,\n  bg = \"white\"\n)\ntoc()\n\n\n\n\n\n\n\n\n\n\ndplyr-ambient.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\nlibrary(here)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nblank_canvas &lt;- long_grid(\n  x = seq(from = 0, to = 1, length.out = 2000),\n  y = seq(from = 0, to = 1, length.out = 2000)\n) \n\nplot_painted_canvas &lt;- function(canvas, palette = NULL) {\n  if(is.null(palette)) {\n    palette &lt;- c(\"#e5ddc8\",\"#01949a\",\"#004369\",\"#db1f48\")\n  }\n  canvas |&gt; \n    ggplot(aes(x, y, fill = paint)) + \n    geom_raster(show.legend = FALSE) +\n    theme_void() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_gradientn(colours = palette)\n}\n\n# your code here! add to the blank canvas :)\nblank_canvas\n\n\n\n\n\n\n\n\n\n\nfirst-ambient-art.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nx_coords &lt;- seq(from = 0, to = 1, length.out = 800)\ny_coords &lt;- seq(from = 0, to = 1, length.out = 800)\ncanvas &lt;- long_grid(x = x_coords, y = y_coords) \n\nfreq_spatial &lt;- 10\nseed_spatial &lt;- 100\nseed_palette &lt;- 101\n\ndat &lt;- canvas |&gt; \n  mutate(\n    paint = gen_perlin(\n      x = x, \n      y = y, \n      frequency = freq_spatial, \n      seed = seed_spatial\n    )\n  )\n  \npic &lt;- dat |&gt;\n  ggplot(aes(x, y, fill = paint)) + \n  geom_raster(show.legend = FALSE) +\n  theme_void() + \n  coord_equal() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_gradientn(\n    colours = sample_canva(seed_palette)\n  )\n\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nfractal-art.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\nlibrary(here)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nblank_canvas &lt;- long_grid(\n  x = seq(from = 0, to = 1, length.out = 2000),\n  y = seq(from = 0, to = 1, length.out = 2000)\n) \n\nplot_painted_canvas &lt;- function(canvas, palette = NULL) {\n  if(is.null(palette)) {\n    palette &lt;- c(\"#e5ddc8\",\"#01949a\",\"#004369\",\"#db1f48\")\n  }\n  canvas |&gt; \n    ggplot(aes(x, y, fill = paint)) + \n    geom_raster(show.legend = FALSE) +\n    theme_void() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_gradientn(colours = palette)\n}\n\nfractal_art &lt;- function(fractal, generator, palette = NULL, ...) {\n  blank_canvas |&gt;\n    mutate(\n      paint = fracture(\n        noise = generator,\n        fractal = fractal,\n        x = x, \n        y = y, \n        ...\n      )\n    ) |&gt;\n    plot_painted_canvas(palette = palette)\n}\n\n\n\n\n\n\n\n\n\n\nmake-noise-art.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(ambient)\nlibrary(here)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nmake_noise_art &lt;- function(\n    generator = gen_perlin, \n    frequency = 10, \n    seed = 1234,\n    pixels = 2000,\n    palette = c(\"#e5ddc8\", \"#01949a\", \"#004369\", \"#db1f48\"), \n    ...\n) {\n  \n  # define the grid\n  canvas &lt;- long_grid(\n    x = seq(from = 0, to = 1, length.out = pixels),\n    y = seq(from = 0, to = 1, length.out = pixels)\n  ) \n  \n  # use the generator to add paint\n  canvas &lt;- canvas |&gt;\n    mutate(\n      paint = generator(\n        x, y, \n        frequency = frequency, \n        seed = seed, \n        ...\n      )\n    )\n  \n  # use ggplot2 to draw the picture\n  art &lt;- canvas |&gt; \n    ggplot(aes(x, y, fill = paint)) + \n    geom_raster(show.legend = FALSE) +\n    theme_void() +\n    coord_equal() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_fill_gradientn(colours = palette)\n  \n  return(art)\n}\n\n# call make_noise_art with idiosyncratic parameters\nart &lt;- make_noise_art(\n  generator = gen_worley,\n  seed = 1000, \n  palette = sample_canva(100),\n  value = \"distance\",\n  pixels = 1000\n)\n\n# save the plot to file with a generic file name\nggsave(\n  filename = here(\"output\", \"noise-art.png\"), \n  plot = art,\n  width = 1000,\n  height = 1000,\n  units = \"px\",\n  dpi = 300\n)"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html",
    "href": "posts/2023-01-01_playing-with-docker/index.html",
    "title": "Playing with docker and the github container registry",
    "section": "",
    "text": "Docker docker docker baby. This is a post about docker, and on the off chance that you’ve been living under a rock for the last several years, docker1 allows you to run your code within a “container” that isolates it from other processes running on your machine. Containers are a bit like virtual machines, but smaller, more portable, and don’t require you to have a complete copy of a second operating system running on your machine. They’re… actually, you know what? Why don’t I quote the relevant paragraphs from the docker website:\nThey even have pretty pictures on the website. I thought about reproducing their figures for this blog post but why bother? If you want to look at their pictures you can go look at the website and in any case I think we can all agree that making these cute whale graphics with ggplot2 was a much better use of my time, yes?\nAnyway. I’ve been meaning to teach myself docker for a few years now. It’s one of those “things” that has this weird aura of being difficult when it… doesn’t seem to be all that difficult? For a long time I’ve had this feeling of dread or insecurity about it, thinking that it must be “too technical” for me.5 I have no doubt that the internals to docker are complicated, and there are subtleties to using docker well that will take a while to grasp, but when I managed to set aside my fears and read the documentation it turned out that the basics were surprisingly easy."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#installing-docker",
    "href": "posts/2023-01-01_playing-with-docker/index.html#installing-docker",
    "title": "Playing with docker and the github container registry",
    "section": "Installing docker",
    "text": "Installing docker\nThe installation guides on the docker website are good, and have information for various operating systems. I’m doing this on my ubuntu laptop6 so I followed the ubuntu install guide. I also went a little further and followed the post-install instructions for linux so that I could run docker commands without requiring superuser privileges: that’s the reason you won’t see any sudo commands in this post. Obviously, that’s something that will be a bit different on different operating systems and I’m not trying to write a tutorial here, but if you are using this post as a resource you can check that everything is working on your own installation by running this command:\n\ndocker run hello-world\n\n\nUnable to find image 'hello-world:latest' locally\nlatest: Pulling from library/hello-world\n2db29710123e: Pull complete \nDigest: sha256:c77be1d3a47d0caf71a82dd893ee61ce01f32fc758031a6ec4cf1389248bb833\nStatus: Downloaded newer image for hello-world:latest\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n 1. The Docker client contacted the Docker daemon.\n 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n    (amd64)\n 3. The Docker daemon created a new container from that image which runs the\n    executable that produces the output you are currently reading.\n 4. The Docker daemon streamed that output to the Docker client, which sent it\n    to your terminal.\n\nTo try something more ambitious, you can run an Ubuntu container with:\n $ docker run -it ubuntu bash\n\nShare images, automate workflows, and more with a free Docker ID:\n https://hub.docker.com/\n\nFor more examples and ideas, visit:\n https://docs.docker.com/get-started/\n\nOkay that looks good. Docker7 seems to be running on my machine. As an aside, as long as you are online you don’t need to have a copy hello-world itself for this to work: docker will download it for you when you run the command."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#terminology",
    "href": "posts/2023-01-01_playing-with-docker/index.html#terminology",
    "title": "Playing with docker and the github container registry",
    "section": "Terminology",
    "text": "Terminology\nBefore diving in and using docker, it helps to disambiguate three terms:\n\nContainer. A container is an executable. It runs on your machine isolated from other processes, has a namespace on the kernel, etc. Setting the particulars aside, it is a computing environment.\nImage. An image is a read-only template that contains the instruction to build a container. It’s a “snapshot” of a computing environment, constructed from one or more “layers” of build steps. Images are binaries that are stored locally and hosted on various registries. More on that later!\nDockerfile. Finally, there’s the dockerfile.8 That’s a plain text file that you as the user write. It contains the instructions for how to construct an image. They supply, in a (very!) abstract sense, the source code for an image.\n\nSo it works like this. You use a dockerfile to build an image, the image contains the instructions to run a container, and the corresponding commands are quite sensibly called docker build and docker run. Or if you like diagrams with labelled arrows…\n\\[\n\\mbox{dockerfile} \\xrightarrow{\\mbox{build}} \\mbox{image} \\xrightarrow{\\mbox{run}} \\mbox{container}\n\\]\nAt any point you can get a summary of the images on your system by running docker image list. If you’re doing this with a fresh installation and you run the command after running the “hello world” example above,9 you’d get output that looks like this:\n\ndocker image list\n\nREPOSITORY    TAG       IMAGE ID       CREATED         SIZE\nhello-world   latest    feb5d9fea6a5   15 months ago   13.3kB\nYou can do the same thing for containers with docker container ls,10 which by default will show you currently-running containers. To see all containers, running or not, add the --all parameter:\n\ndocker container ls --all\n\n\nCONTAINER ID   IMAGE         COMMAND    CREATED         STATUS                     PORTS     NAMES\nefcf7186776f   hello-world   \"/hello\"   6 minutes ago   Exited (0) 6 minutes ago             bold_davinci\n\nNotice the difference in the “CREATED” time! The image for hello-world is something that someone else created 15 months ago and kindly placed online so I could pull it onto my machine without building it myself. The container is the executable that I created from that image a mere 6 minutes ago when I called docker run. They’re both currently on my laptop, but they are quite different things.\nAh, but I am rambling again, aren’t I? Sorry. Shall we have a go at this then?\n\n\n\n\n\n\nThis was my first attempt at plotting something that looks a bit like the docker whale. It’s nothing fancy: I created a data frame with coordinates corresponding to a circle and then distorted it in two different ways. One distortion produces the whale body, another makes the tail. They are rendered in ggplot2 with geom_polygon(). Later in the process I tweaked the tail a bit."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#motivating-problem",
    "href": "posts/2023-01-01_playing-with-docker/index.html#motivating-problem",
    "title": "Playing with docker and the github container registry",
    "section": "Motivating problem",
    "text": "Motivating problem\nIn my last post I mentioned that, btw I use arch now.11 Well. Sort of. A more accurate statement would be to say that I installed arch linux on a secondary laptop as something to play with and I’m still using ubuntu for my day to day coding. At the moment I’m still getting used to the quirks of arch and encountering odd behaviour when – for example – one of my scripts that ran perfectly well on my ubuntu machine caused RStudio to crash when I ran it on the arch box. The “it works on my machine” problem strikes again… sigh.\nIn an effort to isolate the problem I started reran the unit tests for the package that I thought might be responsible for the crash and they all passed on both machines, but since that package is my queue package and the unit test aren’t as comprehensive as I’d like I would not be at all surprised if there’s an exotic bug that makes it fail only on arch.\nAll this made me think a little about how I typically use CI.12 Like many R developers I’ll use github actions to run my unit tests on mac os, ubuntu, and windows. I’ll run the tests with multiple versions of R including R-devel. If I’m thinking about a CRAN submission I’ll expand the scope and run my tests using other services also.\nI’ve never tested on arch though.\nI’ve never tested on arch because I’ve never had an arch machine to test on before. Or… [docker enters from stage left]… I’ve never had an arch image that I can use to containerise my unit tests before…\nOoh… a side project! Why don’t I try creating some docker images with R running on arch linux? In other words, why don’t I do a really lazy, half-arsed version of the thing that the rocker project has already done to an extremely high standard with ubuntu and debian… except with arch?13\n\n\n\n\n\n\nAdding the boxes was conceptually easy: the expand_grid() function from tidyr creates the necessary data structure, and geom_tile() plots it. One thing I really like about this iteration is that the spacing of the boxes creates a Hermann grid illusion. It’s not as cool as the scintillating grid version, but I used to teach it in introductory cognitive science classes and I have a soft spot for it."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#minimal-example",
    "href": "posts/2023-01-01_playing-with-docker/index.html#minimal-example",
    "title": "Playing with docker and the github container registry",
    "section": "Minimal example",
    "text": "Minimal example\nSometimes the easiest way to tell a story is to begin at the ending, and – spoiler! – I did in fact succeed in my attempt,14 and I am now the proud15 maintainer of two hastily-constructed images hosted on the github container repository. Now that I have these things, it should be really easy for us to put together a simple project that will run R code using these images and – even though I’m going to be using my ubuntu laptop – have it be executed by a container that is running arch.\nOh. The. Thrill.\nBe. Still. My. Beating. Heart.\nOkay, so here it is. Accompanying this post is a project called system-check that consists of a three-line dockerfile and a two-line R script. Let’s ignore the dockerfile for a moment and focus on the R code. Here’s the script:\n\n\n\n./system-check/script.R\n\ncat(c(\"Running on:\", osVersion), sep = \"\\n  \")\ncat(c(\"With locale:\", strsplit(Sys.getlocale(), \";\")[[1]]), sep = \"\\n  \")\n\n\nIf we ignore the parts the code dedicated to making the output pretty, we can see that all it’s doing is printing the osVersion and calling Sys.getlocale(). Here’s what happens when I run the script on my ubuntu laptop, without using docker in any way:\n\nRscript ./system-check/script.R\n\nRunning on:\n  Ubuntu 22.04.1 LTS\nWith locale:\n  LC_CTYPE=en_AU.UTF-8\n  LC_NUMERIC=C\n  LC_TIME=en_AU.UTF-8\n  LC_COLLATE=en_AU.UTF-8\n  LC_MONETARY=en_AU.UTF-8\n  LC_MESSAGES=en_AU.UTF-8\n  LC_PAPER=en_AU.UTF-8\n  LC_NAME=C\n  LC_ADDRESS=C\n  LC_TELEPHONE=C\n  LC_MEASUREMENT=en_AU.UTF-8\n  LC_IDENTIFICATION=C\nThe first part of the output tells me my operating system (ubuntu), and the second part specifies the locale. I’m in Australia so for most things my locale is en_AU.UTF-8. That makes sense, but of course this output is specific to my machine: an arch user running R in the United States should expect to see something very different.\nThat’s where docker comes in.\nThe docker images that I built and am hosting on github simulate exactly that. The computing environments specified by the arch-r-base and arch-r-test images use arch linux as the operating system and have the system locale set to en_US.UTF-8. So if I were to execute this script from within a container running the arch-r-base16 image, I should expect to see different results even though my laptop is running ubuntu and my system locale is en_AU.UTF-8.\nHere’s a dockerfile specifying an image that does exactly that:\n\n\n\n./system-check/Dockerfile\n\nFROM ghcr.io/djnavarro/arch-r-base:release\nCOPY script.R /home/script.R\nCMD Rscript /home/script.R\n\n\nIt’s a sequence of three docker instructions.\n\nLike all dockerfiles, it begins with a FROM17 18 instruction that specifies the name of a preexisting docker image to use as a starting point. I’ve been very explicit here and referenced the image using a fully qualified name that consists of a container repository (ghcr.io), a username (djnavarro), the image name arch-r-base, and an optional tag (release). You don’t always need to be that precise, especially if you’re using an image that you know exists locally.\nThe second step is a COPY instruction that copies the R script to a specific file path within the image. This takes place at build time. This step is necessary because when the container starts up it will be isolated from other processes on the system. It doesn’t have access to the host file system. If you want the container to have access to a file you need to copy it at build time.19\nThe third step is a CMD instruction. Every dockerfile must have a CMD instruction (and much like highlanders there can be only one) specifying a default for what the container should do when it is launched.20\n\nLater on, when you’re starting to feel comfortable with the basic idea of writing dockerfiles, its worth reading the official guide on dockerfile best practices. Lots of little things started to make sense to me when I did that. For now, let’s just acknowledged that yes Virginia we have a dockerfile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI created the random container stacks using dplyr. The boxes are grouped by column (using their x-coordinate), a random height is generated for that column, and rows in the data frame corresponding to boxes above that height are filtered out of the data set before it is passed to ggplot2.\n\n\n\n\nBuilding the image\nOur next step is to build it to an image. The way we do that from the terminal is with the docker build command. For the purposes of this post – which I am writing in quarto and thus has a code execution engine blah blah blah – I am going to assume21 that the working directory is set to the folder containing the post, and that it contains a subfolder called system-check in which the dockerfile and the R script are stored. In other words, system-check is the directory holding the docker project.\nThe simplest way to build an image from this project is like this:\n\ndocker build system-check\n\nThis command tells docker to look for a dockerfile in the system-check folder, and make an image using whatever it finds there. That’s a perfectly fine way to do it, but my personal preference is to give the resulting image a name, using the --tag flag. So the command, which I’ve broken over a few lines to highlight its structure, now looks like this:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\nThe reason I’ve done this is that later on when I call the docker run command I can refer to the image by name, which does make life simpler. Under normal circumstances I’d probably have called the image system-check rather than my-system-check (why create new names when I don’t need to?) but for the purposes of this post I think it’s helpful to be clear that when I refer to the image name I’m referring to the thing I created using --tag, not the name of the folder that holds the dockerfile!\nOkay, enough talk. Let’s run it this time:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\n\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM ghcr.io/djnavarro/arch-r-base:release\nrelease: Pulling from djnavarro/arch-r-base\n597018910566: Pull complete \n8150bcc6bc64: Pull complete \ne49e8a34689c: Pull complete \nc14eff78251d: Pull complete \n42b358854199: Pull complete \nbabcc0d99cfd: Pull complete \nDigest: sha256:f9ff0f7b431ed1b975823c871949ccbc15c3e3d7dce23775f793f9f64bb2779e\nStatus: Downloaded newer image for ghcr.io/djnavarro/arch-r-base:release\n ---&gt; 0a9929e54a6b\nStep 2/3 : COPY script.R /home/script.R\n ---&gt; b9913096b118\nStep 3/3 : CMD Rscript /home/script.R\n ---&gt; Running in 1314ee0ff2fb\nRemoving intermediate container 1314ee0ff2fb\n ---&gt; 489003ffb5d0\nSuccessfully built 489003ffb5d0\nSuccessfully tagged my-system-check:latest\n\nThe output here shows you that the build process unfolds as a sequence of three steps: one for each of our docker instructions. It also gives you the impression (correctly!) that the first step is considerably more complex than the other two. That makes sense: the arch-r-base image is itself constructed from a sequence of steps, and those steps have produced an image that is built from several “layers”. Each of those hexadecimal hashes refers to one of the layers.22\nWhen you run this on your own system you’ll see little progress bars as the different layers of the image are downloaded. For example, that line that says 597018910566: Pull complete? That’s referring to the very first layer in the arch-r-base image (which is arch linux itself) and that layer is about 280MB or something like that, so you get a little progress bar to let you know how its going. That’s super helpful if you ever find yourself using the arch-r-test image, because one of the layers in that image includes a texlive installation (ugh) so that layer is (I’m so sorry) about 2GB in size.\nDownloading large images is a huge pain, and generally I would try to avoid creating an image with a layer that large. Thankfully, docker is smart enough to check the local cache before trying to download anything.23 We can see this in action if we repeat the exact same command:\n\ndocker build \\\n  --tag my-system-check \\\n  system-check\n\nSending build context to Docker daemon  3.072kB\nStep 1/3 : FROM ghcr.io/djnavarro/arch-r-base:release\n ---&gt; 0a9929e54a6b\nStep 2/3 : COPY script.R /home/script.R\n ---&gt; Using cache\n ---&gt; b9913096b118\nStep 3/3 : CMD Rscript /home/script.R\n ---&gt; Using cache\n ---&gt; 489003ffb5d0\nSuccessfully built 489003ffb5d0\nSuccessfully tagged my-system-check:latest\nThis finishes instantaneously because docker24 25 notices that I already have a copy of this image so it uses the cache for everything.\nWe can confirm that this has worked by running docker image list:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED          SIZE\nmy-system-check                 latest    489003ffb5d0   26 minutes ago   955MB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   13 hours ago     955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago    13.3kB\n\nNow, you might be wondering about those image sizes. Did I really just create two 955MB images? That seems a bit much. It’s certainly true that the image is 955MB in size: after all, the image does have to describe an entire operating system running R, so it’s not surprising that it isn’t tiny. But it looks as if I just wasted an entire GB of space by making two of them. Thankfully, docker is not that silly. The my-system-check image is almost identical to arch-r-base. In fact, it’s just one very small layer added on top of the layers that comprise the arch-r-base image. If you dig into the documentation on storage you discover that docker quite sensibly allows images to share layers, so even though arch-r-base and my-system-check are individually 955MB in size, they are also collectively 955MB in size thanks to layer sharing.\nThe sheer excitement of working with computers is just too much for me to bear sometimes.\n\n\nRun in a container\nOkay, we are ready to go baby! The image is set up, and all we have to do is run it in a container using docker run. The docker run command is quite powerful, and has a lot of arguments you can use to control how the image executes.26 I’m not going to use any of that flexibility here. This is just a vanilla command asking docker to run the my-system-check image:\n\ndocker run my-system-check\n\nRunning on:\n  Arch Linux\nWith locale:\n  LC_CTYPE=en_US.UTF-8\n  LC_NUMERIC=C\n  LC_TIME=en_US.UTF-8\n  LC_COLLATE=en_US.UTF-8\n  LC_MONETARY=en_US.UTF-8\n  LC_MESSAGES=en_US.UTF-8\n  LC_PAPER=en_US.UTF-8\n  LC_NAME=C\n  LC_ADDRESS=C\n  LC_TELEPHONE=C\n  LC_MEASUREMENT=en_US.UTF-8\n  LC_IDENTIFICATION=C\nIt’s an awfully elaborate way to say “btw I use arch”, but yes… the image does what we hoped it would. It’s executed the R script on arch linux with a en_US.UTF-8 locale. I have successfully faked it27 as an arch user."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#fancier-example",
    "href": "posts/2023-01-01_playing-with-docker/index.html#fancier-example",
    "title": "Playing with docker and the github container registry",
    "section": "Fancier example",
    "text": "Fancier example\nFor the next example I’ll add a little bit of extra complexity. The real reason I wanted the arch-r images in the first place was to make it easier to run unit tests for an R package on a system running arch linux. If I were going to do this properly I’d set it up in a way that could be incorporated into a CI workflow with github actions, but I’m not going to be that fancy for this blog post. Instead, I’ll set it up so that I can generate containers running arch linux that can clone a package repository from github into the container, and then run the unit tests. I’ll even give it a bit of flexibility so that the user can decide at build time28 which github repository the container points to.\nAs before the project – which I’ve called test-on-arch – consists of two files. There’s an R script that executes at run time, and the dockerfile executed at build time. Here they are:\n\n\n\n./test-on-arch/Dockerfile\n\nFROM ghcr.io/djnavarro/arch-r-test:release\n\n# copy the testing script\nCOPY clone-and-check.R /home/clone-and-check.R\n\n# pass args through environment variables\nARG user\nARG repo\nARG cran=https://cloud.r-project.org\nENV user=$user\nENV repo=$repo\nENV cran=$cran\n\n# run the testing script\nCMD Rscript /home/clone-and-check.R\n\n\n\n\n\n./test-on-arch/clone-and-check.R\n\n# get the system environment variables\nuser &lt;- Sys.getenv(\"user\")\nrepo &lt;- Sys.getenv(\"repo\")\ncran &lt;- Sys.getenv(\"cran\")\n\n# define github url and a path for the local package install\nurl &lt;- paste(\"https://github.com\", user, repo, sep = \"/\")\ndir &lt;- paste(\"/home/project\", repo, sep=\"/\")\n\n# clone repo, install dependencies, and run checks\ngert::git_clone(url, dir, verbose = TRUE)\nremotes::install_deps(dir, dependencies = TRUE, repos = cran)\nrcmdcheck::rcmdcheck(dir)\n\n\nLike last time, the place to start is with the R script. It expects to find user, repo, and cran values as environment variables. Once it finds those, it clones the user/repo repository from github, installs any dependencies of the package from cran, and then uses rcmdcheck to check the downloaded package.\nNow let’s look at how the dockerfile sets up the computing environment to enable this script to be run on arch linux:\n\nJust like we saw in the last example, the dockerfile begins with a FROM instruction. This time around though I’m using the arch-r-test image rather than the arch-r-base image. Much like the base image, the test image runs arch linux and installs R in the environment. However, it also installs several other system dependencies and R packages that come in handy when running R CMD check, which makes it a bit more useful in this context.\nThe next step in the dockerfile is the COPY instruction that ensures that the image has a copy of the R script. There’s nothing new here so we can move on.\nThe next two steps use the ARG instruction. This is a new one for us: it’s a mechanism for allowing the user to specify arguments that will be passed to docker when building the image. That’s handy because it means I can customise the image that gets built. The obvious use here is that I can specify the user and the repo for the package that I want to check! (Later on we’ll see how this is done using the --build-arg argument to docker build)\nNext up is another ARG step, used to specify the url for the cran repository that the container should use to download any R packages. Notice, however, that this time I’ve specified a default value, so you don’t actually have to specify cran when you call docker build: if you don’t it will just use the default url\nThe ARG steps pass the user input to docker, but they don’t set any environment variables (remember, our R script is expecting to find environment variables). That’s the job of the ENV instructions that appear in the next three steps.29\nFinally, we have the CMD instruction, which specifies a default action for the container: run the script.\n\n\n\n\n\n\n\nAfter a little bit of tinkering I decided to make the tail a little fatter and use theme_minimal() with a border added as a way of subtly communicating the fact that ggplot2 is doing the work. The grid lines are almost invisible in a single whale plot like this but become more prominent in the facetted plots where there are more of them.\n\n\n\n\nBuilding the image\nSetting aside the fact that our test-on-arch project has a lot of flaws and limitations, it will serve the purposes we need it to. Let’s say I want to create an image that will check the queue package hosted at github.com/djnavarro/queue. To do that I’ll need to set user=djnavarro and repo=queue when I build the image, which I can do with the --build-arg argument:\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch\n\nNotice that I’ve chosen to call this image test-queue. A nice thing about being able to name the images independently from the dockerfile is that it’s easy to create multiple images using the same dockerfile (just with different arguments) and give them meaningful names. And sure, this particular example is very silly because literally everything I’m doing here at the build stage could be done just as efficiently at the run stage. But whatever.\nLet’s see what happens when I try to execute this build command. The arch-r-test image is considerably larger than arch-r-base. This one isn’t a frugal image! It takes a while, so I’m going to go have a smoke while I wait30 but the nice thing is that if you’ve done it once you don’t have to do it again. Anyway…\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch\n\n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\nrelease: Pulling from djnavarro/arch-r-test\n597018910566: Already exists \n8150bcc6bc64: Already exists \n198fc6066fb9: Pull complete \nb1600153860f: Pull complete \ned6330815f89: Pull complete \nfb2d11f79510: Pull complete \nff05f09f5a58: Pull complete \n9abaa14ad138: Pull complete \nDigest: sha256:f4605c32e18168589bd32248f5af97f8f1b57bd4de5fa6e1b54e53db13ab9514\nStatus: Downloaded newer image for ghcr.io/djnavarro/arch-r-test:release\n ---&gt; 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---&gt; d7c276834cf8\nStep 3/9 : ARG user\n ---&gt; Running in efeeb43f874d\nRemoving intermediate container efeeb43f874d\n ---&gt; d5d055328ea4\nStep 4/9 : ARG repo\n ---&gt; Running in 75f6d1ff1502\nRemoving intermediate container 75f6d1ff1502\n ---&gt; 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---&gt; Running in 3f620871b0d7\nRemoving intermediate container 3f620871b0d7\n ---&gt; 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---&gt; Running in c7a7811e374e\nRemoving intermediate container c7a7811e374e\n ---&gt; b8e01e708a08\nStep 7/9 : ENV repo=$repo\n ---&gt; Running in 2f01c723898c\nRemoving intermediate container 2f01c723898c\n ---&gt; 0939221c1a35\nStep 8/9 : ENV cran=$cran\n ---&gt; Running in 37399a0bbe70\nRemoving intermediate container 37399a0bbe70\n ---&gt; ccba9748fdd2\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---&gt; Running in 5d3eb7184e21\nRemoving intermediate container 5d3eb7184e21\n ---&gt; 76926d5616d7\nSuccessfully built 76926d5616d7\nSuccessfully tagged test-queue:latest\n\nNotice that during the first step when downloading arch-r-test, I didn’t have to download the whole thing. Two of the layers in arch-r-test are shared with the arch-r-base image, and docker is smart enough to notice that I already have those layers in my cache. That’s what the Already exists part of the output indicates. Admittedly it doesn’t save us much in this case because its the texlive installation that causes pain, but it’s a nice feature nevertheless.\nAs a little sanity check – because, dear reader, I have been sitting here waiting very patiently while a large image downloaded over a slow connection and would like to confirm that I don’t have to do that again – let’s repeat the exercise from earlier and try building it a second time just to reassure ourselves that the cache is doing its job:\n\ndocker build \\\n  --tag test-queue \\\n  --build-arg user=djnavarro \\\n  --build-arg repo=queue \\\n  test-on-arch \n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\n ---&gt; 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---&gt; Using cache\n ---&gt; d7c276834cf8\nStep 3/9 : ARG user\n ---&gt; Using cache\n ---&gt; d5d055328ea4\nStep 4/9 : ARG repo\n ---&gt; Using cache\n ---&gt; 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---&gt; Using cache\n ---&gt; 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---&gt; Using cache\n ---&gt; b8e01e708a08\nStep 7/9 : ENV repo=$repo\n ---&gt; Using cache\n ---&gt; 0939221c1a35\nStep 8/9 : ENV cran=$cran\n ---&gt; Using cache\n ---&gt; ccba9748fdd2\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---&gt; Using cache\n ---&gt; 76926d5616d7\nSuccessfully built 76926d5616d7\nSuccessfully tagged test-queue:latest\nNot going to lie, I breathed a little sigh of relief. Docker used the cached layers, and that all happened instantaneously. Okay cool. I’m going to stop doing these checks from now on, but one last time let’s take a peek at the list of images I have stored locally:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED              SIZE\ntest-queue                      latest    76926d5616d7   About a minute ago   4.99GB\nmy-system-check                 latest    b7426ffb1484   12 minutes ago       955MB\nghcr.io/djnavarro/arch-r-test   release   4f873f316861   17 hours ago         4.99GB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   17 hours ago         955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago        13.3kB\n\n\n\nRun in a container\nOkay where were we? Ah yes, we’ve built our image so now it’s time to run it. Does my little queue package build cleanly and pass its unit tests on arch? Let’s find out…\n\ndocker run test-queue\n\n\nTransferred 766 of 766 objects...done!\nChecked out 34 of 34 commits... done!\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘.../DESCRIPTION’ ... OK\n* preparing ‘queue’:\n* checking DESCRIPTION meta-information ... OK\n* installing the package to build vignettes\n* creating vignettes ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\n* building ‘queue_0.0.2.tar.gz’\n\n── R CMD check ─────────────────────────────────────────────────────────────────\n* using log directory ‘/tmp/Rtmp1Nld2I/file131069108/queue.Rcheck’\n* using R version 4.2.2 (2022-10-31)\n* using platform: x86_64-pc-linux-gnu (64-bit)\n* using session charset: UTF-8\n* checking for file ‘queue/DESCRIPTION’ ... OK\n* this is package ‘queue’ version ‘0.0.2’\n* package encoding: UTF-8\n* checking package namespace information ... OK\n* checking package dependencies ... OK\n* checking if this is a source package ... OK\n* checking if there is a namespace ... OK\n* checking for executable files ... OK\n* checking for hidden files and directories ... OK\n* checking for portable file names ... OK\n* checking for sufficient/correct file permissions ... OK\n* checking whether package ‘queue’ can be installed ... OK\n* checking installed package size ... OK\n* checking package directory ... OK\n* checking ‘build’ directory ... OK\n* checking DESCRIPTION meta-information ... OK\n* checking top-level files ... OK\n* checking for left-over files ... OK\n* checking index information ... OK\n* checking package subdirectories ... OK\n* checking R files for non-ASCII characters ... OK\n* checking R files for syntax errors ... OK\n* checking whether the package can be loaded ... OK\n* checking whether the package can be loaded with stated dependencies ... OK\n* checking whether the package can be unloaded cleanly ... OK\n* checking whether the namespace can be loaded with stated dependencies ... OK\n* checking whether the namespace can be unloaded cleanly ... OK\n* checking loading without being on the library search path ... OK\n* checking dependencies in R code ... NOTE\nNamespaces in Imports field not imported from:\n  ‘callr’ ‘cli’ ‘R6’ ‘tibble’\n  All declared Imports should be used.\n* checking S3 generic/method consistency ... OK\n* checking replacement functions ... OK\n* checking foreign function calls ... OK\n* checking R code for possible problems ... OK\n* checking Rd files ... OK\n* checking Rd metadata ... OK\n* checking Rd cross-references ... OK\n* checking for missing documentation entries ... OK\n* checking for code/documentation mismatches ... OK\n* checking Rd \\usage sections ... OK\n* checking Rd contents ... OK\n* checking for unstated dependencies in examples ... OK\n* checking installed files from ‘inst/doc’ ... OK\n* checking files in ‘vignettes’ ... OK\n* checking examples ... OK\n* checking for unstated dependencies in ‘tests’ ... OK\n* checking tests ...\n  Running ‘testthat.R’\n OK\n* checking for unstated dependencies in vignettes ... OK\n* checking package vignettes in ‘inst/doc’ ... OK\n* checking running R code from vignettes ...\n  ‘queue.Rmd’ using ‘UTF-8’... OK\n NONE\n* checking re-building of vignette outputs ... OK\n* checking PDF version of manual ... OK\n* DONE\n\nStatus: 1 NOTE\nSee\n  ‘/tmp/Rtmp1Nld2I/file131069108/queue.Rcheck/00check.log’\nfor details.\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n── R CMD check results ──────────────────────────────────────── queue 0.0.2 ────\nDuration: 38.5s\n\n❯ checking dependencies in R code ... NOTE\n  Namespaces in Imports field not imported from:\n    ‘callr’ ‘cli’ ‘R6’ ‘tibble’\n    All declared Imports should be used.\n\n0 errors ✔ | 0 warnings ✔ | 1 note ✖\n\nOkay yes, this is the expected result. That note would of course get me in trouble on CRAN, but it’s what I was expecting to see: I get the same note on ubuntu. I just haven’t gotten around to fixing it yet. The only part that is different to what I see on ubuntu is this:\n\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n\nYeah. This is interesting. I deliberately didn’t try to faff about with systemd in these images, so this is an expected warning. It’s not a problem with queue or with arch, just a consequence of how I built the images. That would have some consequences for testing a lot of packages, but I’m not trying to recreate the rocker project here so I’m not too fussed about it in this little exercise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe colour scheme is sampled using ggthemes::canva_palettes, picking one of the ones that provides a blue/green palette.\n\n\n\n\n\nTwo images, one dockerfile\nThe advantage to passing arguments is that you can build many images from the same dockerfile, and docker will reuse the cached layers intelligently. We’ve seen this already, but here’s another example. Let’s try using the test-on-arch dockerfile to build an image that checks the praise package. Up to this point I’ve never tried testing the praise package on arch before, but (of course????) this builds immediately and without downloading anything, because everything that actually matters about this build was already done when I built the test-queue image earlier:\n\ndocker build \\\n  --tag test-praise \\\n  --build-arg user=rladies \\\n  --build-arg repo=praise \\\n  test-on-arch \n\nSending build context to Docker daemon  3.072kB\nStep 1/9 : FROM ghcr.io/djnavarro/arch-r-test:release\n ---&gt; 4f873f316861\nStep 2/9 : COPY clone-and-check.R /home/clone-and-check.R\n ---&gt; Using cache\n ---&gt; d7c276834cf8\nStep 3/9 : ARG user\n ---&gt; Using cache\n ---&gt; d5d055328ea4\nStep 4/9 : ARG repo\n ---&gt; Using cache\n ---&gt; 7edce4d95863\nStep 5/9 : ARG cran=https://cloud.r-project.org\n ---&gt; Using cache\n ---&gt; 51a7ec6700ba\nStep 6/9 : ENV user=$user\n ---&gt; Running in 3a9b1843d5b4\nRemoving intermediate container 3a9b1843d5b4\n ---&gt; aa2578d71155\nStep 7/9 : ENV repo=$repo\n ---&gt; Running in 1d15632dd6ca\nRemoving intermediate container 1d15632dd6ca\n ---&gt; 057a61970d7c\nStep 8/9 : ENV cran=$cran\n ---&gt; Running in e5586a32b05a\nRemoving intermediate container e5586a32b05a\n ---&gt; 48852232e4b7\nStep 9/9 : CMD Rscript /home/clone-and-check.R\n ---&gt; Running in 0fb9a526210c\nRemoving intermediate container 0fb9a526210c\n ---&gt; a02feea26152\nSuccessfully built a02feea26152\nSuccessfully tagged test-praise:latest\nOnce again, we can take a look at the list of images:\n\ndocker image list\n\n\nREPOSITORY                      TAG       IMAGE ID       CREATED          SIZE\ntest-praise                     latest    a02feea26152   20 seconds ago   4.99GB\ntest-queue                      latest    76926d5616d7   4 minutes ago    4.99GB\nmy-system-check                 latest    b7426ffb1484   14 minutes ago   955MB\nghcr.io/djnavarro/arch-r-test   release   4f873f316861   17 hours ago     4.99GB\nghcr.io/djnavarro/arch-r-base   release   0a9929e54a6b   17 hours ago     955MB\nhello-world                     latest    feb5d9fea6a5   15 months ago    13.3kB\n\nAgain note the value of layer sharing. If these were all independent images we’d be looking at 17GB on disk. In fact, because arch-r-test reuses the layers from arch-r-base and all the other images are trivial additions to one of these two images, the total size of all these images is in fact “only” 5GB… i.e., the size of the arch-r-test image. And again, the only reason that one is so big is that I was really fussy about tex installations and bundled an entire texlive distribution with extra fonts and everything because I have no desire deal with tests whining about missing tex stuff.\nAnyway, let’s get back on track and run the test-praise image in a container:\n\ndocker run test-praise\n\n\nTransferred 431 of 431 objects...done!\nChecked out 26 of 26 commits... done!\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘.../DESCRIPTION’ ... OK\n* preparing ‘praise’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘praise_1.0.0.tar.gz’\n\n── R CMD check ─────────────────────────────────────────────────────────────────\n* using log directory ‘/tmp/Rtmpi7Ngun/file12ad64a83/praise.Rcheck’\n* using R version 4.2.2 (2022-10-31)\n* using platform: x86_64-pc-linux-gnu (64-bit)\n* using session charset: UTF-8\n* checking for file ‘praise/DESCRIPTION’ ... OK\n* this is package ‘praise’ version ‘1.0.0’\n* checking package namespace information ... OK\n* checking package dependencies ... OK\n* checking if this is a source package ... OK\n* checking if there is a namespace ... OK\n* checking for executable files ... OK\n* checking for hidden files and directories ... OK\n* checking for portable file names ... OK\n* checking for sufficient/correct file permissions ... OK\n* checking whether package ‘praise’ can be installed ... OK\n* checking installed package size ... OK\n* checking package directory ... OK\n* checking DESCRIPTION meta-information ... OK\n* checking top-level files ... OK\n* checking for left-over files ... OK\n* checking index information ... OK\n* checking package subdirectories ... OK\n* checking R files for non-ASCII characters ... OK\n* checking R files for syntax errors ... OK\n* checking whether the package can be loaded ... OK\n* checking whether the package can be loaded with stated dependencies ... OK\n* checking whether the package can be unloaded cleanly ... OK\n* checking whether the namespace can be loaded with stated dependencies ... OK\n* checking whether the namespace can be unloaded cleanly ... OK\n* checking dependencies in R code ... OK\n* checking S3 generic/method consistency ... OK\n* checking replacement functions ... OK\n* checking foreign function calls ... OK\n* checking R code for possible problems ... OK\n* checking Rd files ... OK\n* checking Rd metadata ... OK\n* checking Rd cross-references ... OK\n* checking for missing documentation entries ... OK\n* checking for code/documentation mismatches ... OK\n* checking Rd \\usage sections ... OK\n* checking Rd contents ... OK\n* checking for unstated dependencies in examples ... OK\n* checking examples ... OK\n* checking for unstated dependencies in ‘tests’ ... OK\n* checking tests ...\n  Running ‘testthat.R’\n OK\n* checking PDF version of manual ... OK\n* DONE\n\nStatus: OK\n\nSystem has not been booted with systemd as init system (PID 1). Can't operate.\nFailed to connect to bus: Host is down\nWarning: Your system is mis-configured: ‘/var/db/timezone/localtime’ is not a symlink\nWarning: ‘/var/db/timezone/localtime’ is not identical to any known timezone file\nWarning message:\nIn system(\"timedatectl\", intern = TRUE) :\n  running command 'timedatectl' had status 1\n── R CMD check results ─────────────────────────────────────── praise 1.0.0 ────\nDuration: 25.1s\n\n0 errors ✔ | 0 warnings ✔ | 0 notes ✔\n\nOnce again we see the warning about systemd, and once again I am ignoring it. The thing that matters here, as far as I’m concerned, is that the unit tests for the praise package pass on arch.\n\n\nA small caution\nBefore we move onto the third project I want to talk about one more example using this one, as a way of cautioning anyone who might feel inclined to use it without fixing its many deficiencies. Let’s try using test-on-arch to run the unit tests for ggplot2, shall we? Unlike praise and queue, ggplot2 is a large and complicated package with substantial dependencies and a lot of unit tests. That’s going to be a problem given that test-on-arch clones the entire repository from scratch every time it’s called. Building the image is easy, because the build stage for test-on-arch doesn’t do anything except copy the script and pass a few arguments…\n\ndocker build \\\n  --tag test-ggplot2 \\\n  --build-arg user=tidyverse \\\n  --build-arg repo=ggplot2 \\\n  test-on-arch \n\nBut when we call docker run things become unpleasant for us even before we’ve had a chance to start running the unit tests, because the git clone operation is very time consuming…\n\ndocker run test-ggplot2 \n\nTransferred 15676 of 74694 objects...\n\n…uh, right. Look this is going to take a while, so maybe we should move on?\nThe main reason I wanted to point to this is to highlight that the clone step occurs at run time, and the entire clone operation is repeated every time we call it. That’s not a smart way to do this. If you really wanted to design a docker workflow for testing packages on arch, you’d want to make some smarter design choices than this! The test-on-arch project I’ve used in this blog post is a toy, nothing more.31"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#hosting-images",
    "href": "posts/2023-01-01_playing-with-docker/index.html#hosting-images",
    "title": "Playing with docker and the github container registry",
    "section": "Hosting images",
    "text": "Hosting images\nFor the third example, let’s look at the arch-r-base image itself. In addition to the dockerfile there are two small text files used to specify locale information. The two locale files aren’t very interesting and could easily have been included as strings in the dockerfile, but I found it neater to keep them separate. The locale-gen file specifies locales that the image understands, and locale.conf specifies configuration details. (Both are configuration files on linux). In any case, here’s the whole thing:\n\n\n\n\nbase/Dockerfile\n\nFROM archlinux:base-devel\n\nLABEL org.opencontainers.image.source \"https://github.com/djnavarro/arch-r\" \nLABEL org.opencontainers.image.authors \"Danielle Navarro &lt;djnavarro@protonmail.com&gt;\" \nLABEL org.opencontainers.image.description DESCRIPTION\nLABEL org.opencontainers.image.licenses \"GPL-3.0\"\n\n# set the locale\nCOPY base/locale.gen /etc/locale.gen\nCOPY base/locale.conf /etc/locale.conf\nRUN locale-gen\nENV LANG=en_US.UTF-8\nENV LC_ALL=en_US.UTF-8\n\n# install R and set default command\nRUN pacman -Syu --noconfirm r\nCMD R --no-save\n\n\n\n\n\n\nbase/locale.gen\n\nC.UTF8 UTF-8\nen_US.UTF-8 UTF-8\n\n\n\n\n\nbase/locale.conf\n\nLANG=en_US.UTF-8\nLC_ALL=en_US.UTF-8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe names are sampled using the babynames package. I deliberately chose to ignore name frequency, sampling uniformly at random from the names in the data set. You end up with more interesting choices that way.\n\n\n\nTruly exciting stuff, I know. Thankfully only some of it is new. The FROM instruction uses the archlinux:base-devel image hosted here. The RUN instruction is used to execute commands at build time, so you can see in this example I’ve used it to create system locale information (by calling locale-gen) and to install R (using the pacman package manager used on arch linux).\nThe other new thing here is the LABEL instruction used to supply metadata about the image. This is particularly important if you’re planning to make your image public, as I have done with the arch-r-base and arch-r-test images. The labelling that I’ve supplied here follows the specifications provided by the open container initiative, or at least attempts to. I’m still new to this, but as far as I can tell this is correct? Anyway, you can see that it specifies the location of the source code, the author of the image, and the licence. That’s the main thing.\nYou are probably wondering, though, why the description just reads “DESCRIPTION” and doesn’t have an actual… you know… description. The reason for that is that I’m hosting these through the github container registry that links my github repository to the images automatically. Specifically, I’m using a github action that automates the build process and populates the description on the arch-r-base package page using the description field from the arch-r github repository. Leaving the value for that field as “DESCRIPTION” ensures that all works smoothly.\nSpeaking of which, I’m not in any way an expert on github actions – this is my first attempt at creating a workflow and I cribbed heavily from other workflows I found online – but for whatever it’s worth I figure I should share. Here’s the workflow I’m using:\n\n\n\n\n.github/workflows/build-image.yaml\n\nname: publish arch-r images\n\non:\n  push:\n    branches: ['release']\n    \nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - dockerfile: ./base/Dockerfile\n            image: ghcr.io/djnavarro/arch-r-base\n          - dockerfile: ./test/Dockerfile\n            image: ghcr.io/djnavarro/arch-r-test\n            \n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: checkout repository\n        uses: actions/checkout@v2\n\n      - name: login to the container registry\n        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: extract metadata (tags, labels) for docker\n        id: meta\n        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n        with:\n          images: ${{ matrix.image }}\n\n      - name: build and push docker image\n        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n        with:\n          context: .\n          file: ${{ matrix.dockerfile }}\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n\nFor this workflow to run, I needed to edit the permissions associated with my github PAT to include some additional scopes. If, like me, you’ve created your PAT using the default scopes provided by usethis::create_github_token(), you’ll need a few more to run workflows that build and modify docker images if you want to work with github packages. This workflow doesn’t use all these, but the permissions typically required for to work with container images on github are these:\n\nread:packages scope to download container images and read metadata.\nwrite:packages scope to download and upload container images and read and write metadata.\ndelete:packages scope to delete container images.\n\nIn any case, this github actions workflow triggers an automatic deployment to the github container registry whenever there is a new push to the release branch of the repository. This is what creates the ghcr.io/djnavarro/arch-r-base:release and ghcr.io/djnavarro/arch-r-test:release images. I’m entirely certain that this could be done in a more sophisticated way, but it does work, and that was my main goal for this post."
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#resources",
    "href": "posts/2023-01-01_playing-with-docker/index.html#resources",
    "title": "Playing with docker and the github container registry",
    "section": "Resources",
    "text": "Resources\nAnd that brings us to the end of the post. There’s not much else to say really. I played around with docker. Learned a few things. Had some fun. Drew some whales. Normal stuff, really. But if you’re at all keen on following up on any of the things in this post, here are some resources I relied on when writing this:\n\nThe docker reference documentation: docs.docker.com/reference\nDockerfile best practices docs.docker.com/develop/develop-images/dockerfile_best-practices\nInstructions on giving docker sudo privileges for linux users: docs.docker.com/engine/install/linux-postinstall\nThe rocker project by Carl Boettiger, Dirk Eddelbuettel, Noam Ross, and Shima Tatsuya: rocker-project.org\nSource code for the rocker repositories: github.com/rocker-org/rocker\nBlog post on docker by Colin Fay: colinfay.me/docker-r-reproducibility\nSlides on docker by Noam Ross: github.com/noamross/nyhackr-docker-talk\nDocker for beginners by Prakhar Srivastav: docker-curriculum.com\nReferencing docker images by Nigel Brown windsock.io/referencing-docker-images\nWorking with the github container registry: docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry\nInformation about open containers labels: github.com/opencontainers/image-spec/blob/main/annotations.md"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#postscript-making-dockerplots-in-ggplot2",
    "href": "posts/2023-01-01_playing-with-docker/index.html#postscript-making-dockerplots-in-ggplot2",
    "title": "Playing with docker and the github container registry",
    "section": "Postscript: Making “dockerplots” in ggplot2",
    "text": "Postscript: Making “dockerplots” in ggplot2\nI had a lot of fun making the whales. They’re cute, and they make me happy. The function that generates these is called sample_whales(), and you can find the source code by expanding the folded code block below. Enjoy!\n\n\nSource code for sample_whales()\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(dplyr)\n\nsample_whales &lt;- function(seed = NULL, nrow = 4, ncol = 6) {\n\n  if(is.null(seed)) seed &lt;- sample(1000, 1)\n  set.seed(seed)\n\n  nwhales &lt;- nrow * ncol\n\n  # define a circle\n  circle &lt;- tibble(\n    th = seq(0, 2*pi, length.out = 1000),\n    x = cos(th),\n    y = sin(th)\n  )\n\n  # distort a circle to create the whale body\n  whale_body &lt;- circle |&gt;\n    mutate(\n      y = if_else(y &gt; 0, 0, y),\n      y = if_else(x &lt; 0, -abs(y) ^ .6, -abs(y) ^ 1.7)\n    )\n\n  # distort a circle to create the whale tail\n  whale_tail &lt;- circle |&gt;\n    mutate(\n      weight = (abs(th - pi)/pi) ^ 1.3,\n      angle = pi * 1.2,\n      x = x * weight + .35 * (1 - weight),\n      x_scaled = x * .6,\n      y_scaled = y * .4,\n      x = x_scaled * cos(angle) - y_scaled * sin(angle),\n      y = x_scaled * sin(angle) + y_scaled * cos(angle),\n      x = x + 1.35,\n      y = y + 0.25\n    )\n\n  # bind the body to the tail to make a whale\n  whale &lt;- bind_rows(whale_body, whale_tail)\n\n  # fully stacked set of boxes\n  box_stack &lt;- expand_grid(\n    x = seq(-.7, .5, .3),\n    y = seq(.25, 1.5, .3)\n  )\n\n  # sample names using babynames package\n  names &lt;- unique(sample(\n    x = babynames::babynames$name,\n    size = ceiling(nwhales * 1.2)\n  ))\n\n  # sample colours using a blue palette from ggthemes\n  shades &lt;- sample(\n    x = ggthemes::canva_palettes$`Cool blues`,\n    size = nrow * ncol,\n    replace = TRUE\n  )\n\n  boxes &lt;- list()\n  whales &lt;- list()\n  for(i in 1:(nrow * ncol)) {\n\n    # assign the whales a name and a look\n    whales[[i]] &lt;- whale |&gt;\n      mutate(\n        name = names[[i]],\n        look = shades[[i]]\n      )\n\n    # assign the whales a name and colour,\n    # and randomly remove boxes off the stack\n    boxes[[i]] &lt;- box_stack |&gt;\n      mutate(\n        name = names[[i]],\n        look = shades[[i]]\n      ) |&gt;\n      group_by(x) |&gt;\n      mutate(max_height = runif(1, min = .05, max = 1.8)) |&gt;\n      filter(y &lt; max_height)\n  }\n\n  # collapse lists to data frames\n  boxes &lt;- bind_rows(boxes)\n  whales &lt;- bind_rows(whales)\n\n  # last minute tinkering... :-)\n  boxes &lt;- boxes |&gt; mutate(y = y - .3, x = x + .01)\n  whales &lt;- whales |&gt; mutate(y = y - .31)\n\n  # draw the plot\n  ggplot(mapping = aes(x, y, fill = look, colour = look)) +\n    geom_polygon(data = whales, linewidth = 2) +\n    geom_tile(\n      data = boxes,\n      width = .18,\n      height = .18,\n      linewidth = 2,\n      linejoin = \"bevel\"\n    ) +\n    facet_wrap(vars(name), nrow = nrow, ncol = ncol) +\n    coord_equal(xlim = c(-1.5, 1.5), ylim = c(-1.5, 1.5)) +\n    scale_x_continuous(labels = NULL, name = NULL) +\n    scale_y_continuous(labels = NULL, name = NULL) +\n    scale_fill_identity() +\n    scale_color_identity() +\n    theme_minimal(base_size = 14) +\n    theme(\n      axis.ticks = element_blank(),\n      panel.border = element_rect(fill = NA, colour = \"grey90\")\n    )\n}"
  },
  {
    "objectID": "posts/2023-01-01_playing-with-docker/index.html#footnotes",
    "href": "posts/2023-01-01_playing-with-docker/index.html#footnotes",
    "title": "Playing with docker and the github container registry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLook, I know it’s technically supposed to be “Docker” not “docker” and it’s supposed to be “GitHub” not “github”. But my body was “supposed” to use testosterone as its primary sex hormone too, and we’ve all seen how little regard I had for that. Sometimes conventions are worth breaking out of sheer bloody-mindedness.↩︎\nSTEM people need to find new words for things. What is a “kernel”? Is it the bits of an operating system that run essential processes? Is it a specialised function that applies only to input arguments of specific types (i.e., what R folks would call a “method” in the functional object oriented programming sense, as opposed to the encapsulated object-oriented programming paradigm that dominates in other languages)? Or is it the thing the governs the transformation from data space to feature space in a support vector machine or other inferential systems built on reproducing kernel Hilbert spaces? For fuck’s sake people LEARN A NEW WORD.↩︎\nI’d like to propose using “egg” in lieu of “kernel” for any new tech nomenclature. Not only does it show you have some wit and know your audience.↩︎\nYour audience consists of queers. Nobody else reads this far into a nested footnote series.↩︎\nSometimes I think that the “not technical enough” concept is just straight up misogyny, both internalised and… external. I mean, I taught myself Bayesian nonparametrics and algorithmic information theory and even wrote respected academic papers in both those fields in addition to my own discipline of mathematical psychology. I was an editor at Science (yes, the journal). I wrote a quite successful statistics textbook. I’m an author on the ggplot2 book. I was a successful tenured academic in a mathematical science with no formal training in mathematics. I’ve taught myself several programming languages. Last year I wrote quite a lot of Apache Arrow content that everyone seems to like. So, um, yeah. Perhaps I should stop paying attention to the opinions of boys who condescend to me and tell me I’m not technical enough because… I’m stronger in R than in Python or C++? Tiresome.↩︎\nYes I know I use Arch now, hush. you’ll see why I’m doing this from ubuntu in a moment…↩︎\nI suppose, for the sake of precision, I should draw attention to the part of the output that refers to the docker client and the docker daemon. Docker takes a client-server approach. When I’m typing these commands I’m interacting with the docker client, which passes my requests over to the docker daemon. The daemon is the process that does most of the work. It pulls images from registries (e.g., docker hub, github container registry, etc), it builds images, it creates containers, etc. In this case, the client and the daemon are both running on the same machine, but they don’t actually have to. The daemon could totally run on a remote system. However, the distinction between the client and the daemon isn’t important for this post so I’m going to ignore it and collectively refer to both of them working together as “docker”.↩︎\nI’m sure that’s supposed to be “Docker file”. Per my earlier footnote, I don’t care.↩︎\nAs a little aside. In order to create this output with the “appearance” of starting with a fresh docker installation I – quite nobly – cleared out all my cached containers and images so that I could start from a clean system. Should you ever want to do the same, it’s a two step process. Assuming you don’t have any containers running, your first step is to delete any containers on your system that aren’t running (i.e., all of them) with docker container prune. Then you can delete any “dangling” images that aren’t associated with a container (i.e., all of them) with docker image prune --all. You’re welcome.↩︎\nNo I don’t know why they use list for images and ls for containers. That seems unhelpful.↩︎\nI strongly feel there is qualitative dissertation to be written mapping the btw-i-use-arch guy onto the men-explain-things-to-me guy from the Rebecca Solnit essay. As far as I can tell they are essentially the same person, just inhabiting different semantic domains. One day I will write the story of the guy at a conference who breathlessly explained a paper to me and how my work would be improved considerably if I’d read it while I was quietly wondering how to explain to him that it was my paper… sigh. Men.↩︎\nAm I the only one who still thinks that CI should stand for “confidential informant” rather than “continuous integration”?↩︎\nQuite obviously, I do not actually recommend anyone use the images I’ve set up. I mean, surely my phrasing here makes 1000% clear that this is a cute project I threw together in a couple of days for my own amusement. If you are looking to do reproducible computing in R you should be using the images provided by rocker. If you use my images and something goes wrong then to be perfectly frank you only have yourself to blame.↩︎\nSet your sights low enough and it is very easy to achieve your goals.↩︎\nNo. Just no.↩︎\nIt’s an open question how long I’m going to last in this post before making an Archer joke.↩︎\nTechnically it’s possible for an ARG instruction to precede a FROM instruction but I’m yet to actually see that in the wild.↩︎\nMuch like SQL clauses, docker instructions are written in uppercase by convention. They don’t actually have to be uppercase, but again, I’ve never seen a dockerfile written any other way. Along the same lines, your dockerfile doesn’t actually have to be called “Dockerfile”, but it’s the default and everyone uses it.↩︎\nAlternatively, you can use the VOLUME instruction to create a mount point and use that as a way to share a folder between the host and the container at run time, but that’s more fiddly and there’s really no need for that in this simple example. But if you want an easy-to-follow example using the VOLUME instruction in an R project, Colin Fay uses it in his docker for R users post.↩︎\nThe user can override the default by when calling docker run but I’m not going to cover that in this post↩︎\nCorrectly.↩︎\nTo a first approximation you can imagine that every docker instruction produces a layer, and it is my understanding that this is how it used to be. But for efficiency reasons more recent versions of docker only produce persistent layers from RUN, COPY, and ADD instructions. Other instructions produce temporary intermediate images, but do not create persistent layers in the final image.↩︎\nOr, as Sterling would phrase it, “I swear I had something for this.”↩︎\nNow that I’ve started making Archer jokes, it’s very hard not to turn “docker” into a euphemism. Hm. I should call him.↩︎\nLook all I’m saying is that “Queering the dock: images as tops, containers as bottoms” would make a terrible thesis and I would read the hell out of it.↩︎\nIn truth I didn’t actually need to construct the my-system-check image at all: I could have just run arch-r-base in a container with a few arguments tweaked. But that would defeat the point of the exposition, obviously.↩︎\nPhrasing.↩︎\nOkay yeah I could do this at run time too, but I want an excuse to talk about the ARG instruction.↩︎\nOkay yes, clever person, I could have chosen to pass environment variables at run time using the --env argument to docker run. I didn’t need to do this at build time using ARG. But that would defeat the point of the exposition wouldn’t it? I wanted to use ARG and ENV in the main text, and quietly mention the --env argument to docker run in an aside. And I have now accomplished exactly that, haven’t I?↩︎\nI am, after all, “on smoko” (which in my case means I am unemployed and bored out of my mind) but incidentally if you want to see the most fabulous cover ever (Wet Leg covering The Chats), here it is.↩︎\nI know the mystery will be too much for some people so I’d better resolve it: no, the ggplot2 tests didn’t pass on the arch image. Some of the dependencies didn’t install properly, and then eventually it threw an error trying to build the vignettes. If I had the energy I’d dig into it and figure out why… but I don’t.↩︎"
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html",
    "href": "posts/2024-06-21_table1/index.html",
    "title": "Making tables in R with table1",
    "section": "",
    "text": "I’ll never be good enough  You make me wanna die  And everything you love will burn up in the light  And every time I look inside your eyes  You make me wanna die      - The Pretty Reckless\nIt’s no secret that my health hasn’t been so great these last few months. Nothing life-threatening, I hasten to add, but severe enough that I’ve spent a depressing amount of 2024 in bed, and not in the fun way. I’m fortunate enough to have a remote job, and the workload this year hasn’t been as demanding as it was last year. I’ve been able to manage, yes,1 but it has been rough. I’ve necessarily been focusing what little energy I’ve had on my kids and on my day to day work. I’ve had no bandwidth at all to write, or make art, or learn new things. A sorry state of affairs, and one that sucks much of the joy out of life.\nHappily, things have started to turn around in recent weeks. I’ve had a little more energy, I’ve been able to work from my desk rather than my bed, and while the artistic impulse hasn’t come back yet I’ve started to write once more. I told myself that I’d start with something fairly simple for my first attempt at writing – Danielle, perhaps you could write up a few notes about a package you use at work? Nothing complicated. Just a little something on the table1 package2 by Benjamin Rich, perhaps? Nice and simple, short and sweet. Won’t take very long at all will it my dear?\nYeah, right."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#getting-started",
    "href": "posts/2024-06-21_table1/index.html#getting-started",
    "title": "Making tables in R with table1",
    "section": "Getting started",
    "text": "Getting started\nThe table1 package is one of those “niche” packages that is designed to solve exactly one problem, and solve that problem well: it is designed to produce tables of descriptive statistics of the sort that typically appear as “Table 1” in an academic paper (hence the name). It’s not a general purpose tool for table construction like gt or flextable, and compared to those packages it has a number of limitations. However, because the scope of the package is narrower, it’s able to solve the specific problem that it is designed for in an extremely efficient manner. It’s used a lot in my workplace and while I was a little skeptical at first I’ve come to love it.\nSo let’s get this party started shall we? First, I’ll need to load a few packages in order to make this post even remotely legible. Besides the table1 package itself, I’ll load the palmerpenguins package so that I have a data set I can play with, and dplyr for any data wrangling I need to do later on:\n\nlibrary(palmerpenguins)\nlibrary(table1)\nlibrary(dplyr)\n\nThe palmerpenguins data set that I’ll be using in this post is one I’ve used many times before, and it’s nicely documented on the package website. Suffice it to say, the data set contains a collection of measurements from three penguin species, and the data set looks like this:\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007\n 2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007\n 3 Adelie  Torgersen           40.3          18                 195        3250 female  2007\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;    2007\n 5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007\n 6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007\n 7 Adelie  Torgersen           38.9          17.8               181        3625 female  2007\n 8 Adelie  Torgersen           39.2          19.6               195        4675 male    2007\n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;    2007\n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;    2007\n# ℹ 334 more rows\n\n\nTo illustrate the basic usage of the table1 package, I’ll create a table that provides descriptive statistics for the bill_length_mm and island variables, computed separately by each species represented in the data set. We can do this with very little difficulty by passing a one-sided formula to the table1() function. The formula we want looks like this:\n~ island + bill_length_mm | species\nOn the left we have the two variables that contain the measurements we want to describe (bill_length_mm and island), and on the right we have the stratification variable that supplies the grouping (species). When calling the table1() function, all we have to do is pass this formula and the data frame itself:3 4\ntable1(~ island + bill_length_mm | species, penguins)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nisland\n\n\n\n\n\n\nBiscoe\n44 (28.9%)\n0 (0%)\n124 (100%)\n168 (48.8%)\n\n\nDream\n56 (36.8%)\n68 (100%)\n0 (0%)\n124 (36.0%)\n\n\nTorgersen\n52 (34.2%)\n0 (0%)\n0 (0%)\n52 (15.1%)\n\n\nbill_length_mm\n\n\n\n\n\n\nMean (SD)\n38.8 (2.66)\n48.8 (3.34)\n47.5 (3.08)\n43.9 (5.46)\n\n\nMedian [Min, Max]\n38.8 [32.1, 46.0]\n49.6 [40.9, 58.0]\n47.3 [40.9, 59.6]\n44.5 [32.1, 59.6]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\nThe output here is a table showing frequency counts for the discrete variable (island) and some standard summary statistics for the continuous variable (bill_length_mm). This table isn’t perfect but it’s surprisingly good given how little effort I had to put in when creating it, and I suspect this ease-of-use factor is the main reason why this package gets used so much in my workplace. Like everything in life though, the devil is in the details, and if you want to make the most of the package it’s helpful to dive into those details to get a good sense of what the package can (and cannot) do."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#applying-labels",
    "href": "posts/2024-06-21_table1/index.html#applying-labels",
    "title": "Making tables in R with table1",
    "section": "Applying labels",
    "text": "Applying labels\nThe table I produced above immediately illustrates the first problem a data analyst has to grapple with when using the table1 package: variable labels. In most respects this “off the shelf” table is pretty good: it’s almost good enough to use. But there’s one big eyesore: the raw variable names island and bill_length_mm appear as row labels in the output. These are both excellent variable names for programming, but they’re not very nice when exposed in a table. To fix this, we can use the label() function supplied by the table1 package to associate each of these variables with a pretty, human-readable label:5\nlabel(penguins$island) &lt;- \"Island\"\nlabel(penguins$bill_length_mm) &lt;- \"Bill Length (mm)\"\n\ntable1(~ island + bill_length_mm | species, penguins)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nIsland\n\n\n\n\n\n\nBiscoe\n44 (28.9%)\n0 (0%)\n124 (100%)\n168 (48.8%)\n\n\nDream\n56 (36.8%)\n68 (100%)\n0 (0%)\n124 (36.0%)\n\n\nTorgersen\n52 (34.2%)\n0 (0%)\n0 (0%)\n52 (15.1%)\n\n\nBill Length (mm)\n\n\n\n\n\n\nMean (SD)\n38.8 (2.66)\n48.8 (3.34)\n47.5 (3.08)\n43.9 (5.46)\n\n\nMedian [Min, Max]\n38.8 [32.1, 46.0]\n49.6 [40.9, 58.0]\n47.3 [40.9, 59.6]\n44.5 [32.1, 59.6]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\nTo understand what’s really going on here, it’s helpful to recognise that the label() function is purely a convenience function. All it’s really doing is setting the “label” metadata attribute for the relevant object. If you really wanted to, you could do exactly the same thing in base R via the attr() function:\nattr(penguins$bill_depth_mm, \"label\") &lt;- \"Bill Depth (mm)\"\n\ntable1(~ bill_depth_mm | species, penguins)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nBill Depth (mm)\n\n\n\n\n\n\nMean (SD)\n18.3 (1.22)\n18.4 (1.14)\n15.0 (0.981)\n17.2 (1.97)\n\n\nMedian [Min, Max]\n18.4 [15.5, 21.5]\n18.5 [16.4, 20.8]\n15.0 [13.1, 17.3]\n17.3 [13.1, 21.5]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\nThat being said, I am slowly coming to like the setLabel() convenience function that table1 provides rather than using label() or attr(). The setLabel() function has the nice property that it returns the labelled object itself and so it plays very nicely with a dplyr workflow. If you have a data frame with several variables that need to be labelled, you can use mutate() and setLabel() to apply all the labels in one step, like this:\npenguins &lt;- penguins |&gt; \n  mutate(\n    flipper_length_mm = setLabel(flipper_length_mm, \"Flipper Length (mm)\"),\n    body_mass_g = setLabel(body_mass_g, \"Body Mass (g)\"),\n    sex = setLabel(sex, \"Sex\"),\n    year = setLabel(year, \"Year\")\n  )\n\ntable1(~ flipper_length_mm + body_mass_g + sex + year | species, penguins)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n201 (14.1)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n197 [172, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n4200 (802)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n4050 [2700, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nSex\n\n\n\n\n\n\nfemale\n73 (48.0%)\n34 (50.0%)\n58 (46.8%)\n165 (48.0%)\n\n\nmale\n73 (48.0%)\n34 (50.0%)\n61 (49.2%)\n168 (48.8%)\n\n\nMissing\n6 (3.9%)\n0 (0%)\n5 (4.0%)\n11 (3.2%)\n\n\nYear\n\n\n\n\n\n\nMean (SD)\n2010 (0.822)\n2010 (0.863)\n2010 (0.792)\n2010 (0.818)\n\n\nMedian [Min, Max]\n2010 [2010, 2010]\n2010 [2010, 2010]\n2010 [2010, 2010]\n2010 [2010, 2010]"
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#customing-cell-content",
    "href": "posts/2024-06-21_table1/index.html#customing-cell-content",
    "title": "Making tables in R with table1",
    "section": "Customing cell content",
    "text": "Customing cell content\n\nSomebody mixed my medicine  Somebody’s in my head again  Well, I’ll drink what you leak and I’ll smoke what you sigh  See across the room with a look in your eye      –The Pretty Reckless\n\nOne thing I really like about the table1 package is that it supplies very sensible defaults for tables of descriptive statistics: continuous variables are summarised not only via means and standard deviations, but you also get the medians, ranges, and missing data summaries. Categorical variables are summarised with counts and percentages, and again you get a missing data summary. Very often this is exactly the summary you want, and no customisation at all is required.\nInevitably, though, every data analyst comes across as situation that requires a different collection of summary statistics. At that point, you need to dive a little deeper and understand the syntax table1 uses to modify the summaries that it produces.\n\nUsing abbreviated codes\nThe table1 package has a very practical and flexible mechanism for customising the descriptive statistics that it produces, but one that needs a bit of unpacking to understand. If you really want to do so, you can write an entire “rendering” function from scratch that affords very fine grained control over the output (more on that later!) but most of the time you don’t actually want to go to all that effort. In most situations, all you really want to do is swap out one widely-used descriptive statistic for a different widely-used descriptive statistic. It would be no fun for the analyst if they had to write an entire rendering function from scratch just to switch from reporting arithmetic means to reporting geometric means. To that end, table1 provides a compact syntax using “abbreviated codes” that covers a lot of common use cases.\nAs a concrete example, let’s consider the task I described above: reporting geometric means and standard deviations. This is a very common task in pharmacometrics because a lot of observed data are approximately log-normal in distribution, and in my everyday work I find I have to do this a lot. Luckily for me, the table1 package recognises the strings \"GMEAN\" and \"GSD\" as abbreviated codes, and internally will replace them with function calls that compute the geometric mean and geometric standard deviation. To define a custom render that produces these two statistics, all I have to do is define a named vector like this one:\n\nrender_geometric &lt;- c(\n  \"Geometric mean\" = \"GMEAN\", \n  \"Geometric SD\" = \"GSD\"\n)\n\nIn this compressed syntax, the names define the row labels that will be printed in the output table (e.g., \"Geometric mean\" becomes a row label), and the values are interpreted using the abbreviated code (e.g., the \"GMEAN\" string is replaced by the value of the geometric mean). To apply this custom render to my table only to the continuous variables in the summary table, all I have to do is include render.continuous = render_geometric in the call to table1():\ntable1(\n  x = ~ flipper_length_mm + body_mass_g + sex + year | species, \n  data = penguins, \n  render.continuous = render_geometric\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nGeometric mean\n190\n196\n217\n200\n\n\nGeometric SD\n1.04\n1.04\n1.03\n1.07\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nGeometric mean\n3670\n3710\n5050\n4130\n\n\nGeometric SD\n1.13\n1.11\n1.11\n1.21\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nSex\n\n\n\n\n\n\nfemale\n73 (48.0%)\n34 (50.0%)\n58 (46.8%)\n165 (48.0%)\n\n\nmale\n73 (48.0%)\n34 (50.0%)\n61 (49.2%)\n168 (48.8%)\n\n\nMissing\n6 (3.9%)\n0 (0%)\n5 (4.0%)\n11 (3.2%)\n\n\nYear\n\n\n\n\n\n\nGeometric mean\n2010\n2010\n2010\n2010\n\n\nGeometric SD\n1.00\n1.00\n1.00\n1.00\n\n\n\n\n\n\nAs you can see from the output, the custom render has been applied to the two continuous variables flipper_length_mm and body_mass_g but not the categorical variables sex and year, just as you’d expect given that the argument I specified is called render.continuous. However, there are two features that might be a little surprising:\n\nThe missing data summary for the continuous variables is unaffected\nIf you look at the documentation for table1() you’ll notice it has a render argument but not a render.continuous argument\n\nI’ll unpack both of those things later in the blog post, but I wanted to mention them now because these things confused me a little when I first started using table1. For now, let’s just accept that it works and move on.\nThe table1 package comes equipped with quite a few of these abbreviated codes, which makes life considerably easier. For instance if we needed to compute the 10th, 50th, and 90th percentiles of each continuous variable, we could use the \"q10\", \"q50\", and \"q90\" keywords, like so:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g + sex | species, \n  data = penguins, \n  render.continuous = c(\n    \"10th percentile\" = \"q10\", \n    \"50th percentile\" = \"q50\",\n    \"90th percentile\" = \"q90\"\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\n10th percentile\n181\n187\n209\n185\n\n\n50th percentile\n190\n196\n216\n197\n\n\n90th percentile\n198\n205\n228\n221\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\n10th percentile\n3150\n3300\n4400\n3300\n\n\n50th percentile\n3700\n3700\n5000\n4050\n\n\n90th percentile\n4300\n4200\n5700\n5400\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nSex\n\n\n\n\n\n\nfemale\n73 (48.0%)\n34 (50.0%)\n58 (46.8%)\n165 (48.0%)\n\n\nmale\n73 (48.0%)\n34 (50.0%)\n61 (49.2%)\n168 (48.8%)\n\n\nMissing\n6 (3.9%)\n0 (0%)\n5 (4.0%)\n11 (3.2%)\n\n\n\n\n\n\nVery handy.\n\n\nThe quote from “My medicine” at the start of this section isn’t accidental. The abbreviated code syntax in table1 is a powerful tool, but when I started reading table1 code without understanding the keyword matching involved it did feel a little like “someone’s in my head again”, substituting code where a string should be\n\n\nSupported aliases\nThe natural question you might have as the user of the package is, of course, what abbreviated codes does the table1 package understand? As documented here in the package vignette, you can find a complete listing by playing around with the stats.default() function.6 So let’s do that. For continuous variables, this is the list of supported aliases:\n\ncontinuous &lt;- 1:10\nnames(stats.default(continuous)) \n\n [1] \"N\"      \"NMISS\"  \"SUM\"    \"MEAN\"   \"SD\"     \"CV\"     \"GMEAN\"  \"GSD\"    \"GCV\"   \n[10] \"MEDIAN\" \"MIN\"    \"MAX\"    \"q01\"    \"q02.5\"  \"q05\"    \"q10\"    \"q25\"    \"q50\"   \n[19] \"q75\"    \"q90\"    \"q95\"    \"q97.5\"  \"q99\"    \"Q1\"     \"Q2\"     \"Q3\"     \"IQR\"   \n[28] \"T1\"     \"T2\"    \n\n\nHere’s what each of these mean:7\n\n\"N\", \"NMISS\": these compute the number of non-missing observations and number of missing observations respectively\n\"SUM\", \"MEAN\", \"SD\", \"MEDIAN\", \"MIN\", \"MAX\": these all correspond to to the functions of the same name, e.g., \"SUM\" produces a call to sum(), with missing values removed\n\"CV\": the coefficient of variation, i.e., 100 times the standard deviation divided by the absolute value of the mean\n\"GMEAN\", \"GSD\", \"GCV\": the geometric mean, geometric standard deviation, and geometric coefficient of variation\nq01, q02.5, \"q05\", \"q10\", \"q25\", \"q50\", \"q75\", \"q90\", \"q95\", \"q97.5\", \"q99\": these are understood to refer to specific quantiles, e.g., \"q25\" is translated as a function call that computes the 25th percentile\n\"Q1\", \"Q2\" \"Q3\": these are used to compute quartiles (25th, 50th, and 75th percentiles)\n\"T1\", \"T2\": these are used to compute tertiles (33rd and 67th percentiles)\n\"IQR\": this computes the interquartile range\n\nIn all cases except for \"NMISS\", the relevant statistics are computed after removing missing values. Turning now to categorical variables, we can again use the stats.default() function to find the supported abbreviated codes:8\n\ncategorical &lt;- c(\"a\", \"b\", \"c\")\nnames(stats.default(categorical)[[1]])\n\n[1] \"FREQ\"    \"PCT\"     \"PCTnoNA\" \"NMISS\"  \n\n\nThe interpretation of these is as follows:\n\n\"FREQ\": the frequency count for a category\n\"PCT\": the percent relative frequency, with missing values included in the denominator\n\"PCTnoNA\": the percent relative frequency, after missing values are removed\n\"NMISS\": the number of missing values, as before\n\nThe nice thing about these abbreviated codes is that they cover a surprisingly wide variety of use cases. More often than not I’ve found that the descriptive statistics I need can be specified using this mechanism. From the analyst perspective this is great: you really don’t want to waste time writing more code than you have to, so if you can specify your table of descriptive statistics without bothering to write a function, you’re doing well.\n\n\n\nWriting render functions\n\nI am strong, love is evil  It’s a version of perversion that is only for the lucky people  Take your time and do with me what you will  I won’t mind, you know I’m ill, you know I’m ill  So hit me like a man  And love me like a woman      – The Pretty Reckless\n\n\n\nProbably no surprise to anyone who knows me that “Hit me like a man” is my favourite Pretty Reckless song. But also appropriate to how I feel about the render function syntax. The functionality is powerful once you know how to use it, but it’s also simple and lovely\nAlas life is not always kind to us, and it’s not uncommon to run into situations where your table of descriptive statistics requires the computation of something that doesn’t have an abbreviated code in table1. When that happens, the only recourse is for the user to write a rendering function that takes the data as input and returns a vector of strings to be printed into the table. As an example, suppose you have a need to report Winsorised summary statistics for your continuous variables:\n\nrender_winsorized &lt;- function(x, cutoff = .05, ...) {\n  lo &lt;- quantile(x, cutoff, na.rm = TRUE)\n  hi &lt;- quantile(x, 1 - cutoff, na.rm = TRUE)\n  x[x &lt; lo] &lt;- lo\n  x[x &gt; hi] &lt;- hi\n  strs &lt;- c(\n    \"\",\n    \"Winsorized mean\" = sprintf(\"%1.2f\", mean(x, na.rm = TRUE)),\n    \"Winsorized SD\" = sprintf(\"%1.2f\", sd(x, na.rm = TRUE))\n  )\n  return(strs)\n}\n\nNotice that this render_winsorized() function returns a named vector of strings that follows the same convention that we followed with the simpler render_geometric example earlier: the names of the output string become the row labels, and the values are printed into the table itself. Along similar lines, we can define a rendering function to be applied to the categorical variables in the data. Here’s a very simple one that reports only the absolute frequencies for each category:\n\nrender_counts &lt;- function(x, ...) c(\"\", table(stringr::str_to_title(x)))\n\nHaving defined our render functions, we produce the desired table by passing render_winsorized() as the handler for continuous variables and render_counts() as the handler for categorical variables:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g + sex | species, \n  data = penguins, \n  render.continuous = render_winsorized,\n  render.categorical = render_counts\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nWinsorized mean\n189.91\n195.99\n217.23\n200.85\n\n\nWinsorized SD\n5.75\n6.43\n6.38\n13.52\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nWinsorized mean\n3696.52\n3738.68\n5074.80\n4200.80\n\n\nWinsorized SD\n432.98\n336.76\n471.74\n765.82\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nSex\n\n\n\n\n\n\nFemale\n73\n34\n58\n165\n\n\nMale\n73\n34\n61\n168\n\n\nMissing\n6 (3.9%)\n0 (0%)\n5 (4.0%)\n11 (3.2%)\n\n\n\n\n\n\nOur table is mostly done, but we still don’t have a method for adjusting how the missing data summaries are produced. To do that we need to define one more rendering function and pass it as the render.missing argument:\nrender_missing &lt;- function(x, ...) c(\"Missing\" = sum(is.na(x)))\n\ntable1(\n  x = ~ flipper_length_mm + body_mass_g + sex | species, \n  data = penguins, \n  render.continuous = render_winsorized,\n  render.categorical = render_counts,\n  render.missing = render_missing\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nWinsorized mean\n189.91\n195.99\n217.23\n200.85\n\n\nWinsorized SD\n5.75\n6.43\n6.38\n13.52\n\n\nMissing\n1\n0\n1\n2\n\n\nBody Mass (g)\n\n\n\n\n\n\nWinsorized mean\n3696.52\n3738.68\n5074.80\n4200.80\n\n\nWinsorized SD\n432.98\n336.76\n471.74\n765.82\n\n\nMissing\n1\n0\n1\n2\n\n\nSex\n\n\n\n\n\n\nFemale\n73\n34\n58\n165\n\n\nMale\n73\n34\n61\n168\n\n\nMissing\n6\n0\n5\n11\n\n\n\n\n\n\n\n\nUnpacking render functions\nThere’s still a bit of a mystery here, because the table1() function doesn’t have arguments render.continuous, render.categorical, or render.missing: instead, it has a render argument. What’s actually going on here is that the default value for render is the render.default() function exported by table1, and render.default() accepts render.continuous, render.categorical, or render.missing as arguments. In other words, what’s happening in the code above is that my custom functions end up being passed to render.default() via the dots.\nThere’s nothing to prevent you from bypassing this whole process by writing your own render function that handles all the input variables. For example, here’s a very simple rendering function that counts the number of non-missing observations, and prints it in the same row as the variable name:\nrender_n &lt;- function(x, ...) sum(!is.na(x))\n\ntable1(\n  x = ~ flipper_length_mm + body_mass_g + sex | species, \n  data = penguins, \n  render = render_n\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n151\n68\n123\n342\n\n\nBody Mass (g)\n151\n68\n123\n342\n\n\nSex\n146\n68\n119\n333\n\n\n\n\n\n\nI have to confess it took me waaaaay too long to realise that I could do this in table1. True, I don’t often have a need to bypass the render.default() function, but there are definitely times when that’s a handy little bit of functionality. Sigh. Sometimes I’m quite dense."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#table-annotations",
    "href": "posts/2024-06-21_table1/index.html#table-annotations",
    "title": "Making tables in R with table1",
    "section": "Table annotations",
    "text": "Table annotations\n\nFollow me down to the river  Drink while the water is clean  Follow me down to the river tonight  I’ll be down here on my knees      –The Pretty Reckless\n\n\n\nOkay I’ll admit it. This lyric isn’t really connected to the text. I mean, the video clip is a collection of annotations showing the lyrics, I guess. But honestly I just like the song\nTime to switch gears a little. In the previous section I talked about how to customise the statistics that are reported in the table cells. Implicit in this discussion is the fact that a custom render function allows you to customise the row labels associated with each statistic, in exactly the same way that variable labels allow you to customise the variable descriptions that appear in the leftmost column of the table. Taken together, these two mechanisms (render functions and variable labels) give the user a lot of control over what appears in the leftmost column of the table. But what about the header row? How do we customise that in table1?\n\nStrata column labels\nTo start with, let’s consider the columns that associated with a particular stratum. In the penguins tables I’ve been creating, the strata are defined by the species variable so are three columns that are associated with a specific stratum. By default table1() will add a description for each such column in the header row that contains the category name (e.g., “Gentoo”) and the number of observations that belong to this category. But perhaps we don’t want those sample size numbers? Maybe all we want is the category name. To customise how each stratum is labelled in the header row, the table1() function has an argument called render.strat that takes a function as its value. The strata rendering function takes three arguments: the label is the value in the data that defines that category (e.g., \"Gentoo\"), n is the number of observations that have been assigned to the category, and transpose is a logical variable indicating whether the table is transposed (more on that later). The output of the function is a string that specifies (as HTML) what should appear in the header row. To illustrate the idea, here’s a very simple stratum rendering function that only prints the category label:\nrender_strat &lt;- function(label, n, transpose = FALSE) {\n  sprintf(\"&lt;span class='stratlabel'&gt;%s&lt;/span&gt;\", label)\n}\nThe only subtlety to this render_strat() function is that it outputs some HTML that wraps the label in an HTML span tag and assigns it to a class that we can (and will) use later on to create some fancy styling using CSS. But I’m getting ahead of myself a little. For now, it’s enough to note that render_strat() creates a very simple label that just prints out the category label. Here it is in action:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  render.strat = render_strat\n)\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\nOverall\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n201 (14.1)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n197 [172, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n4200 (802)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n4050 [2700, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\nIndeed it performs as expected: the sample size annotations are gone and we now have very minimalistic labels for each of the three penguin species. So let’s move along.\n\n\nOther column labels\nThere are two other columns that typically appear in a table1 output object: on the left we have column that contains the row labels, and on the right we have a table that contains the descriptive statistics for the “overall” data set where we collapse across all strata. By default, the row label column on the left has no label and the aggregated column on the right uses that label “Overall”. Both of these are customisable in the call to table1(), using the overall and rowlabelhead arguments. An example of this is illustrated below:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  overall = \"All Species\",\n  rowlabelhead = \"Measurement\"\n)\n\n\n\n\n\n\n\n\n\n\n\nMeasurement\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nAll Species\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n201 (14.1)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n197 [172, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n4200 (802)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n4050 [2700, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\n\n\nFootnotes and captions\n\nOh lord, heaven knows, we belong way down below  Oh lord, tell us so, we belong way down below      –The Pretty Reckless\n\nIn addition to allowing you to customise the header row, the table1 package also supports the addition of captions and footnotes. Shockingly, it turns out you can specify these with table1() by using the caption and footnote arguments. Both of these take a single string as their input, and you can use HTML tags here. For example, here’s a footer that acknowledges the two packages I’ve relied on most in this post, specifying the package names in boldface:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  footnote = \"Created using &lt;b&gt;table1&lt;/b&gt; and &lt;b&gt;palmerpenguins&lt;/b&gt;\"\n)\n\n\n\n\nAdelie(N=152)\nChinstrap(N=68)\nGentoo(N=124)\nOverall(N=344)\n\nCreated using table1 and palmerpenguins\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n201 (14.1)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n197 [172, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n4200 (802)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n4050 [2700, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\nSpecifying captions is very similar. Here’s a simple example:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  caption = \"Flipper length and body mass by species among the Palmer penguins\"\n)\n\n\nFlipper length and body mass by species among the Palmer penguins\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n201 (14.1)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n197 [172, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\nBody Mass (g)\n\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n4200 (802)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n4050 [2700, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n2 (0.6%)\n\n\n\n\n\n\n\n\n\nOne day, just for one brief moment, I would like to look this cool. It’s never going to happen of course, but a girl can dream"
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#table-structure",
    "href": "posts/2024-06-21_table1/index.html#table-structure",
    "title": "Making tables in R with table1",
    "section": "Table structure",
    "text": "Table structure\nSo far we’ve discussed how to control what statistics are computed in the table, and in the process I’ve also talked about how to specify the row and columns labels that are associated with the various statistics. I’ve also talked about other marginalia associated with a table, which in table1 is really just the footnote and caption. What I haven’t talked about yet is how to control the structure of the table that gets produced. For the most part, this structure is controlled by the formula that you use to specify the table. When I specify a table like this\n~ flipper_length_mm + body_mass_g | species\nI will generally get a table that has one column for each unique value of species (the strata), and a block of rows associated with each of the variables (flipper_length_mm and body_mass_g) that supply the relevant descriptive statistics. By default we also get one additional “overall” column that collapses the strata and reports descriptive statistics for the entire data set. Most of the time this is exactly the structure we want, but not always. To that end, table1 allows you to customise the structure in various ways.\n\nRemoving “overall”\nThe simplest way to modify the table structure is to remove the “overall” column that collapses the strata. We can do this using the overall argument. In the last section I showed how to use this argument to change the label associated with this column by passing a string, but if instead we set overall = FALSE, that column will be removed entirely:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  overall = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\nMean (SD)\n190 (6.54)\n196 (7.13)\n217 (6.48)\n\n\nMedian [Min, Max]\n190 [172, 210]\n196 [178, 212]\n216 [203, 231]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n\n\nBody Mass (g)\n\n\n\n\n\nMean (SD)\n3700 (459)\n3730 (384)\n5080 (504)\n\n\nMedian [Min, Max]\n3700 [2850, 4780]\n3700 [2700, 4800]\n5000 [3950, 6300]\n\n\nMissing\n1 (0.7%)\n0 (0%)\n1 (0.8%)\n\n\n\n\n\n\n\n\nNested stratifications\nUp to this point in the post I’ve used only a single variable to define the strata in the table. At the start of the post I mentioned that you can drop the stratification entirely simply by specifying a one-sided formula like ~ flipper_length_mm + body_mass_g that doesn’t include anything on the right hand side of the | separator. But that’s rarely of interest to us in real world data analysis. The thing we’re more likely to want is a nested stratification, where we define strata based on all unique combinations of two variables. For example, let’s suppose I wanted to compute descriptive statistics for each species of penguin, but for each species the statistics should be computed separately for each island. Happily, the table1 package supports this kind of two-level stratification. All I have to do is write species * island on the right hand side of the separator like so:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species * island,\n  data = penguins,\n  overall = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n\n\nChinstrap\n\n\nGentoo\n\n\n\n\nBiscoe\n(N=44)\nDream\n(N=56)\nTorgersen\n(N=52)\nDream\n(N=68)\nBiscoe\n(N=124)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\n\nMean (SD)\n189 (6.73)\n190 (6.59)\n191 (6.23)\n196 (7.13)\n217 (6.48)\n\n\nMedian [Min, Max]\n190 [172, 203]\n190 [178, 208]\n191 [176, 210]\n196 [178, 212]\n216 [203, 231]\n\n\nMissing\n0 (0%)\n0 (0%)\n1 (1.9%)\n0 (0%)\n1 (0.8%)\n\n\nBody Mass (g)\n\n\n\n\n\n\n\nMean (SD)\n3710 (488)\n3690 (455)\n3710 (445)\n3730 (384)\n5080 (504)\n\n\nMedian [Min, Max]\n3750 [2850, 4780]\n3580 [2900, 4650]\n3700 [2900, 4700]\n3700 [2700, 4800]\n5000 [3950, 6300]\n\n\nMissing\n0 (0%)\n0 (0%)\n1 (1.9%)\n0 (0%)\n1 (0.8%)\n\n\n\n\n\n\nAs it happens, only the Adelie penguins appear on all three islands: the Chinstrap penguins appear only on Dream Island, and the Gentoo penguins appear only on Biscoe Island. So the table here contains three columns for the Adelie penguins, and only one each for the Chinstrap and Gentoo penguins.\nThere are some limitations to this functionality. You can’t stratify by more than two variables, and the stratification variables cannot contain any missing values. Even so, the functionality is pretty handy, and it is sensitive to the order in which you specify the two stratification variables. If I write island * species rather than species * island, I get this table instead:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | island * species,\n  data = penguins,\n  overall = FALSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiscoe\n\n\nDream\n\n\nTorgersen\n\n\n\n\nAdelie\n(N=44)\nGentoo\n(N=124)\nAdelie\n(N=56)\nChinstrap\n(N=68)\nAdelie\n(N=52)\n\n\n\n\nFlipper Length (mm)\n\n\n\n\n\n\n\nMean (SD)\n189 (6.73)\n217 (6.48)\n190 (6.59)\n196 (7.13)\n191 (6.23)\n\n\nMedian [Min, Max]\n190 [172, 203]\n216 [203, 231]\n190 [178, 208]\n196 [178, 212]\n191 [176, 210]\n\n\nMissing\n0 (0%)\n1 (0.8%)\n0 (0%)\n0 (0%)\n1 (1.9%)\n\n\nBody Mass (g)\n\n\n\n\n\n\n\nMean (SD)\n3710 (488)\n5080 (504)\n3690 (455)\n3730 (384)\n3710 (445)\n\n\nMedian [Min, Max]\n3750 [2850, 4780]\n5000 [3950, 6300]\n3580 [2900, 4650]\n3700 [2700, 4800]\n3700 [2900, 4700]\n\n\nMissing\n0 (0%)\n1 (0.8%)\n0 (0%)\n0 (0%)\n1 (1.9%)\n\n\n\n\n\n\nI’ve found this functionality useful many times in my everyday life.\n\n\nAdding extra columns\nAnother kind of “structural” customisation that table1 allows is adding new columns. A common use case for this functionality is to add a column that reports a p-value associated with a particular row. For example, suppose what I wanted my table to do is run a one-way ANOVA for each of the continuous variables in the table, to test to see if the categories have different group means. To handle something like this we’ll again need to write a custom render function that accepts data from all groups – as a list of vectors – and returns a string that should be printed into the relevant cell in the new “p-values” column. This render_p_value() function will do this for me:\n\nrender_p_value &lt;- function(x, ...) {\n  dat &lt;- bind_rows(\n    purrr::map(x, ~ data.frame(value = .)), \n    .id = \"group\"\n  )\n  mod &lt;- aov(value ~ group, dat)\n  p &lt;- summary(mod)[[1]][1, 5]\n  return(scales::label_pvalue()(p))\n}\n\nI don’t want to dive into the details of what this function is doing, but if you’re familiar with the standard interface for linear models in R it should look very familiar. If not, here’s the gist: the first part of the code rearranges the list of vectors input into a data frame format, the second part estimates parameters for the model, runs the usual F-test, and extracts the p-value from the output. Finally, it returns the p-value as a prettily-formatted string.\nThe render_p_value() function is the one we’ll use to render our new column, but for the purposes of this example I’ll also define a custom renderer for the contents of the strata columns as well, so that the only thing it does is report the mean value for the group. You don’t have to do this, I’m only doing it because I want my table to be as simple as possible.\n\nrender_mean &lt;- function(x, ...) sprintf(\"%1.1f\", mean(x, na.rm = TRUE))\n\nNow that we have our rendering functions, I can specify one or more additional columns in my table by passing a named list of functions to the extra.col argument (the names are used to specify the column labels). In my case I’m only adding a single extra column with the p-value so my list has only a single function, but it’s not too hard to imagine scenarios where I’d want to add more than one (e.g., maybe I want to report the degrees of freedom associated with my F-test). Anyway, here’s the code:\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  render = render_mean,\n  extra.col = list(\"p-value\" = render_p_value)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nOverall\n(N=344)\np-value\n\n\n\n\nFlipper Length (mm)\n190.0\n195.8\n217.2\n200.9\n&lt;0.001\n\n\nBody Mass (g)\n3700.7\n3733.1\n5076.0\n4201.8\n&lt;0.001\n\n\n\n\n\n\nVery nice.\n\n\nTransposing tables\n\nLet me open up the discussion with  I’m not impressed with any motherfucking word I say  See I lied that I cried when he came inside  And now I’m burning a highway to Hades      –The Pretty Reckless\n\nBefore I dive a little deeper and talk about table structures that go beyond what you can do with the formula interface to table1() there’s one more thing I should talk about. I don’t actually want to talk about this topic because it makes me cry a little bit every time I encounter it, but I’ll be good and try anyway.\nThat topic is transposing a table.\nNormally when you specify a table with a formula, the stratification is used to create the columns, and the variable list is used to define the rows. You can flip this if you like by setting transpose = TRUE when calling table1(), but in my experience this is a bit messy and often requires a lot of tinkering with your render functions to make the results look good. To that end, here’s a simple rendering function that I’ll use in this example. All it does is compute the mean and standard deviation for a continuous variable:\n\nrender_mean_sd &lt;- function(x, ...) {\n  m &lt;- mean(x, na.rm = TRUE)\n  s &lt;- sd(x, na.rm = TRUE)\n  sprintf(\"%1.1f (%1.1f)\", m, s)\n}\n\nWith the help of this render_mean_sd() function and the simple render_strat() function I defined earlier, here’s an example of a transposed table in which the stratification variable (species) defines the rows, and the four variables on the left are used to define columns:\ntable1(\n  x = ~ bill_length_mm + bill_depth_mm + flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  transpose = TRUE,\n  rowlabelhead = \"Species\",\n  render = render_mean_sd,\n  render.strat = render_strat\n)\n\n\n\n\nSpecies\nBill Length (mm)\nBill Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\nAdelie\n38.8 (2.7)\n18.3 (1.2)\n190.0 (6.5)\n3700.7 (458.6)\n\n\nChinstrap\n48.8 (3.3)\n18.4 (1.1)\n195.8 (7.1)\n3733.1 (384.3)\n\n\nGentoo\n47.5 (3.1)\n15.0 (1.0)\n217.2 (6.5)\n5076.0 (504.1)\n\n\nOverall\n43.9 (5.5)\n17.2 (2.0)\n200.9 (14.1)\n4201.8 (802.0)\n\n\n\n\n\n\nIt works, and there are definitely use cases for this. But the functionality is limited. It only works for a single stratification level (i.e., you can’t do species * island in this context), and it only looks pretty because my render_mean_sd() function doesn’t contain any labels. Things get messy pretty fast when working with transposed tables, and in all honesty I’ve never been willing to use this functionality in a client project.\n\n\nArbitrary stratification\n\nWhy’d you bring a shotgun to the party?      –The Pretty Reckless\n\nUp to this point in the post every table I’ve created with table1() has used a formula to specify the basic structure of the output. In real life this is almost always what you want to do, because this “formula interface” is pretty flexible and extremely easy to work with. I’d even go so far as to say that this formula interface is one of the most appealing aspects to the table1 package. However, in point of fact the formula interface that everyone uses is actually a bit of sweet syntactic sugar laid atop a lower-level interface. Very occasionally you encounter a situation where the formula interface isn’t expressive enough to do what you want, and when that happens you have to “bring a shotgun to the party” and use the low level interface which takes a list of data frames (one per stratum) as input.\nTo motivate the discussion, I’ll give an example of something that I’ve occasionally wanted to do but isn’t possible with the formula interface. Earlier in this post I showed you how to create a nested stratification where we pass two stratification variables in the formula, and the table contains one stratum for every unique combination of the two variables. Sometimes, though, you want to create a table that stratifies by two variables but only shows the marginal stratifications. For example, I might want a table that includes descriptive statistics for each species of penguin, and next to that my table would have descriptive statistics for each sex. In this situation I’m not interested in the cross-tabulation. That is, I don’t care about which species a male penguin belongs to, and I don’t care about the sex of the various Adelie penguins either. I’m interested in species and sex completely independently of each other. The formula interface doesn’t support this kind of stratification, so I’m going to have to do it manually.\nLet’s see how this is done. First, I’m going to make a few tweaks to the data that aren’t really very important, but will make my table a little nicer. Specifically, I’ll convert the sex variable to title case so that I get nice labels later, and I’ll convert year to a factor so that table1() treats it as a categorical variable:\n\npenguins &lt;- penguins |&gt;\n  mutate(\n    sex = stringr::str_to_title(sex),\n    year = factor(year)\n  )\n\nNow let’s move along to the important step. I’m going to create a new variable called penguins_strata that is a list of data frames.\n\npenguins_strata &lt;- c(\n    split(penguins, ~species), \n    split(penguins, ~sex),\n    list(\"All\" = penguins)\n  )\n\npenguins_strata\n\n$Adelie\n# A tibble: 152 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt; \n 1 Adelie  Torgersen           39.1          18.7               181        3750 Male  \n 2 Adelie  Torgersen           39.5          17.4               186        3800 Female\n 3 Adelie  Torgersen           40.3          18                 195        3250 Female\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193        3450 Female\n 6 Adelie  Torgersen           39.3          20.6               190        3650 Male  \n 7 Adelie  Torgersen           38.9          17.8               181        3625 Female\n 8 Adelie  Torgersen           39.2          19.6               195        4675 Male  \n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 142 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n$Chinstrap\n# A tibble: 68 × 8\n   species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt; \n 1 Chinstrap Dream            46.5          17.9               192        3500 Female\n 2 Chinstrap Dream            50            19.5               196        3900 Male  \n 3 Chinstrap Dream            51.3          19.2               193        3650 Male  \n 4 Chinstrap Dream            45.4          18.7               188        3525 Female\n 5 Chinstrap Dream            52.7          19.8               197        3725 Male  \n 6 Chinstrap Dream            45.2          17.8               198        3950 Female\n 7 Chinstrap Dream            46.1          18.2               178        3250 Female\n 8 Chinstrap Dream            51.3          18.2               197        3750 Male  \n 9 Chinstrap Dream            46            18.9               195        4150 Female\n10 Chinstrap Dream            51.3          19.9               198        3700 Male  \n# ℹ 58 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n$Gentoo\n# A tibble: 124 × 8\n   species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt; \n 1 Gentoo  Biscoe           46.1          13.2               211        4500 Female\n 2 Gentoo  Biscoe           50            16.3               230        5700 Male  \n 3 Gentoo  Biscoe           48.7          14.1               210        4450 Female\n 4 Gentoo  Biscoe           50            15.2               218        5700 Male  \n 5 Gentoo  Biscoe           47.6          14.5               215        5400 Male  \n 6 Gentoo  Biscoe           46.5          13.5               210        4550 Female\n 7 Gentoo  Biscoe           45.4          14.6               211        4800 Female\n 8 Gentoo  Biscoe           46.7          15.3               219        5200 Male  \n 9 Gentoo  Biscoe           43.3          13.4               209        4400 Female\n10 Gentoo  Biscoe           46.8          15.4               215        5150 Male  \n# ℹ 114 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n$Female\n# A tibble: 165 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt; \n 1 Adelie  Torgersen           39.5          17.4               186        3800 Female\n 2 Adelie  Torgersen           40.3          18                 195        3250 Female\n 3 Adelie  Torgersen           36.7          19.3               193        3450 Female\n 4 Adelie  Torgersen           38.9          17.8               181        3625 Female\n 5 Adelie  Torgersen           41.1          17.6               182        3200 Female\n 6 Adelie  Torgersen           36.6          17.8               185        3700 Female\n 7 Adelie  Torgersen           38.7          19                 195        3450 Female\n 8 Adelie  Torgersen           34.4          18.4               184        3325 Female\n 9 Adelie  Biscoe              37.8          18.3               174        3400 Female\n10 Adelie  Biscoe              35.9          19.2               189        3800 Female\n# ℹ 155 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n$Male\n# A tibble: 168 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex  \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750 Male \n 2 Adelie  Torgersen           39.3          20.6               190        3650 Male \n 3 Adelie  Torgersen           39.2          19.6               195        4675 Male \n 4 Adelie  Torgersen           38.6          21.2               191        3800 Male \n 5 Adelie  Torgersen           34.6          21.1               198        4400 Male \n 6 Adelie  Torgersen           42.5          20.7               197        4500 Male \n 7 Adelie  Torgersen           46            21.5               194        4200 Male \n 8 Adelie  Biscoe              37.7          18.7               180        3600 Male \n 9 Adelie  Biscoe              38.2          18.1               185        3950 Male \n10 Adelie  Biscoe              38.8          17.2               180        3800 Male \n# ℹ 158 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n$All\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;chr&gt; \n 1 Adelie  Torgersen           39.1          18.7               181        3750 Male  \n 2 Adelie  Torgersen           39.5          17.4               186        3800 Female\n 3 Adelie  Torgersen           40.3          18                 195        3250 Female\n 4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;  \n 5 Adelie  Torgersen           36.7          19.3               193        3450 Female\n 6 Adelie  Torgersen           39.3          20.6               190        3650 Male  \n 7 Adelie  Torgersen           38.9          17.8               181        3625 Female\n 8 Adelie  Torgersen           39.2          19.6               195        4675 Male  \n 9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;  \n10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;  \n# ℹ 334 more rows\n# ℹ 1 more variable: year &lt;fct&gt;\n\n\nAs you can see from this lengthy output, the penguins_strata variable is a list of six data frames. There are three data frames corresponding to each of the three species, two data frames corresponding to each unique sex category, and a final data frame that contains the entire data set. Later on when I construct the table, the table1() function will render each of these six data frames into a single column with the appropriate descriptive statistics.\nThe second input we need is a “labels” list that does two jobs:\n\nNotice that so far I’ve defined my strata, but haven’t specified the variables. The labels list has to do this. It specifies which variables should to be used when computing descriptive statistics. For the purposes of this example, I would like to tabulate the number of penguins (in each stratum) found on each island and the number of penguins observed in each year\nIn my head, the thing I want to do here is stratify the data separately by each species and by each sex. But the penguins_strata object doesn’t say anything about this. It’s just a flat list of six data frames. It doesn’t “know” that three of these data frames refer to different species and two of them refer to different sexes. For that matter, it doesn’t know that the last data frame isn’t associated with a grouping variable either. So we’ll also need to specify a list of groups that supplies the names of these grouping variables.\n\nIn other words, we need something like this:\n\npenguins_labels &lt;- list(\n    variables = list(\n      island = \"Island\", # names denote variables, values supply labels\n      year = \"Year\"\n    ), \n    groups = list(\"Species\", \"Sex\", \"\") # this is a list of labels only\n  )\n\npenguin_groups &lt;- c(3, 2, 1) # first three data frames are group 1, etc\n\nAt this point we have:\n\npenguins_strata, a variable that contains all the data organised into a list with one data frame per strata\npenguins_labels, a list that specifies the variables for which descriptive statistics are requested and the labels that should be assigned to variables and strata groups; and\npenguins_groups, a vector that specifies how the strata columns should be grouped\n\nIt’s quite a bit of setup work, but having done all the hard parts during the setup, the call to table1() is now very simple:\ntable1(penguins_strata, penguins_labels, groupspan = penguin_groups)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\n\n\nSex\n\n\n\n\n\nAdelie\n(N=152)\nChinstrap\n(N=68)\nGentoo\n(N=124)\nFemale\n(N=165)\nMale\n(N=168)\nAll\n(N=344)\n\n\n\n\nIsland\n\n\n\n\n\n\n\n\nBiscoe\n44 (28.9%)\n0 (0%)\n124 (100%)\n80 (48.5%)\n83 (49.4%)\n168 (48.8%)\n\n\nDream\n56 (36.8%)\n68 (100%)\n0 (0%)\n61 (37.0%)\n62 (36.9%)\n124 (36.0%)\n\n\nTorgersen\n52 (34.2%)\n0 (0%)\n0 (0%)\n24 (14.5%)\n23 (13.7%)\n52 (15.1%)\n\n\nYear\n\n\n\n\n\n\n\n\n2007\n50 (32.9%)\n26 (38.2%)\n34 (27.4%)\n51 (30.9%)\n52 (31.0%)\n110 (32.0%)\n\n\n2008\n50 (32.9%)\n18 (26.5%)\n46 (37.1%)\n56 (33.9%)\n57 (33.9%)\n114 (33.1%)\n\n\n2009\n52 (34.2%)\n24 (35.3%)\n44 (35.5%)\n58 (35.2%)\n59 (35.1%)\n120 (34.9%)\n\n\n\n\n\n\nAnd there it is. A table with two marginal stratifications and an overall column that provides descriptive statistics for two categorical variables."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#styling-tables",
    "href": "posts/2024-06-21_table1/index.html#styling-tables",
    "title": "Making tables in R with table1",
    "section": "Styling tables",
    "text": "Styling tables\nThe last topic I want to cover in this post is the visual styling of tables produced by table1(). In a moment I’ll show you what a table1 object looks like under the hood, but the short version is that the table structure is specified using HTML, and the visual styling is performed with the help of CSS. Most of the time when you’re using the table1 package you don’t really have to think too much about this, because the package comes with a collection of built-in styles that are usually good enough for a data analyst to use, but sometimes you want to go a little further. So we should talk a little about styling.\n\nBuilt-in styles\nLet’s start with the built-in styles that come for free with the table1 package. As described in the package vignette, if you’re not particularly interested in writing your own CSS code you have these options available to you:\n\nzebra: alternating shaded and unshaded rows\ngrid: show all grid lines\nshade: shade the header in gray\ntimes: use a serif font\ncenter: center all columns\n\nEach of these is associated with a CSS class that has the Rtable1- prefix, e.g., the zebra style corresponds to the Rtable1-zebra CSS class. You can have one of these classes applied to your table using the topclass argument. For instance, here’s a “zebra” style table:\n\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  topclass = \"Rtable1-zebra\",\n  render = render_mean,\n  render.strat = render_strat,\n  footnote = \"Source: palmerpenguins\"\n)\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\nOverall\n\nSource: palmerpenguins\n\n\n\n\nFlipper Length (mm)\n190.0\n195.8\n217.2\n200.9\n\n\nBody Mass (g)\n3700.7\n3733.1\n5076.0\n4201.8\n\n\n\n\n\n\nBecause these built-in styles are all CSS classes, you can apply more than one to your table. For example, if I want a table with zebra-style stripes, a shaded header bar, and text in Times New Roman font, I can specify topclass = \"Rtable1-zebra Rtable1-shade Rtable1-times\" and get the desired result:\n\ntable1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  topclass = \"Rtable1-zebra Rtable1-shade Rtable1-times\",\n  render = render_mean,\n  render.strat = render_strat,\n  footnote = \"Source: palmerpenguins\"\n)\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\nOverall\n\nSource: palmerpenguins\n\n\n\n\nFlipper Length (mm)\n190.0\n195.8\n217.2\n200.9\n\n\nBody Mass (g)\n3700.7\n3733.1\n5076.0\n4201.8\n\n\n\n\n\n\n\n\nUsing custom CSS\n\nSigned with the devil  Signed with the devil  Signed with the devil, oh      –The Pretty Reckless\n\nIn everyday data analysis work the build-in style classes that come with the table1 package are good enough to create pretty outputs. But sometimes they are not. A client or a journal might want a table to be formatted in a very specific style, and at that point you’re going to have to write your own CSS code. I have a love/hate relationship with CSS. It’s such a powerful tool for styling HTML objects, but somehow it never feels natural to me and I feel like I’m making a pact with dark powers every time I use. Unfortunately I’m at the point in the post where I have to deal with demonic forces. Let’s just hope we all come through this unscathed yeah?\nTo help with this disussion I’ll start by creating a table, but instead of printing it to the output, I’ll assign it to a variable called tbl. As you can see from the code below, I’ve specified topclass = \"mytable\" so that I can write some CSS that will be applied only to this table (or, I suppose, any other table that has CSS class mytable, but I’m only going to make one):\n\ntbl &lt;- table1(\n  x = ~ flipper_length_mm + body_mass_g | species,\n  data = penguins,\n  topclass = \"mytable\",\n  render = render_mean,\n  render.strat = render_strat,\n  footnote = \"Source: palmerpenguins\"\n)\n\nNext, if we want to write some CSS that will target this table, it helps a great deal to be able to see the actual HTML associated with the tbl object. Here it is:\n\ncat(as.character(tbl))\n\n&lt;table class=\"mytable\"&gt;\n&lt;thead&gt;\n&lt;tr&gt;\n&lt;th class='rowlabel firstrow lastrow'&gt;&lt;/th&gt;\n&lt;th class='firstrow lastrow'&gt;&lt;span class='stratlabel'&gt;Adelie&lt;/span&gt;&lt;/th&gt;\n&lt;th class='firstrow lastrow'&gt;&lt;span class='stratlabel'&gt;Chinstrap&lt;/span&gt;&lt;/th&gt;\n&lt;th class='firstrow lastrow'&gt;&lt;span class='stratlabel'&gt;Gentoo&lt;/span&gt;&lt;/th&gt;\n&lt;th class='firstrow lastrow'&gt;&lt;span class='stratlabel'&gt;Overall&lt;/span&gt;&lt;/th&gt;\n&lt;/tr&gt;\n&lt;tfoot&gt;&lt;tr&gt;&lt;td colspan=\"5\" class=\"Rtable1-footnote\"&gt;&lt;p&gt;Source: palmerpenguins&lt;/p&gt;\n&lt;/td&gt;&lt;/tr&gt;&lt;/tfoot&gt;\n&lt;/thead&gt;\n&lt;tbody&gt;\n&lt;tr&gt;\n&lt;td class='rowlabel firstrow lastrow'&gt;Flipper Length (mm)&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;190.0&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;195.8&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;217.2&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;200.9&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td class='rowlabel firstrow lastrow'&gt;Body Mass (g)&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;3700.7&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;3733.1&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;5076.0&lt;/td&gt;\n&lt;td class='firstrow lastrow'&gt;4201.8&lt;/td&gt;\n&lt;/tr&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n\n\nThis output reveals the CSS class names associated with the specific components of the table. So, let’s suppose that the client has indicated that the table footnote needs to be in italics and – for reasons known but to god – the header text needs to be shown in hot pink. Thanks to the blood magic of CSS nesting, I can write a little snippet of CSS that specifies that for any table of CSS class mytable, the footnote should be in italics and the stratification labels should be shown in hot pink:\n.mytable {\n  .Rtable1-footnote {\n    font-style: italic;\n  }\n  .stratlabel {\n    color: hotpink\n  }\n}\nUnder the hood, I have saved this exact CSS snippet to a tiny stylesheet that is imported within this post, so when I print out the tbl object I get the desired result:\ntbl\n\n\n\n\nAdelie\nChinstrap\nGentoo\nOverall\n\nSource: palmerpenguins\n\n\n\n\nFlipper Length (mm)\n190.0\n195.8\n217.2\n200.9\n\n\nBody Mass (g)\n3700.7\n3733.1\n5076.0\n4201.8\n\n\n\n\n\nAnother day, another encounter with CSS that I have survived. I will take the victory."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#epilogue",
    "href": "posts/2024-06-21_table1/index.html#epilogue",
    "title": "Making tables in R with table1",
    "section": "Epilogue",
    "text": "Epilogue\n\nFor the ways that I hurt when I’m hiking up my skirt  For the man that I hate I’m going to hell      –The Pretty Reckless\n\nThere’s a lot I’m not saying in this post. There’s a lot of hidden detail in the table1 package, and additional tricks that you can deploy to make it work to your advantage. But a post has to end somewhere and besides, if you’ve hit the point where the tools I’ve talked about in this post can’t solve your specific problem you’re probably at the point where table1 is the wrong fit.\nIt’s never wise to try to use force."
  },
  {
    "objectID": "posts/2024-06-21_table1/index.html#footnotes",
    "href": "posts/2024-06-21_table1/index.html#footnotes",
    "title": "Making tables in R with table1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt always strikes me as a bitter failure of public policy that when someone falls sick, their first thought is always something along the lines of “can I still work?” Very few people actually love their jobs so much that they want to work through a serious illness, but the fear that the company will discard you the moment something bad happens is built into our society at a low level. If you’re not dead you work. Because capitalism.↩︎\nThe package source code is on github, and the package vignette provides a lot of useful detail that you can’t necessarily find by browsing the help files.↩︎\nThe output of a call to table1() has S3 class “table1”, and internally specifies an HTML table (more on that later). When printed in a quarto or R markdown document like this one, in the normal course of events the table1:::knit_print.table() method is called, in which case the table1 object is coerced to a data frame and the end result looks the same as a data frame would look when knitr::kable() is called. However, this is slightly different to how the table looks if you call it interactively in an R session where the S3 method called is table1:::print.table1(). Because I want the output in this post to look as close as possible to the typical output when calling table1() in a regular R session, I’ve set results = \"asis\" for all my code chunks in this document, thereby ending up with tables that look the same as the ones you see interactively in the R session.↩︎\nThe stratification variable (i.e. species) isn’t actually necessary to create a table, and if you wanted you could produce a table using a formula like ~ island + bill_length_mm. In practice, however, I’ve found that I never do this: almost every table I’ve created in real life has a stratification variable.↩︎\nThe table1 package also supports units as a separate piece of metadata via the units() function, but I have to admit I never really use that one.↩︎\nIf you’re a foolish person like I am you can also dig into the source code to find the answer, because why would I be smart and read the package vignette before reading the source code?↩︎\nIf you want a more precise answer, you can use a command like parse.abbrev.render.code(\"GMEAN\") to return the actual function that is executed whenever a \"GMEAN\" is computed during the table rendering process.↩︎\nIf you’re curious as to why I’m extracting the first element of the output in this code, try playing around with stats.default() and looking at the differences between how the output is structured for continuous versus categorical inputs.↩︎"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html",
    "href": "posts/2024-12-20_art-from-code-3/index.html",
    "title": "Art from code III: Polygon tricks",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the third post in that series.\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#semi-transparent-polygons",
    "href": "posts/2024-12-20_art-from-code-3/index.html#semi-transparent-polygons",
    "title": "Art from code III: Polygon tricks",
    "section": "Semi-transparent polygons",
    "text": "Semi-transparent polygons\nA commonly used trick in generative art is to simulate graded textures by plotting many slightly-different and mostly-transparent polygons over the top of one another. I showed an example of this at the end of the previous section, in fact. However, it was all tangled up in the discussion of fractals and spatial noise patterns, so it might be useful to revisit it here.\nIn this section I’m going to adapt the recursive polygon-deformation technique described in Tyler Hobbes’ guide to simulating water colour paint. It’s a simple method and works surprisingly well sometimes. The approach I take here isn’t precisely identical to his, but it’s pretty close.\nLet’s start by creating a square tibble that contains x and y columns specifying the coordinates for a square, and a seg_len column that specifies the length of that of the edge connecting that point to the next one (i.e., the point specified by the next row):\n\nsquare &lt;- tibble(\n  x = c(0, 1, 1, 0, 0),\n  y = c(0, 0, 1, 1, 0),\n  seg_len = c(1, 1, 1, 1, 0)\n)\n\nThis representation defines a closed path: the fifth and final point is the same location as the first one. You don’t technically need this for geom_polygon(), but it’s convenient for other reasons to set it up so that the final “segment” has length 0.\nNext let’s write a simple plotting function to display a polygon:\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(color = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(size = 2)\n  }\n  return(pic)\n}\n\nshow_polygon(square)\n\n\n\n\n\n\n\n\nYes, that is indeed a square.\nThe next step in our process is to think about ways that we can deform this polygon. A simple method would be to insert a new vertex: we select one of the edges and split it in half by creating a new point in between the two endpoints. If we then add a little noise to perturb the location of the new point, the polygon will be slightly deformed.\nHow should we select the edge to break in two? One possibility is to select completely at random, but I’m going to try something slightly different and choose edges with probability proportional to their length. A bias to break longer edges will help ensure we don’t end up with polygons with one or two very long edges and many tiny edges. Here’s a function that does this:\n\nsample_edge &lt;- function(polygon) {\n  sample(nrow(polygon), 1, prob = polygon$seg_len)\n}\n\nAs a side bonus, this algorithm will never select the “edge” that starts with the final point (e.g., the “fifth” point in square never gets selected) because the corresponding edge has length zero. Thanks to this we can safely assume that no matter which row gets selected by sample_edge(), it can’t be the last one. For every possible row ind it can return, there will always be a row ind + 1 in the polygon.\nNext step is to realise that if we break an edge into two edges, we’ll need to compute the length of these two new edges: so we might as well have a helper function that takes the co-ordinates of two points as input, and returns the length of an edge connecting them.\n\nedge_length &lt;- function(x1, y1, x2, y2) {\n  sqrt((x1 - x2)^2 + (y1 - y2)^2)\n}\n\nFinally, as a convenience, here’s a function that takes a size argument and returns a random number between -size/2 and size/2. It’s just a wrapper around runif() but I find it helps me remember why I’m using the random number generator and it makes my code a little easier for me to read:\n\nedge_noise &lt;- function(size) {\n  runif(1, min = -size/2, max = size/2)\n}\n\nNow that I’ve got my helper functions, here’s the code for an insert_edge() function that selects an edge and breaks it into two edges. In addition to expecting a polygon as input (a tibble like square that has columns x, y, and seg_len), it takes a noise argument: a number used to scale the amount of noise added when edge_noise() is called:\n\ninsert_edge &lt;- function(polygon, noise) {\n  \n  # sample and edge and remember its length\n  ind &lt;- sample_edge(polygon)\n  len &lt;- polygon$seg_len[ind]\n  \n  # one endpoint of the old edge\n  last_x &lt;- polygon$x[ind]\n  last_y &lt;- polygon$y[ind]\n  \n  # the other endpoint of the old edge\n  next_x &lt;- polygon$x[ind + 1]\n  next_y &lt;- polygon$y[ind + 1]\n  \n  # location of the new point to be inserted: noise \n  # is scaled proportional to the length of the old edge\n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  # the new row for insertion into the tibble, \n  # containing coords and length of the 'new' edge\n  new_row &lt;- tibble(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  # update the length of the 'old' edge\n  polygon$seg_len[ind] &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  # insert a row into the tibble\n  bind_rows(\n    polygon[1:ind, ],\n    new_row,\n    polygon[-(1:ind), ]\n  )\n}\n\nHere’s the function in action:\nset.seed(2)\npolygon &lt;- square \npolygon &lt;- insert_edge(polygon, noise = .5); show_polygon(polygon)\npolygon &lt;- insert_edge(polygon, noise = .5); show_polygon(polygon)\npolygon &lt;- insert_edge(polygon, noise = .5); show_polygon(polygon)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ve no intention of manually calling insert_edge() over and over, so the time has come to write a grow_polygon() function that sequentially inserts edges into a polygon for a fixed number of iterations, and at a specific noise level. I’ll also set it up so the user can optionally elect to specify the seed used to generate random numbers. If the user doesn’t specify a seed, the random number generator state is left as-is:\n\ngrow_polygon &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge(polygon, noise)\n  return(polygon)\n}\n\nThe images below show what our recursively deformed polygon looks like after 30, 100, and 1000 iterations:\nsquare |&gt; \n  grow_polygon(iterations = 30, noise = .5, seed = 2) |&gt; \n  show_polygon(show_vertices = FALSE)\nsquare |&gt; \n  grow_polygon(iterations = 100, noise = .5, seed = 2) |&gt; \n  show_polygon(show_vertices = FALSE)\nsquare |&gt; \n  grow_polygon(iterations = 1000, noise = .5, seed = 2) |&gt; \n  show_polygon(show_vertices = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have functions grow_polygon() and show_polygon() that will create and display a single deformed polygon, let’s generalise them. The grow_multipolygon() function below creates many deformed polygons by calling grow_polygon() repeatedly, and the show_multipolygon() function is a minor variation on show_polygon() that plots many polygons with a low opacity:\n\ngrow_multipolygon &lt;- function(base_shape, n, seed = NULL, ...) {\n  if(!is.null(seed)) set.seed(seed)\n  polygons &lt;- list()\n  for(i in 1:n) {\n    polygons[[i]] &lt;- grow_polygon(base_shape, ...)\n  }\n  polygons &lt;- bind_rows(polygons, .id = \"id\")\n  polygons\n}\n\nshow_multipolygon &lt;- function(polygon, fill, alpha = .02, ...) {\n  ggplot(polygon, aes(x, y, group = id)) +\n    geom_polygon(colour = NA, alpha = alpha, fill = fill, ...) + \n    coord_equal() + \n    theme_void()\n}\n\nSo now here’s what we do. We take the original square and deform it a moderate amount. Running grow_polygon() for about 100 iterations seems to do the trick. This then becomes the base_shape to be passed to grow_multipolygon(), which we then use to create many polygons (say, n = 50) that are all derived from this base shape. Finally, we use show_multipolygon() to plot all 50 polygons. Each individual polygon is plotted with very low opacity, so the overall effect is to create a graded look:\n\ntic()\ndat &lt;- square |&gt; \n  grow_polygon(iterations = 100, noise = .5, seed = 2) |&gt;\n  grow_multipolygon(n = 50, iterations = 1000, noise = 1, seed = 2)\ntoc()\n\n38.787 sec elapsed\n\nshow_multipolygon(dat, fill = \"#d43790\")\n\n\n\n\n\n\n\n\nIt’s a little slow to produce results, but at least the results are pretty!\n\n\n\n\n\n\nExercise\n\n\n\n\nLet’s look at single polygons first. All the code you need to work with those is included in the grow-polygon.R script in the materials. Try modifying the iterations, noise, and seed arguments to see what kind of output is created at different parameter values.\nThe grow-multipolygons.R file contains the extra machinery to create these textured plots. Try playing around with the code for this. I’ve tweaked the parameter settings so that it runs faster than the code shown here, but doesn’t produce output that looks quite as nice."
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#growing-polygons-faster",
    "href": "posts/2024-12-20_art-from-code-3/index.html#growing-polygons-faster",
    "title": "Art from code III: Polygon tricks",
    "section": "Growing polygons faster",
    "text": "Growing polygons faster\nAs an aside, you may have noticed that the code I’ve written here is inefficient: I’ve got vectors growing in a loop, which is very inefficient in R. There’s a few ways we could speed this up. The most time consuming would be to rewrite the resource intensive loops in C++ and then call it from R using a package like Rcpp or cpp11. I’ll show an example of this technique later in the workshop, but in this case I’ll do something a little simpler.\nThe big problem with the previous code is that I’ve got atomic vectors (numeric vectors in this case) growing inside the loop, which tends to cause the entire vector to be copied at every iteration. One solution to this is to store each point as its own list, and treat the polygon as a list of points. That way, when I modify the polygon to add a new point, R will alter the container object (the list), but the objects representing the points themselves don’t get copied. Happily, only a few minor modifications of the code are needed to switch to this “list of points” representation:\n\nsquare_l &lt;- transpose(square)\n\nsample_edge_l &lt;- function(polygon) {\n  sample(length(polygon), 1, prob = map_dbl(polygon, ~ .x$seg_len))\n}\n\ninsert_edge_l &lt;- function(polygon, noise) {\n  \n  ind &lt;- sample_edge_l(polygon)\n  len &lt;- polygon[[ind]]$seg_len\n  \n  last_x &lt;- polygon[[ind]]$x\n  last_y &lt;- polygon[[ind]]$y\n  \n  next_x &lt;- polygon[[ind + 1]]$x\n  next_y &lt;- polygon[[ind + 1]]$y\n  \n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  new_point &lt;- list(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  polygon[[ind]]$seg_len &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  c(\n    polygon[1:ind],\n    list(new_point),\n    polygon[-(1:ind)]\n  )\n}\n\ngrow_polygon_l &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge_l(polygon, noise)\n  return(polygon)\n}\n\ngrow_multipolygon_l &lt;- function(base_shape, n, seed = NULL, ...) {\n  if(!is.null(seed)) set.seed(seed)\n  polygons &lt;- list()\n  for(i in 1:n) {\n    polygons[[i]] &lt;- grow_polygon_l(base_shape, ...) |&gt;\n      transpose() |&gt;\n      as_tibble() |&gt;\n      mutate(across(everything(), unlist))\n  }\n  polygons &lt;- bind_rows(polygons, .id = \"id\")\n  polygons\n}\n\nThat’s a fairly large code chunk, but if you compare each part to the earlier versions you can see that these functions have almost the same structure as the original ones. Most of the changes are little changes to the indexing, like using polygon[[ind]]$x to refer to coordinate rather than polygon$x[ind].\nThe code to generate images using the list-of-points version is almost identical to the original version. All we’re doing differently is using square_l, grow_polygon_l(), and grow_multipolygon_l() where previously we’d used square, grow_polygon(), and grow_multipolygon():\n\ntic()\ndat &lt;- square_l |&gt; \n  grow_polygon_l(iterations = 100, noise = .5, seed = 2) |&gt;\n  grow_multipolygon_l(n = 50, iterations = 1000, noise = 1, seed = 2) \ntoc()\n\n31.624 sec elapsed\n\n\nThat’s a pretty substantial improvement in performance relative to the original version, with only very minor rewriting of the code. And yes, it does produce the same result:\n\nshow_multipolygon(dat, fill = \"#d43790\")"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#using-the-method-splotches",
    "href": "posts/2024-12-20_art-from-code-3/index.html#using-the-method-splotches",
    "title": "Art from code III: Polygon tricks",
    "section": "Using the method: splotches",
    "text": "Using the method: splotches\nOkay, so that’s the method. What I generally find when making art is that it’s a little awkward to play around and explore when it takes a long time to render pieces, so it’s handy to have a version of your generative art tools that will quickly produce results, even if those results aren’t quite as nice. It’s a little like having the ability to make rough sketches: something you can do easily before committing to doing something in detail. With that in mind, the splotch() function below wraps a slightly cruder version of the method than the one I showed earlier. It generates fewer polygons, and those polygons have fewer vertices.\n\nsplotch &lt;- function(seed, layers = 10) {\n  set.seed(seed)\n  square_l &lt;- transpose(tibble(\n    x = c(0, 1, 1, 0, 0),\n    y = c(0, 0, 1, 1, 0),\n    seg_len = c(1, 1, 1, 1, 0)\n  ))\n  square_l |&gt; \n    grow_polygon_l(iterations = 10, noise = .5, seed = seed) |&gt;\n    grow_multipolygon_l(n = layers, iterations = 500, noise = .8, seed = seed) \n}\n\nThe results aren’t quite as nice as the full fledged version, but they are fast:\n\ntic()\nsplotch_1 &lt;- splotch(seed = 12) \nsplotch_2 &lt;- splotch(seed = 34)\nsplotch_3 &lt;- splotch(seed = 56)\nsplotch_4 &lt;- splotch(seed = 78)\ntoc()\n\n6.09 sec elapsed\n\n\nBecause splotch() is fast and a little crude, it can be a handy way to explore colour choices:\nshow_multipolygon(splotch_1, \"#f51720\", alpha = .2)\nshow_multipolygon(splotch_2, \"#f8d210\", alpha = .2)\nshow_multipolygon(splotch_3, \"#059dc0\", alpha = .2)\nshow_multipolygon(splotch_4, \"#81b622\", alpha = .2)"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#using-the-method-smudged-hexagons",
    "href": "posts/2024-12-20_art-from-code-3/index.html#using-the-method-smudged-hexagons",
    "title": "Art from code III: Polygon tricks",
    "section": "Using the method: Smudged hexagons",
    "text": "Using the method: Smudged hexagons\nThe goal of splotch() is to have a tool we can play around with and explore the method. That’s nice and all, but can we also use the method to make something fun? Here’s one example: since we are R users and love our hexagons, let’s write a function that paints hexagons using this recursive deformation method. The goal is to create a shape with a naturalistic look, as if it had been painted or coloured, with some of the edges smudged or blurred. The smudged_hexagon() function attempts to do that:\n\nsmudged_hexagon &lt;- function(seed, noise1 = 0, noise2 = 2, noise3 = 0.5) {\n  set.seed(seed)\n  \n  # define hexagonal base shape\n  theta &lt;- (0:6) * pi / 3\n  hexagon &lt;- tibble(\n    x = sin(theta),\n    y = cos(theta),\n    seg_len = edge_length(x, y, lead(x), lead(y))\n  )\n  hexagon$seg_len[7] &lt;- 0\n  hexagon &lt;- transpose(hexagon)\n  base &lt;- hexagon |&gt; \n    grow_polygon_l(\n      iterations = 60, \n      noise = noise1\n    )\n  \n  # define intermediate-base-shapes in clusters\n  polygons &lt;- list()\n  ijk &lt;- 0\n  for(i in 1:3) {\n    base_i &lt;- base |&gt; \n      grow_polygon_l(\n        iterations = 50, \n        noise = noise2\n      )\n    \n    for(j in 1:3) {\n      base_j &lt;- base_i |&gt; \n        grow_polygon_l(\n          iterations = 50, \n          noise = noise2\n        )\n      \n      # grow 10 polygons per intermediate-base\n      for(k in 1:10) {\n        ijk &lt;- ijk + 1\n        polygons[[ijk]] &lt;- base_j |&gt;\n          grow_polygon_l(\n            iterations = 500, \n            noise = noise3\n          ) |&gt;\n          transpose() |&gt;\n          as_tibble() |&gt;\n          mutate(across(.fn = unlist))\n      }\n    }\n  }\n  \n  # return as data frame\n  bind_rows(polygons, .id = \"id\")\n}\n\nHere it is in action:\n\ntic()\ndat &lt;- smudged_hexagon(seed = 1)\ntoc()\n\n20.272 sec elapsed\n\ndat |&gt; show_multipolygon(fill = \"#d4379005\")\n\n\n\n\n\n\n\n\nsmudged_hexagon(seed = 11) |&gt; show_multipolygon(fill = \"#d4379005\")\nsmudged_hexagon(seed = 44) |&gt; show_multipolygon(fill = \"#d4379005\")\nsmudged_hexagon(seed = 88) |&gt; show_multipolygon(fill = \"#d4379005\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndat &lt;- bind_rows(\n  smudged_hexagon(seed = 11),\n  smudged_hexagon(seed = 44),\n  smudged_hexagon(seed = 88),\n  .id = \"source\"\n) |&gt;\n  mutate(\n    id = paste(id, source),\n    x = x + as.numeric(source)\n  ) |&gt;\n  arrange(id)\n\nggplot(dat, aes(x, y, group = id, fill = factor(source))) +\n  geom_polygon(alpha = .02, show.legend = FALSE) + \n  theme_void() + \n  scale_fill_manual(values = c(\n    \"#ff1b8d\", \"#ffda00\", \"#1bb3ff\"\n  )) +\n  coord_equal() \n\n\n\n\n\n\n\n\nThis one makes me happy :-)\n\n\n\n\n\n\nExercise\n\n\n\nCode for these two systems is included in the splotch.R and smudged-hexagon.R scripts in the materials.\n\nExplore the effect of the layers argument in the splotch() system, and how it interacts with the alpha argument to show_multipolygon()\nCreate a modified version of the smudged_hexagon() system that creates a smudged triangle, or, if you’re feeling more ambitious and want to read slightly further down the page to look for the code you need, a smudged heart shape!\nIn the “layered” smudged hexagon example (the one with three hexagons of different colours), why did I include the calls to mutate() and arrange()? What would have happened without that code?"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#slightly-misshapen-objects",
    "href": "posts/2024-12-20_art-from-code-3/index.html#slightly-misshapen-objects",
    "title": "Art from code III: Polygon tricks",
    "section": "Slightly misshapen objects",
    "text": "Slightly misshapen objects\nThe second case of polygon trickery that I want to talk about is adapted from an example kindly shared with me by Will Chase. Will posted some code on twitter showing how to very gently deform the outline of a shape to give it a slightly hand drawn look, and I’ll expand on that example here. Let’s suppose I want to draw the outline of a heart. I do a little googling and discover some formulas that I can use for that purpose. If I have a vector describing the angle around circle from 0 to 2\\(\\pi\\), I can compute the x- and y-coordinates for a heart shape using these functions:\n\nheart_x &lt;- function(angle) {\n  x &lt;- (16 * sin(angle) ^ 3) / 17\n  return(x - mean(x))\n}\n\nheart_y &lt;- function(angle) {\n  y &lt;- (13 * cos(angle) - 5 * cos(2 * angle) - 2 * cos(3 * angle) -\n          cos(4 * angle)) / 17\n  return(y - mean(y))\n}\n\nHere’s what it looks like when I draw a heart using these formulas:\n\nheart_shape &lt;- tibble(\n  angle = seq(0, 2 * pi, length.out = 50),\n  x = heart_x(angle),\n  y = heart_y(angle)\n)\nshow_polygon(heart_shape)\n\n\n\n\n\n\n\n\nI use hearts drawn with these formulas quite frequently in my art. They’re easy to compute, the shape often produces interesting patterns when other processes are applied to it, and of course it’s meaningfully associated with positive emotions and affection! However, the problem with using this formula is that the hearts it draws are very precise and mechanical. Sometimes that’s fine: precise, crisp shapes are often exactly the look we’re going for. But other times we might want an outline that looks a little more naturalistic. For instance, I asked my 9 year old daughter to draw a few heart shapes for me that I could use as an example. Here’s what she drew:\n\nknitr::include_graphics(\"hand-drawn-hearts.jpg\")\n\n\n\n\n\n\n\n\nSetting aside the fact that in one case she decided that she actually wanted to draw a frog face rather than a heart – unlike DALL-E, humans have a tendency to flat out refuse to follow the text prompts when you ask them to make art for you – these hearts have a qualitatively different feel to the crisp and clean look of the artificial ones.\nWhat we’d like to do is gently and smoothly deform the outline of the original shape to produce something that captures some of the naturalistic feel that the hand-drawn hearts have. As always we’re not going to try to perfectly reproduce all the features of the original, just capture “the vibe”."
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#perlin-blobs",
    "href": "posts/2024-12-20_art-from-code-3/index.html#perlin-blobs",
    "title": "Art from code III: Polygon tricks",
    "section": "Perlin blobs",
    "text": "Perlin blobs\nLet’s start with a slightly simpler version of the problem: instead of deforming a heart shape we’ll deform a circle using Perlin noise. Our base shape is a circle that looks like this:\n\ncircle &lt;- tibble(\n  angle = seq(0, 2*pi, length.out = 50),\n  x = cos(angle),\n  y = sin(angle)\n)\nshow_polygon(circle)\n\n\n\n\n\n\n\n\nWe can create gently distorted circles using the perlin_blob() function shown below. Here’s how it works. First it defines coordinates in the shape of a perfect circle (that’s the variables x_base and y_base). Then we use gen_perlin() to calculate some spatially varying noise at each of those co-ordinates. Or, more precisely, we generate fractal noise at those coordinates using gen_perlin() as the generator and fbm() as the fractal function, but that’s not a super important detail rignt now. What is important is to realise that, although we want to use the numbers returned by our fractal generator to slightly modify the radius of the circle at that location, those numbers can be negative. So we’ll rescale them using the helper function normalise_radius() so that the minimum distance from the origin is r_min and the maximum distance from the origin is r_max. This rescaling helps to ensure that the output is regular.\nIn any case, after computing the (Perlin-noise distorted) radius associated with each coordinate, we compute the final x and y values for the “Perlin blob” by multiplying the coordinates of the base shape by the radius. Here’s the code:\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_blob &lt;- function(n = 100, \n                        freq_init = 0.3,\n                        octaves = 2, \n                        r_min = 0.5, \n                        r_max = 1) {\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    x_base = cos(angle),\n    y_base = sin(angle),\n    radius = fracture(\n      x = x_base, \n      y = y_base, \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    x = radius * x_base,\n    y = radius * y_base\n  )\n}\n\nHere are three outputs from our perlin_blob() function:\nset.seed(1); perlin_blob() |&gt; show_polygon(FALSE)\nset.seed(2); perlin_blob() |&gt; show_polygon(FALSE)\nset.seed(3); perlin_blob() |&gt; show_polygon(FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo give you a feel for how this function behaves, here’s a few images showing the effect of changing the freq_init parameter. This argument is used to set the overall noise level when generating fractal noise patterns:\nset.seed(1); perlin_blob(freq_init = .2) |&gt; show_polygon(FALSE)\nset.seed(1); perlin_blob(freq_init = .4) |&gt; show_polygon(FALSE)\nset.seed(1); perlin_blob(freq_init = .8) |&gt; show_polygon(FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe effect of the radius parameters is slightly different to the effect of the noise parameter. Shifting the r_min and r_max arguments has the effect of “globally flattening” the pattern of variation because the overall shape can only vary within a narrow bound. But it’s quite possible to set a high value for freq_init (causing noticeable distortions to the radius to emerge even at small scales) while constraining the global shape to be almost perfectly circular. The result is a rough-edged but otherwise perfect circle:\nset.seed(1); \nperlin_blob(\n  n = 1000,\n  freq_init = 10, \n  r_min = .95, \n  r_max = 1\n) |&gt; \n  show_polygon(FALSE)\n\n\n\n\n\n\n\n\n\n\n\nAt these parameter settings the output of perlin_blob() reminds me more of a cookie shape than a hand-drawn circle. I’ve never used those settings in art before, but I can imagine some tasty applications!\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the perlin-blob.R script in the materials. You can also find analogous code for the Perlin heart system describe in in the next section in the perlin-heart.R script.\n\nTry playing around with these two systems. See if you can reproduce the same “qualitative variations” with perlin_heart() that I showed above with perlin_blob()\nExplore the effect of modifying the arguments. It should be possible to produce some very wild looking shapes!"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#perlin-hearts",
    "href": "posts/2024-12-20_art-from-code-3/index.html#perlin-hearts",
    "title": "Art from code III: Polygon tricks",
    "section": "Perlin hearts",
    "text": "Perlin hearts\nModifying this system so that it draws distorted heart shapes rather than distorted circles is not too difficult. There’s a few different ways we can do this, but the way I find most pleasing is to start with a distorted circle and then apply the heart_x() and heart_y() transformations:\n\nperlin_heart &lt;- function(n = 100, \n                         freq_init = 0.3,\n                         octaves = 2, \n                         r_min = 0.5, \n                         r_max = 1,\n                         x_shift = 0,\n                         y_shift = 0,\n                         id = NA,\n                         seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    x_base = cos(angle),\n    y_base = sin(angle),\n    radius = fracture(\n      x = x_base, \n      y = y_base, \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    id = id\n  )\n}\n\nHere are three outputs from our perlin_heart() function:\nperlin_heart(seed = 1) |&gt; show_polygon(FALSE)\nperlin_heart(seed = 2) |&gt; show_polygon(FALSE)\nperlin_heart(seed = 3) |&gt; show_polygon(FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of my favourite systems is a very simple one that draws many of these Perlin hearts on a grid, filling each one with a colour selected from a randomly sampled palette. To replicate that here I’ll need a palette generator and once again I’ll fall back on our old favourite sample_canva()\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nNow that we have a palette generator we can use the functional programming toolkit from purrr to do the work for us. In this case I’m using pmap_dfr() to call the perlin_heart() at a variety of different settings. I’ve included the x_shift, y_shift and id values among the settings to make it a little easier to plot the data:\nperlin_heart_grid &lt;- function(nx = 10, ny = 6, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  heart_settings &lt;- expand_grid(\n    r_min = .3, \n    r_max = .4, \n    x_shift = 1:nx, \n    y_shift = 1:ny\n  ) |&gt;\n    mutate(id = row_number()) \n  \n  heart_data &lt;-  pmap_dfr(heart_settings, perlin_heart)\n  \n  heart_data |&gt;\n    ggplot(aes(x, y, group = id, fill = sample(id))) +\n    geom_polygon(size = 0, show.legend = FALSE) +\n    theme_void() +\n    scale_fill_gradientn(colours = sample_canva(seed)) +\n    coord_equal(xlim = c(0, nx + 1), ylim = c(0, ny + 1))\n}\n\nperlin_heart_grid(seed = 451)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included as the perlin-heart-grid.R script in the materials. To check that you understand it, try modifying it in the following ways:\n\nOnly show 50% of the hearts\nAdd a small amount of random noise to the position of each heart\nGive all the hearts the “rough edged biscuit” look\n\n\n\nWe can elaborate on this idea in various ways. For example, the perlin_heart2() function shown below modifies the original idea by adding a additional width variable computed in a similar way to radius:\n\nperlin_heart2 &lt;- function(n = 100, \n                          freq_init = 0.3,\n                          octaves = 2, \n                          r_min = 0.5, \n                          r_max = 1,\n                          w_min = 0,\n                          w_max = 4,\n                          rot = 0,\n                          x_shift = 0,\n                          y_shift = 0,\n                          id = NA,\n                          seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    \n    radius = fracture(\n      x = cos(angle), \n      y = sin(angle), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    \n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    \n    width = fracture(\n      x = cos(angle + rot), \n      y = sin(angle + rot), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise(to = c(w_min, w_max)),\n    \n    id = id\n  )\n}\n\nHere are three outputs from our perlin_heart2() function, showing the effect of varying the rot parameter. Because the width of outline varies, rot causes the whole pattern of variable thickness to rotate around the heart. As you might imagine, this is going to turn out to be very handy in a moment when we start animating these things!\nshow_width &lt;- function(polygon) {\n  ggplot(polygon, aes(x, y, size = width)) +\n    geom_path(fill = NA, show.legend = FALSE) + \n    coord_equal() + \n    scale_size_identity() +\n    theme_void()\n}\n\nperlin_heart2(n = 1000, rot = 0, seed = 2) |&gt; show_width()\nperlin_heart2(n = 1000, rot = pi / 2, seed = 2) |&gt; show_width()\nperlin_heart2(n = 1000, rot = pi, seed = 2) |&gt; show_width()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere’s an example where I plot several hearts at once courtesy of the magic of pmap_dfr():\nperlin_heart_grid2 &lt;- function(nx = 4, ny = 2, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  heart_settings &lt;- expand_grid(\n    r_min = .3, \n    r_max = .4, \n    w_min = .01,\n    w_max = 6,\n    x_shift = 1:nx, \n    y_shift = 1:ny\n  ) |&gt;\n    mutate(\n      n = 200,\n      x_shift = x_shift + runif(n(), -.1, .1),\n      y_shift = y_shift + runif(n(), -.1, .1),\n      rot = runif(n(), -.1, .1),\n      id = row_number()\n    ) \n  \n  heart_data &lt;- pmap_dfr(heart_settings, perlin_heart2)\n  \n  heart_data |&gt;\n    ggplot(aes(x, y, group = id, colour = sample(id), size = width)) +\n    geom_path(show.legend = FALSE) +\n    theme_void() +\n    scale_size_identity() +\n    scale_colour_gradientn(colours = sample_canva(seed)) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    coord_fixed(xlim = c(0, nx + 1), ylim = c(0, ny + 1))\n}\n\nperlin_heart_grid2(seed = 666)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included as the perlin-heart-grid-2.R script in the materials. Can you modify it so that each heart is plotted in a single colour rather than in the banded pattern? With different hearts having different colours?"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#animated-perlin-hearts",
    "href": "posts/2024-12-20_art-from-code-3/index.html#animated-perlin-hearts",
    "title": "Art from code III: Polygon tricks",
    "section": "Animated perlin hearts",
    "text": "Animated perlin hearts\nThe final example for this session uses the gifsky package to create an animated version of the variable-width hearts from the last section, by “rotating” or “sliding” the variable-with curves along the contours of the Perlin hearts. The design of the functions in this system is very similar in spirit to that adopted in the static systems. The main difference is that the output is created by calling the save_gif() function. We pass it an expression that, in the normal course of events, would create many plots – that’s what the generate_all_frames() function does – and it captures these plots and turns them into a single animated gif:\n\nperlin_heart_data &lt;- function(nhearts = 10, scatter = .05, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  palette &lt;- sample_canva(seed) |&gt;\n    (\\(x) colorRampPalette(x)(nhearts))()\n  \n  heart_settings &lt;- tibble(\n    id = 1:nhearts,\n    n = 500,\n    r_min = .35, \n    r_max = .4,\n    w_min = -10, \n    w_max = 10,\n    x_shift = runif(nhearts, -scatter/2, scatter/2),\n    y_shift = runif(nhearts, -scatter/2, scatter/2),\n    rot = runif(nhearts, -pi, pi)\n  )\n  \n  heart_settings |&gt;\n    pmap_dfr(perlin_heart2) |&gt;\n    group_by(id) |&gt;\n    mutate(\n      shade = sample(palette, 1),\n      width = abs(width)\n    )\n}\n\ngenerate_one_frame &lt;- function(dat) {\n  \n  pic &lt;- dat |&gt;\n    ggplot(aes(x, y, group = id, size = width, colour = shade)) +\n    geom_path(show.legend = FALSE) +\n    theme_void() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_colour_identity() +\n    scale_size_identity() +\n    coord_fixed(xlim = c(-.6, .6), ylim = c(-.6, .6))\n  \n  print(pic)\n}\n\nrotate_vector &lt;- function(x, percent) {\n  \n  len &lt;- length(x)\n  ind &lt;- ceiling(len * percent)\n  if(ind == 0) return(x)\n  if(ind == len) return(x)\n  c(x[(ind+1):len], x[1:ind])\n}\n\ngenerate_all_frames &lt;- function(dat, nframes = 100) {\n  \n  for(frame in 1:nframes) {\n    dat |&gt;\n      group_by(id) |&gt;\n      mutate(width = width |&gt; rotate_vector(frame / nframes)) |&gt;\n      generate_one_frame()\n  }\n}\n\nanimated_perlin_heart &lt;- function(seed, ...) {\n  \n  save_gif(\n    expr = perlin_heart_data(seed = seed, ...) |&gt; generate_all_frames(),\n    gif_file = paste0(\"animated-perlin-heart-\", seed, \".gif\"),\n    height = 1000,\n    width = 1000,\n    delay = .1,\n    progress = TRUE,\n    bg = \"#ffffff\"\n  )\n  invisible(NULL)\n}\n\ntic()\nanimated_perlin_heart(seed = 100)\ntoc()\n\n12.868 sec elapsed\n\nknitr::include_graphics(\"animated-perlin-heart-100.gif\") \n\n\n\n\n\n\n\n\nanimated_perlin_heart(seed = 123)\nanimated_perlin_heart(seed = 456)\nanimated_perlin_heart(seed = 789)\n\nknitr::include_graphics(\"animated-perlin-heart-123.gif\")\nknitr::include_graphics(\"animated-perlin-heart-456.gif\")\nknitr::include_graphics(\"animated-perlin-heart-789.gif\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the perlin-heart-animated.R script in the materials. Personally I just find this one fun to play with, so instead of setting an exercise I’ll let you take a look!"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#textured-lines",
    "href": "posts/2024-12-20_art-from-code-3/index.html#textured-lines",
    "title": "Art from code III: Polygon tricks",
    "section": "Textured lines",
    "text": "Textured lines\n\nlibrary(e1071)\n\nThere’s one other topic I want to mention in this post. To motivate the topic, I’ll start by writing a function that uses statistical tools to generate random smooth curves in two dimensions:\n\nsmooth_loess &lt;- function(x, span) {\n  n &lt;- length(x)\n  dat &lt;- tibble(time = 1:n, walk = x)\n  mod &lt;- loess(walk ~ time, dat, span = span)\n  predict(mod, tibble(time = 1:n))\n}\n\nsmooth_path &lt;- function(n = 1000, smoothing = .4, seed = NULL) { \n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    x = smooth_loess(rbridge(1, n), span = smoothing),\n    y = smooth_loess(rbridge(1, n), span = smoothing),\n    stroke = 1\n  )\n}\n\nHere’s an example of the paths it produces:\n\npath &lt;- smooth_path(seed = 123)\n\npath |&gt; \n  ggplot(aes(x, y)) +\n  geom_path(size = 2) + \n  coord_equal() +\n  theme_void() \n\n\n\n\n\n\n\n\nThe path it self is smooth but slightly misshapen (i.e., it doesn’t feel “precise” in the same way that the very first heart felt precise), and you can imagine creating a generative art system that uses this kind of technique, but it doesn’t feel hand drawn. The problem here is that while the path feels fairly natural, the stroke itself is too perfect. It’s a solid line with no texture or grading to it. That spoils the illusion of naturalness to an extent.\nIt’s not too difficult to improve on this if, instead of plotting one smooth curve to represent the path, we plot a very large number of points or small segments with irregular breaks and spacing. In this section I won’t go into a lot of detail on design choices and the various ways you can do this, but I’ll mention that Ben Kovach has a lovely post on making generative art feel natural that discusses this in more detail.\nFor now, I’ll limit myself to presenting some code for a system that implements this idea:\n\nperturb &lt;- function(path, noise = .01, span = .1) {\n  path |&gt; \n    group_by(stroke) |&gt;\n    mutate(\n      x = x + rnorm(n(), 0, noise),\n      y = y + rnorm(n(), 0, noise),\n      x = smooth_loess(x, span),\n      y = smooth_loess(y, span),\n      alpha = runif(n()) &gt; .5,\n      size = runif(n(), 0, .2)\n    )\n}\n\nbrush &lt;- function(path, bristles = 100, seed = 1, ...) {\n  set.seed(seed)\n  dat &lt;- list()\n  for(i in 1:bristles) {\n    dat[[i]] &lt;- perturb(path, ...)\n  }\n  return(bind_rows(dat, .id = \"id\"))\n}\n\nstroke &lt;- function(dat, geom = geom_path, colour = \"black\", ...) {\n  dat |&gt;  \n    ggplot(aes(\n      x = x, \n      y = y, \n      alpha = alpha, \n      size = size, \n      group = paste0(stroke, id)\n    )) + \n    geom(\n      colour = colour, \n      show.legend = FALSE,\n      ...\n    ) + \n    coord_equal() +\n    scale_alpha_identity() +\n    scale_size_identity() +\n    theme_void()\n}\n\nThe plots below show a couple of examples of how you can apply this idea to our original curve:\npath |&gt;\n  brush() |&gt;\n  stroke()\npath |&gt;\n  brush(bristles = 200, span = .08) |&gt;\n  mutate(size = size * 3) |&gt;\n  stroke(geom = geom_point, stroke = 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis doesn’t in any sense exhaust the possibilities, but I hope it’s a useful hint about how to get started if you ever find yourself trying to figure out how to draw naturalistic looking pen strokes. Also, the fact that I’ve included the code means I get to apply the idea to the Perlin hearts system:\n\nperlin_heart(n = 500, seed = 123) |&gt;\n  mutate(stroke = 1) |&gt;\n  brush(bristles = 100, noise = .02) |&gt;\n  stroke()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the textured-lines.R script. Try to use this a jumping-off point for designing your own system for generating textured lines. There’s no real reason why you should do it my way. How would you do this yourself?"
  },
  {
    "objectID": "posts/2024-12-20_art-from-code-3/index.html#materials",
    "href": "posts/2024-12-20_art-from-code-3/index.html#materials",
    "title": "Art from code III: Polygon tricks",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\ngrow-multipolygon.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nsquare &lt;- tibble(\n  x = c(0, 1, 1, 0, 0),\n  y = c(0, 0, 1, 1, 0),\n  seg_len = c(1, 1, 1, 1, 0)\n)\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, colour = \"black\", ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(fill = NA, colour = colour, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(size = 2, colour = colour)\n  }\n  return(pic)\n}\n\nsample_edge &lt;- function(polygon) {\n  sample(nrow(polygon), 1, prob = polygon$seg_len)\n}\n\nedge_length &lt;- function(x1, y1, x2, y2) {\n  sqrt((x1 - x2)^2 + (y1 - y2)^2)\n}\n\nedge_noise &lt;- function(size) {\n  runif(1, min = -size/2, max = size/2)\n}\n\ninsert_edge &lt;- function(polygon, noise) {\n  \n  # sample and edge and remember its length\n  ind &lt;- sample_edge(polygon)\n  len &lt;- polygon$seg_len[ind]\n  \n  # one endpoint of the old edge\n  last_x &lt;- polygon$x[ind]\n  last_y &lt;- polygon$y[ind]\n  \n  # the other endpoint of the old edge\n  next_x &lt;- polygon$x[ind + 1]\n  next_y &lt;- polygon$y[ind + 1]\n  \n  # location of the new point to be inserted: noise \n  # is scaled proportional to the length of the old edge\n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  # the new row for insertion into the tibble, \n  # containing coords and length of the 'new' edge\n  new_row &lt;- tibble(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  # update the length of the 'old' edge\n  polygon$seg_len[ind] &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  # insert a row into the tibble\n  bind_rows(\n    polygon[1:ind, ],\n    new_row,\n    polygon[-(1:ind), ]\n  )\n}\n\ngrow_polygon &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge(polygon, noise)\n  return(polygon)\n}\n\ngrow_multipolygon &lt;- function(base_shape, n, seed = NULL, ...) {\n  if(!is.null(seed)) set.seed(seed)\n  polygons &lt;- list()\n  for(i in 1:n) {\n    polygons[[i]] &lt;- grow_polygon(base_shape, ...)\n  }\n  polygons &lt;- bind_rows(polygons, .id = \"id\")\n  polygons\n}\n\nshow_multipolygon &lt;- function(polygon, fill, alpha = .02, ...) {\n  ggplot(polygon, aes(x, y, group = id)) +\n    geom_polygon(colour = NA, alpha = alpha, fill = fill, ...) + \n    coord_equal() + \n    theme_void()\n}\n\n# simplified version of the one in the workshop\ntic()\ndat &lt;- square |&gt; \n  grow_polygon(iterations = 100, noise = .5, seed = 2) |&gt;\n  grow_multipolygon(n = 10, iterations = 300, noise = 1, seed = 2)\npic &lt;- show_multipolygon(dat, fill = \"#d43790\", alpha = .2)\nplot(pic)\ntoc()\n\n\n\n\n\n\n\n\n\n\ngrow-polygon.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nsquare &lt;- tibble(\n  x = c(0, 1, 1, 0, 0),\n  y = c(0, 0, 1, 1, 0),\n  seg_len = c(1, 1, 1, 1, 0)\n)\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, colour = \"black\", ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(fill = NA, colour = colour, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(size = 2, colour = colour)\n  }\n  return(pic)\n}\n\nsample_edge &lt;- function(polygon) {\n  sample(nrow(polygon), 1, prob = polygon$seg_len)\n}\n\nedge_length &lt;- function(x1, y1, x2, y2) {\n  sqrt((x1 - x2)^2 + (y1 - y2)^2)\n}\n\nedge_noise &lt;- function(size) {\n  runif(1, min = -size/2, max = size/2)\n}\n\ninsert_edge &lt;- function(polygon, noise) {\n  \n  # sample and edge and remember its length\n  ind &lt;- sample_edge(polygon)\n  len &lt;- polygon$seg_len[ind]\n  \n  # one endpoint of the old edge\n  last_x &lt;- polygon$x[ind]\n  last_y &lt;- polygon$y[ind]\n  \n  # the other endpoint of the old edge\n  next_x &lt;- polygon$x[ind + 1]\n  next_y &lt;- polygon$y[ind + 1]\n  \n  # location of the new point to be inserted: noise \n  # is scaled proportional to the length of the old edge\n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  # the new row for insertion into the tibble, \n  # containing coords and length of the 'new' edge\n  new_row &lt;- tibble(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  # update the length of the 'old' edge\n  polygon$seg_len[ind] &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  # insert a row into the tibble\n  bind_rows(\n    polygon[1:ind, ],\n    new_row,\n    polygon[-(1:ind), ]\n  )\n}\n\ngrow_polygon &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge(polygon, noise)\n  return(polygon)\n}\n\n\n\n# modify this code\npic &lt;- square |&gt; \n  grow_polygon(iterations = 1000, noise = .5, seed = 2) |&gt; \n  show_polygon(show_vertices = FALSE)\n\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nperlin-blob.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(colour = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(colour = \"black\", size = 2)\n  }\n  return(pic)\n}\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_blob &lt;- function(n = 100, \n                        freq_init = 0.3,\n                        octaves = 2, \n                        r_min = 0.5, \n                        r_max = 1) {\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    x_base = cos(angle),\n    y_base = sin(angle),\n    radius = fracture(\n      x = x_base, \n      y = y_base, \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    x = radius * x_base,\n    y = radius * y_base\n  )\n}\n\nset.seed(3); \npic &lt;- perlin_blob(freq_init = .4) |&gt; show_polygon(FALSE)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nperlin-heart-animated.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\nlibrary(here)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(colour = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(colour = \"black\", size = 2)\n  }\n  return(pic)\n}\n\nheart_x &lt;- function(angle) {\n  x &lt;- (16 * sin(angle) ^ 3) / 17\n  return(x - mean(x))\n}\n\nheart_y &lt;- function(angle) {\n  y &lt;- (13 * cos(angle) - 5 * cos(2 * angle) - 2 * cos(3 * angle) -\n          cos(4 * angle)) / 17\n  return(y - mean(y))\n}\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_heart2 &lt;- function(n = 100, \n                          freq_init = 0.3,\n                          octaves = 2, \n                          r_min = 0.5, \n                          r_max = 1,\n                          w_min = 0,\n                          w_max = 4,\n                          rot = 0,\n                          x_shift = 0,\n                          y_shift = 0,\n                          id = NA,\n                          seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    \n    radius = fracture(\n      x = cos(angle), \n      y = sin(angle), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    \n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    \n    width = fracture(\n      x = cos(angle + rot), \n      y = sin(angle + rot), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise(to = c(w_min, w_max)),\n    \n    id = id\n  )\n}\n\nperlin_heart_data &lt;- function(nhearts = 10, scatter = .05, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  palette &lt;- sample_canva(seed) |&gt;\n    (\\(x) colorRampPalette(x)(nhearts))()\n  \n  heart_settings &lt;- tibble(\n    id = 1:nhearts,\n    n = 500,\n    r_min = .35, \n    r_max = .4,\n    w_min = -10, \n    w_max = 10,\n    x_shift = runif(nhearts, -scatter/2, scatter/2),\n    y_shift = runif(nhearts, -scatter/2, scatter/2),\n    rot = runif(nhearts, -pi, pi)\n  )\n  \n  heart_settings |&gt;\n    pmap_dfr(perlin_heart2) |&gt;\n    group_by(id) |&gt;\n    mutate(\n      shade = sample(palette, 1),\n      width = abs(width)\n    )\n}\n\ngenerate_one_frame &lt;- function(dat) {\n  \n  pic &lt;- dat |&gt;\n    ggplot(aes(x, y, group = id, size = width, colour = shade)) +\n    geom_path(show.legend = FALSE) +\n    theme_void() +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_colour_identity() +\n    scale_size_identity() +\n    coord_fixed(xlim = c(-.6, .6), ylim = c(-.6, .6))\n  \n  print(pic)\n}\n\nrotate_vector &lt;- function(x, percent) {\n  \n  len &lt;- length(x)\n  ind &lt;- ceiling(len * percent)\n  if(ind == 0) return(x)\n  if(ind == len) return(x)\n  c(x[(ind+1):len], x[1:ind])\n}\n\ngenerate_all_frames &lt;- function(dat, nframes = 100) {\n  \n  for(frame in 1:nframes) {\n    dat |&gt;\n      group_by(id) |&gt;\n      mutate(width = width |&gt; rotate_vector(frame / nframes)) |&gt;\n      generate_one_frame()\n  }\n}\n\nanimated_perlin_heart &lt;- function(seed, ...) {\n  \n  gif_file &lt;- paste0(\"animated-perlin-heart-\", seed, \".gif\")\n  save_gif(\n    expr = perlin_heart_data(seed = seed, ...) |&gt; generate_all_frames(),\n    gif_file = here(\"output\", gif_file),\n    height = 1000,\n    width = 1000,\n    delay = .1,\n    progress = TRUE,\n    bg = \"#222222\"\n  )\n  invisible(NULL)\n}\n\ntic()\nanimated_perlin_heart(seed = 99)\ntoc()\n\n\n\n\n\n\n\n\n\n\nperlin-heart-grid-2.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(colour = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(colour = \"black\", size = 2)\n  }\n  return(pic)\n}\n\nheart_x &lt;- function(angle) {\n  x &lt;- (16 * sin(angle) ^ 3) / 17\n  return(x - mean(x))\n}\n\nheart_y &lt;- function(angle) {\n  y &lt;- (13 * cos(angle) - 5 * cos(2 * angle) - 2 * cos(3 * angle) -\n          cos(4 * angle)) / 17\n  return(y - mean(y))\n}\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_heart2 &lt;- function(n = 100, \n                          freq_init = 0.3,\n                          octaves = 2, \n                          r_min = 0.5, \n                          r_max = 1,\n                          w_min = 0,\n                          w_max = 4,\n                          rot = 0,\n                          x_shift = 0,\n                          y_shift = 0,\n                          id = NA,\n                          seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    \n    radius = fracture(\n      x = cos(angle), \n      y = sin(angle), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    \n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    \n    width = fracture(\n      x = cos(angle + rot), \n      y = sin(angle + rot), \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise(to = c(w_min, w_max)),\n    \n    id = id\n  )\n}\n\nperlin_heart_grid2 &lt;- function(nx = 4, ny = 2, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  heart_settings &lt;- expand_grid(\n    r_min = .3, \n    r_max = .4, \n    w_min = .01,\n    w_max = 6,\n    x_shift = 1:nx, \n    y_shift = 1:ny\n  ) |&gt;\n    mutate(\n      n = 200,\n      x_shift = x_shift + runif(n(), -.1, .1),\n      y_shift = y_shift + runif(n(), -.1, .1),\n      rot = runif(n(), -.1, .1),\n      id = row_number()\n    ) \n  \n  heart_data &lt;-  pmap_dfr(heart_settings, perlin_heart2)\n  \n  heart_data |&gt;\n    ggplot(aes(x, y, group = id, colour = sample(id), size = width)) +\n    geom_path(show.legend = FALSE) +\n    theme_void() +\n    scale_size_identity() +\n    scale_colour_gradientn(colours = sample_canva(seed)) +\n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    coord_fixed(xlim = c(0, nx + 1), ylim = c(0, ny + 1))\n}\n\npic &lt;- perlin_heart_grid2(seed = 666)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nperlin-heart-grid.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(colour = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(colour = \"black\", size = 2)\n  }\n  return(pic)\n}\n\nheart_x &lt;- function(angle) {\n  x &lt;- (16 * sin(angle) ^ 3) / 17\n  return(x - mean(x))\n}\n\nheart_y &lt;- function(angle) {\n  y &lt;- (13 * cos(angle) - 5 * cos(2 * angle) - 2 * cos(3 * angle) -\n          cos(4 * angle)) / 17\n  return(y - mean(y))\n}\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_heart &lt;- function(n = 100, \n                         freq_init = 0.3,\n                         octaves = 2, \n                         r_min = 0.5, \n                         r_max = 1,\n                         x_shift = 0,\n                         y_shift = 0,\n                         id = NA,\n                         seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    x_base = cos(angle),\n    y_base = sin(angle),\n    radius = fracture(\n      x = x_base, \n      y = y_base, \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    id = id\n  )\n}\n\nperlin_heart_grid &lt;- function(nx = 10, ny = 6, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  heart_settings &lt;- expand_grid(\n    r_min = .3, \n    r_max = .4, \n    x_shift = 1:nx, \n    y_shift = 1:ny\n  ) |&gt;\n    mutate(id = row_number()) \n  \n  heart_data &lt;-  pmap_dfr(heart_settings, perlin_heart)\n  \n  heart_data |&gt;\n    ggplot(aes(x, y, group = id, fill = sample(id))) +\n    geom_polygon(size = 0, show.legend = FALSE) +\n    theme_void() +\n    scale_fill_gradientn(colours = sample_canva(seed)) +\n    coord_equal(xlim = c(0, nx + 1), ylim = c(0, ny + 1))\n}\n\npic &lt;- perlin_heart_grid(seed = 451)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nperlin-heart.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(gifski)\n\nshow_polygon &lt;- function(polygon, show_vertices = TRUE, ...) {\n  \n  pic &lt;- ggplot(polygon, aes(x, y)) +\n    geom_polygon(colour = \"black\", fill = NA, show.legend = FALSE, ...) + \n    coord_equal() + \n    theme_void()\n  \n  if(show_vertices == TRUE) {\n    pic &lt;- pic + geom_point(colour = \"black\", size = 2)\n  }\n  return(pic)\n}\n\nheart_x &lt;- function(angle) {\n  x &lt;- (16 * sin(angle) ^ 3) / 17\n  return(x - mean(x))\n}\n\nheart_y &lt;- function(angle) {\n  y &lt;- (13 * cos(angle) - 5 * cos(2 * angle) - 2 * cos(3 * angle) -\n          cos(4 * angle)) / 17\n  return(y - mean(y))\n}\n\nnormalise_radius &lt;- function(x, min, max) {\n  normalise(x, from = c(-0.5, 0.5), to = c(min, max))\n}\n\nperlin_heart &lt;- function(n = 100, \n                         freq_init = 0.3,\n                         octaves = 2, \n                         r_min = 0.5, \n                         r_max = 1,\n                         x_shift = 0,\n                         y_shift = 0,\n                         id = NA,\n                         seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    angle = seq(0, 2*pi, length.out = n),\n    x_base = cos(angle),\n    y_base = sin(angle),\n    radius = fracture(\n      x = x_base, \n      y = y_base, \n      freq_init = freq_init,\n      noise = gen_perlin, \n      fractal = fbm, \n      octaves = octaves\n    ) |&gt;\n      normalise_radius(r_min, r_max),\n    x = radius * heart_x(angle) + x_shift,\n    y = radius * heart_y(angle) + y_shift,\n    id = id\n  )\n}\n\nset.seed(3); \npic &lt;- perlin_heart(freq_init = .4) |&gt; show_polygon(FALSE)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nsmudged-hexagon.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(here)\n\nedge_length &lt;- function(x1, y1, x2, y2) {\n  sqrt((x1 - x2)^2 + (y1 - y2)^2)\n}\n\nedge_noise &lt;- function(size) {\n  runif(1, min = -size/2, max = size/2)\n}\n\nsample_edge_l &lt;- function(polygon) {\n  sample(length(polygon), 1, prob = map_dbl(polygon, ~ .x$seg_len))\n}\n\ninsert_edge_l &lt;- function(polygon, noise) {\n  \n  ind &lt;- sample_edge_l(polygon)\n  len &lt;- polygon[[ind]]$seg_len\n  \n  last_x &lt;- polygon[[ind]]$x\n  last_y &lt;- polygon[[ind]]$y\n  \n  next_x &lt;- polygon[[ind + 1]]$x\n  next_y &lt;- polygon[[ind + 1]]$y\n  \n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  new_point &lt;- list(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  polygon[[ind]]$seg_len &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  c(\n    polygon[1:ind],\n    list(new_point),\n    polygon[-(1:ind)]\n  )\n}\n\ngrow_polygon_l &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge_l(polygon, noise)\n  return(polygon)\n}\n\ngrow_multipolygon_l &lt;- function(base_shape, n, seed = NULL, ...) {\n  if(!is.null(seed)) set.seed(seed)\n  polygons &lt;- list()\n  for(i in 1:n) {\n    polygons[[i]] &lt;- grow_polygon_l(base_shape, ...) |&gt;\n      transpose() |&gt;\n      as_tibble() |&gt;\n      mutate(across(.fn = unlist))\n  }\n  polygons &lt;- bind_rows(polygons, .id = \"id\")\n  polygons\n}\n\nshow_multipolygon &lt;- function(polygon, fill, alpha = .02, ...) {\n  ggplot(polygon, aes(x, y, group = id)) +\n    geom_polygon(colour = NA, alpha = alpha, fill = fill, ...) + \n    coord_equal() + \n    theme_void()\n}\n\nsmudged_hexagon &lt;- function(seed, noise1 = 0, noise2 = 2, noise3 = 0.5) {\n  set.seed(seed)\n  \n  # define hexagonal base shape\n  theta &lt;- (0:6) * pi / 3\n  hexagon &lt;- tibble(\n    x = sin(theta),\n    y = cos(theta),\n    seg_len = edge_length(x, y, lead(x), lead(y))\n  )\n  hexagon$seg_len[7] &lt;- 0\n  hexagon &lt;- transpose(hexagon)\n  base &lt;- hexagon |&gt; \n    grow_polygon_l(\n      iterations = 60, \n      noise = noise1\n    )\n  \n  # define intermediate-base-shapes in clusters\n  polygons &lt;- list()\n  ijk &lt;- 0\n  for(i in 1:3) {\n    base_i &lt;- base |&gt; \n      grow_polygon_l(\n        iterations = 50, \n        noise = noise2\n      )\n    \n    for(j in 1:3) {\n      base_j &lt;- base_i |&gt; \n        grow_polygon_l(\n          iterations = 50, \n          noise = noise2\n        )\n      \n      # grow 10 polygons per intermediate-base\n      for(k in 1:10) {\n        ijk &lt;- ijk + 1\n        polygons[[ijk]] &lt;- base_j |&gt;\n          grow_polygon_l(\n            iterations = 500, \n            noise = noise3\n          ) |&gt;\n          transpose() |&gt;\n          as_tibble() |&gt;\n          mutate(across(.fn = unlist))\n      }\n    }\n  }\n  \n  # return as data frame\n  bind_rows(polygons, .id = \"id\")\n}\n\n\ntic()\ndat &lt;- smudged_hexagon(seed = 88)\npic &lt;- dat |&gt; show_multipolygon(fill = \"#d4379005\")\nggsave(\n  filename = here(\"output\", \"smudged-hexagon.png\"), \n  plot = pic,\n  width = 2000,\n  height = 2000,\n  units = \"px\",\n  dpi = 300,\n  bg = \"#222222\"\n)\ntoc()\n\n\n\n\n\n\n\n\n\n\nsplotch.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ambient)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(here)\n\nedge_length &lt;- function(x1, y1, x2, y2) {\n  sqrt((x1 - x2)^2 + (y1 - y2)^2)\n}\n\nedge_noise &lt;- function(size) {\n  runif(1, min = -size/2, max = size/2)\n}\n\nsample_edge_l &lt;- function(polygon) {\n  sample(length(polygon), 1, prob = map_dbl(polygon, ~ .x$seg_len))\n}\n\ninsert_edge_l &lt;- function(polygon, noise) {\n  \n  ind &lt;- sample_edge_l(polygon)\n  len &lt;- polygon[[ind]]$seg_len\n  \n  last_x &lt;- polygon[[ind]]$x\n  last_y &lt;- polygon[[ind]]$y\n  \n  next_x &lt;- polygon[[ind + 1]]$x\n  next_y &lt;- polygon[[ind + 1]]$y\n  \n  new_x &lt;- (last_x + next_x) / 2 + edge_noise(len * noise)\n  new_y &lt;- (last_y + next_y) / 2 + edge_noise(len * noise)\n  \n  new_point &lt;- list(\n    x = new_x,\n    y = new_y,\n    seg_len = edge_length(new_x, new_y, next_x, next_y)\n  )\n  \n  polygon[[ind]]$seg_len &lt;- edge_length(\n    last_x, last_y, new_x, new_y\n  )\n  \n  c(\n    polygon[1:ind],\n    list(new_point),\n    polygon[-(1:ind)]\n  )\n}\n\ngrow_polygon_l &lt;- function(polygon, iterations, noise, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  for(i in 1:iterations) polygon &lt;- insert_edge_l(polygon, noise)\n  return(polygon)\n}\n\ngrow_multipolygon_l &lt;- function(base_shape, n, seed = NULL, ...) {\n  if(!is.null(seed)) set.seed(seed)\n  polygons &lt;- list()\n  for(i in 1:n) {\n    polygons[[i]] &lt;- grow_polygon_l(base_shape, ...) |&gt;\n      transpose() |&gt;\n      as_tibble() |&gt;\n      mutate(across(.fn = unlist))\n  }\n  polygons &lt;- bind_rows(polygons, .id = \"id\")\n  polygons\n}\n\nshow_multipolygon &lt;- function(polygon, fill, alpha = .02, ...) {\n  ggplot(polygon, aes(x, y, group = id)) +\n    geom_polygon(colour = NA, alpha = alpha, fill = fill, ...) + \n    coord_equal() + \n    theme_void()\n}\n\nsplotch &lt;- function(seed, layers = 10) {\n  set.seed(seed)\n  square_l &lt;- transpose(tibble(\n    x = c(0, 1, 1, 0, 0),\n    y = c(0, 0, 1, 1, 0),\n    seg_len = c(1, 1, 1, 1, 0)\n  ))\n  square_l |&gt; \n    grow_polygon_l(iterations = 10, noise = .5, seed = seed) |&gt;\n    grow_multipolygon_l(n = layers, iterations = 500, noise = .8, seed = seed) \n}\n\ntic()\ndat &lt;- splotch(seed = 1)\npic &lt;- dat |&gt; show_multipolygon(fill = \"white\", alpha = .2)\nggsave(\n  filename = here(\"output\", \"splotch.png\"), \n  plot = pic,\n  width = 2000,\n  height = 2000,\n  units = \"px\",\n  dpi = 300,\n  bg = \"black\"\n)\ntoc()\n\n\n\n\n\n\n\n\n\n\ntextured-lines.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(e1071)\n\nsmooth_loess &lt;- function(x, span) {\n  n &lt;- length(x)\n  dat &lt;- tibble(time = 1:n, walk = x)\n  mod &lt;- loess(walk ~ time, dat, span = span)\n  predict(mod, tibble(time = 1:n))\n}\n\nsmooth_path &lt;- function(n = 1000, smoothing = .4, seed = NULL) { \n  if(!is.null(seed)) set.seed(seed)\n  tibble(\n    x = smooth_loess(rbridge(1, n), span = smoothing),\n    y = smooth_loess(rbridge(1, n), span = smoothing),\n    stroke = 1\n  )\n}\n\nperturb &lt;- function(path, noise = .01, span = .1) {\n  path |&gt; \n    group_by(stroke) |&gt;\n    mutate(\n      x = x + rnorm(n(), 0, noise),\n      y = y + rnorm(n(), 0, noise),\n      x = smooth_loess(x, span),\n      y = smooth_loess(y, span),\n      alpha = runif(n()) &gt; .5,\n      size = runif(n(), 0, .2)\n    )\n}\n\nbrush &lt;- function(path, bristles = 100, seed = 1, ...) {\n  set.seed(seed)\n  dat &lt;- list()\n  for(i in 1:bristles) {\n    dat[[i]] &lt;- perturb(path, ...)\n  }\n  return(bind_rows(dat, .id = \"id\"))\n}\n\nstroke &lt;- function(dat, geom = geom_path, colour = \"white\", ...) {\n  dat |&gt;  \n    ggplot(aes(\n      x = x, \n      y = y, \n      alpha = alpha, \n      size = size, \n      group = paste0(stroke, id)\n    )) + \n    geom(\n      colour = colour, \n      show.legend = FALSE,\n      ...\n    ) + \n    coord_equal() +\n    scale_alpha_identity() +\n    scale_size_identity() +\n    theme_void() + \n    theme(plot.background = element_rect(\n      fill = \"#222222\", \n      colour = \"#222222\"\n    ))\n}\n\npath &lt;- smooth_path(seed = 123)\npic &lt;- path |&gt; brush() |&gt; stroke()\nplot(pic)"
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html",
    "href": "posts/2024-12-23_art-from-code-6/index.html",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the sixth post in that series.\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(deldir)\nlibrary(ggthemes)\nlibrary(voronoise)\nlibrary(tictoc)\nlibrary(ambient)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(truchet)\nlibrary(sf)\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}"
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#rectangle-subdivision",
    "href": "posts/2024-12-23_art-from-code-6/index.html#rectangle-subdivision",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Rectangle subdivision",
    "text": "Rectangle subdivision\nOne of my favourite generative artists in the R community is Ijeamaka Anyene, partly because she’s fabulous but also partly because her approach is so different to mine and she makes things I’d never think to try. She has a talent for designing colourful pieces in a minimalist, geometric style. Minimalism in art is not something I’m good at: I have a habit of overcomplicating my pieces! However, in this first section I’m going to resist the temptation to add complexity, and build a system inspired by Ijeamaka’s recursive rectangle subdivision art. She has a blog post on her approach, by the way.\nRecursive rectangle subdivisions come up a lot in real life. Suppose you have a parcel of land and want to divide it in two parts A and B. In doing so you create a boundary. Later, land unit A wants to divide: this adds a new boundary, splitting A into A1 and A2, but leaving unit B untouched. If this process repeats often enough, you end up with subdivisions that have a very recognisable structure. Here’s a subdivision depicting a 1939 land use survey map for a part of the San Fernando valley in Los Angeles\n\n\n\n\n\n\n\n\n\nLet’s design a generative art system that mimics this process. Suppose we have some data frame blocks where each row represents one rectangular block, and one of the columns it stores is the area of that rectangle. Now imagine that our subdivision process deliberately targets larger blocks: the probability of choosing the a block for subdivision is proportional to its area. The choose_rectangle() function below takes the blocks data frame as input, and randomly selects a row with probability proportional to blocks$area. It returns the row number for the selected rectangle:\n\nchoose_rectangle &lt;- function(blocks) {\n  sample(nrow(blocks), 1, prob = blocks$area)\n}\n\nFor this system we assume that you can only subdivide a rectangle in one of two ways: horizontally, or vertically. We aren’t going to allow diagonal lines or anything that would produce other kinds of polygons. The input to a subdivision is a rectangle, and the output should be two rectangles.\nIf we’re going to do that, we need to select a “break point”. The choose_break() function will do that for us. It takes a lower and upper value as input, and returns a value (expressed as the distance from the lower boundary) specifying where the break is inserted:\n\nchoose_break &lt;- function(lower, upper) {\n  round((upper - lower) * runif(1))\n}\n\nNotice that I’ve called round() here to ensure that the outputs will always be integer value. As a consequence, all of our subdivisions will line up on a grid of some kind: that comes in handy later if, for example, we want to plot the result as a bitmap or a raster object.\nNext, we need a function that can subdivide a rectangle! For the moment, let’s assume that we’re splitting horizontally, so in a moment we’ll write a function called split_rectangle_x() to do this for us. It’s going to take a rectangle as the main argument, which is presumably going to be a tibble that contains columns that define a rectangle. To make life a little simpler, here’s a convenience function create_rectangles() that creates this tibble for us:\n\ncreate_rectangles &lt;- function(left, right, bottom, top, value) {\n  tibble(\n    left = left,\n    right = right,\n    bottom = bottom,\n    top = top,\n    width = right - left,\n    height = top - bottom,\n    area = width * height,\n    value = value\n  )\n}\n\nNote that this function can create multiple rectangles. It doesn’t check to see if the rectangles overlap, though. If I wanted to write rigorous code I would probably prevent it from allowing rectangles to overlap, but I’m not being super rigorous here. It’s not production code!\nAnyway, here are a couple of rectangles that represent a vertical split, where one of them sits above the other:\n\nrect &lt;- create_rectangles(\n  left = 1, \n  right = 10, \n  bottom = c(1, 4), \n  top = c(4, 10),\n  value = 1:2\n)\nrect\n\n# A tibble: 2 × 8\n   left right bottom   top width height  area value\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1     1    10      1     4     9      3    27     1\n2     1    10      4    10     9      6    54     2\n\n\nNow we can write our horizontal subdivision function, split_rectangle_x(), and it’s vertical counterpart split_rectangle_y(). Each of these takes a single rectangle as input, calls choose_break() to determine where the break point should be, and then creates two new rectangles that will replace the old one. When called, they’ll automatically recalculate the width, height, and areas for both rectangles. The value of the first rectangle (the one to the left or on the lower side) remains unchanged, and and the new_value argument is used to assign a value to the second rectangle:\n\nsplit_rectangle_x &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(left, right)\n    new_left  &lt;- c(left, left + split)\n    new_right &lt;- c(left + split, right)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(new_left, new_right, bottom, top, new_value)\n  })\n}\n\n\nsplit_rectangle_y &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(bottom, top)\n    new_bottom &lt;- c(bottom, bottom + split)\n    new_top &lt;- c(bottom + split, top)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(left, right, new_bottom, new_top, new_value)\n  })\n}\n\nWhile we are here, we can write a split_rectangle() function that randomly decides whether to split horizontally or vertically, and then calls the relevant function to do the splitting:\n\nsplit_rectangle &lt;- function(rectangle, value) {\n  if(runif(1) &lt; .5) {\n    return(split_rectangle_x(rectangle, value))\n  }\n  split_rectangle_y(rectangle, value)\n}\n\nHere it is in action:\n\nset.seed(1)\nsplit_rectangle(rectangle = rect[1, ], value = 3)\n\n# A tibble: 2 × 8\n   left right bottom   top width height  area value\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1     4      1     4     3      3     9     1\n2     4    10      1     4     6      3    18     3\n\n\nNotice that it is possible to create a block with zero area. That’s okay: that block will never be selected for later subdivision. We could filter out all zero-area rectangles if we wanted to, but I’m too lazy to bother!\nNow we are in a position to define a function called split_block() that takes block, a tibble of one or more rectangles as input, selects one to be subdivided using choose_rectangle(), and then splits it with split_rectangle(). The old, now-subdivided rectangle is removed from the block, the two new ones are added, and the updated block of rectangles is returned:\n\nsplit_block &lt;- function(blocks, value) {\n  old &lt;- choose_rectangle(blocks) \n  new &lt;- split_rectangle(blocks[old, ], value)\n  bind_rows(blocks[-old, ], new)\n}\n\nHere it is at work:\n\nsplit_block(rect, value = 3)\n\n# A tibble: 3 × 8\n   left right bottom   top width height  area value\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    10      1     4     9      3    27     1\n2     1    10      4     5     9      1     9     2\n3     1    10      5    10     9      5    45     3\n\n\nNow that we have a create_rectangles() function that can generate a new rectangle and a split_block() function that can pick one rectangle and split it, we can write subdivision() function quite succinctly. We repeatedly apply the split_block() function until it has created enough splits for us. I could write this as a loop, but it feels more elegant to me to use the reduce() function from the purrr package to do this:\n\nsubdivision &lt;- function(ncol = 1000, \n                        nrow = 1000, \n                        nsplits = 50, \n                        seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  blocks &lt;- create_rectangles(\n    left = 1, \n    right = ncol, \n    bottom = 1, \n    top = nrow, \n    value = 0\n  )\n  reduce(1:nsplits, split_block, .init = blocks)\n}\n\nsubdivision(nsplits = 5)\n\n# A tibble: 6 × 8\n   left right bottom   top width height   area value\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1     1  1000    661  1000   999    339 338661     1\n2     1   207      1   661   206    660 135960     0\n3   207  1000    255   661   793    406 321958     3\n4   207   776      1   255   569    254 144526     2\n5   776   950      1   255   174    254  44196     4\n6   950  1000      1   255    50    254  12700     5\n\n\nAs you can see, in this version of the system I’ve arranged it so that the value column represents the iteration number upon which the corresponding rectangle was created.\nFinally we get to the part where we make art! The develop() function below uses geom_rect() to draw the rectangles, mapping the value to the fill aesthetic:\n\ndevelop &lt;- function(div, seed = NULL) {\n  \n  div |&gt; \n    ggplot(aes(\n      xmin = left, \n      xmax = right, \n      ymin = bottom, \n      ymax = top,\n      fill = value\n    )) +\n    geom_rect(\n      colour = \"#ffffff\", \n      linewidth = 3,\n      show.legend = FALSE\n    ) +\n    scale_fill_gradientn(\n      colours = sample_canva2(seed)\n    ) +\n    coord_equal() +\n    theme_void() +\n    theme(\n      plot.background = element_rect(\n        fill = \"#ffffff\"\n      )\n    ) \n}\n\nsubdivision(seed = 1) |&gt; develop() \n\n\n\n\n\n\n\n\nThe uneven spacing here is not accidental. Because the rectangles are plotted with a thick white border, and plotted against a white background, very thin rectangles are invisible. That leads to a slightly irregular pattern among the visible rectangles. I quite like it!\nHere are a few more outputs from the system:\nsubdivision(nsplits = 100, seed = 123) |&gt; develop()\nsubdivision(nsplits = 200, seed = 102) |&gt; develop()\nsubdivision(nsplits = 500, seed = 103) |&gt; develop()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the subdivision.R script in the materials"
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#mosaica",
    "href": "posts/2024-12-23_art-from-code-6/index.html#mosaica",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Mosaica",
    "text": "Mosaica\nRemember earlier when I said I have this compulsive tendency to make my generative art systems unnecessarily elaborate? I was not lying. Now that I’ve created this simple and clean subdivision() system my first instinct is to use it as the basis for something more complicated. The fill_rectangle() function below takes a single rectangle as input, divides it into a grid of squares with edge length 1, and then assigns each of those squares a fill value generated with a randomly sampled fractal (using the ambient package to do the work):\n\nfill_rectangle &lt;- function(left, right, bottom, top, width, \n                           height, area, value, nshades = 100) {\n  \n  set.seed(value)\n  fractals &lt;- list(billow, fbm, ridged)\n  generators &lt;- list(gen_simplex, gen_perlin, gen_worley)\n  \n  expand_grid(\n    x = left:right, \n    y = bottom:top, \n  ) |&gt;\n    mutate(\n      fill = 10 * value + fracture(\n        x = x * sample(-3:3, 1),\n        y = y * sample(-3:3, 1),\n        noise = sample(generators, 1)[[1]],\n        fractal = sample(fractals, 1)[[1]],\n        octaves = sample(10, 1),\n        frequency = sample(10, 1) / 20,\n        value = \"distance2\"\n      ) |&gt;\n        normalise(to = c(1, nshades)) |&gt; \n        round()\n    )\n}\n\nI’ll also write a draw_mosaic() function that plots a collection of these unit-square sized tiles:\n\ndraw_mosaic &lt;- function(dat, palette) {\n  background &lt;- sample(palette, 1)\n  dat |&gt;\n    ggplot(aes(x, y, fill = fill)) +\n    geom_tile(show.legend = FALSE, colour = background, size = .2) +\n    scale_size_identity() +\n    scale_colour_gradientn(colours = palette) +\n    scale_fill_gradientn(colours = palette) +\n    scale_x_continuous(expand = expansion(add = 5)) +\n    scale_y_continuous(expand = expansion(add = 5)) +\n    coord_equal() +\n    theme_void() +\n    theme(plot.background = element_rect(fill = background)) \n}\n\nWhen combined with the original subdivision() function I can now write a generative art system called mosaica() that uses subdivision() to partition a grid into rectangular units, then applies fill_rectangle() to separate each of these rectangles into unit squares and fill each of these squares with a colour based on a spatial noise pattern generated using ambient. Then it draws a picture:\n\nmosaica &lt;- function(ncol = 60, \n                    nrow = 60, \n                    nsplits = 30, \n                    seed = NULL) {\n  \n  subdivision(ncol, nrow, nsplits, seed) |&gt;\n    pmap_dfr(fill_rectangle) |&gt; \n    slice_sample(prop = .995) |&gt;\n    filter(!is.na(fill)) |&gt;\n    draw_mosaic(palette = sample_canva2(seed))\n}\n\nmosaica(ncol = 200, nrow = 100, nsplits = 200, seed = 1302)\n\n\n\n\n\n\n\n\nIt makes me happy :)\nmosaica(seed = 1977)\nmosaica(seed = 2022)\nmosaica(seed = 1969)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmosaica(nrow = 100, seed = 1)\nmosaica(nrow = 100, seed = 2)\nmosaica(nrow = 100, seed = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the mosaica.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#voronoi-tesselation",
    "href": "posts/2024-12-23_art-from-code-6/index.html#voronoi-tesselation",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Voronoi tesselation",
    "text": "Voronoi tesselation\nLet’s switch gears a little. So far we’ve only looked at rectangular tilings, but there are many other ways to tile a two dimensional plane. One method for constructing an irregular tiling – one that generative artists are especially fond of – is to generate a collection of points and then computing the Voronoi tesselation (also known as a Voronoi diagram) of those points. Wikipedia definitions are, once again, helpful:\n\nA Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. In the simplest case, these objects are just finitely many points in the plane (called seeds, sites, or generators). For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other.\n\nExtremely conveniently for our purposes, the ggforce package provides two handy geom functions – geom_voronoi_segment() and geom_voronoi_tile() – that plot the Voronoi tesselation for a set of points. All you have to do as the user is specify the x and y aesthetics (corresponding to the coordinate values of the points), and ggplot2 will do all the work for you. Let’s see what we can do using these tools!\nIn the beginning there were points…\n\nset.seed(61)\ndat &lt;- tibble(\n  x = runif(20),\n  y = runif(20),\n  val = runif(20)\n)\n\npic &lt;- ggplot(dat, aes(x, y, fill = val)) +\n  coord_equal(xlim = c(-.3, 1.3), ylim = c(-.3, 1.3)) +\n  guides(fill = guide_none()) +\n  theme_void()\n\npic + geom_point(size = 3)\n\n\n\n\n\n\n\n\nThe points themselves are not very artistically impressive, but we can make something more interesting if we add the Voronoi tesselation. The minimal way to do this is with geom_voronoi_segment()\n\npic + \n  geom_voronoi_segment() + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nWe can already see the beginnings of something pleasing. I mean, if I’m being honest this is already quite pretty in a minimalist way but – as I keep saying – I have an urge to tinker and see what elaborations we can add. First, let’s switch from geom_voronoi_segment() to geom_voronoi_tile():\n\npic + \n  geom_voronoi_tile() + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nSetting the max.radius argument prevents any tile extending beyond a fixed distance from the point generating the tile, giving the image as a whole a “bubbly” look:\n\npic + \n  geom_voronoi_tile(max.radius = .2) + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nHm. Those sharp corners between tiles aren’t the prettiest thing I’ve ever seen. Let’s round those corners a little bit, shall we? The radius argument lets us do that:\n\npic + \n  geom_voronoi_tile(max.radius = .2, radius = .02) + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nNext, let’s shrink all the tiles a tiny bit to create small gaps between adjacent tiles:\n\npic + \n  geom_voronoi_tile(\n    max.radius = .2, \n    radius = .02,\n    expand = -.005\n  ) + \n  geom_point(size = 3)\n\n\n\n\n\n\n\n\nLet’s remove the points themselves, leaving only the rounded tiles:\n\npic + \n  geom_voronoi_tile(\n    max.radius = .2, \n    radius = .02,\n    expand = -.005\n  )\n\n\n\n\n\n\n\n\nFinally, we’ll create another tiling and use it as a background texture:\n\nbg_dat &lt;- tibble(\n  x = runif(500, min = -.5, max = 1.5),\n  y = runif(500, min = -.5, max = 1.5)\n)\npic + \n  geom_voronoi_tile(\n    data = bg_dat,\n    fill = \"#333333\", \n    radius = .01,\n    expand = -.0025\n  ) +\n  geom_voronoi_tile(\n    colour = \"white\", \n    max.radius = .2, \n    radius = .02,\n    expand = -.005\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nA script reproducing this piece is included in the voronoi-tiles.R file in the materials."
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#voronoi-baroque-part-i",
    "href": "posts/2024-12-23_art-from-code-6/index.html#voronoi-baroque-part-i",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Voronoi baroque: Part I",
    "text": "Voronoi baroque: Part I\nWhen I first started playing around with Voronoi tesselations the pieces I made looked a lot like the worked example: the Voronoise series I posted on my art site contains pieces that look like the one above, generated from random collections of points. What I started realising a little later is that if you feed a structured set of points into your Voronoi tesselations, you can create some very elaborate patterns. I’ve played around with this idea in a few series (my favourite so far is Sadists Kiss).\nI’ll illustrate the approach by reusing an earlier system. The unboxy() function shown below reimplements the “unboxing” system that I talked about in the section on iterated function systems:\n\nunboxy &lt;- function(iterations, layers) {\n  \n  coeffs &lt;- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  point0 &lt;- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  funs &lt;- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  update &lt;- function(point, t) {\n    l &lt;- sample(layers, 1)\n    f &lt;- sample(funs, 1)[[1]]\n    z &lt;- point[3]\n    point[3] &lt;- 1\n    point &lt;- f(point %*% coeffs[,,l])\n    point[3] &lt;- (point[3] + z)/2\n    return(point)\n  }\n  \n  points &lt;- accumulate(1:iterations, update, .init = point0)\n  points &lt;- matrix(unlist(points), ncol = 3, byrow = TRUE)\n  points &lt;- as_tibble(as.data.frame(points)) \n  names(points) &lt;- c(\"x\", \"y\", \"val\")\n  return(points)\n}\n\nI’m not going to explain the inner workings of this function here (because they’re already discussed elsewhere), but in case you need a refresher or haven’t read the relevant page, here’s a look at the kinds of data this function produces, and a scatterplot showing the very non-random spatial patterns it generates:\nset.seed(1)\ndat &lt;- unboxy(iterations = 1000, layers = 5) \n\ndat\nggplot(dat, aes(x, y)) + \n  geom_point(colour = \"white\", show.legend = FALSE) +\n  coord_equal(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) + \n  theme_void()\n\n\n\n# A tibble: 1,001 × 3\n        x       y     val\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  0.579  0.236   0.666 \n 2 -1.95   0.935  -0.0757\n 3 -0.884  1.09    0.672 \n 4 -0.929  2.04    0.401 \n 5  0.861  1.44    2.01  \n 6  0.404  1.29    1.46  \n 7  0.826  0.834  -0.909 \n 8  0.999 -2.00    1.94  \n 9  0.850  0.0301  1.61  \n10 -1.80   1.38   -0.381 \n# ℹ 991 more rows\n\n\n\n\n\n\n\n\n\n\nNow let’s plot the Voronoi tesselation corresponding to these points, once again relying on our old friend sample_canva2() to generate the palette:\n\npic &lt;- ggplot(dat, aes(x, y, fill = val)) +\n  theme_void() + \n  coord_equal(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) + \n  scale_fill_gradientn(colours = sample_canva2()) + \n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\npic +\n  geom_voronoi_tile(\n    colour = \"#222222\",\n    size = .2, \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nRounding the corners and expanding the tiles gives the piece a different feel…\n\npic +\n  geom_voronoi_tile(\n    radius = .01,\n    expand = .01,\n    colour = \"#222222\",\n    size = .2, \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nSo does this…\n\npic +\n  geom_voronoi_tile(\n    max.radius = .1,\n    radius = .01,\n    expand = -.0001,\n    colour = \"#222222\",\n    size = .2, \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nThe possibilities are surprisingly rich, and quite a lot of fun to play around with!\n\n\n\n\n\n\nExercise\n\n\n\nA script containing code for this system is included in the materials, as the voronoi-unbox.R file."
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#voronoi-baroque-part-ii",
    "href": "posts/2024-12-23_art-from-code-6/index.html#voronoi-baroque-part-ii",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Voronoi baroque: Part II",
    "text": "Voronoi baroque: Part II\nOkay, I need to confess something. Voronoi tiling art was the thing that finally pushed me to learn the ggproto object oriented programming system used by ggplot2. It’s not because I’m a masochist and enjoy the pain of learning an OOP system that isn’t used for anything except ggplot2. No-one is that much of a masochist, surely. No, it was because I wanted the ability to intercept and modify the Voronoi tiles during the plot construction process. Because… yeah, I don’t even remember why I wanted that. Evil reasons, probably.\nEnter stage left, the voronoise package. It’s not on CRAN – because I can’t think of a single good reason to send it to CRAN – but you can install it from GitHub with\nremotes::install_github(\"djnavarro/voronoise\")\nThe voronoise package only does one thing: it supplies geom_voronoise(), a geom that behaves just like geom_voronoi_tile() except for the fact you can pass it a “perturbing function” that modifies the tiles. Annoyingly – because I was not a very good software developer at the time and I was not thinking about what someone else (i.e., future me) would use it for later – the default arguments to geom_voronoise() aren’t the same as the defaults for geom_voronoi_tile(), which means it’s a good idea to explicitly specify things like max.radius etc even if you’re “just going to use the defaults”. Sorry. That was my mistake. I cannot stress enough that voronoise is not a good package. But… it’ll do for my purposes today.\nHere’s a simple example where the perturb argument is used to shift all the tiles to the right by a random offset:\n\npic +\n  geom_voronoi_tile( # original tiling in grey\n    max.radius = .1,\n    radius = .01,\n    expand = -.0001,\n    fill = \"#444444\",\n    colour = \"#222222\",\n    size = .2, \n    show.legend = FALSE\n  ) +\n  voronoise::geom_voronoise( # perturbed tiling\n    max.radius = .1,\n    radius = .01,\n    expand = -.0002,\n    perturb = \\(data) data |&gt; \n      group_by(group) |&gt; \n      mutate(x = x + runif(1, min = 0, max = .2)), \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nThat’s kind of neat. Another approach I’ve been fond of in the past is to use something like this sift() function, which computes a crude approximation to the area of each tile and perturbs only those tiles smaller than a certain size:\n\nsift &lt;- function(data) {\n  data &lt;- data |&gt;\n    group_by(group) |&gt;\n    mutate(\n      tilesize = (max(x) - min(x)) * (max(y) - min(y)),\n      x = if_else(tilesize &gt; .02, x, x + rnorm(1)/10), \n      y = if_else(tilesize &gt; .02, y, y + rnorm(1)/10)\n    ) |&gt;\n    ungroup()\n  return(data)\n}\n\n\nvoronoi_baroque &lt;- function(\n    seed, \n    perturb, \n    max.radius = NULL, \n    radius = 0, \n    expand = 0,\n    ...\n) {\n  \n  set.seed(seed)\n  \n  blank &lt;- ggplot(mapping = aes(x, y, fill = val)) +\n    theme_void() + \n    coord_equal(xlim = c(-2.75, 2.75), ylim = c(-2.75, 2.75)) + \n    guides(fill = guide_none(), alpha = guide_none()) +\n    scale_fill_gradientn(colours = sample_canva2(seed)) + \n    scale_alpha_identity() + \n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))\n  \n  blank + \n    geom_voronoise(\n      data = unboxy(iterations = 10000, layers = 5),\n      perturb = perturb,\n      max.radius = max.radius,\n      radius = radius,\n      expand = expand,\n      ...,\n      show.legend = FALSE\n    )\n}\n\nvoronoi_baroque(1234, sift)\nvoronoi_baroque(4000, sift)\nvoronoi_baroque(2468, sift)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe fun thing about voronoi_baroque() is that you can write whatever perturbation function you like… up to a point, of course. I cannot stress enough that the voronoise package is not particularly reliable!\n\nshake &lt;- function(data) {\n  data |&gt; \n    group_by(group) |&gt;\n    mutate(\n      x = x + runif(1)/10, \n      y = y + runif(1)/10\n    ) |&gt;\n    ungroup()\n}\n\nvoronoi_baroque(21, shake)\nvoronoi_baroque(43, shake)\nvoronoi_baroque(17, shake)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the voronoi-baroque.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#truchet-tiles",
    "href": "posts/2024-12-23_art-from-code-6/index.html#truchet-tiles",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Truchet tiles",
    "text": "Truchet tiles\nOne final topic to mention before I wrap this one up: truchet tiles. Truchet tiles are square tiles decorated with asymmetric patterns, designed so that whenever you lay them out randomly, the patterns will connect up in aesthetically pleasing ways. To be honest, I’ve not explore them much myself but Antonio Páez has written the truchet package that you can use to play with these. It’s not currently on CRAN, but you can install from GitHub with:\nremotes::install_github(\"paezha/truchet\")\nThe basic idea in the truchet package is to represent the patterns compactly as a geometry column. If you’re familiar with the sf package this sort of output will be familiar to you:\n\nset.seed(123)\nmosaic &lt;- st_truchet_ms(\n  tiles = c(\"dr\", \"tn\", \"ane\"), \n  p1 = 0.2, # scale 1 \n  p2 = 0.6, # scale 2\n  p3 = 0.2, # scale 3\n  xlim = c(1, 6),\n  ylim = c(1, 6)\n)\nmosaic\n\nSimple feature collection with 797 features and 1 field\nGeometry type: GEOMETRY\nDimension:     XY\nBounding box:  xmin: 0.1666667 ymin: 0.1666667 xmax: 6.833333 ymax: 6.833333\nCRS:           NA\nFirst 10 features:\n   color                       geometry\n1      1 MULTIPOLYGON (((0.8292294 5...\n2      2 POLYGON ((0.4956387 6.16655...\n3      2 POLYGON ((0.8340757 5.53053...\n4      1 MULTIPOLYGON (((2.829229 2....\n5      2 POLYGON ((2.495639 3.166552...\n6      2 POLYGON ((2.834076 2.530531...\n7      1 MULTIPOLYGON (((1.829229 2....\n8      2 MULTIPOLYGON (((2.164615 2....\n9      1 POLYGON ((2.5 1.166667, 2.4...\n10     2 POLYGON ((2.833333 0.5, 2.8...\n\n\nIf you’re not familiar, the key things to note are that the geometry column stores the pattern as a polygon (or collection of polygons), and that geom_sf() understands this geometry column. So you can use code like this to plot your truchet tiling:\n\nmosaic |&gt; \n  ggplot(aes(fill = color)) +\n  geom_sf(color = NA, show.legend = FALSE) + \n  scale_fill_gradientn(colours = c(\"#222222\", \"#ffffff\")) + \n  theme_void()\n\n\n\n\n\n\n\n\nIn this example you’ll notice that I don’t actually specify a mapping for geometry. That’s a little unusual for ggplot2, but it is standard to name the column containing a “simple features geometry” as geometry, so geom_sf() will look for a column by that name.\nThat’s about all I wanted to mention about the truchet package. It makes pretty things and you can check out the package website for more information :)\n\nset.seed(123)\nst_truchet_ss(\n  tiles = c(\n    \"silk_1\", \"silk_2\", \n    \"rainbow_1\", \"rainbow_2\",\n    \"cloud_1\", \"cloud_2\"\n  ),\n  xlim = c(1, 9),\n  ylim = c(1, 6)\n) |&gt;\n  ggplot() +\n  geom_sf(colour = \"#222222\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nExample code for truchet tiles is included in the truchet-example.R script below."
  },
  {
    "objectID": "posts/2024-12-23_art-from-code-6/index.html#materials",
    "href": "posts/2024-12-23_art-from-code-6/index.html#materials",
    "title": "Art from code VI: Tiles and tessellations",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\nmosaica.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(ambient)\nlibrary(tidyr)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nchoose_rectangle &lt;- function(blocks) {\n  sample(nrow(blocks), 1, prob = blocks$area)\n}\n\nchoose_break &lt;- function(lower, upper) {\n  round((upper - lower) * runif(1))\n}\n\ncreate_rectangles &lt;- function(left, right, bottom, top, value) {\n  tibble(\n    left = left,\n    right = right,\n    bottom = bottom,\n    top = top,\n    width = right - left,\n    height = top - bottom,\n    area = width * height,\n    value = value\n  )\n}\n\nsplit_rectangle_x &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(left, right)\n    new_left  &lt;- c(left, left + split)\n    new_right &lt;- c(left + split, right)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(new_left, new_right, bottom, top, new_value)\n  })\n}\n\nsplit_rectangle_y &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(bottom, top)\n    new_bottom &lt;- c(bottom, bottom + split)\n    new_top &lt;- c(bottom + split, top)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(left, right, new_bottom, new_top, new_value)\n  })\n}\n\nsplit_rectangle &lt;- function(rectangle, value) {\n  if(runif(1) &lt; .5) {\n    return(split_rectangle_x(rectangle, value))\n  }\n  split_rectangle_y(rectangle, value)\n}\n\nsplit_block &lt;- function(blocks, value) {\n  old &lt;- choose_rectangle(blocks) \n  new &lt;- split_rectangle(blocks[old, ], value)\n  bind_rows(blocks[-old, ], new)\n}\n\nsubdivision &lt;- function(ncol = 1000, \n                        nrow = 1000, \n                        nsplits = 50, \n                        seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  blocks &lt;- create_rectangles(\n    left = 1, \n    right = ncol, \n    bottom = 1, \n    top = nrow, \n    value = 0\n  )\n  reduce(1:nsplits, split_block, .init = blocks)\n}\n\nfill_rectangle &lt;- function(left, right, bottom, top, width, \n                           height, area, value, nshades = 100) {\n  \n  set.seed(value)\n  fractals &lt;- list(billow, fbm, ridged)\n  generators &lt;- list(gen_simplex, gen_perlin, gen_worley)\n  \n  expand_grid(\n    x = left:right, \n    y = bottom:top, \n  ) |&gt;\n    mutate(\n      fill = 10 * value + fracture(\n        x = x * sample(-3:3, 1),\n        y = y * sample(-3:3, 1),\n        noise = sample(generators, 1)[[1]],\n        fractal = sample(fractals, 1)[[1]],\n        octaves = sample(10, 1),\n        frequency = sample(10, 1) / 20,\n        value = \"distance2\"\n      ) |&gt;\n        normalise(to = c(1, nshades)) |&gt; \n        round()\n    )\n}\n\ndraw_mosaic &lt;- function(dat, palette) {\n  background &lt;- sample(palette, 1)\n  dat |&gt;\n    ggplot(aes(x, y, fill = fill)) +\n    geom_tile(show.legend = FALSE, colour = background, size = .2) +\n    scale_size_identity() +\n    scale_colour_gradientn(colours = palette) +\n    scale_fill_gradientn(colours = palette) +\n    scale_x_continuous(expand = expansion(add = 5)) +\n    scale_y_continuous(expand = expansion(add = 5)) +\n    coord_equal() +\n    theme_void() +\n    theme(plot.background = element_rect(fill = background)) \n}\n\nmosaica &lt;- function(ncol = 60, \n                    nrow = 60, \n                    nsplits = 30, \n                    seed = NULL) {\n  \n  subdivision(ncol, nrow, nsplits, seed) |&gt;\n    pmap_dfr(fill_rectangle) |&gt; \n    slice_sample(prop = .995) |&gt;\n    filter(!is.na(fill)) |&gt;\n    draw_mosaic(palette = sample_canva2(seed))\n}\n\npic &lt;- mosaica(ncol = 100, nrow = 100, nsplits = 200, seed = 1302)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nsubdivision.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nchoose_rectangle &lt;- function(blocks) {\n  sample(nrow(blocks), 1, prob = blocks$area)\n}\n\nchoose_break &lt;- function(lower, upper) {\n  round((upper - lower) * runif(1))\n}\n\ncreate_rectangles &lt;- function(left, right, bottom, top, value) {\n  tibble(\n    left = left,\n    right = right,\n    bottom = bottom,\n    top = top,\n    width = right - left,\n    height = top - bottom,\n    area = width * height,\n    value = value\n  )\n}\n\nsplit_rectangle_x &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(left, right)\n    new_left  &lt;- c(left, left + split)\n    new_right &lt;- c(left + split, right)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(new_left, new_right, bottom, top, new_value)\n  })\n}\n\nsplit_rectangle_y &lt;- function(rectangle, new_value) {\n  with(rectangle, {\n    split &lt;- choose_break(bottom, top)\n    new_bottom &lt;- c(bottom, bottom + split)\n    new_top &lt;- c(bottom + split, top)\n    new_value &lt;- c(value, new_value)\n    create_rectangles(left, right, new_bottom, new_top, new_value)\n  })\n}\n\nsplit_rectangle &lt;- function(rectangle, value) {\n  if(runif(1) &lt; .5) {\n    return(split_rectangle_x(rectangle, value))\n  }\n  split_rectangle_y(rectangle, value)\n}\n\nsplit_block &lt;- function(blocks, value) {\n  old &lt;- choose_rectangle(blocks) \n  new &lt;- split_rectangle(blocks[old, ], value)\n  bind_rows(blocks[-old, ], new)\n}\n\nsubdivision &lt;- function(ncol = 1000, \n                        nrow = 1000, \n                        nsplits = 50, \n                        seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  blocks &lt;- create_rectangles(\n    left = 1, \n    right = ncol, \n    bottom = 1, \n    top = nrow, \n    value = 0\n  )\n  reduce(1:nsplits, split_block, .init = blocks)\n}\n\ndevelop &lt;- function(div, seed = NULL) {\n  \n  div |&gt; \n    ggplot(aes(\n      xmin = left, \n      xmax = right, \n      ymin = bottom, \n      ymax = top,\n      fill = value\n    )) +\n    geom_rect(\n      colour = \"#ffffff\", \n      size = 3,\n      show.legend = FALSE\n    ) +\n    scale_fill_gradientn(\n      colours = sample_canva2(seed)\n    ) +\n    coord_equal() +\n    theme_void()\n}\n\npic &lt;- subdivision(seed = 1) |&gt; develop() \nplot(pic)\n\n\n\n\n\n\n\n\n\n\ntruchet-example.R\n\n\n\n\n\n\nlibrary(truchet)\nlibrary(ggplot2)\n\nset.seed(123)\nmosaic &lt;- st_truchet_ms(\n  tiles = c(\"dr\", \"tn\", \"ane\"), \n  p1 = 0.2, # scale 1 \n  p2 = 0.6, # scale 2\n  p3 = 0.2, # scale 3\n  xlim = c(1, 6),\n  ylim = c(1, 6)\n)\n\npic &lt;- mosaic |&gt; \n  ggplot(aes(fill = color)) +\n  geom_sf(color = NA, show.legend = FALSE) + \n  scale_fill_gradientn(colours = c(\"#222222\", \"#ffffff\")) + \n  theme_void()\n\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nvoronoi-baroque.R\n\n\n\n\n\n\nlibrary(tibble)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(voronoise)\nlibrary(dplyr)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nunboxy &lt;- function(iterations, layers) {\n  \n  coeffs &lt;- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  point0 &lt;- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  funs &lt;- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  update &lt;- function(point, t) {\n    l &lt;- sample(layers, 1)\n    f &lt;- sample(funs, 1)[[1]]\n    z &lt;- point[3]\n    point[3] &lt;- 1\n    point &lt;- f(point %*% coeffs[,,l])\n    point[3] &lt;- (point[3] + z)/2\n    return(point)\n  }\n  \n  points &lt;- accumulate(1:iterations, update, .init = point0)\n  points &lt;- matrix(unlist(points), ncol = 3, byrow = TRUE)\n  points &lt;- as_tibble(as.data.frame(points)) \n  names(points) &lt;- c(\"x\", \"y\", \"val\")\n  return(points)\n}\n\nsift &lt;- function(data) {\n  data &lt;- data |&gt;\n    group_by(group) |&gt;\n    mutate(\n      tilesize = (max(x) - min(x)) * (max(y) - min(y)),\n      x = if_else(tilesize &gt; .02, x, x + rnorm(1)/10), \n      y = if_else(tilesize &gt; .02, y, y + rnorm(1)/10)\n    ) |&gt;\n    ungroup()\n  return(data)\n}\n\nshake &lt;- function(data) {\n  data |&gt; \n    group_by(group) |&gt;\n    mutate(\n      x = x + runif(1)/10, \n      y = y + runif(1)/10\n    ) |&gt;\n    ungroup()\n}\n\nvoronoi_baroque &lt;- function(\n    seed, \n    perturb, \n    max.radius = NULL, \n    radius = 0, \n    expand = 0,\n    ...\n) {\n  \n  set.seed(seed)\n  \n  blank &lt;- ggplot(mapping = aes(x, y, fill = val)) +\n    theme_void() + \n    coord_equal(xlim = c(-2.75, 2.75), ylim = c(-2.75, 2.75)) + \n    guides(fill = guide_none(), alpha = guide_none()) +\n    scale_fill_gradientn(colours = sample_canva2(seed)) + \n    scale_alpha_identity() + \n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0))\n  \n  blank + \n    geom_voronoise(\n      data = unboxy(iterations = 10000, layers = 5),\n      perturb = perturb,\n      max.radius = max.radius,\n      radius = radius,\n      expand = expand,\n      ...,\n      show.legend = FALSE\n    )\n}\n\npic &lt;- voronoi_baroque(43, shake)\nplot(pic)\n\n\n\n\n\n\n\n\n\n\nvoronoi-tiles.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(tibble)\n\nset.seed(61)\n\ndat &lt;- tibble(\n  x = runif(20),\n  y = runif(20),\n  val = runif(20)\n)\n\nbg_dat &lt;- tibble(\n  x = runif(500, min = -.5, max = 1.5),\n  y = runif(500, min = -.5, max = 1.5)\n)\n\npic &lt;- ggplot(dat, aes(x, y, fill = val)) +\n  coord_equal(xlim = c(-.3, 1.3), ylim = c(-.3, 1.3)) +\n  guides(fill = guide_none()) +\n  theme_void() + \n  theme(panel.background = element_rect(\n    fill = \"#222222\", colour = \"#222222\"\n  ))\n\npic2 &lt;- pic + \n  geom_voronoi_tile(\n    data = bg_dat,\n    fill = \"#333333\", \n    radius = .01,\n    expand = -.0025\n  ) +\n  geom_voronoi_tile(\n    colour = \"white\", \n    max.radius = .2, \n    radius = .02,\n    expand = -.005\n  )\n\nplot(pic2)\n\n\n\n\n\n\n\n\n\n\nvoronoi-unbox.R\n\n\n\n\n\n\nlibrary(tibble)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggforce)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nunboxy &lt;- function(iterations, layers) {\n  \n  coeffs &lt;- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  point0 &lt;- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  funs &lt;- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  update &lt;- function(point, t) {\n    l &lt;- sample(layers, 1)\n    f &lt;- sample(funs, 1)[[1]]\n    z &lt;- point[3]\n    point[3] &lt;- 1\n    point &lt;- f(point %*% coeffs[,,l])\n    point[3] &lt;- (point[3] + z)/2\n    return(point)\n  }\n  \n  points &lt;- accumulate(1:iterations, update, .init = point0)\n  points &lt;- matrix(unlist(points), ncol = 3, byrow = TRUE)\n  points &lt;- as_tibble(as.data.frame(points)) \n  names(points) &lt;- c(\"x\", \"y\", \"val\")\n  return(points)\n}\n\nset.seed(1)\ndat &lt;- unboxy(iterations = 1000, layers = 5) \n\npic &lt;- ggplot(dat, aes(x, y, fill = val)) +\n  theme_void() + \n  coord_equal(xlim = c(-2.5, 2.5), ylim = c(-2.5, 2.5)) + \n  scale_fill_gradientn(colours = sample_canva2()) + \n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0))\n\npic2 &lt;- pic +\n  geom_voronoi_tile(\n    colour = \"#222222\",\n    size = .2, \n    show.legend = FALSE\n  )\n\nplot(pic2)"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html",
    "href": "posts/2023-01-10_kubernetes/index.html",
    "title": "Deploying R with kubernetes",
    "section": "",
    "text": "NOTE: The donuts app is no longer live, so the images linking to it are blank now. It was starting to cost me money! :-)\nStory time. There was a very weird moment in machine learning history, about 20 years ago, when the probabilistic AI folks were completely obsessed with Bayesian nonparametrics, and a disproportionate number of papers at NeurIPS had titles like “[Cutesy prefix]: An infinite dimensional model of [something really boring]”. In most cases, you’d dig into the paper and discover that they hadn’t done anything very special. All they’d done is implement a Bayesian model of [boring thing] that was ambiguous about the number of [components], and instead of thinking about what prior constraints make sense for the problem they were trying to solve, the authors used a Chinese restaurant process (CRP) to specify the conditional prior distribution over allocations of observations to components. The CRP has the mildly-interesting property that for any finite sample size there is a non-negligible conditional probability that the next observation belongs to a hitherto unobserved component, and asymptotically the partitions over observations it generates have a countably infinite number of components. Alas, exactly zero of these papers happened to have an infinitely large data set to train the model on, and without fail the results in the papers didn’t appear to have anything “infinite dimensional” about them whatsoever.\nI say this with love and gentleness, dear reader, because I wrote quite a few of those papers myself.\nWhy do I tell this story in a blog post that has absolutely nothing to do with machine learning, statistics, or Bayesian inference? Because in a fit of pique, somewhere around 2006, I decided to do the damned reading myself and learned quite a lot of Bayesian nonparametrics. Not because I thought it would be useful, but because I was curious and I was getting extremely irritated at overconfident machine learning boys telling me that as a mere psychologist I couldn’t possibly understand the depth of their thinking.\nWhich brings me, naturally enough, to kubernetes."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#ggplot2-on-kubernetes",
    "href": "posts/2023-01-10_kubernetes/index.html#ggplot2-on-kubernetes",
    "title": "Deploying R with kubernetes",
    "section": "ggplot2 on kubernetes",
    "text": "ggplot2 on kubernetes\nLet’s start at the ending, shall we? The art shown below is generated at donut.djnavarro.net, and it is more-or-less unique. The site is designed to serve a different image every time it is accessed, using the timestamp as the seed to a generative art system written in R with ggplot2. If you refresh this page, the artwork will change:\n\n\nUnder the hood, the site is a kubernetes app running containerised R code with google kubernetes engine. Sounds fancy, right?\nWell, maybe. Shall we take a look at how it works? Perhaps, like so many other things in this world, it will turn out not to be anywhere near as complicated as it is made out to be."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#um.-what-is-kubernetes-do-i-care",
    "href": "posts/2023-01-10_kubernetes/index.html#um.-what-is-kubernetes-do-i-care",
    "title": "Deploying R with kubernetes",
    "section": "Um. What is kubernetes? Do I care?",
    "text": "Um. What is kubernetes? Do I care?\nThere’s nothing I love more than looking at the website for a software tool and trying to work out what it does by reading how the developers have chosen to describe it. On the kubernetes website they’ve gone with the headline “Production-Grade Container Orchestration”, and started with this:\n\nKubernetes, also known as K8s, is an open-source system for automating deployment, scaling, and management of containerized applications.\n\nAs opening lines go it wouldn’t get you a lot of attention on grindr1 but context makes a difference and it’s not so terrible as a description of what kubernetes does. It’s a useful tool if you need to deploy an application on a cluster2 and have that application run smoothly as you “scale” your cluster by adding more “nodes”3 to the cluster. For the application I’m about to write, kubernetes is overkill. I don’t actually need kubernetes to run something this simple, but this is a learning exercise. I’m doing it so that I can familiarise myself with core concepts. When I get to the part of the post that actually does something with kubernetes I’ll start introducing terminology, but for now that’s enough for us.\nShould you care as an R user? I mean, probably not. If you want a proper answer, Roel Hogervorst has an excellent blog post called “WTF is Kubernetes and Should I Care as R User?” I won’t duplicate content here: you should read the original post. But the short answer is that you probably won’t need to run your own application using kubernetes, but you might need to contribute code to a larger application that uses it. If so, you may want to play around with kubernetes to make sure you understand what it does."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#write-the-r-code",
    "href": "posts/2023-01-10_kubernetes/index.html#write-the-r-code",
    "title": "Deploying R with kubernetes",
    "section": "Write the R code",
    "text": "Write the R code\n\nLikes to watch me in the glass room, bathroom  Chateau Marmont, slippin’ on my red dress, puttin’ on my makeup  Glass room, perfume, cognac, lilac fumes  Says it feels like heaven to him    – Lana Del Rey4\n\nLet’s begin at the beginning. Anytime you want to write something, it helps to have something to say. There is nothing more tiresome than an op-ed writer trying to fill out 1000 words to make the Saturday paper deadline, but tech writing that tries to demonstrate a tool5 without anything even remotely resembling an application runs a very close second. So let’s at least pretend we have a use case for this yeah?\nIn real life I am an unemployed 40-something woman who smokes and drinks too much and makes very poor choices around men, but in my spare moments I make generative art using R. It’s an extremely unprofitable hobby6 but it’s not completely without market value. Among other things the lovely folks at Posit were kind enough to pay me to put together an “art from code” workshop last year, and thanks to their kindness and my weird priorities there is now a nice little online tutorial that you can use to learn how to make generative art in R. What I’m going to do in this post is build a little kubernetes app that creates generative in R. It won’t be very fancy, but hopefully you can see how an app like this could be expanded7 to create a platform for “long form generative art” with R, not dissimilar to what artblocks or fxhash allow generative artists to do with javascript. The artist supplies code (in this case using R) that generates artwork, and the server uses that code to generate an arbitrary number of pieces that… idk, I guess you could sell them? Whatever. Do I look like a capitalist to you?\nTo build something like this we’ll need some R code that creates generative art. I won’t try to make anything too fancy here. In fact, I’ll reuse code for the “donuts” system I used in my multi-threaded task queues post. It’s a good choice for this application because the donuts system is something that is extremely easy to implement in R because the ggplot2 package provides tooling for creating data visualisations that use polar geometry8\nHere’s how you build the system. I won’t go into detail because this system is a very minor variation on this one in my art-from-code tutorial, but here’s the gist. First, the plot is going to need a colour scheme, so we’ll define a function that samples a palette randomly with the assistance of the ggthemes package:\n\n\n\nserver.R\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\n\nNext, we’ll have a function that generates a table full of random numbers that we will later on map onto various plot aesthetics to make a pretty picture:\n\n\n\nserver.R\n\nsample_data &lt;- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat &lt;- tibble::tibble(\n    x0 = stats::runif(n),\n    y0 = stats::runif(n),\n    x1 = x0 + stats::runif(n, min = -.2, max = .2),\n    y1 = y0 + stats::runif(n, min = -.2, max = .2),\n    shade = stats::runif(n),\n    size = stats::runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\n\n\nNow comes the part of the system that does most of the artistic work, by defining a visual layout for any plots that are created using the system:\n\n\n\nserver.R\n\ndonut_style &lt;- function(data = NULL, palette) {\n  ggplot2::ggplot(\n    data = data,\n    mapping = ggplot2::aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      linewidth = size\n    )) +\n    ggplot2::coord_polar(clip = \"off\") +\n    ggplot2::scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(-1, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_x_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1),\n      oob = scales::oob_keep\n    ) +\n    ggplot2::scale_colour_gradientn(colours = palette) +\n    ggplot2::scale_linewidth(range = c(0, 6)) +\n    ggplot2::theme_void() +\n    ggplot2::theme(\n      panel.background = ggplot2::element_rect(\n        fill = palette[1], colour = palette[1]\n      )\n    ) +\n    ggplot2::guides(\n      colour = ggplot2::guide_none(),\n      linewidth = ggplot2::guide_none(),\n      fill = ggplot2::guide_none(),\n      shape = ggplot2::guide_none()\n    )\n}\n\n\nThe last step is a function that puts it all together. The donut() function takes a single integer-valued input and returns a plot object that happens to look slightly pretty:\n\n\n\nserver.R\n\ndonut &lt;- function(seed) {\n\n  dat &lt;- sample_data(n = 200, seed = seed) |&gt;\n    dplyr::mutate(y1 = y0, size = size / 3)\n\n  line_spec &lt;- sample(c(\"331311\", \"11\", \"111115\"), 1)\n\n  pic &lt;- donut_style(palette = sample_canva(seed = seed)) +\n    ggplot2::geom_segment(data = dat, linetype = line_spec)\n\n  if(stats::runif(1) &lt; .5) {\n    pic &lt;- pic +\n      ggplot2::geom_segment(\n        data = dat |&gt; dplyr::mutate(y1 = y1 - .2, y0 = y0 - .2),\n        linetype = line_spec\n      )\n  }\n  if(stats::runif(1) &lt; .5) {\n    pic &lt;- pic +\n      ggplot2::geom_segment(\n        data = dat |&gt; dplyr::mutate(y1 = y1 - .4, y0 = y0 - .4),\n        linetype = line_spec\n      )\n  }\n\n  pic\n}\n\n\nHere it is in action:\nfor(seed in 1:6) plot(donut(seed))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNot my finest work, but still pretty enough to be fun."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-an-api",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-an-api",
    "title": "Deploying R with kubernetes",
    "section": "Expose an API",
    "text": "Expose an API\nThe next step in the process is to define a public API that specifies how visitors to the website can interact with the underlying R code. That’s not something we typically do with R code because we aren’t usually in the business of writing web applications in R, but thanks to endless joy that is the plumber this task can be accomplished with a few lines of code decoration:\n\n\n\nserver.R\n\n#* draws a donut plot\n#* @serializer svg list(width = 10, height = 10)\n#* @get /\nfunction(seed = NA) {\n  if(is.na(seed)) {\n    seed &lt;- as.integer(Sys.time())\n  }\n  print(donut(seed))\n}\n\n\nThere are a few things to note here:\n\nThe code decoration on line 93 specifies that the function defined in lines 94-99 will be called whenever an HTML GET request is sent to the / endpoint. Or, in simpler language, whenever someone visits the main page for the website that eventually ended up being hosted at donut.djnavarro.net.\nThe code decoration on line 92 how the output from the R function (an in-memory data structure) will be serialised (to a binary stream) and transmitted to the user.9 In this case, the output is a plot object that would normally be handled by the R graphics device. What I’ve used plumber to do here, is have this output converted to an svg file. It’s that svg file that the website will serve to the user.\nFinally, notice that the function does take a seed argument.10 I’ve set NA as the default value rather than the more conventional NULL because plumber won’t accept a NULL default in this context.\n\nNoting that all the code I’ve presented so far belongs to a file called server.R (the link goes to the github repo for this “donut” side-project), I can start the web server running locally on port 3456 like this:\n\nplumber::plumb(file=\"server.R\")$run(port = 3456)"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#containerise-it",
    "href": "posts/2023-01-10_kubernetes/index.html#containerise-it",
    "title": "Deploying R with kubernetes",
    "section": "Containerise it",
    "text": "Containerise it\nAt this point in the process I have a perfectly functional webserver… that only runs on my machine which just happens to have the dependencies installed, and is only accessible locally from that machine. We’ll need to fix both of those problems.\nLet’s start by fixing the first one by running the website from within a docker container. Under normal circumstances I’d walk you through that process, but since I wrote a long blog post about docker just the other day, I’ll jump straight to showing you the Dockerfile:\n\n\n\n\nDockerfile\n\nFROM rocker/r-ver:4.2.2\n\nLABEL org.opencontainers.image.source \"https://github.com/djnavarro/donut\"\nLABEL org.opencontainers.image.authors \"Danielle Navarro &lt;djnavarro@protonmail.com&gt;\"\nLABEL org.opencontainers.image.description DESCRIPTION\nLABEL org.opencontainers.image.licenses \"MIT\"\n\nRUN Rscript -e 'install.packages(c(\"ggplot2\", \"scales\", \"tibble\", \"dplyr\", \"plumber\", \"ggthemes\"))'\nCOPY server.R /home/server.R\nEXPOSE 80\nCMD Rscript -e 'plumber::plumb(file=\"/home/server.R\")$run(host=\"0.0.0.0\", port = 80)'\n\n\n\nEvery instruction in this dockerfile is something I covered in the last post, except for the EXPOSE instruction on line 10. That one tells the container to listen on port 80. It doesn’t necessarily publish the output anywhere accessible from outside the container11 but it does mean that the plumber web server running inside the container is listening on port 80 and can create a response when it receives a request. I’ll deal with the publishing issue later."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#push-it-to-the-registry",
    "href": "posts/2023-01-10_kubernetes/index.html#push-it-to-the-registry",
    "title": "Deploying R with kubernetes",
    "section": "Push it to the registry",
    "text": "Push it to the registry\nThe next step in the process is to host the image created from this dockerfile on a public registry.12 I talked about that process in the last post too, so again I’ll keep things simple. The process I followed for the donut project is essentially identical to the one I used in this section of the docker post. I’ve created a github actions workflow that automatically builds the image on github and hosts it with the github container registry. The resulting image name is ghcr.io/djnavarro/donut:main and here’s the build-image.yaml workflow I’m using:\n\n\n\n\n.github/workflows/build-image.yaml\n\nname: publish donut image\n\non:\n  push:\n    branches: ['main']\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build-and-push-image:\n    runs-on: ubuntu-latest\n    strategy:\n      fail-fast: false\n      matrix:\n        include:\n          - dockerfile: ./Dockerfile\n            image: ghcr.io/djnavarro/donut\n\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n      - name: checkout repository\n        uses: actions/checkout@v2\n\n      - name: login to the container registry\n        uses: docker/login-action@f054a8b539a109f9f41c372932f1ae047eff08c9\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n\n      - name: extract metadata (tags, labels) for docker\n        id: meta\n        uses: docker/metadata-action@98669ae865ea3cffbcbaa878cf57c20bbf1c6c38\n        with:\n          images: ${{ matrix.image }}\n\n      - name: build and push docker image\n        uses: docker/build-push-action@ad44023a93711e3deb337508980b4b5e9bcdc5dc\n        with:\n          context: .\n          file: ${{ matrix.dockerfile }}\n          push: true\n          tags: ${{ steps.meta.outputs.tags }}\n          labels: ${{ steps.meta.outputs.labels }}\n\n\n\nAt long, long last we have all the precursors in place. We have a little web application that runs inside a docker container, and the image describing that container is hosted on a registry.13 We can get started on the kubernetes side of things…"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#create-a-kubernetes-cluster",
    "href": "posts/2023-01-10_kubernetes/index.html#create-a-kubernetes-cluster",
    "title": "Deploying R with kubernetes",
    "section": "Create a kubernetes cluster",
    "text": "Create a kubernetes cluster\nAt the risk of stating the bloody obvious, if you want to use kubernetes to deploy an application on a cluster… you’re probably going to need a cluster running kubernetes. You can get yourself one of these in lots of different ways but the way I’m going to do it is with GKE, the google kubernetes engine. You’ll need a google account to do this, and yes this is something that they charge actual money for, but the good news is that when you sign up for google cloud services you get a few hundred dollars of credit to start with. That’s pretty useful for novices: it’s nice to be able to play around and learn the basics before you have to start worrying about what it’s going to cost.\nIf you go down that path you can access your projects from the cloud console, located at console.cloud.google.com. Once there you can navigate to the various pages you’ll need by clicking on links and menu items, but google offers a lot of different cloud services and it does take a little while for the interface to start feeling familiar, so I’ll link to the pages you need directly as well.\nBefore you can create a cluster of your very own, you need to create a project. Pretty much everything you do with google cloud services is organised into projects so that’s where we’ll start. To create a project, go to console.cloud.google.com/projectcreate and follow the prompts. Give your project a fancy name that makes you sound cool: I called mine donut-art.\nNow that you have a project, you’ll need to enable the specific google cloud services that your project will need access to. In this example the only thing I’ll need is GKE itself, but in other situations you might need access to google cloud storage or something like that. To enable GKE on your current project go to console.cloud.google.com/kubernetes/. If it hasn’t already been enabled for the project the page will ask if you want to. Even more conveniently, if you don’t have a cluster running it will ask if you want to create one.14 It will give you two options: an “autopilot” cluster is one where google will automatically manage the configuration for you, whereas for a “standard” cluster you’ll have to be more explicit about how many nodes you want and how they are organised. There are situations where you need to use the standard cluster,15 but this is not one of those so I went with the autopilot approach because it’s simpler. I didn’t need to change any of the defaults: I called my cluster donut-cluster, and created it in the region australia-southeast1.16\nHere’s a screenshot showing you what the relevant bit of the google kubernetes engine console looks like for me now that I have a cluster up and running:\n\nIf I click on the “donut-cluster” link it takes me to a page with a lot more detail, but you can see that some of the information is the same:"
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#command-line-tools",
    "href": "posts/2023-01-10_kubernetes/index.html#command-line-tools",
    "title": "Deploying R with kubernetes",
    "section": "Command line tools",
    "text": "Command line tools\nTime for a little digression.\nTake a look at the menu shown in the last screenshot. If I click on the “connect” button and it will reveal a command I can use to connect to the cluster from the terminal on my laptop… but it requires me to have the gcloud command line tools installed. Now, if you don’t want to install the tools locally you can avoid this by selecting the “run in cloud shell” option that also appears on the dialog box. However, I dislike the cloud shell and prefer to work from my own terminal. So, the next step is to install the command line tools. For this project, the two things I need are gcloud (to interact with google cloud services) and kubectl (to interact with kubernetes).\n\nInstalling gcloud\nIt turns out that installing the command line tools is relatively straightforward, and is made a lot easier thanks to the gcloud installation instructions which are detailed and not too hard to follow. In addition to the basic tools, I installed the “gke-gcloud-auth-plugin” which are needed for authentication. Once the command line tools are installed, authentication from your terminal is a one-line command:\n\ngcloud auth login\n\nI can now interact with my google cloud projects from my command line.\n\n\nInstalling kubectl\nThe second tool I need for this project is kubectl, a command line tool used to control a kubernetes cluster. You can find installation instructions for different operating systems by visiting the kubernetes install tools page. I’m doing this from a linux machine, so I also found it useful to enable autocompletion of kubectl commands within the bash shell. The instructions for this are included in the kubectl install page for linux.\n\n\nConnect to the cluster\nNow that I have gcloud and kubectl running, I can connect to my cluster. The first thing to do is use gcloud to get the credentials needed to connect to my cluster:\n\n  export USE_GKE_GCLOUD_AUTH_PLUGIN=True\n  gcloud container clusters get-credentials donut-cluster \\\n    --zone australia-southeast1 \\\n    --project donut-art\n\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for donut-cluster.\nThen I can use kubectl to verify that it can connect to my cluster:\n\nkubectl cluster-info\n\nKubernetes control plane is running at blah blah blah\nGLBCDefaultBackend is running at blah blah blah\nKubeDNS is running at blah blah blah\nKubeDNSUpstream is running at blah blah blah\nMetrics-server is running at blah blah blah\nOkay, I may have edited the output slightly. The missing bits are the various URLs. They aren’t very interesting… the main thing to notice is that yes, kubectl can connect to my cluster and the cluster is up and running."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#kubernetes-terminology",
    "href": "posts/2023-01-10_kubernetes/index.html#kubernetes-terminology",
    "title": "Deploying R with kubernetes",
    "section": "Kubernetes terminology",
    "text": "Kubernetes terminology\nNot surprisingly, kubernetes has a lot of terminology. That’s quite daunting when you’re getting started and I’m not going to attempt a complete glossary here. Instead, let’s start with these four terms, since we’ll use them a lot:\n\nContainer. This has the same meaning it has in other contexts: a container is a self-contained executable that bundles up dependencies and runs isolated from other processes on the machine. Kubernetes supports other types of containers besides docker containers, but let’s pretend we’re only talking about docker here.\nPod. A pod is the smallest deployable unit you can create: it is an abstraction that refers to one or more containers working together. An application can have many pods running on many machines (nodes) but each pod runs on one machine. Pods are considered ephemeral. Kubernetes will have no qualms about shutting down a pod if it doesn’t seem to be doing its job, or creating new pods to replace it if it needs to,\n\nDeployment. A deployment is an abstraction that specifies a collection of pods that your application runs. Essentially it describes your “desired state” for the application. When you “apply” a deployment kubernetes will start the application running (more or less), and try to make the thing that’s actually running look like your stated deployment configuration.\nService. A service is an abstraction that specifies how the pods running on your kubernetes cluster communicate with the outside world. They’re awfully handy things to have if you want your application to be accessible on the web.\n\nConceptually, it’s also helpful to know these terms at the very beginning, even though frankly I’m not going to do anything with them here:\n\nNode. A node refers one of the machines running in your cluster.\nControl Plane. The control plane refers to a collection of processes that run together on a single node and are in charge of actually running the whole thing. We won’t need to do anything to the control plane in this post other than leave it alone and let it do its job, but it does help to know the term because it shows up everywhere.\n\nMore terms will appears as we go along – and I’ll try to explain all those when they pop up – but these are the ones that I wish I’d understood properly before I started trying to play with kubernetes clusters."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#create-a-deployment",
    "href": "posts/2023-01-10_kubernetes/index.html#create-a-deployment",
    "title": "Deploying R with kubernetes",
    "section": "Create a deployment",
    "text": "Create a deployment\nThe way to configure your kubernetes cluster is with manifest files that are written in yaml format and use the kubectl apply command to update your cluster using the instructions laid out in the manifest file. You can use a manifest to modify any aspect to your cluster configuration, and later in this post I’ll show a few more manifests, but for now here’s the deployment.yaml file I’m using to specify a deployment for the donuts application:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: donut-example\n  name: donut\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: donut-example\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: donut-example\n    spec:\n      containers:\n        - name: donut\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n\n\nFor the moment, let’s censor the metadata and everything that uses the metadata so that we can focus on what the rest of the manifest is doing:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      containers:\n        - name: [names are metadata]\n          image: ghcr.io/djnavarro/donut:main\n          imagePullPolicy: Always\n          resources:\n            requests:\n              memory: \"64Mi\"\n              cpu: \"250m\"\n            limits:\n              memory: \"128Mi\"\n              cpu: \"500m\"\n          ports:\n            - containerPort: 80\n\n\nThere’s a lot going on here, so for the moment let’s focus on the bottom.17 The section of the code from lines 13-25 is used to specify the docker containers18 that my cluster is going to deploy. There’s only one container listed in this section, and the line that reads image: ghcr.io/djnavarro/donut:main is the way that I’ve specified the docker image to use when creating the container. There are other settings I’ve used to set up this container: I’ve asked kubernetes to allocate memory and cpu resources to the container, and I’ve exposed container port 80 (which, if you can remember back that far, is where the plumber web API is running inside the container). I’ve also set imagePullPolicy: Always to ensure that every time I update this deployment kubernetes will pull the image from the registry afresh. I did that because more often than not while I was writing code for the kubernetes deployment I was tweaking the image too, and I wanted to make sure that I was always trying to deploy the most recent version of the image.\nOkay, now that we know what’s going on in that section of the code, let’s collapse that part and think about the manifest file like this:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    metadata:\n      [some metadata]\n    spec:\n      [details of one or more containers]\n\n\nOkay, so now let’s think about the bottom part of this code too. Lines 9-13 of this condensed pseudo-manifest describe some kind of template. But a template for what? Well, as clearly stated on line 13 in human(ish) language, it’s a template for “one or more containers”. In kubernetes terminology, a deployable thing that holds one or more containers is a pod… so this section of the code is describing a pod template. It’s an instruction to kubernetes that says… “hey, when you create a pod as part of this deployment, here’s the template you should use”. So we can simplify again:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  replicas: 2\n  selector:\n    [use some metadata]\n  template:\n    [details of the pod template]\n\n\nIn this condensed form we can see that lines 5-10 provide a specification of the deployment itself. I’ve given it a pod template that tells kubernetes what the pods should look like, and I’ve specified the number of “replicas”. How many copies of this pod do I want it to run in this deployment: for no good reason at all I decided to run two (i.e., two replicas).\nIf we simplify the manifest yet again, we can see the top-level description:\n\n\n\ndeployment.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata: \n  [some metadata]\nspec:\n  [details of the deployment]\n\n\nThe only other things to point out right now is that the “kind” field is used to tell kubernetes what type of object to create (a Deployment), and the “apiVersion” field is used to specify which version of the kubernetes API to use when interpreting the manifest. That’s handy to note because later on I’ll be using APIs that are a bit more specific to the google kubernetes engine.\nOkay, so now that we have some sense of what’s going on in the deployment.yaml file (ignoring the fact that I’ve glossed over the metadata bits), let’s actually apply it to our cluster:19\n\nkubectl apply -f deployment.yaml\n\nTo see if it’s working we can use kubectl get deployments:\n\nkubectl get deployments\n\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\ndonut   2/2     2            2           19h\nIt is alive.\nFor more details on this part of the process, check out the kubernetes documentation on deploying a stateless application. You may also want to look at the page on managing container resources at this point."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-no-https",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-no-https",
    "title": "Deploying R with kubernetes",
    "section": "Expose the deployment (no https)",
    "text": "Expose the deployment (no https)\nAt this point my little donut application is running happily on the cluster, but it doesn’t have a public IP address. No-one can visit it. To expose the deployment to the world you’ll need to start a service running that takes care of this for you. Exactly how you go about doing this depends on whether you want to enable https on the website. If you’re not too fussed about https the process is fairly simple and you can find details on how to do it by reading the tutorial on exposing an external IP address, and you may find the kubernetes documentation on services helpful too. The TL;DR is that it’s simple enough that you don’t even need to bother with a manifest file:\n\nkubectl expose deployment donut --type=LoadBalancer --name=donut-service\n\nThe application is now online. It has a public IP address that you can find, and if you own a domain that you want to map to that IP address all you have to do is create a DNS record that points the URL at the appropriate IP. With any luck your domain provider will have some decent documentation for this. For instance, I use google domains for djnavarro.net domain, and they have some pretty decent instructions on configuring DNS records that I could use to point donut.djnavarro.net at the IP address for my kubernetes application.\nUnfortunately for me, I am a masochist, and as such I chose the option that delivers pain."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-with-https",
    "href": "posts/2023-01-10_kubernetes/index.html#expose-the-deployment-with-https",
    "title": "Deploying R with kubernetes",
    "section": "Expose the deployment (with https)",
    "text": "Expose the deployment (with https)\n\nI like the kick in the face  And the things you do to me  I love the way that it hurts  I don’t miss you, I miss the misery    – Halestorm\n\nConfiguring the kubernetes application to use https is a bit of a pain in the ass.20 I’m deploying all this through google kubernetes engine, so the approach I took was to follow the guide for using google-managed ssl certificates. The guide is excellent… apart from a couple of minor… issues… that really fucked me21 when I tried to follow it. I’ll mention those as I go.\n\nGet a static IP\nThe first thing you have to do is create a static IP address that you’ll later use for your cluster. The gcloud compute addresses create command does that for you. Here’s how I did that for my cluster:\n\ngcloud compute addresses create donut-ip-address \\\n  --global \\\n  --project donut-art\n\n\n\n\n\n\nThis prints out a very boring message that informs you that the ID address has been created. More helpfully, now that the IP address exists you can ask google to tell you what it is:\n\ngcloud compute addresses describe donut-ip-address \\\n  --global \\\n  --project donut-art\n\nWhen you do this, the output prints out the IP address and some other details. Later on, this is the address you’ll create a DNS record for so that – in my case – the https://donut.djnavarro.net/ address points to the correct location.\n\n\nGet a certificate\nThe next step in the process is to create a managed certificate. Somebody needs to certify that my website is what it says it is.22 I’m going to need a manifest file for this, which I’ve saved as managed-cert.yaml:\n\n\n\nmanaged-cert.yaml\n\napiVersion: networking.gke.io/v1\nkind: ManagedCertificate\nmetadata:\n  name: managed-cert\nspec:\n  domains:\n    - donut.djnavarro.net\n\n\nNotice that the apiVersion field here is using something specific to GKE: I’m using google infrastructure here and they’ve kindly23 provided an API that makes it easy to use their managed certificates. The yaml here is pretty simple: I’m asking google to supply me with a certificate for my kubernetes application, which will be valid for the domain donut.djnavarro.net (you can list more than one here but I didn’t).\nNow that I have a manifest, I apply it to my cluster in the usual way:\n\nkubectl apply -f managed-cert.yaml\n\nOkay, at this point in the guide it warns you that it might take an hour or so for the certificate to be provisioned. I manage so many websites now that I’d stopped paying attention to this warning because like, 90% of the time, the thing actually happens in 20 seconds. Yeah nah, not this time babe. This one actually took an hour. We’ll come back to it. I mean, if you want to check you can try this command:\n\nkubectl describe managedcertificate managed-cert\n\nIt will print out a bunch of stuff, but when you scroll through the output you’ll very likely come across something that says that the certificate is “Provisioning”. It did work for me but it took a while so let’s move on while that is happening.\n\n\nCreate a service\nNext up is the step that fucked me in the worst possible way. It failed, because I did a copy-paste on a bit of code that I needed to edit. I did that because the guide on the google website doesn’t flag this as something you need to edit. Worse yet, it failed silently because kubernetes had no bloody way to know my manifest was fucked up. Worst of all, for at least three hours I was convinced that my error was in the later step because this step failed silently.\nSiiiiiiiiiiiiiigh. Computers were a mistake.\nAnyway, let’s start by looking at the manifest file, which I’ve called mc-service.yaml:\n\n\n\nmc-service.yaml\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: mc-service\nspec:\n  selector:\n    app.kubernetes.io/name: donut-example\n  type: NodePort\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n\n\nAt this point in the process I freely admit I’m at the edge of my own knowledge, and I don’t want to say too much about something I only barely understand myself, but there are two things I will point out about this:\n\nNotice that under spec.selector (lines 5-7) I’m referring to the name of my deployment (donut-example). That’s what the kubernetes docs tell you to do when setting up a NodePort service (see here), but the guide on the the corresponding google page that I linked to earlier (i.e., this one) misled me. It made me think I was supposed to use the name of the service (mc-service). You want this service to point at your deployment!\nNotice I’m doing everything on port 80? You probably don’t need to do this, but I found some discussion online about an old issue with kubernetes where they were hardcoding port 80 somewhere. I’m pretty certain that’s been properly resolved now and I don’t need to use port 80 for everything, but it was one of the tweaks I made on the way to figuring out the problem with spec.selector and… well… fuck it. The current version works and I’m new to kubernetes so I’m not changing it today.\n\nIn any case, now that I have a manifest file I can apply it to the cluster:\n\nkubectl apply -f mc-service.yaml\n\n\n\nCreate the DNS record\nThe next step in the process was to create a DNS record (with google domains in my case) for donut.djnavarro.net that points this subdomain to the appropriate IP address. I talked about this earlier in the post, so let’s move on…\n\n\nCreate an ingress\nIf the gods were kind we would be done, but of course we are not. I have a managed certificate and I have a service that exposes my deployment. That doesn’t mean that my application is configured to serve pages over https. To do this I need to create an ingress that manages external access to the service I created earlier and handles the SSL bit. Which is the thing I really need in order to make https work. Again…\nSiiiiiiiiiiiigh.\nOkay, here’s my mc-ingress.yaml manifest file for that:\n\n\n\nmc-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mc-ingress\n  annotations:\n    kubernetes.io/ingress.global-static-ip-name: donut-ip-address\n    networking.gke.io/managed-certificates: managed-cert\n    kubernetes.io/ingress.class: \"gce\"\nspec:\n  defaultBackend:\n    service:\n      name: mc-service\n      port:\n        number: 80\n\n\nThis manifest uses the static IP address I created (donut-ip-address), as well as the TLS certificate that I’ve asked google to provide me (managed-cert), and it specifies the mc-service I created as the backend. These things together give me https… apparently.\nAs usual, I apply the manifest to my cluster:\n\nkubectl apply -f mc-ingress.yaml \n\nI can inspect the results with kubectl get ingress:\n\nkubectl get ingress\n\nNAME         CLASS    HOSTS   ADDRESS         PORTS   AGE\nmc-ingress   &lt;none&gt;   *       34.149.195.33   80      98s\nI might still have to wait for the certificate provisioning to finish, so I’d better check again:\n\nkubectl describe managedcertificate managed-cert\n\nHere’s the relevant bit of the output showing what it looks like once it’s all working:\nSpec:\n  Domains:\n    donut.djnavarro.net\nStatus:\n  Certificate Name:    mcrt-b2204ff4-ad92-4811-a56d-f007190bb659\n  Certificate Status:  Active\n  Domain Status:\n    Domain:     donut.djnavarro.net\n    Status:     Active\nAt last. I have https. It works, which I can verify simply by visiting https://donut.djnavarro.net and seeing if my app is working. Obviously I know that it is, because the embedded image at the start of the post is doing what it’s supposed to, but just for fun I’ll do it again:\n\n\nExcellent. It works."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#epilogue",
    "href": "posts/2023-01-10_kubernetes/index.html#epilogue",
    "title": "Deploying R with kubernetes",
    "section": "Epilogue",
    "text": "Epilogue\nThe application I built is very limited, and I think it’s important to point to the things that I know it does poorly. I am sure there are others, but the big one is storage. As currently designed, the app generates a new image every time the site is visited. That’s wasteful, especially if you’re going to reuse images. You can do better than this by enabling google cloud storage, connecting to it as a volume, and writing generated images to storage when they are create. The plumber app would then check for the relevant files before trying to generate a new one. There’s a useful tutorial on volumes if you want to explore this with the google kubernetes engine.\nIt’s also worth pointing out that I have completely ignored helm, the package manager for kubernetes. Helm is excellent, and when the time comes that you want to deploy an application that someone else has designed properly, the thing you actually do is use a helm “chart”. For example, the one time I actually got spark running properly on a kubernetes cluster, the way I did it was using a terribly-useful helm chart provided by Bitnami: bitnami.com/stack/spark/helm. There’s a lot of useful tooling built up around kubernetes, and you might as well take advantage of that!\nThat being said… yes there’s a lot more to talk about, but I’m done with this post. I’m tired and unemployed, and since nobody is paying me for any of this I’m going to call it quits for today."
  },
  {
    "objectID": "posts/2023-01-10_kubernetes/index.html#footnotes",
    "href": "posts/2023-01-10_kubernetes/index.html#footnotes",
    "title": "Deploying R with kubernetes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough who knows: there’s a lot of strange on grindr.↩︎\nLots of computers working as a single unit.↩︎\nAlso known colloquially as “computers”. A node in a cluster is one of the machines that makes up the cluster. I’m endlessly entertained by the human ability to make concepts impenetrable to outsiders by writing in jargon that is never explained to novices.↩︎\nOh look it’s a trans who likes Lana, how original.↩︎\nThe author does not count.↩︎\nUnless you are Thomas Lin Pedersen, but I think it is very clear that he is much taller than me.↩︎\nAfter a staggering amount of work, mind you…↩︎\nTo be honest I am still not convinced coord_polar() is particularly useful for visualisation because I very rarely have anything to say that is naturally expressed as a pie chart, but it’s brilliant for generative art.↩︎\nThis isn’t the right place for a deep dive on serialising R objects, but I’ve written about it before.↩︎\nArguments can be passed to a plumber endpoint through the query string. For instance, the URL http://donut.djnavarro.net/?seed=6 fixes the seed to 6, so the output will always be the same as the red and black donut shown in the bottom right of the output above.↩︎\nHere’s a blog post on the difference between exposing and publishing ports in docker if you need it!↩︎\nIt doesn’t have to be public: kubernetes can pull images from private registries too, but I’m not going to bother with that sort of thing here↩︎\nIf this were something fancier you’d probably have more than one container, but I’m keeping this as simple as possible for the sake of what’s left of my sanity.↩︎\nI should mention that yes you can do all this with the gcloud command line tool, but I haven’t reached the point in the post where I talk about that yet, and in any case the point-and-click process is actually pretty easy.↩︎\nAn example would be if you plan to deploy spark on kubernetes. I’ve been playing around with that a little and for that you really need to have google back off and not delete nodes whenever the autopilot thinks you don’t need them. But that’s not the case for the donut app so I’m keeping it simple.↩︎\nAlso known as “Sydney” to those of us who live here↩︎\nToo obvious.↩︎\nKubernetes supports other types of containers besides docker, but let’s not complicate matters.↩︎\nThe -f argument in kubectl apply here tells it to use the manifest file.↩︎\nWhich may be your thing in an appropriate mutually consensual context, but it’s not everyone’s cup of tea.↩︎\nNot in the fun way.↩︎\nI could go on a long ramble about all my issues with authority figures at this point, but frankly I don’t think that certification authorities are a bad thing so perhaps just this once I’ll be a good girl and use the damn service. It’s convenient and it’s useful.↩︎\nlol no. This is a figure of speech. Nothing that google or any large tech company does is a kindness. It is self interest on their part, of course, but it’s convenient for my purposes here.↩︎"
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html",
    "href": "posts/2023-06-30_makefiles/index.html",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "",
    "text": "I have a secret to confess: I have been writing code for over 20 years, and until about a month ago I have been loathe even to try using Make. For too long I have feared catastrophic implosion should I be reckless enough attempt to dive into these dark waters.1 Even now, as the sunlight fades and I pass below the surface into the treacherous realm below, I can hear the ominous sounds of compressive stress upon my psyche. I imagine the betentacled krakenlike beasts native to this realm congregating outside the hull.\nDrums, drums in the deep.\nBut I am here now and I cannot get out. I shall have to complete this blog post in the hope that a wizard and his merry little troupe of clueless hobbits may one day discover the tale of my tragic descent and eventual demise at the hands of build automation balrogs."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#farewell-to-the-broad-sunlit-uplands",
    "href": "posts/2023-06-30_makefiles/index.html#farewell-to-the-broad-sunlit-uplands",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Farewell to the broad, Sunlit Uplands",
    "text": "Farewell to the broad, Sunlit Uplands\nEvery tragic narrative begins with a fatal mistake, the hubris of the doomed making the terrible choice that sealed their fate well before the story gets underway. In this case, that mistake was deciding that now is the time to read a 1200 page book on C++. Absolutely cursed decision. There was no way I wasn’t going to end up swallowed by a yawning hellmouth once that choice had been made. But – as the saying goes – when one descends into the abyss to be crushed by lovecraftian horrors, it’s all about the journey and not the destination.\nHere’s how the sad story unfolded. Having read through the first hundred or so pages of the C++ necronomicon (the “fucking around” stage), I started encountering the inevitable consequences of the fact that (a) C++ is a compiled language, and (b) I am a person who obsessively takes notes as she reads and writes her own code to accompany the notes. And so it came to pass that (in the “finding out” stage of this tragedy) I was barely one chapter into the book and I’d written almost 50 little baby C++ programs, every one of them a helpless monster gnashing it’s tiny teeth in ravenous hunger, demanding to be compiled before it can do anything useful.\nOh no, my precious abominations, I said to them. I already have human children to feed and care for, I’ll not fall into the trap of lovingly passing each of you individually to the compiler for nurture and sustenance with bespoke hand crafted calls to clang++. That way lies madness and chaos. No, I shall hire a metaphorical nanny/butler/build-manager to feed you and compile you when you need compiling, to politely inform me each time a little C++ demon has grown into to a new binary file, and to take care of sundry other drudgeries with which I do not wish to be burdened. I shall write a Makefile.\nAnd with that my doomed submersible slipped below the waves."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#the-decay-of-that-colossal-wreck",
    "href": "posts/2023-06-30_makefiles/index.html#the-decay-of-that-colossal-wreck",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "The decay of that colossal Wreck",
    "text": "The decay of that colossal Wreck\nAs the light fades away visions of my Ozymandian future cross my eyes. I imagine the Works that I will construct, upon which even the Mighty will gaze and despair. Hints of make targets that I will specify and the wonders that will get built with automations.\nBehold!\nHere is the Makefile I wrote for my side project. It’s a minor incantation at best, a small spell to feed my tiresome C++ babies into the maw of clang++ whenever necessary, and renders all my boring markdown scratchings into graven html with the help of pandoc.\n\ncpp := $(patsubst src/%.cpp, bin/%, $(wildcard src/*.cpp))\nnotes := $(patsubst notes/%.md, docs/%.html, $(wildcard notes/*.md))\nstatic := docs/.nojekyll docs/CNAME docs/style.css\n\nall: dirs $(cpp) $(static) $(notes)\n\ndirs:\n    @mkdir -p ./bin\n    @mkdir -p ./docs\n\n$(cpp): bin/%: src/%.cpp\n    @echo \"[compiling]\" $&lt;\n    @clang++-15 --std=c++20 $&lt; -o $@\n\n$(static): docs/%: static/%\n    @echo \"[copying]\" $&lt; \n    @cp $&lt; $@\n\n$(notes): docs/%.html: notes/%.md\n    @echo \"[rendering]\" $&lt;\n    @pandoc $&lt; -o $@ --template=./pandoc/template.html \\\n        --standalone --mathjax --toc --toc-depth 2\n\nclean:\n    @echo \"[deleting] docs\"\n    @echo \"[deleting] bin\"\n    @rm -rf docs\n    @rm -rf bin\n\nIt is not very impressive, I know. But it does work, and it does help. So perhaps I should say a little about how I got to there from here?\n\n\n\n\nThe Ulysses mosaic at the Bardo Museum in Tunis, Tunisia (2nd century AD), depicting the temptation of Odysseus by the Sirens. (Image appears to be public domain)"
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#love-me-while-your-wrists-are-bound",
    "href": "posts/2023-06-30_makefiles/index.html#love-me-while-your-wrists-are-bound",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Love me while your wrists are bound",
    "text": "Love me while your wrists are bound\nIf I’m going to write something about Makefiles, I should perhaps start by acknowledging a few important truths:\n\nI’m not an expert. Everything I know about Makefiles is from makefiletutorial.com. This post is not going to tell you anything you cannot find in Chase Lambert’s lovely tutorial.\nThere are many alternatives to Make. I’ve seen many projects use CMake for build automation, for example. Alternatively, if you’re working in R you might prefer to use the targets package by Will Landau (user manual here). There is nothing particularly special about Make per se that made me decide to learn it: it just happens to be a thing that has been around for a long time, and it was irritating me that I didn’t know how to use it.\nLike all things created by humans, it is cursed. Makefiles are indeed the Night That is Dark and Full of Terrors. The red priestesses warned us.\n\nWith that out of the way, let’s begin. Reduced to its simplest form a Makefile is a collection of build targets, each of which is defined using syntax that looks something like this:\n\ntargets: prerequisites\n    command\n    command\n    command\n\nIt seems simple enough. The top level command provides the name of the target. In the simplest case, a target is a specific file that make needs to build, and the name of the target is the path to that file, though it’s also possible to specify targets using arbitrary names\nOptionally, a target can have a set of prerequisites associated with that target. Prerequisites provide a method for specifying the dependencies for a build target. If the files listed as prerequisites have changed more recently than the output target, the build target is deemed to be “out of date”, and the commands listed beneath it will be executed in order to rebuild that target.\nA concrete example might help to make this a little clearer:\n\nbin/collatz: src/collatz.cpp\n    clang++ --std=c++20 src/collatz.cpp -o bin/collatz\n\nLet’s unpack what each part of this target means:\n\nbin/collatz is the target, and is specified as the path to the output file that we’re asking make to build for us.\nsrc/collatz.cpp is a prerequisite file. If the src/collatz.cpp file has been modified more recently than the bin/collatz file created by the compilation command underneath, then that command will be executed when make is called\nThe third line is a shell command. In this instance, the command takes the src/collatz.cpp source file and uses clang to compile it to a binary executable file bin/collatz. (The --std=c++20 flag indicates that C++ version 20 should be assumed)\n\nTargets and their prerequisites provide a mechanism by which a Makefile can be used to track the dependencies among the various files in your project. It’s worth noting a few special cases:\n\nIf a target has no prerequisites it is always deemed out of date, so the commands will be executed every time.\nIf the name of the target doesn’t correspond to an actual output file, it’s considered to be a “phony” target and is always considered out of date, and hence the commands will always be executed.\nA target can be explicitly labelled as “phony” even if the target name happens to be the same as a file in the project using the .PHONY keyword. We’ll see an example of this later.\n\nIt seems lovely, does it not? Of course it does my sweet Odysseus. You’ve been listening to the Sirens again, and fortunate indeed that your loved ones have tied you to the mast to prevent you from casting yourself overboard and drowning.\n\n“But Danielle, this seems so simple! It is lovely, alluring and sweet. I see no sign of eldritch horrors or evil creatures lurking in the depths here”\n\nYou say that, so I presume that you have absolutely noticed that all those command lines in the code snippet above are indented with tabs and not spaces, yes? No? Those tabs are like little glass knives buried in the sand beneath your soft, bare feet. You must use tabs for indentations in your Makefile, or it won’t work.\n\n“But Danielle, my IDE is set to automatically convert tabs to spaces! This is going to mess me up and now I have to faff about making exceptions for specific files”\n\nIndeed. Don’t say I didn’t warn you.\n\n\n\n\n\nThe cover art to “A Hope in Hell”, the fourth of The Sandman comics. Written by Neil Gaiman, Sam Kieth and Mike Dringenberg, and part of the “Preludes and Nocturnes” collection. Likely a copyrighted image, but hopefully okay to reproduce here under fair use etc."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#hope-in-hell",
    "href": "posts/2023-06-30_makefiles/index.html#hope-in-hell",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Hope in hell",
    "text": "Hope in hell\nPerhaps we won’t die, we whisper to ourselves as we open a blank Makefile, and point our vessel towards Scylla and Charybdis with the kind of blind optimism that typically ends with the Coroners Court issuing a lengthy report several months later. After all, our project is so very small. We are but hobbits crossing the Brandywine river looking for mushrooms or something, surely the Willow at the heart of the Old Forest won’t eat us?\nSorry. Got a little distracted there, didn’t I? I’m going to blame Morpheus… I haven’t slept very well lately and my writing gets very weird when that happens.\nGetting back on track now. When your project is very small, it isn’t hard to write a basic Makefile. Again, it helps to use concrete examples. Let us imagine a project that has this structure:\n\n\n./_examples/version1\n├── .gitignore\n├── Makefile\n└── src\n    ├── collatz.cpp\n    ├── species.cpp\n    └── swap.cpp\n\n\nIn this happy fantasy Narnia – which absolutely will never turn into a Fillory because happy endings are real, and life really truly is more than one barely-sublimated trauma after another – we have a very easy thing to work with. In the src folder we have three .cpp files that each correspond to a small C++ program that needs to be compiled.\nBeing the sort of person who likes to separate inputs from outputs, we decide that the executable binary files should all be stored in a bin folder. Being also the cautious sort of person who understands the difference between inputs and outputs, our project has a .gitignore file that ensures that nothing we write to bin is placed under version control.\nWe also have a a file called Makefile,2 whose contents are as follows:\n\n# the \"all\" target is a set of other targets\nall: dir bin/collatz bin/species bin/swap\n\n# the \"dir\" target creates a directory for the binaries\ndir:\n    mkdir -p ./bin\n\n# the \"bin/collatz\" target compiles the collatz.cpp program\nbin/collatz: src/collatz.cpp\n    clang++ --std=c++20 src/collatz.cpp -o bin/collatz\n\n# the \"bin/species\" target compiles the species.cpp program\nbin/species: src/species.cpp\n    clang++ --std=c++20 src/species.cpp -o bin/species\n\n# the \"bin/swap\" target compiles the swap.cpp program\nbin/swap: src/swap.cpp\n    clang++ --std=c++20 src/swap.cpp -o bin/swap\n\n# the \"clean\" target deletes all binary files\nclean:\n    rm -rf bin\n\nThe central part of the Makefile is familiar. We’re taking the “compile a C++ source file” recipe that I previously used as an example of makefile target, and repeating it three times over. It’s so utterly dull that it actually reads better if we strip the comments:\n\nbin/collatz: src/collatz.cpp\n    clang++ --std=c++20 src/collatz.cpp -o bin/collatz\n\nbin/species: src/species.cpp\n    clang++ --std=c++20 src/species.cpp -o bin/species\n\nbin/swap: src/swap.cpp\n    clang++ --std=c++20 src/swap.cpp -o bin/swap\n\nIt’s repetitive, but for this toy project it works. If we want this project to build, we require that all three of these C++ source files be compiled to binaries.\nSisyphus should be so lucky.\nThe nature of make targets is that you can call them by name. In the snippet above I have three targets. To build each of these I could type this mind-meltingly tedious sequence of commands at the terminal:\nmake bin/collatz\nmake bin/species\nmake bin/swap\nIt works fine when there are only a few targets, but becomes extremely painful once there are dozens of them. Life is short, and this is not the kind of masochism I enjoy. Building each target individually is simply not on my to-do list. Not now, not as Valyria sinks into its Doom, and not as Rome is burning. My fiddling time is preserved for something better than this, my babes.\nTo accommodate the need of the dead things like myself, make makes it possible to group multiple targets together:\nall: dir bin/collatz bin/species bin/swap\nThis is very helpful. Instead of typing this to make all four targets…\nmake dir\nmake bin/collatz\nmake bin/species\nmake bin/swap\n…I can now type this and get the same result:\nmake all\nIn fact, even this can be shortened, because “all” happens to be the first target listed in the Makefile. If you don’t specify a target to build, make will use the first target in the file. It is conventional, then, to call the first target “all”, and have that target consist of a list of all the other targets needed to build the whole project. Consequently, I can do this:3\n\nmake\n\nHere’s what we get as output…\n\n\nmkdir -p ./bin\nclang++ --std=c++20 src/collatz.cpp -o bin/collatz\nclang++ --std=c++20 src/species.cpp -o bin/species\nclang++ --std=c++20 src/swap.cpp -o bin/swap\n\n\n…and our project now contains the binary files:\n\n\n./_examples/version1\n├── .gitignore\n├── Makefile\n├── bin\n│   ├── collatz\n│   ├── species\n│   └── swap\n└── src\n    ├── collatz.cpp\n    ├── species.cpp\n    └── swap.cpp\n\n\nNice.\nSo, okay. This is the explanation of lines 1-19 of our Makefile. What’s going on in lines 20-22?\nI’m so glad you asked.\nWhat happens if you want to burn it all down and revert to the initial (unbuilt) state of the project? make doesn’t provide that functionality automatically, but it is traditional for writers of Makefiles to include a target called clean that includes commands that will perform this clean up job for you.4 That’s generally a good thing to do, and for this project the cleanup process is very simple. All we have to do delete the bin folder and everything in it, so that’s what our “clean” target does.\nBecause we have this target in the Makefile, all we have to do is type make clean:\n\nmake clean\n\n\n\nrm -rf bin\n\n\nAnd just like that, we are back to the clean (unbuilt) state for our project:\n\n\n./_examples/version1\n├── .gitignore\n├── Makefile\n└── src\n    ├── collatz.cpp\n    ├── species.cpp\n    └── swap.cpp\n\n\nHope yet lives, despite our descent into Hell."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#the-filetree-yggdrasil-reaching-to-the-heavens",
    "href": "posts/2023-06-30_makefiles/index.html#the-filetree-yggdrasil-reaching-to-the-heavens",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "The filetree Yggdrasil, reaching to the heavens",
    "text": "The filetree Yggdrasil, reaching to the heavens\nIn the Makefile I used in the last section, I created a separate target for every file, and wrote the code manually for every one of them. It’s a little repetitive, but when you only have a handful of files that need to be processed (… regardless of whether “processing” means compiling a source file, rendering a markdown document, or anything else), it’s not too onerous. However, it’s very common for a project to grow much too large for this to be ideal. For example, here’s the filetree for the side-project (including source files and output files) that motivated me to learn how to write Makefiles in the first place:\n\n\n\n\n\nThe world tree Yggdrasil from Norse mythology, as depicted by Friedrich Wilhelm Heine in 1886. (Public domain image)\n\n\n\n\n./_examples/learning-cpp\n├── .gitignore\n├── LICENSE.md\n├── Makefile\n├── README.md\n├── bin\n│   ├── add-with-logging\n│   ├── add-with-overloading\n│   ├── append-c-strings\n│   ├── array-danielle\n│   ├── array-iterator\n│   ├── beta-sample\n│   ├── beta-sample-2\n│   ├── char-code\n│   ├── circles\n│   ├── collatz\n│   ├── employee\n│   ├── enumerated-types\n│   ├── extended-raw-string-literal\n│   ├── file-extension\n│   ├── file-extension-2\n│   ├── gender-switch\n│   ├── gender-switch-2\n│   ├── helloworld\n│   ├── helloworld-using\n│   ├── immovable-reference\n│   ├── mean-value\n│   ├── na-na-hey-hey\n│   ├── pass-by-reference-to-const\n│   ├── pointer-free-store\n│   ├── pointer-stack\n│   ├── poisson-conditional\n│   ├── poisson-initialised-conditional\n│   ├── poisson-sample\n│   ├── raw-string-literal\n│   ├── scope-resolution\n│   ├── simple-reference\n│   ├── simple-string\n│   ├── species-first-pass\n│   ├── stoi\n│   ├── string-class-examples\n│   ├── string-class-handy\n│   ├── string-class-logical\n│   ├── string-escapes\n│   ├── string-to-numeric\n│   ├── string-vectors\n│   ├── structured-binding\n│   ├── structured-binding-asl\n│   ├── swap\n│   ├── try-catch\n│   ├── typecasting\n│   └── validation-check\n├── docs\n│   ├── .nojekyll\n│   ├── CNAME\n│   ├── chapter-01.html\n│   ├── chapter-02.html\n│   ├── chapter-03.html\n│   ├── chapter-04.html\n│   ├── index.html\n│   └── style.css\n├── notes\n│   ├── chapter-01.md\n│   ├── chapter-02.md\n│   ├── chapter-03.md\n│   ├── chapter-04.md\n│   └── index.md\n├── pandoc\n│   ├── README.md\n│   └── template.html\n├── src\n│   ├── add-with-logging.cpp\n│   ├── add-with-overloading.cpp\n│   ├── append-c-strings.cpp\n│   ├── array-danielle.cpp\n│   ├── array-iterator.cpp\n│   ├── beta-sample-2.cpp\n│   ├── beta-sample.cpp\n│   ├── char-code.cpp\n│   ├── circles.cpp\n│   ├── collatz.cpp\n│   ├── employee.cpp\n│   ├── employee.h\n│   ├── enumerated-types.cpp\n│   ├── extended-raw-string-literal.cpp\n│   ├── file-extension-2.cpp\n│   ├── file-extension.cpp\n│   ├── gender-switch-2.cpp\n│   ├── gender-switch.cpp\n│   ├── helloworld-using.cpp\n│   ├── helloworld.cpp\n│   ├── immovable-reference.cpp\n│   ├── mean-value.cpp\n│   ├── na-na-hey-hey.cpp\n│   ├── pass-by-reference-to-const.cpp\n│   ├── pointer-free-store.cpp\n│   ├── pointer-stack.cpp\n│   ├── poisson-conditional.cpp\n│   ├── poisson-initialised-conditional.cpp\n│   ├── poisson-sample.cpp\n│   ├── raw-string-literal.cpp\n│   ├── scope-resolution.cpp\n│   ├── simple-reference.cpp\n│   ├── simple-string.cpp\n│   ├── species-first-pass.cpp\n│   ├── stoi.cpp\n│   ├── string-class-examples.cpp\n│   ├── string-class-handy.cpp\n│   ├── string-class-logical.cpp\n│   ├── string-escapes.cpp\n│   ├── string-to-numeric.cpp\n│   ├── string-vectors.cpp\n│   ├── structured-binding-asl.cpp\n│   ├── structured-binding.cpp\n│   ├── swap.cpp\n│   ├── try-catch.cpp\n│   ├── typecasting.cpp\n│   └── validation-check.cpp\n└── static\n    ├── .nojekyll\n    ├── CNAME\n    └── style.css\n\n\nIt’s not a huge project by any stretch of the imagination, but it’s big enough to illustrate the point. If I had to write a separate target telling make how to process each of these files I’d quickly lose my mind. Not only that, it would be difficult to maintain if – for example – I needed to change the command used to compile the C++ source files.\nIn practice, then, we want to write Makefiles that use pattern matching to process every file that matches that pattern. For instance, in the “learning-cpp” project shown above, one of the pattern rules I need is one that automatically compiles every .cpp file in the src folder to a binary file of the same name in the bin folder.5 Conveniently enough, that’s exactly the same problem we needed to solve for the toy example. So let’s revisit it, keeping in mind that although you don’t really need pattern rules for a project as tiny as the toy project I’m using here, you really do need them as soon as your project moves into the real world.\n\n\n\n\n“The kraken, as seen by the eye of imagination”. Public domain image by John Gibson, published in Monsters of the sea, legendary and authentic, 1887"
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#release-the-kraken-of-the-imagination",
    "href": "posts/2023-06-30_makefiles/index.html#release-the-kraken-of-the-imagination",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Release the kraken of the imagination",
    "text": "Release the kraken of the imagination\nNow comes the part of the post where turbulent waters are encountered, and we the dark beasts of the depths might claim us. That is to say, we’ll start creating targets programmatically within our Makefile. To that end we’ll return to the toy project. As before, our project has the following source files:\n\n\n./_examples/version2\n├── .gitignore\n├── Makefile\n└── src\n    ├── collatz.cpp\n    ├── species.cpp\n    └── swap.cpp\n\n\nHowever, our Makefile this time around is a little different:\n\n# lists of filenames\nsrc_files := $(wildcard src/*.cpp)\nbin_files := $(patsubst src/%.cpp, bin/%, $(src_files))\n\n# the \"all\" target is much simpler now\nall: dir $(bin_files)\n\ndir:\n    mkdir -p ./bin\n\n# each C++ binary is a target, the source is its prerequisite\n$(bin_files): bin/%: src/%.cpp\n    clang++ --std=c++20 $&lt; -o $@\n\nclean:\n    rm -rf bin\n\nLet’s go through this line by line. First, we can use wildcard matching to find all files in the src folder that end with the .cpp file extension:\nsrc_files := $(wildcard src/*.cpp)\nIt may not be immediately apparent to – oh, say, humans – but this is in fact a function call. The name of the function is wildcard, the $( ) syntax with the function name inside the parentheses is the way you call functions in make,6 and src/*.cpp is the argument passed to the function.\nIt may also not be obvious upon first inspection – because again, why would it be? – why I’ve used := instead of = in my assignment statement. The goal here is to create a new variable called src_files that contains the names of the various source files, that much is clear. But why use :=, exactly? The answer, of course, is that make supports several different kinds of assignment operators, and confusingly enough = is not the operator for “simple” assignment:\n\nUse := if you want “simple assignment”: the assignment happens once and only once, the first time the assignment statement is encountered\nUse = if you want “recursive assignment”: the assignment is reevaluated every time the value of the right hand side changes (e.g., in this example, if a later make target changes the list of source files in the src folder, the value of src_files changes too)\nUse ?= if you want “conditional assignment”: the assignment only happens if the variable doesn’t already have a value (sure, normal humans would use an if-statement for this, but as we all know keystrokes are a precious resource and must be conserved; preserving human sanity is of course a much less important goal)\nUse += if you want the value of the right hand side to be added to the variable rather than replacing its existing value.\n\n\n\nIt sure doesn’t seem like I should have had to write a small manuscript simply to explain one very modest line of code, does it? But such is the nature of make.\nIn any case, the thing that matters here is we’ve scanned the src folder and created a variable called src_files that lists all the C++ source code files in that folder. In other words, src_files is now a synonym for this:\nsrc/collatz.cpp src/species.cpp src/swap.cpp\nThis will now form the basis by which we construct a list of build targets. Because our project is very simple and has a nice one-to-one mapping between source files and output files, what we really want to construct now is a variable that contains a list of build targets like this:\nbin/collatz bin/species bin/swap\nIf we could be assured that the binary files always exist, we could use the same trick to list all binaries in the bin folder. But because those might not exist (e.g., if we delete the binaries when calling make clean), we can’t be assured of that. So instead, we’ll use the patsubst function to do a pattern substitution: we’ll take the src_files variable as input, strip the .cpp extension from the files, and replace src with bin. Here’s what that looks like:\nbin_files := $(patsubst src/%.cpp, bin/%, $(src_files))\nThe patsubst function takes three arguments, and – of course – they are specified in a weird order. The data argument appears in the third position, because again… why not? The pattern to which we match the data appears in the first position, and the replacement pattern appears in the second position.7 Anyway, the point here is that what this function call does is as follows: it takes all the filenames in src_files, matches them against src/%.cpp to find the “stem” (e.g., the stem for src/collatz.cpp is the part that matches the % operator, i.e., collatz), and then uses the replacement pattern bin/% to construct output values from the stems (e.g., collatz is transformed to bin/collatz). And so we end up with a variable bin_files that contains the list of target files we want to build:\nbin/collatz bin/species bin/swap\nNow that we have this, we can define the “all” target using this variable, as follows:\nall: dir $(bin_files)\nFrom the make perspective this is equivalent to:\nall: dir bin/collatz bin/species bin/swap\nOr, to put it another way, by using the bin_files variable, we can programmatically ensure that the “all” target includes a target for every binary file that needs to be compiled.\nHaving defined a list of targets programmatically, our next task8 is to write a static pattern rule that programmatically defines the targets themselves. Specifically, for every target listed in bin_files, we want (1) to assert that it relies on the corresponding source file as a prerequisite, and (2) to specify a build action that compiles the binary from the corresponding source.\nHere’s some code that does this:\n$(bin_files): bin/%: src/%.cpp\n    clang++ --std=c++20 $&lt; -o $@\nThe underlying syntax here is as follows:\ntargets: target-pattern: prerequisites-patterns\n    commands\nFor our example, the bin_files variable contains the list of targets specified by the pattern rule. The bin/% part (the target pattern) and the src/%.cpp part (the prerequisites pattern) are used for pattern substitution purposes. It’s essentially the same task that we saw when I called patsubst using these patterns earlier: in the previous example I used them to construct the name of a binary file from the corresponding source file, this time I’m going the other direction and constructing the name of the source file (to use as a rerequisite) from the binary file (which is used as the target).\nOkay, now let’s turn to the second line of the code snippet. In the orginal version of the code I wrote targets like this:\nbin/collatz: src/collatz.cpp\n    clang++ --std=c++20 src/collatz.cpp -o bin/collatz\nBut in the static pattern rule version I’ve used $&lt; to refer to the prerequisite file (e.g., the source file src/collatz.cpp) and $@ to refer to the file name of the target (e.g., the binary file bin/collatz). These are both examples of automatic variables in make. There are quite a lot of these: $@, $%, $&lt;, $?, $^, $+, $|, $*. Some of these have “D” and “F” variants that specifically refer to directory paths or filenames: $(@D) and $(@F) are variations on $@, $(*D) and $(*F) are variants of $* and so on. If you desperately want to learn all these details the linked page explains them all. For our purposes it’s enough to note that in the example above, I’ve used $&lt; to refer to the source file and $@ to refer to the output file.\nRight. After all that as explanatory background we can run make, and happily see that the results are indeed the same as before:\n\nmake\n\n\n\nmkdir -p ./bin\nclang++ --std=c++20 src/collatz.cpp -o bin/collatz\nclang++ --std=c++20 src/species.cpp -o bin/species\nclang++ --std=c++20 src/swap.cpp -o bin/swap\n\n\nAnd now that we’ve built the project we see that the filetree now contains the binaries:\n\n\n./_examples/version2\n├── .gitignore\n├── Makefile\n├── bin\n│   ├── collatz\n│   ├── species\n│   └── swap\n└── src\n    ├── collatz.cpp\n    ├── species.cpp\n    └── swap.cpp\n\n\n\n\n\n\n\n“Me(dusa) too”. Oil painting of the mythological character, Medusa, reimagined through a contemporary feminist lens, in response to the #metoo movement.9 Art by Judy Takács. Released by the artist as CC-BY."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#the-tragedy-of-medusa-and-what-is-permitted-to-be-seen-and-said",
    "href": "posts/2023-06-30_makefiles/index.html#the-tragedy-of-medusa-and-what-is-permitted-to-be-seen-and-said",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "The tragedy of Medusa, and what is permitted to be seen and said",
    "text": "The tragedy of Medusa, and what is permitted to be seen and said\nThe last step in putting together a Makefile for our toy project is to tidy some of the code, and make choices about what messages are printed to the terminal when make is called. Let’s start with the tidying. It was convenient for expository purposes to create the list of targets as a two-step process, so that I could talk about the wildcard function before introducing the patsubst function:\nsrc_files := $(wildcard src/*.cpp)\nbin_files := $(patsubst src/%.cpp, bin/%, $(src_files))\nBut realistically this doesn’t need to be two lines, so I’ll shorten it to a single line that generates the list of compilation targets:\ncompile := $(patsubst src/%.cpp, bin/%, $(wildcard src/*.cpp))\nThe second task is to add some code modifying the messages printed when targets are built. To do this, I’ll preface all my commands with the @ symbol, which silences their raw output, thereby preventing them from being printed to the terminal whenever make is called. In place of the automatic printing, I’ll use echo to write my own, more human-friendly output lines. So now my Makefile looks like this:\n\ncompile := $(patsubst src/%.cpp, bin/%, $(wildcard src/*.cpp))\n\nall: dir $(compile)\n\ndir:\n    @mkdir -p ./bin\n\n$(compile): bin/%: src/%.cpp\n    @echo \"compiling\" $&lt; \"to\" $@\n    @clang++ --std=c++20 $&lt; -o $@\n\nclean:\n    @echo \"deleting binary files\"\n    @rm -rf bin\n\nLet’s have a look at what happens when we call make using this version of the Makefile. The same files are compiled, but the printed messages are prettier:\n\nmake\n\n\n\ncompiling src/collatz.cpp to bin/collatz\ncompiling src/species.cpp to bin/species\ncompiling src/swap.cpp to bin/swap\n\n\nMuch nicer.\n\n\n\n\nMosaic by Sebald Beham depicting Hercules and Iolaus slaying the many-headed Hydra of Lerna, 1545. Public domain image."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#the-fourth-wall-shatters-into-little-shards-of-recursion",
    "href": "posts/2023-06-30_makefiles/index.html#the-fourth-wall-shatters-into-little-shards-of-recursion",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "The fourth wall shatters into little shards of Recursion",
    "text": "The fourth wall shatters into little shards of Recursion\nAt this point, this post has covered all the tricks that I’m using in the Makefile for the accursed C++ side project that motivated me to learn make. What this post hasn’t yet covered, though, are some of the tricks that I needed to use for… um… this post. This quarto blog post is a project, and it has a Makefile. But the folder that contains all the source for this blog post also contains source files for all the sub-projects that I’ve used as the examples… and each of those has its own Makefile. Our simple project has become a multi-headed monster, a poisonous serpentine water beast.\nTo create a Makefile that works in this situation we need to call call make recursively, and though much beloved by computer scientists, I personally view recursion as the little death and the exsanguination of hope. To do this with make some care is required. The thing you don’t want to do is literally use the make command inside a Makefile. That’s exactly the kind of intuitive strategy that get us slain by the poison breath of the Hydra. Instead, we use the $(MAKE) variable as an alias for make. To illustrate this let’s take a look at the actual Makefile used to build this post:\n\npost := 2023-06-30_makefiles\nhtml := ../../_site/posts/$(post)/index.html\nexamples = version1 version2 version3\n\n# explicitly state that these targets aren't file names\n.PHONY: all clean clean_quarto\n.PHONY: $(patsubst %, build_%, $(examples))\n.PHONY: $(patsubst %, clean_%, $(examples))\n\nall: $(patsubst %, build_%, $(examples)) $(html)\n\n$(patsubst %, build_%, $(examples)): build_%: _examples/%\n    @echo \"------------ building\" $&lt; \"------------\"\n    @$(MAKE) -C $&lt; --no-print-directory\n\n$(html): index.qmd\n    @echo \"------------ rendering quarto ------------\"\n    @echo \"rendering\" $@\n    @quarto render $&lt; --quiet\n\n$(patsubst %, clean_%, $(examples)): clean_%: _examples/%\n    @$(MAKE) clean -C $&lt; --no-print-directory\n\nclean_quarto: \n    @rm -rf ../../_site/posts/$(post)\n    @rm -rf ../../_freeze/posts/$(post)\n\nclean: $(patsubst %, clean_%, $(examples)) clean_quarto\n\nThere are some other new tricks in play here. When I call make via the $(MAKE) alias, I’m passing some additional flags: the -C flag tells make to change directories (I could also have used --directory here in place of -C), and the --no-print-directory flag asks make to do so without printing an annoyingly long message informing me that it has done so. As usual $&lt; refers to a prerequisite (e.g., _examples/version1). In other words, this command…\n@$(MAKE) -C $&lt; --no-print-directory\n… has essentially the same effect as a bash command that changes to the appropriate directory, calling make there, and then returning to the original directory:\ncd _examples/version1\nmake\ncd ../..\nThere’s another trick in play here too. At the start of the file I’ve made use of .PHONY to declare explicitly that many of my targets don’t refer to real files, and are merely labels for recipes. I’ve been lazy about that up till now,10 but it does matter in a lot of contexts.\nIn any case, here’s what I get as output when I make this post:\n\nmake\n\n------------ building _examples/version1 ------------\nmkdir -p ./bin\nclang++ --std=c++20 src/collatz.cpp -o bin/collatz\nclang++ --std=c++20 src/species.cpp -o bin/species\nclang++ --std=c++20 src/swap.cpp -o bin/swap\n------------ building _examples/version2 ------------\nmkdir -p ./bin\nclang++ --std=c++20 src/collatz.cpp -o bin/collatz\nclang++ --std=c++20 src/species.cpp -o bin/species\nclang++ --std=c++20 src/swap.cpp -o bin/swap\n------------ building _examples/version3 ------------\ncompiling src/collatz.cpp to bin/collatz\ncompiling src/species.cpp to bin/species\ncompiling src/swap.cpp to bin/swap\n------------ rendering quarto ------------\nrendering ../../_site/posts/2023-06-23_makefiles/index.html\nEach of the example projects gets built, with a pretty header line to explain which project is building at each step of the process, and then finally the quarto document is rendered also. Somewhat awkwardly though, there’s some indirect recursion going on also: the quarto document calls make several times internally in order to generate much of the output shown in this post. It doesn’t actually break anything, but it does mean it’s a little harder for make to infer when one of the submakes is out of date. Indirect recursion is a strange beast at the best of times, but fortunately it doesn’t cause a lot of problems in this case."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#epilogue-and-the-death-of-the-author",
    "href": "posts/2023-06-30_makefiles/index.html#epilogue-and-the-death-of-the-author",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Epilogue, and the Death of the Author",
    "text": "Epilogue, and the Death of the Author\nThis was a strange post, and I honestly have no idea how to wrap it all up. If you do want to learn more about Makefiles, I highly recommend the walkthrough at makefiletutorial.com. It’s how I learned. As for the rest of the narrative… I don’t know what that was all about? I was bored, I guess."
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#postscript",
    "href": "posts/2023-06-30_makefiles/index.html#postscript",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Postscript",
    "text": "Postscript\nAfter sharing this post on mastodon some folks suggested a few other resources related to make and other build automation tools. So here’s a list of resources I’ve either used in this post, or someone else suggested to me afterwards:\n\nmakefiletutorial.com is the tutorial I learned from\nhere’s the documentation for GNU make\na blog post by Mike Bostock: why use make\na blog post by Jake Howard: just! stop using make\nfor R users, there is the usethis::use_make() function which was new to me\n\nLinks to some related tools:\n\ntargets\ncmake\nsnakemake\nrake\njust\ninvoke\ntask"
  },
  {
    "objectID": "posts/2023-06-30_makefiles/index.html#footnotes",
    "href": "posts/2023-06-30_makefiles/index.html#footnotes",
    "title": "Makefiles. Or, the balrog and the submersible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI did ponder briefly the question of whether this joke is in poor taste. On the one hand, it probably is. On the other hand, I can’t help but notice there’s a remarkable number of people who suddenly come out of the woodwork to handwringing about the horrors of ordinary people making jokes at the expense of reckless rich people who came to a relatively painless end due to their own overwhelming hubris, while Dave Chappelle and Ricky Gervais are both out there making bank by mocking and belittling the most vulnerable people in society. Pick your battles my sweet things. Pick your battles.↩︎\nTraditionally a Makefile is simply named Makefile or makefile. It doesn’t have to be, but if you call it something else you need to explicitly tell make where to find the file using the -f flag. A command like make -f my_make_file, for example, specifies that the Makefile is called my_make_file.↩︎\nAdmittedly, this implicitly assumes that I’m executing the make command from the same directory as the Makefile itself. That creates some awkwardness for this blog post because the quarto file is not in the same folder as the Makefile. So when you look at the source code for this post you’ll see I’m doing something slightly different. But let’s put those particular nightmares on layby shall we? Instead, let’s see what horrors escape from the particular Pandora’s box that happens to sit before us.↩︎\nDon’t include “clean” in the list of “all” targets, obviously: that would defeat the point entirely.↩︎\nThis seems as good a moment as any to mention that yes, I am indeed aware of the implicit rules that are very often used in Makefiles to do common tasks like compiling C code without explicitly calling the compiler. I’ve chosen not to use those here because, quite frankly, implicit compilation rules make me uncomfortable.↩︎\nOh yes, make uses infix notation for functions. Of course it does, for the same reason that it mandates tab indentation… because make is the very quintessence of evil design. It’s useful enough to weasel its way into your projects, at which point it then slowly drives you toward the pit of despair by making design choices that seem chosen deliberately to make you feel like an idiot. Case in point, you can use ${ } instead of $( ) to call a function if you like. Because why not?↩︎\nThe fact that this happens to be the same batshit argument ordering used in the base R gsub() function makes me suspect that there is some historical reason for this that involves being lectured about grep for about an hour. Anyway there’s a reason why almost everyone who uses R in real world vastly prefers the stringr pattern matching API over the base R API. But I digress.↩︎\nI’m skipping over the dir target on lines 8 and 9, because the code here is the same as it was in the original version. It’s very boring: it just makes sure that a bin folder exists.↩︎\nYou probably know why this piece speaks to me, and why I chose to include it even though it’s a slight departure from the narrative. If not, well, I’ll leave it for you to guess.↩︎\nThe .ALLCAPS thing going on here tells us that .PHONY is one of the special built-in target names that have particular meaning in make.↩︎"
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html",
    "title": "Data serialisation in R",
    "section": "",
    "text": "I still alive, and that’s what matters. The traumatic experience of the last week is fading, leaving a pale residue of fear and the few scraps of writing that are the sole surviving documentation of these days. It is a tale of fright, a desperate agony, and like any good tragedy it starts with the hope and naive optimism of youth…\nI’ve decided the time has come for me to do a deep dive into data serialisation in R. Serialisation is one of those terms that comes up from time to time in data science, and it’s popped up so many times on my twitter feed that I feel like I need a better grasp of how serialisation works in R. It’s a topic that folks who work with big data or have a computer science background likely understand quite well, but a lot of people who use R come from other backgrounds. If you’re a social scientist who mostly works with small CSV files, for example, there’s no particular reason why you’d have encountered this. In my case, I’ve worked as a mathematical psychologist and computational modeller for about 20 years, and until very recently I’ve never had never had to think about it in any detail. The issue only came up for me when I started reading about Apache Arrow (a topic for another post, perhaps) and realised that I needed to have a better understanding of what all this data serialisation business is about, and how R handles it.\nThis post is aimed at anyone who is in a similar situation to me!\nOh you sweet summer child. You really think you are prepared for the dark? That’s adorable.\nImage by Andrey Zvyagintsev. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#what-is-serialisation",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#what-is-serialisation",
    "title": "Data serialisation in R",
    "section": "What is serialisation?",
    "text": "What is serialisation?\nIn general serialisation refers to any process that takes an object stored in memory and converts into a stream of bytes that can be written to a file or transmitted elsewhere. Any time we write data to a file, we are “serialising” it according to some encoding scheme. Suppose, for instance, I have a data frame called art:\n\nart\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nThis data frame is currently stored in memory on my machine, and it has structure. R represents this data frame as a list of length 6. Each element of this list is a pointer to another data structure, namely an atomic vector (e.g., numeric vector). The list is accompanied by additional metadata that tells R that this particular list is a data frame. The details of how this is accomplished don’t matter for this post. All that matters for now is that the in-memory representation of art is a structured object. It’s little more complicated than a stream of data, but if I want to save this data to a file it needs to be converted into one. The process of taking an in-memory structure and converting it to a sequence of bytes is called serialisation.\nSerialisation doesn’t have to be fancy. The humble CSV file can be viewed as a form of serialisation for a data frame, albeit one that does not store all the metadata associated with the data frame. Viewed this way, write.csv() can be viewed as a serialisation function for tabular data:\n\nwrite.csv(art, file = \"art.csv\", row.names = FALSE)\n\nWhen I call this function R uses the art object to write text onto the disk, saved as the file “art.csv”. If I were to open this file in a text editor, I’d see this:\n\n\n\"resolution\",\"series\",\"sys_id\",\"img_id\",\"short_name\",\"format\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n1000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n2000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n4000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n500,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"jpg\"\n8000,\"watercolour\",\"sys02\",\"img34\",\"teacup-ocean\",\"png\"\n\n\nAlthough this view is human-readable, it is slightly misleading. The text in shown above isn’t the literal sequence of bytes. It’s how those bytes are displayed when the have been unserialised and displayed on screen as UTF-8 plain text. To get a sense of what serialised text actually looks like we can use the charToRaw() function. The first few characters of the text file are \"resolu\" which looks like this when series of bytes:\n\ncharToRaw('\"resolu')\n\n[1] 22 72 65 73 6f 6c 75\n\n\nThe raw vector shown in the output above uses one byte to represent each character. For instance, the character \"l\" is represented with the byte 6c in the usual hexadecimal representation. We can unpack that byte into its consituent 8-bit representation using rawToBits()\n\n\"u\" |&gt;\n  charToRaw() |&gt;\n  rawToBits()\n\n[1] 01 00 01 00 01 01 01 00\n\n\n(Note that the base pipe |&gt; is rendered as a triangle-shaped ligature in Fira Code)\nReturning to the “art.csv” data file, I can use file() and readBin() to define a simple helper function that opens a binary connection to the file, reads in the first 100 bytes (or whatever), closes the file, and then returns those bytes as a raw vector:\n\nread_bytes &lt;- function(path, max_bytes = 100) {\n  con &lt;- file(path, open = \"rb\")\n  bytes &lt;- readBin(con, what = raw(), n = max_bytes)\n  close(con)\n  return(bytes)\n}\n\nHere are the first 100 bytes of the “art.csv” file:\n\nread_bytes(\"art.csv\")\n\n  [1] 22 72 65 73 6f 6c 75 74 69 6f 6e 22 2c 22 73 65 72 69 65 73 22 2c 22 73 79\n [26] 73 5f 69 64 22 2c 22 69 6d 67 5f 69 64 22 2c 22 73 68 6f 72 74 5f 6e 61 6d\n [51] 65 22 2c 22 66 6f 72 6d 61 74 22 0a 31 30 30 30 2c 22 77 61 74 65 72 63 6f\n [76] 6c 6f 75 72 22 2c 22 73 79 73 30 32 22 2c 22 69 6d 67 33 34 22 2c 22 74 65\n\n\nThe read.csv() function is similar to read_bytes() in spirit: when I call read.csv(\"art.csv\"), R opens a connection to the “art.csv” file. It then reads that sequence of bytes into memory, and then closes the file. However, unlike my simple read_bytes() function, it does something useful with that information. The sequence of bytes gets decoded (unserialised), and the result is that R reconstructs the original art data frame:\n\nart &lt;- read.csv(\"art.csv\")\nart\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nThrilling stuff.\n\nDo you feel that slow dread yet, my dear? Do you feel yourself slipping? You are on the edge of the cliff. You can still climb back to safety if you want. You don’t have to fall. The choice is still yours.\n\n\n\n\n\n\nImage by Daniel Jensen. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#how-does-rds-serialisation-work",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#how-does-rds-serialisation-work",
    "title": "Data serialisation in R",
    "section": "How does RDS serialisation work?",
    "text": "How does RDS serialisation work?\nData can be serialised in different ways. The CSV format works reasonably well for rectangular data structures like data frames, but doesn’t work well if you need to serialise something complicated like a nested list. The JSON format is a better choice for those cases, but it too has some limitations when it comes to storing R objects. To serialise an R object we need to store the metadata (classes, names, and other attributes) associated with the object, and if the object is a function there is a lot of other information relevant to its execution besides the source code (e.g., enclosing environment). Because R needs this information, it relies on the native RDS format to do the work. As it happens I have an “art.rds” file on disk that stores the same data frame in the RDS format. When I use readRDS() to unserialise the file, it recreates the same data frame:\n\nreadRDS(\"art.rds\")\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nHowever, when I read this file using read_bytes() it’s also clear that “art.rds” contains a very different sequence of bytes to “art.csv”:\n\nread_bytes(\"art.rds\")\n\n  [1] 1f 8b 08 00 00 00 00 00 00 03 8b e0 62 60 60 60 66 60 61 64 62 60 66 05 32\n [26] 19 58 43 43 dc 74 2d 80 62 c2 40 0e 1b 10 f3 02 31 50 11 f3 0b 08 66 bf 00\n [51] c1 fc 0b 20 98 f1 0b 04 cb 3b 40 30 83 00 58 3d 0b 03 27 90 e6 2e 4f 2c 49\n [76] 2d 4a ce cf c9 2f 2d 1a 4a 42 a8 be 60 2d ae 2c 36 30 1a 18 0e 9a 4b 32 73\n\n\nThis is hardly surprising since RDS and CSV are different file formats. But while I have a pretty good mental model of what the contents of a CSV file look like, I don’t have a very solid grasp of what the format of an RDS file is. I’m curious.\n\nOh sweetie, I tried to warn you…\n\n\nThe serialize() function\nTo get a sense of how the RDS format works, it’s helpful to note that R has a serialize() function and an unserialize() function that provide low-level access to the same mechanisms that underpin saveRDS() and readRDS().\n\nbytes &lt;- serialize(art, connection = NULL)\n\nAs you can see, this is the same sequence of bytes returned by read_bytes()…\n\nbytes[1:100]\n\n  [1] 58 0a 00 00 00 03 00 04 03 00 00 03 05 00 00 00 00 05 55 54 46 2d 38 00 00\n [26] 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03 e8 00 00 03 e8 00 00 07\n [51] d0 00 00 07 d0 00 00 0f a0 00 00 0f a0 00 00 01 f4 00 00 01 f4 00 00 1f 40\n [76] 00 00 1f 40 00 00 00 10 00 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\n\n…oh wait, no it’s not. What gives???? The “art.rds” file begins with 1f 8b 08 00, whereas serialize() returns a sequence of bytes that begins with 58 0a 00 00. These are not the same at all! Why is this happening???\n\n\nRDS uses gzip compression\nAfter digging a little into the help documentation, I realised that this happens because the default behaviour of saveRDS() is to write a compressed RDS file using gzip compression. In contrast, serialize() does not employ any form of compression. The art.rds file that I have stored on disk is that gzipped version, but it’s easy enough to save an uncompressed RDS file, simply by setting compress = FALSE:\n\nsaveRDS(art, file = \"art_nozip.rds\", compress = FALSE)\n\nSo now when I inspect the uncompressed file using read_bytes(), the output is the same one I obtained when I called serialize(art) earlier:\n\nread_bytes(\"art_nozip.rds\")\n\n  [1] 58 0a 00 00 00 03 00 04 03 00 00 03 05 00 00 00 00 05 55 54 46 2d 38 00 00\n [26] 03 13 00 00 00 06 00 00 00 0d 00 00 00 0a 00 00 03 e8 00 00 03 e8 00 00 07\n [51] d0 00 00 07 d0 00 00 0f a0 00 00 0f a0 00 00 01 f4 00 00 01 f4 00 00 1f 40\n [76] 00 00 1f 40 00 00 00 10 00 00 00 0a 00 04 00 09 00 00 00 0b 77 61 74 65 72\n\n\nThat’s a relief. I was getting very anxious there, but I feel a little better now. My sanity is restored.\n\n…for now.\n\n\n\nThe unserialize() function\nThat was frustrating. Anyway getting back to the main thread, the inverse of the serialize() function is unserialize(). It’s very similar to the readRDS() function that you’d normally use to read an RDS file, but you can apply it to a raw vector like bytes. Once again we reconstruct the original data frame:\n\nunserialize(bytes)\n\n   resolution      series sys_id img_id   short_name format\n1        1000 watercolour  sys02  img34 teacup-ocean    jpg\n2        1000 watercolour  sys02  img34 teacup-ocean    png\n3        2000 watercolour  sys02  img34 teacup-ocean    jpg\n4        2000 watercolour  sys02  img34 teacup-ocean    png\n5        4000 watercolour  sys02  img34 teacup-ocean    jpg\n6        4000 watercolour  sys02  img34 teacup-ocean    png\n7         500 watercolour  sys02  img34 teacup-ocean    jpg\n8         500 watercolour  sys02  img34 teacup-ocean    png\n9        8000 watercolour  sys02  img34 teacup-ocean    jpg\n10       8000 watercolour  sys02  img34 teacup-ocean    png\n\n\nYay.\n\nYou can sense it can’t you? It will only get worse for you, my sweet. Look upon the grim visage of those that have passed this way before. Their lifeless bones are a warning.\n\n\n\n\n\n\nImage by Chelms Varthoumlien. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#serialising-to-plain-text-rds",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#serialising-to-plain-text-rds",
    "title": "Data serialisation in R",
    "section": "Serialising to plain text RDS",
    "text": "Serialising to plain text RDS\nOkay, so what I’ve learned so far is that in most cases, an RDS file is just a gzipped version of … something. It’s the gzipped version of whatever the hell it is that serialize() creates. What I don’t yet know is how the serialize() function operates. What secret magic does it use? How does it construct this sequence of bytes? What do the contents of this file actually include?\nI’ll start simple. Trying to understand how a complicated object is serialised might be painful, so I’ll set the art data frame to one side. Instead, I’ll serialise a numeric vector containing three elements, and … I guess I’ll set ascii = TRUE so that R uses UTF-8 to serialise the object to plain text format rather than … writing a binary file?\n\nClever girl. Yes, the default behaviour is binary serialization. Unless otherwise specified using the xdr argument, serialize() enforces a big-endian representation on the binary encoding. But you didn’t want to go there did you? It frightened you, didn’t it? The abyss stares back at you, sweetness, and you are beginning to attract its attention\n\n\nbytes &lt;- serialize(\n  object = c(10.1, 2.2, 94.3), \n  connection = NULL,\n  ascii = TRUE\n)\n\nWhen I print out the bytes vector I still don’t get text though?\n\nbytes\n\n [1] 41 0a 33 0a 32 36 32 39 31 32 0a 31 39 37 38 38 38 0a 35 0a 55 54 46 2d 38\n[26] 0a 31 34 0a 33 0a 31 30 2e 31 0a 32 2e 32 0a 39 34 2e 33 0a\n\n\nI was expecting text. Where is my text??? I dig a little deeper and realise my mistake. What I’m looking at here is the sequence of bytes that correspond to the UTF-8 encoded text. If I want to see that text using actual letters, I need to use rawToChar(). When I do that I see something that looks vaguely like data:\n\nrawToChar(bytes)\n\n[1] \"A\\n3\\n262912\\n197888\\n5\\nUTF-8\\n14\\n3\\n10.1\\n2.2\\n94.3\\n\"\n\n\nIt is a little easier to read if I use cat() to print the output:\n\nbytes |&gt;\n  rawToChar() |&gt;\n  cat()\n\nA\n3\n262912\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\n\nIt’s… not immediately obvious how this output should be interpreted? I don’t know what all these lines mean, but I recognise the last three lines: those are the three values stored in the vector I serialised. Now I just need to work out what the rest of it is all about.\nBut before I do, I’ll check that this is exactly the same text that I see if I create an RDS file using the following command and then open that file in a text editor:\n\nsaveRDS(\n  object = c(10.1, 2.2, 94.3), \n  file = \"numbers.rds\", \n  ascii = TRUE, \n  compress = FALSE\n)\n\nOkay, it checks out. My excitement can barely be contained.\n\nWilting already, aren’t you? Poor little flower, you’ve been cut from the stem. You’re dead already but you don’t even know it. All that is left is to wither away under the blistering glare of knowledge.\n\n\n\n\n\n\nImage by Daria Shevtsova. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#interpreting-the-rds-format",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#interpreting-the-rds-format",
    "title": "Data serialisation in R",
    "section": "Interpreting the RDS format",
    "text": "Interpreting the RDS format\nAll right, lets see if I can interpret the contents of an RDS file. Rather than tediously writing the file to disk using saveRDS() and then loading it again, I’ll cheat slightly and write a show_rds() function that serialises an object and prints the results directly to the R console:\n\nshow_rds &lt;- function(object, header = TRUE) {\n  rds &lt;- object |&gt;\n    serialize(connection = NULL, ascii = TRUE) |&gt;\n    rawToChar() |&gt;\n    strsplit(split = \"\\n\") |&gt;\n    unlist()\n  if(header == FALSE) rds &lt;- rds[-(1:6)]\n  cat(rds, sep = \"\\n\")\n}\n\nJust to make sure it’s doing what it’s supposed to I’ll make sure it gives the output I’m expecting. Probably a good idea given how many times I’ve been surprised so far…\n\nshow_rds(object = c(10.1, 2.2, 94.3))\n\nA\n3\n262912\n197888\n5\nUTF-8\n14\n3\n10.1\n2.2\n94.3\n\n\nOkay, phew. That looks good.\nI guess my next task is to work out what all this output means. The last three lines are obvious: that’s the data! What about the line above the data? That line reads 3 and is followed by three data values. I wonder if that’s a coincidence? I’ll see what happens if I try to serialise just 2 numbers. Does that line change to 2?\n\nshow_rds(object = c(10.1, 2.2))\n\nA\n3\n262912\n197888\n5\nUTF-8\n14\n2\n10.1\n2.2\n\n\nYes. Yes it does. I am learning things.\nHere’s what I know so far:\nA\n3\n262402\n197888\n5\nUTF-8\n14\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\nOkay, so what’s next? The 14 in the preceding line. What does that mean?\nI puzzled over this for a while, and ended up needing to consult an occult tome of dangerous lore – the R Internals Manual – to find a partial answer. On the very first page of the Infernals Manual there is a table listing the SEXPTYPE codes that R uses internally to specify what kind of entity is encoded by an R object. Here are a few of these SEXPTYPE codes:\n\n\n\nValue\nSEXPTYPE\nVariable type\n\n\n\n\n10\nLGLSXP\nlogical\n\n\n13\nINTSXP\ninteger\n\n\n14\nREALSXP\nnumeric\n\n\n16\nSTRSXP\ncharacter\n\n\n19\nVECSXP\nlist\n\n\n\nSo… when I serialise a plain numeric vector, the RDS file writes the number 14 to the file. In that case I will tentatively update my beliefs about the RDS file\nA\n3\n262402\n197888\n5\nUTF-8\n14     # the object is numeric\n3      # the object has length 3\n10.1   # first value is 10.1\n2.2    # second value is 2.2\n94.3   # third value is 94.3\n\nOh no dear. You have strayed so far from the light already. That 14 carries much more meaning than your fragile mind is prepared to handle. Soon you will know better. Soon you will unravel entirely. You can feel it coming, can’t you?\n\n\n\n\n\n\nImage by Roxy Aln Available by CC0 licence on unsplash.\n\n\n\n\n\nThe RDS header\nAt this point, I have annotated every part of the RDS file that corresponds to the actual object. Consulting the section of the Infernal Manual devoted to serialisation, I learn that the six lines at the beginning of the file are known as the RDS header. Reading further I learn that the first line specifies the encoding scheme (A for ASCII, X for binary big-endian). The second line specifies which version of the RDS file format is used. The third line indicates the version of R that wrote the file. Finally, the fourth line is the minimum version of R required to read the file.\nIf I annotate my RDS header to reflect this knowledge, I get this:\nA       # use ASCII encoding\n3       # use version 3 of the RDS format\n262402  # written with R version 4.1.2\n197888  # minimum R version that can read it is 3.5\n5\nUTF-8 \nI am confused. Where did those numbers come from? Why does version 4.1.2 correspond to the number 262402, and why does 3.5 get encoded as 197888? The Manual is silent, and my thoughts become bleak. Am I losing my mind? Is the answer obvious??? What mess have I gotten myself into?\nIn desperation, I look at the R source code which reveals unto me the magic formula:\n\nencode_r_version &lt;- function(major, minor, patch) {\n  (major * 65536) + (minor * 256) + patch\n}\n\nYessss. This all makes sense now…\n\nencode_r_version(4, 1, 2)\nencode_r_version(3, 5, 0)\n\n[1] 262402\n[1] 197888\n\n\n…so much sense.\nWhat about the other two lines in the header? Prior to RDS version 3 – which was released in R version 3.5 – those two lines didn’t exist in the header. Those are now used to specify the “native encoding” of the file, according to the Manual.\n“But isn’t that ASCII????”, whispers a voice in my head. “Is that not what the A is for?”\nNot quite. The RDS file format isn’t restricted to ASCII characters. In the usual case, the RDS file can encode any UTF-8 character and the native encoding line reads UTF-8. There is another possibility though: the file may use the Latin-1 alphabet. Because of this, there is some ambiguity that needs to be resolved. The RDS file needs to indicate which character set is used for the encoding.\nMy annotated header now looks like this:\nA      # the file uses ASCII encoding\n3      # the file uses version 3 of the RDS format\n262402 # the file was written in R version 4.1.2\n197888 # the minimum R version that can read it is 3.5\n5\nUTF-8  # the file encodes UTF-8 characters not Latin-1\nOkay, that makes a certain kind of sense, but what’s the story behind that 5? What does that mean? What dark secret does it hide?\nIt took me so very long to figure this one out. As far as I can tell this line isn’t discussed in the R Internals Manual, but I worked it out by looking at the source code for serialize. That line reads 5 because it’s telling the parser that the string that follows on the next line (i.e., UTF-8) contains five characters. Presumably if I’d used Latin-1 encoding, the corresponding line would have been 7.\nThis is doing my head in, but I think I’m okay?\n\nAre you sure? Really? You don’t sound too certain\n\n\n\n\n\n\nImage by Liza Polyanskaya. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nLogical, integer, and numeric vectors\nNow that I have a sense of how the RDS header works, I’ll set header = FALSE whenever I call show_rds() from now on. That way I won’t have to look at that same six lines of output over and over and they will no longer haunt my dreams.\n\nOh no my dear. Hiding won’t save you.\n\nI think the time has come to look at how RDS encodes other kinds of data. For three of the four commonly used atomic vector types (logical, integer, and numeric), the RDS format looks exactly as I expected given what I learned earlier. As shown in the table above, the SEXPTYPE code for a logical vector is 10, so a logical vector with four elements looks like this:\n\nshow_rds(\n  object = c(TRUE, TRUE, FALSE, NA), \n  header = FALSE\n)\n\n10\n4\n1\n1\n0\nNA\n\n\nTRUE values are represented by 1 in the RDS file, and FALSE values are represented by 0. Missing values are represented as NA.\nFor an integer vector, the output is again familiar. The SEXPTYPE here is 13, so a vector of four integer looks like this:\n\nshow_rds(\n  object = c(-10L, 20L, 30L, NA),\n  header = FALSE\n)\n\n13\n4\n-10\n20\n30\nNA\n\n\nNumeric vectors I’ve already seen. They have SEXPTYPE of 14, so a numeric vector of length 3 starts with 14 on the first line, 3 on the second line, and then the numbers themselves appear over the remaining three lines. However, there is a catch. There always is when dealing with real numbers. Numeric values are subject to the vagaries of floating point arithmetic when represented in memory, and the encoding is not exact. As a consequence, it is entirely possible that something like this happens:\n\nshow_rds(\n  object = c(10.3, 99.9, 100),\n  header = FALSE\n)\n\n14\n3\n10.3\n99.90000000000001\n100\n\n\nFloating point numbers always make my head hurt. It is best not to dwell too long upon them lest my grip on sanity loosen.\n\nToo late. Far, far too late.\n\n\n\n\n\n\nImage by Hoshino Ai. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nCharacter vectors\nWhat about character vectors?\n\nAdorable that you think these will be safer waters in which to swim my dear. A wiser woman would turn back now and return to the shallows. Yet there you go, drifting out to sea. Fool.\n\n\nLet’s create a simple character vector. According to the table above, character vectors have SEXPTYPE 16, so I’d expect that a character vector with three elements would start with 16 on the first line and 3 on the second line, which would then be followed by the contents of each cell.\nAnd that’s… sort of true?\n\nshow_rds(\n  object = c(\"text\", \"is\", \"strange\"),\n  header = FALSE\n)\n\n16\n3\n262153\n4\ntext\n262153\n2\nis\n262153\n7\nstrange\n\n\nThe format of this output is roughly what I was expecting, except for the fact that each string occupies three lines. For instance, these three lines correspond to the word \"strange\":\n262153\n7\nstrange\nThis puzzled me at first. Eventually, I remembered that the source code for R is written in C, and C represents strings as an array. So where R treats the word \"strange\" a single object with length 1, C treats it as a string array containing 7 characters. In the R source code, the object encoding a string is called a CHARSXP. So lines two and three begin to make sense:\n262153\n7        # the string has \"length\" 7\nstrange  # the 7 characters in the string\nWhat about the first line? Given everything I’ve seen previously it’s pretty tempting to guess that it means something similar to the SEXPTYPE codes that we’ve seen earlier. Perhaps in the same way that numeric is SEXPTYPE 14 and logical is SEXPTYPE 10, maybe there’s some sense in which a single string has a “SEXPTYPE” of 262153? That can’t be right though. According to the R Internals Manual, a CHARSXP object has a SEXPTYPE code of 9, not 262153. I must be misunderstanding something? Why is it 262153?\n\nFrightened by the first wave, are you? All in good time my love. The secrets of 262153 will reveal themselves soon.\n\n\n\n\n\n\nImage by Tim Marshall Available by CC0 licence on unsplash.\n\n\n\n\n\n\nLists\nWhat about lists? Lists are more complicated than atomic vectors, because they’re just containers for other data structures that can have different lengths and types. As mentioned earlier, they have SEXPTYPE 19, so a list with three elements will of course start with 19 on the first line and 3 on the second line. Here’s an example:\n\nshow_rds(\n  object = list(\n    c(TRUE, FALSE), \n    10.2, \n    c(\"strange\", \"thing\")\n  ),\n  header = FALSE\n)\n\n19\n3\n10\n2\n1\n0\n14\n1\n10.2\n16\n2\n262153\n7\nstrange\n262153\n5\nthing\n\n\nThis output makes my brain hurt, but it does make sense if I stare at it long enough. It begins with the two lines specifying that it’s a list of length three. This is then followed by the RDS representation for the logical vector c(TRUE, FALSE), the RDS representation for the numeric vector 10.2, and finally the RDS representation for the character vector c(\"strange\", \"thing\").\nI have started using annotations and whitespace to make it clearer:\n19 # it's a list\n3  # of length 3\n\n  10  # list entry 1 is logical\n   2  # of length 2\n   \n    1       # value is TRUE\n    0       # value is FALSE\n      \n  14  # list entry 2 is numeric \n   1  # of length 1\n   \n    10.2    # value is 10.2\n    \n  16  # list entry 3 is character\n   2  # of length 2\n   \n    262153  # every string starts with this\n         7  # this string has 7 characters\n   strange  # values are: s, t, r, a, n, g, e\n   \n    262153  # every string starts with this\n         5  # this string has 5 characters\n     thing  # values are: t, h, i, n, g\nI feel so powerful! My mind is now afire with knowledge! All the secrets of RDS will be mine…\n\n…and the madness strikes at last. Pride comes before the fall, always.\n\n\n\n\n\n\nImage by Moreno Matković. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nObject attributes\nOne of the key features of R is that vectors are permitted to have arbitrary metadata: names, classes, attributes. If an R object contains metadata, that metadata must be serialised too. That has some slightly surprising effects. Let’s start with this very simple numeric object with two elements:\n\nshow_rds(object = c(100, 200), header = FALSE)\n\n14\n2\n100\n200\n\n\nAs expected it has SEXPTYPE 14 (numeric), length 2, and the values it stores are 100 and 200. Nothing out of the ordinary here. But when I add a name to the object, the output is … complicated.\n\nshow_rds(object = c(a = 100, b = 200), header = FALSE)\n\n526\n2\n100\n200\n1026\n1\n262153\n5\nnames\n16\n2\n262153\n1\na\n262153\n1\nb\n254\n\n\nI … don’t know what I am looking at here. First off, I seem to be having the same problem I had with character strings. If I take the first line of this output at face value I would think that a named numeric vector has SEXPTYPE 526. That can’t be right, can it?\n\nIt isn’t. In the same way that strings don’t have a SEXPTYPE of 262153 (the actual number is 9), the 526 here is a little misleading. This is a numeric vector and like all numeric vectors it is SEXPTYPE 14. You will learn the error of your ways very soon.\n\n\nSetting that mystery aside, I notice that the RDS output is similar to the output we saw when converting a list to RDS. The output contains the numeric vector first (the data), which is then followed by a list that specifies the attributes linked to that object?\n\n Not quite. You’re so close, but it’s a pairlist, not a list. The underlying data structure is different. Don’t let it worry your mind, sweet thing. Preserve your mind for the trials still to come.\n\n\nFor this object, there’s only one attribute that needs to be stored, corresponding to the names associated with each element of the vector. If I annotate the output again, I get this:\n526     # Numeric vector \n2       # with two values\n\n   100     # value 1 \n   200     # value 2\n   \n1026    # Pairlist for attributes\n1       # with one pair of entries\n\n   262153  # The attribute is called \"names\"\n   5       # \n   names   # \n   \n   16      # The attribute has two values\n   2       # \n   \n      262153   # First value is \"a\"\n           1   #\n           a   # \n\n      262153   # Second value is \"b\"\n           1   #\n           b   #\n\n254   # end of pairlist\nThe 254 marking the end of the pairlist confused me for a little while, but it isn’t arbitrary. It represents a NULL value in the RDS format:\n\nshow_rds(NULL, header=FALSE)\n\n254\n\n\n\nYes, my dear. If you look at the relevant part of the R source code, you see that there are a collection of “administrative codes” that are used to denote special values in a SEXPTYPE-like fashion. NULL is the one you’d be most likely to encounter though. Perhaps best not to travel down that road tonight though? Wait until day. You’re getting tired.\n\n\n\n\n\n\nImage by Kelly Sikkema. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#typeflag-packing",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#typeflag-packing",
    "title": "Data serialisation in R",
    "section": "Type/flag packing",
    "text": "Type/flag packing\nThroughout this post, I’ve given the impression that when R serialises an object to RDS format, the first thing it writes is the SEXPTYPE of that object. Technically I wasn’t lying, but this is an oversimplificiation that hides something important. It’s time to unpack this, and to do that I’ll have to dive into the R source code…\n\nDecoding the SEXPTYPE\nAfter digging around in the source code I found the answer. What R actually does in that first entry is write a single integer, and packs multiple pieces of information into the bits that comprise that integer. Only the first eight bits are used to define the SEXPTYPE. Other bits are used as flags indicating other things. Earlier on, I said that a value of 526 actually corresponds to a SEXPTYPE of 14. That becomes clearer when we take a look at the binary representation of 14 and 526. The first eight bits are identical:\n\nintToBits(14)\n\n [1] 00 01 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\nintToBits(526)\n\n [1] 00 01 01 01 00 00 00 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nTo extract the SEXPTYPE, what we want to do is ignore all the later bits. I could write a function that uses intToBits() to unpack an integer into its binary representation, then sets all the bits except the first eight to 0, and then converts back to an integer …but there’s no need. The thing I just described is a “bitwise AND” operation:\n\ndecode_sexptype &lt;- function(x) bitwAnd(x, 255)\n\ndecode_sexptype(14)\n\n[1] 14\n\ndecode_sexptype(526)\n\n[1] 14\n\n\nWhen I said that those 262153 values we encounter every time a string is serialised actually correspond to a SEXPTYPE of 9, this is exactly what I was talking about:\n\ndecode_sexptype(262153)\n\n[1] 9\n\n\nThe attributes pairlist, which gave us a value of 1026 when the RDS is printed out as text?\n\ndecode_sexptype(1026)\n\n[1] 2\n\n\nThose are SEXPTYPE 2, and if we check the R internals manual again, we see that this is indeed the code for a pairlist.\nI feel triumphant, but broken.\n\nGirl, same.\n\n\n\n\n\n\nImage by Aimee Vogelsang. Available by CC0 licence on unsplash.\n\n\n\n\n\n\nWhat’s in the other bits?\nI fear that my mind is lost, but in case anyone uncover these notes and read this far, I should document what I have learned about the contents of the other bits. There are a few different things in there. The two you’d most likely encounter are the object flag (bit 9) and the attributes flag (bit 10). For example, consider the data frame below:\n\ndata.frame(\n  a = 1, \n  b = 2\n)\n\n  a b\n1 1 2\n\n\nhas an integer code of 787. Data frames are just lists with additional metadata, so it’s not surprising that when we extract the SEXPTYPE we get a value of 19:\n\ndecode_sexptype(787)\n\n[1] 19\n\n\nBut data frames are also more than lists. They have an explicit S3 class (\"data.frame\") and they have other attributes too: \"names\" and \"row.names\". If we unpack the integer code 787 into its constituent bits we see that bit 9 and bit 10 are both set to 1:\n\nintToBits(787)\n\n [1] 01 01 00 00 01 00 00 00 01 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nBit 9 is the “object flag”: it specifies whether or not the R data structure has a class attribute. Bit 10 is the more general one, and is called the “attribute flag”: it specifies whether or not the object has any attributes.\n\n\nOkay but what’s up with 262153?\nWho is asking me all these questions anyway?\nIt worries me that I’m now listening to the voices in my head, but okay fine. If we unpack the integer code 262153, we see that there’s something encoded in bit 19:\n\nintToBits(262153)\n\n [1] 01 00 00 01 00 00 00 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 00 00 00\n[26] 00 00 00 00 00 00 00\n\n\nI haven’t found the part of the source code that sets this bit yet, but I’m pretty sure that the role of this bit is to flag whether or not the string should be added to the global string pool. In recent versions of R that’s true for all strings, so in practice every string has an integer code of 262153 rather than 9.\n\n\n\n\n\nImage by Pelly Benassi. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-15_serialisation-with-rds/index.html#are-we-done-yet",
    "href": "posts/2021-11-15_serialisation-with-rds/index.html#are-we-done-yet",
    "title": "Data serialisation in R",
    "section": "Are we done yet?",
    "text": "Are we done yet?\nWell that depends on what you mean by asking the question. If you mean “have we described everything there is to know about the RDS format and how data serialisation works in base R?” then no, we’re absolutely not done. I haven’t said anything about how R serialises functions or expressions:\n\nexpr &lt;- quote(sum(a, b, c))\nfn &lt;- function(x) x + 1 \n\nThese are both R objects and you can save them to RDS files. So of course there’s a serialisation format for those but it’s not a lot of fun. I mean, if you squint at it you can kiiiiiinnnnda see what’s going on with the expression…\n\nshow_rds(expr, header = FALSE)\n\n6\n1\n262153\n3\nsum\n2\n1\n262153\n1\na\n2\n1\n262153\n1\nb\n2\n1\n262153\n1\nc\n254\n\n\n…but if I do the same thing to serialise the function it gets unpleasant. This has been quite an ordeal just getting this far, and I see no need to write about the serialisation of closures. Let someone else suffer through that, because my brain is a wreck.\nSo no, we are not “done”. The RDS format keeps some secrets still.\nBut if you mean “have we reached the point where the author is losing her mind and needs to rest?” then… oh my god yes I am utterly and completely done with this subject, and wish to spend the rest of my night sobbing quietly in darkness.\n\nLet us never speak of this again.\n\n\n\n\n\n\nImage by Andrey Zvyagintsev. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html",
    "href": "posts/2024-02-25_s7/index.html",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "",
    "text": "Warning\n\n\n\nContent note: This post includes mentions of sexual assaults, and other unpleasant topics that relate to LGBT experiences of the world"
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#prelude-i",
    "href": "posts/2024-02-25_s7/index.html#prelude-i",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Prelude I",
    "text": "Prelude I\nIt has been a minute since I wrote anything new, but thankfully I have found time for another side project and, by extension, another blog post. This one is going to be a little different though. Like everything on this blog it is in fact a data science post. I’m going to talk about art, object-oriented programming in R, and the grid graphics system. It being the time of the Sydney Gay and Lesbian Mardi Gras – or “Gay Christmas” as it is affectionately known – I’ll do it with a rainbow palette. Nevertheless, pretty palettes notwithstanding this won’t be a particularly upbeat pride-flag-waving kind of post. There will be art, and there will be code. But there will also be little slivers of darker stories, and in a moment I’ll explain why I’ve made the decision to include them. But let’s start with the art.\n\n# colour scheme\npalette &lt;- c(\n  \"#e50000\", \"#ff8d00\", \"#ffee00\", \n  \"#028121\", \"#004cff\", \"#770088\"\n)\n\n# parameters defining objects\nvalues &lt;- tibble::tibble(\n  x = cos(seq(0, pi * 5/3, length.out = 6)),\n  y = sin(seq(0, pi * 5/3, length.out = 6)),\n  n = 500L,\n  fill = palette,\n  color = fill\n)\n\n# list of blob objects to add to a sketch\nblobs &lt;- purrr::pmap(values, blob)\n\n# define and draw a sketch containing the objects\nblobs |&gt; sketch() |&gt; draw()\n\n\n\n\n\n\n\n\nThe piece is very simple, but has a lovely balance to my eye. Each colour of the pride flag is captured in a single “blob”, with the blobs arranged in a circular overlapping pattern that conveys a sense of movement and progress. More subtly, if you look at the blobs closely you can detect a kind of spatial autocorrelation. Notice how the green, yellow, and orange blobs all have a protrusion on their right hand side at approximately the same place. You’d be tempted to think this is coincidental, but it isn’t. Hidden under the hood there is a vector field that induces correlations on the random process that creates the blob shapes. This is by design.\nThe spatial autocorrelation that provides hidden structure to this artwork becomes a little more obvious if I switch to a different piece constructed with very similar tools:\n\n# parameters defining objects\nvalues &lt;- tibble::tibble(\n  x = rnorm(200L, sd = 1.5),\n  y = rnorm(200L, sd = 1.5),\n  xend = x,\n  yend = y + 1,\n  width = 1,\n  n = 500L,\n  fill = sample(palette, 200L, replace = TRUE),\n  color = fill\n)\n\n# list of ribbon objects to add to a sketch\nribbons &lt;- purrr::pmap(values, ribbon)\n\n# define and draw a sketch containing the objects\nribbons |&gt; \n  sketch() |&gt; \n  draw(xlim = c(-2, 2), ylim = c(-2, 2))\n\n\n\n\n\n\n\n\nIn this piece the blobs are more ribbon shaped, and the piece as a whole feels a little like a rainbow coloured lava lamp. The spatial autocorrelation is more obvious. The ribbons tend to be thick in the same regions of the plot, and thin in other regions.\nEach of these ribbons is a distinct individual object, yet somehow they all have similar experiences when embedded in the environment that defines the art. There is a pattern to how they flow through this world.1\nAn artist looking at this piece might wonder about the trickery used to create this structure. An R programmer looking at it, however, might be wondering something very different. They would likely be asking questions like “what the fuck is this ribbon() function, and what the hell is a sketch(), and how are you rendering this with this draw() function that you piped it to? Absolutely none of this is part of base R, it’s clearly not ggplot2 code, and it’s not even consistent with any well-known data visualisation tool in the R ecosystem. What the hell are you doing here girl?"
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#prelude-ii",
    "href": "posts/2024-02-25_s7/index.html#prelude-ii",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Prelude II",
    "text": "Prelude II\nAs I was saying. Mardi Gras is in full swing in Sydney at the moment, a multi-week festival and parade sprawling across multiple sites across the city. As I write this I am sitting in the sunshine at a bar on Bondi Beach, while a massive stage is being constructed below for the “Bondi Beach Party” event. The surfers, swimmers, and sunbathers are having a lovely day at the beach. Everyone is partying.\nAbout a kilometer away, above the iconic limestone cliffs that surround almost every Sydney beach, is the Marks Park Memorial in Tamarama. The memorial serves as a remembrance for the LGBT victims of hate crimes in Sydney, a stark reminder of the perennial apathy that the NSW police show towards these crimes, and most especially to the gay men who were murdered at these cliffs back in the 1970s and 1980s and whose deaths are to this day the subject of inquests into the appalling behaviour of the police in this city.\nThe contrast is stark. It is one that few people are willing to face up to, particularly cisgender straight people who fancy themselves to be allies of our community but think that allyship means nothing more than waving a little rainbow flag and reciting empty slogans like “love is love”. It’s these people who – oh so frequently – hasten to inform me that things have changed. It’s different now, they confidently tell me. Everyone is so accepting, they say with a certainty I do not share. You don’t need to hide who you are, they reassure me. It’s a surprisingly predictable narrative.\nNone of it is true, of course. I could tell the very recent story of Nex Benedict, the non-binary teenager whose death the Oklahoma authorities seem to be trying very hard not to investigate. I could tell the story of Brianna Ghey, a trans girl stabbed to death in the UK. Closer to home, I could tell the story of Mhelody Bruno, a Filipina trans woman strangled to death by her boyfriend in regional NSW and whose death was barely even investigated by the NSW police and courts. These events all took place in the US, the UK, and Australia, and all quite recently. They are not stories from a time long past.\nI could tell all these stories, but what I have learned is that describing these events has very little impact on the people who keep telling me that actually things are fine now. Those stories are the outliers, they tell me. The bad stories are the exceptions, not the norm.\nAgain, it is not true. The thing that well-meaning cis and straight people have this massive blindspot about, is the fact that so many LGBT people have dark stories from our own experience that we aren’t telling them about, and so they impute a much happier backstory to us than is even remotely plausible. People who barely even know me will, to my utter bewilderment, confidently tell me to my face that – despite being transgender and bisexual – I could not possibly have had any particularly traumatic experiences. The gall of it is shocking, actually."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#act-i-learning-to-love-s7",
    "href": "posts/2024-02-25_s7/index.html#act-i-learning-to-love-s7",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Act I: Learning to love S7",
    "text": "Act I: Learning to love S7\nThis whole thing started because I wanted to learn how to use the relatively new S7 object oriented programming system that is intended to serve as a successor to the informal-but-janky S3 system and the formal-but-painful S4 system that already exist within R.2 It seemed like a very simple exercise when I started, but as with any exercise in learning a new thing I very quickly felt like a dog trying to understand Norway.3\nAs is my habit, I’ve taken my learning process and used it to create art. What I want to do with S7 here is create a system for programmatically constructing “drawable shapes” that abide by particular rules, alongside a toolkit that can take these objects and render them within an abstract “sketch” that can later be rendered to an image with the help of the grid graphics system. In that sense, the basic idea underpinning my tool is not that different – though far less sophisticated – to how ggplot2 works, but I’ve designed it with different considerations in mind. My tools are not designed for data visualisation, they are designed for generative art. Spiritually they have more in common with p5.js than ggplot2.\n\nWriting a new S7 class\nOkay look. It’s kind of like this. While eventually the stated plan is for S7 to become part of base R and be easily available to any R user – no different to S3 and S4 – it’s still in development and there are still quirks. So for now it’s a regular package like any other one and if we want to start building classes with it we’ll have to load the package:\n\nlibrary(S7)\n\nLet’s have a look at how you define classes in S7, and for the moment we’ll keep it simple. All I’m going to do right now is create a new class that represents a circle. The way we do this in S7 is with the new_class() function:\n\ncircle &lt;- new_class(\n  name = \"circle\", \n  properties = list(\n    x = class_numeric,\n    y = class_numeric,\n    radius = class_numeric\n  )\n)\n\nBy doing so, I create a new function called circle() that has arguments corresponding to the x, y and radius properites. I can call this function to create a new circle object. In the code below I create an object cc that – at least in some very abstract sense of the term – corresponds to the unit circle:\n\ncc &lt;- circle(x = 0, y = 0, radius = 1)\ncc\n\n&lt;circle&gt;\n @ x     : num 0\n @ y     : num 0\n @ radius: num 1\n\n\nHere we have a circle object that has three “properties” x, y, and radius. We can think of it as something broadly similar to a named list, but it has rules attached. In the class definition above I specified that x, y, and radius must be numeric values: they are allowed to be integers or doubles, but they can’t be characters. Here’s what happens if I try to create a circle with radius = \"big\":\n\ncircle(x = 0, y = 0, radius = \"big\")\n\nError: &lt;circle&gt; object properties are invalid:\n- @radius must be &lt;integer&gt; or &lt;double&gt;, not &lt;character&gt;\n\n\nSo here we see something that we wouldn’t normally have in, say, an S3 class:4 the class definition explicitly says that the radius property must be numeric, so my circle() function throws an error if I foolishly try to pass it a non-numeric radius.\nOkay so what’s going on with these “properties”? Properties are analogous to “slots” in an S4 object5 and can be accessed using @:\n\ncc@radius\n\n[1] 1\n\n\nOne nice thing about S7 is that you can modify (most) properties of an object directly. So if I want to update my cc object so that it now (nominally) represents a circle with radius 2, I can do this:6\n\ncc@radius &lt;- 2\ncc\n\n&lt;circle&gt;\n @ x     : num 0\n @ y     : num 0\n @ radius: num 2\n\n\nAmazing. Thrilling. The excitement, it cannot be contained.\n\n\nDefault values\n\nThe bar has pride flags plastered on the window, and they’re advertising a drag show. These superficial safety signals do not make me feel safe though, because what I remember about this bar is the feel of a strangers hands at my neck as he forced his tongue down my throat in full view of the patrons in the queer themed rainbow bar upstairs. He was so strong.\n\nOne limitation to the circle() function I just created is that it doesn’t supply default values for the x, y, and radius properties, so the user has to explicitly pass values every time it is called. That’s not always ideal, especially as your classes become more complex (we’ll see this later) and have many properties that only rarely need to be modified by the user.\nTo fix this, it’s helpful to note that when I wrote x = class_numeric in the previous definition for the circle class I was relying on a shorthand notation. If you want to be more explicit in how properties are defined, you can use the new_property() function. One of the arguments to new_property() is default and allows you to provide a default value for that property in case the user doesn’t supply one. So here’s a modified version of our circle class that supplies defaults for all three properties:\n\ncircle &lt;- new_class(\n  name = \"circle\", \n  properties = list(\n    x = new_property(class = class_numeric, default = 0),\n    y = new_property(class = class_numeric, default = 0),\n    radius = new_property(class = class_numeric, default = 1)\n  )\n)\n\nNow that I’ve done so, it’s possible for the user to construct a new circle by supplying values only for those properties where they want to override the default. So this now works:\n\ncircle(x = 2)\n\n&lt;circle&gt;\n @ x     : num 2\n @ y     : num 0\n @ radius: num 1\n\n\nAs you can see, this object describes a circle with default values y = 0 and radius = 1, as well as a user supplied value x = 0.\n\n\nComputed properties\nLet’s be honest. This circle class only represents a circle in the most abstract sense. It’s a list of three numbers that you need to describe a circle, but it doesn’t actually compute any points that lie on the circumference of said circle. If you wanted to draw the circle in question, you’d have to actually compute those points yourself. For a circle this computation is so very straightforward that it seems strange to bother doing it within the circle object, but other kinds of parametrised shapes (notably the blob and ribbon objects that are going to appear later) are more complex and the relationship between the parameters and the vertices of the shape is more complex. In those cases, it seems very sensible to compute the vertices of the shape within the object itself.\nOne very handy feature of S7 classes is that they support computed properties. A computed property is one that is not supplied by the user, and whose value is constructed (initially at the time of object creation, but we’ll revisit this) on the basis of the regular properties of the object. So, for instance, our circle class already has ordinary properties x, y, and radius, but we could add a computed property called points that contains the coordinates of the vertices of the circle that is defined by the x, y, and radius values. In practice, however, that means we’ll need to add another new property to our class. As a mathematical abstraction, there are an infinite number of points that sit on the perimeter of a circle. In real life, we’ll have to approximate this by choosing n evenly spaced locations on the circle.\nTo that end, we can update our circle class like this:\n\ncircle &lt;- new_class(\n  name = \"circle\", \n  properties = list(\n    x = new_property(class_numeric, default = 0),\n    y = new_property(class_numeric, default = 0),\n    radius = new_property(class_numeric, default = 1),\n    n = new_property(class_integer, default = 100L),\n    points = new_property(\n      class = class_data.frame,\n      getter = function(self) {\n        angle &lt;- seq(0, 2 * pi, length.out = self@n)\n        data.frame(\n          x = self@x + self@radius * cos(angle),\n          y = self@y + self@radius * sin(angle)\n        )\n      } \n    )    \n  )\n)\n\nEven a moments inspection of the code makes clear that the points property is rather different to the others. The data stored within points is structured as a data frame (we’ll improve on this later) that has columns corresponding to the x- and y-coordinates of the points. Critically, however, the user does not supply the value for points. Instead, what I have done here is use the getter argument to new_property() to supply a function that is executed whenever a new circle is created. The getter function always takes a single argument self, referring to the circle object itself, and returns the value that will be assigned to the computed property (in this case points).\nThe key thing about the getter function is that you can reference the other properties of the object. So, in the code above, you can see that I’ve used self@x, self@radius, etc, to refer to the values of the x and radius values that were supplied by the user. By doing so, it’s pretty easy to write code that programmatically constructs the value of a computed property, using the user-supplied property values to do so.\nSo let’s have a look at what happens when we use our updated circle class. When I create a new circle, the points property is automatically computed using the user-specified x, y, radius, and n properties:\n\ncc &lt;- circle(x = 1, y = 2, radius = 3, n = 50L)\ncc\n\n&lt;circle&gt;\n @ x     : num 1\n @ y     : num 2\n @ radius: num 3\n @ n     : int 50\n @ points:'data.frame': 50 obs. of  2 variables:\n .. $ x: num  4 3.98 3.9 3.78 3.61 ...\n .. $ y: num  2 2.38 2.76 3.13 3.47 ...\n\n\nAnother nice feature of computed properties in S7 is that they are dynamic. When I update one of the other properties, the computed points property is automatically updated. For example, let’s take the circle object cc that I constructed in the last code snippet, and instruct R to compute it using a mere 5 points along the perimeter:7\n\ncc@n &lt;- 5L\ncc\n\n&lt;circle&gt;\n @ x     : num 1\n @ y     : num 2\n @ radius: num 3\n @ n     : int 5\n @ points:'data.frame': 5 obs. of  2 variables:\n .. $ x: num  4 1 -2 1 4\n .. $ y: num  2 5 2 -1 2\n\n\nThe same trick could be used dynamically to change the size or location of the circle (I leave it to the reader to imagine the potential here in the generative art context).\nA key thing to note about the points property that I just created is that it is read-only. Formally, the way you know that it is read-only is because I have defined a getter function for it but have (deliberately) not defined a setter function that would be executed if the user tries to change points directly. Because of this, points is read-only. The user is permitted to read and extract the property via the @ accessor operator…\n\ncc@points\n\n   x  y\n1  4  2\n2  1  5\n3 -2  2\n4  1 -1\n5  4  2\n\n\nbut they are not allowed to write directly to that property. The only8 way to modify a computed property is by modifying the other properties from which it is computed:\n\ncc@points &lt;- data.frame(x = 1:3, y = 4:6)\n\nError: Can't set read-only property &lt;circle&gt;@points\n\n\nFor the purposes of the tool I want to create, the read-only nature of points is a desirable feature. I don’t want users trying to manually create the points that lie on the circumference of a circle. The abstract concept of a circle is defined by x, y and radius, and if I want to create n evenly spaced points on the circle, then these points are automatically computed as a byproduct of specifying these values.\n\n\nValidators\n\nAm I a woman? I mean, there are men who seem pretty happy to humiliate me, choke me, and rape me as if I were a woman, but is that the validation I want? It says “female” on all my documents now, and I fought tooth and nail for years to make that happen. Is the fight itself enough for me to earn my place as a valid woman? But if so, then why am I repeatedly having to justify my life to the satisfaction of men who ignore me when I talk about the reality of life as a trans woman, who treat every interaction with me like they’re having an academic debate, and whose only interest in my life is using me as a vehicle to argue that I have an unfair advantage in the sporting contests that I’m too frightened to participate in? This must all count for something, surely. Does there come a point at which I too can make a valid claim to be the victim of misogyny?\n\nAnother limitation to the circle class that I’ve just created is that there is no validator function. The role of a validator is to ensure that the values that the user passes during object construction correspond to a valid object. A circle can’t really have a negative radius. The x and y values that define the center of the circle need to be scalar values. The number of points n can’t be non-negative. The class that I’ve just created doesn’t enforce these constraints. Sometimes this means that you can pass nonsense values and end up with something that looks like a valid circle, but isn’t:\n\ncircle(x = 1:2, y = 1)\n\n&lt;circle&gt;\n @ x     : int [1:2] 1 2\n @ y     : num 1\n @ radius: num 1\n @ n     : int 100\n @ points:'data.frame': 100 obs. of  2 variables:\n .. $ x: num  2 3 1.99 2.98 1.97 ...\n .. $ y: num  1 1.06 1.13 1.19 1.25 ...\n\n\nOther times, you can pass nonsense values and get an error message because one of the internal function calls doesn’t work properly:\n\ncircle(n = -4L)\n\n&lt;circle&gt;\n\n\nError in seq.default(0, 2 * pi, length.out = self@n): 'length.out' must be a non-negative number\n\n\nNeither of these outcomes is a good thing. What you really need here is a validator function that checks the user inputs and throws an informative error whenever those inputs are invalid. A nice feature of S7 is that it supports validators “out of the box”, and makes the validator part of the class definition itself.\nSo here is an updated circle class definition that incorporates a validator function. It’s a relatively clean setup, because you can simply have the validator return the string for your error message, or else have a NULL return value if the inputs are good:\n\ncircle &lt;- new_class(\n  name = \"circle\", \n  properties = list(\n    x = new_property(class_numeric, default = 0),\n    y = new_property(class_numeric, default = 0),\n    radius = new_property(class_numeric, default = 1),\n    n = new_property(class_integer, default = 100L),\n    points = new_property(\n      class = class_data.frame,\n      getter = function(self) {\n        angle &lt;- seq(0, 2 * pi, length.out = self@n)\n        data.frame(\n          x = self@x + self@radius * cos(angle),\n          y = self@y + self@radius * sin(angle)\n        )\n      } \n    )    \n  ),\n  validator = function(self) {\n    if (length(self@x) != 1) return(\"x must be length 1\")\n    if (length(self@y) != 1) return(\"y must be length 1\")\n    if (length(self@radius) != 1) return(\"radius must be length 1\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (self@radius &lt; 0) return(\"radius must be a non-negative number\")\n    if (self@n &lt; 1L) return(\"n must be a positive integer\")\n  }\n)\n\nThis probably isn’t the ideal form for the circle validator (e.g., I could probably think more about what messages should be returned) but it illustrates the basic idea. The key thing is that, for example, if I try to pass a negative radius I get an error:\n\ncircle(radius = -0.234)\n\nError: &lt;circle&gt; object is invalid:\n- radius must be a non-negative number\n\n\nOne thing I’m learning very quickly is that when you’re designing a system that is built from many different classes, it is extremely valuable to have validators for all your classes. Debugging becomes much easier on the one hand, and on the other hand you can always move forward in your code knowing that when class A depend on class B in some fashion, you can always rely on knowing that every class B object that class A works with will be a valid instance of class B. A little bit of extra effort when writing the class definition spares you a lot of pain down the track when you’re trying to make sense of how your classes interact with each other.\n\n\nBuilding structure\n\nThe constituent pieces of an oppressive system are often innocuous on their face, very often the result of thoughtlessness rather than malice. The consequences, however, are cruel. Crying in a hotel lobby at a conference when your documentation doesn’t match your appearance and the staff are arguing with each other over whether you’re really the person pictured on the drivers licence you have not yet been able to change. Crying in a lecture theatre in front of an audience because the programmer who designed the IT system didn’t consider name and gender to be malleable properties so even though you’ve updated your records with HR, the IT system has just outed you to all your students. Trying to work out if you’re allowed to use the bathroom next to your office because nobody bothered to write a policy document, and instead walking a kilometer to go off-campus to pee. Living in dread waiting for the next time you fall into the cracks in the system.\n\nUp to this point, all I have done when designing my system is define a circle class that creates objects with a specific structure. In order to expand this into a richer tool that can eventually support the artistic goals I have in mind, I’m going to need to broaden my vision and think about a collection of related classes that play nicely with one another.\nIn order to make this shift, a natural place to start is thinking about what kind of objects should be stored internally within a circle object (and other shapes). For instance, in the code above I stored the computed points as a simple data frame. It would be more sensible to define an actual S7 class for this, with its own validator to make sure that any shape classes I define compute a valid set of points:\n\npoints &lt;- new_class(\n  name = \"points\",\n  properties = list(\n    x = class_numeric,\n    y = class_numeric\n  ),\n  validator = function(self) {\n    if (length(self@x) != length(self@y)) {\n      \"x and y must be the same length\"\n    }\n  }\n)\n\nSimilarly, when the time comes to start drawing these things, my circles (and other shapes) will need to have a defined visual style. Again, I can create a class for this:\n\nstyle &lt;- new_class(\n  name = \"style\",\n  properties = list(\n    color     = new_property(class_character, default = \"black\"),\n    fill      = new_property(class_character, default = \"black\"),\n    linewidth = new_property(class_numeric, default = 1)\n  )\n)\n\nYou’ll notice that style currently doesn’t have a validator, and it doesn’t support a very wide range of properties. The artistic system I want to consider has the constraint that every drawable shape is a polygon of some kind, and needs to have stylistic properties that one associates with polygons. The color, fill, and linewidth properties that exist within this style class are a beginning, but nothing more than that. Similarly, the absence of a validator is a strong hint that I haven’t finished thinking through what I want this class to be. For the current purposes the style class works, but it’s very far from complete.\nLet’s move on, and start thinking about what happens when my system has a richer collection of drawable shapes than just circles. Later on in this post I’m going to define several different kinds of drawable shapes (e.g., the blob and ribbon objects in the initial artwork), and they’ll all have some structural similarities to one another that I want to enforce. To that end I’ll create a class called drawable.9 The drawable class will later be used as the parent class for all the shape classes that I want to be, well, drawable. But it’s not super interesting in itself. Here’s the code:\n\ndrawable &lt;- new_class(\n  name = \"drawable\",\n  properties = list(\n    style = new_property(\n      class = style,\n      default = style()\n    ),\n    points = new_property(\n      class = points,\n      getter = function(self) points(x = numeric(0L), y = numeric(0L))\n    )\n  )\n)\n\nThe true purpose of the drawable class is that it enforces structure. Every drawable shape class that I later define will necessarily have a computed points property, and a user-defined style property. As an added benefit, because the only information I’ll ever need to draw a shape is captured by style and points, I will be able to define a single drawing method that works for all drawable objects regardless of their specific character.\nOkay so now we can define circle as a subclass of drawable. It doesn’t change the code very much. All I’m doing here is adding parent = drawable to the class definition:\n\ncircle &lt;- new_class(\n  name = \"circle\",\n  parent = drawable,\n  properties = list(\n    x = new_property(class_numeric, default = 0),\n    y = new_property(class_numeric, default = 0),\n    radius = new_property(class_numeric, default = 1),\n    n = new_property(class_integer, default = 100L),\n    points = new_property(\n      class = points,\n      getter = function(self) {\n        angle &lt;- seq(0, 2 * pi, length.out = self@n)\n        points(\n          x = self@x + self@radius * cos(angle),\n          y = self@y + self@radius * sin(angle)\n        )\n      }\n    )\n  ),\n  validator = function(self) {\n    if (length(self@x) != 1) return(\"x must be length 1\")\n    if (length(self@y) != 1) return(\"y must be length 1\")\n    if (length(self@radius) != 1) return(\"radius must be length 1\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (self@radius &lt; 0) return(\"radius must be a non-negative number\")\n    if (self@n &lt; 1L) return(\"n must be a positive integer\")\n  }\n)\n\nTo see that our new circle class is indeed a subclass of drawable, we can create a new circle and check its class attribute:\n\ncc &lt;- circle()\nclass(cc)\n\n[1] \"circle\"    \"drawable\"  \"S7_object\"\n\n\nThe primary class is circle, as you’d expect, and the parent class is drawable. Notice also that the object has a third class, S7_object: all S7 objects possess this class.\n\n\nConstructors\nImposing this structure on our drawable objects is valuable, and will pay off later because everything that we need to draw the shape is captured by the embedded style and points objects. However, as an unfortunate side effect of this, I’ve accidentally defined a class that makes it very hard for the user to supply the stylistic features: the arguments to our new circle() function are now:\n\nstyle, which has to be a call to the style() constructor\nx, y, radius, and n, as before\n\nThat means that if I want to define a red circle with radius 2, my poor user (i.e., me) has to do something rather unpleasant:\n\ncircle(style = style(color = \"red\"), radius = 2)\n\n&lt;circle&gt;\n @ style : &lt;style&gt;\n .. @ color    : chr \"red\"\n .. @ fill     : chr \"black\"\n .. @ linewidth: num 1\n @ points: &lt;points&gt;\n .. @ x: num [1:100] 2 2 1.98 1.96 1.94 ...\n .. @ y: num [1:100] 0 0.127 0.253 0.379 0.502 ...\n @ x     : num 0\n @ y     : num 0\n @ radius: num 2\n @ n     : int 100\n\n\nThe reason this is cumbersome is that my drawable class imposes a structure in which all the stylistic features are captured by the style property (shared by all drawables), whereas all the parameters that are used to construct the points property are specific to the particular drawable class (circle, blob, ribbon, etc). This internal structure is a good thing insofar as it ensures I can always extract the two key properties from any drawable:\n\ncc@style\n\n&lt;style&gt;\n @ color    : chr \"black\"\n @ fill     : chr \"black\"\n @ linewidth: num 1\n\ncc@points\n\n&lt;points&gt;\n @ x: num [1:100] 1 0.998 0.992 0.982 0.968 ...\n @ y: num [1:100] 0 0.0634 0.1266 0.1893 0.2511 ...\n\n\nAs you can imagine, this is a handy feature in a generative art tool. Nevertheless it’s a huge pain in the arse from the user perspective when it comes to constructing a new circle object.\nFortunately, S7 makes it possible to fix this by allowing the class definition to include a custom constructor function that allows the developer to customise the arguments that the user supplies.\nHere’s what the circle class looks like with a custom constructor:\n\ncircle &lt;- new_class(\n  name = \"circle\",\n  parent = drawable,\n  properties = list(\n    x      = class_numeric,\n    y      = class_numeric,\n    radius = class_numeric,\n    n      = class_integer,\n    points = new_property(\n      class = points,\n      getter = function(self) {\n        angle &lt;- seq(0, 2 * pi, length.out = self@n)\n        points(\n          x = self@x + self@radius * cos(angle),\n          y = self@y + self@radius * sin(angle)\n        )\n      }\n    )\n  ),\n  validator = function(self) {\n    if (length(self@x) != 1) return(\"x must be length 1\")\n    if (length(self@y) != 1) return(\"y must be length 1\")\n    if (length(self@radius) != 1) return(\"radius must be length 1\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (self@radius &lt; 0) return(\"radius must be a non-negative number\")\n    if (self@n &lt; 1L) return(\"n must be a positive integer\")\n  },\n  constructor = function(x = 0, y = 0, radius = 1, n = 100L, ...) {\n    new_object(\n      drawable(),\n      x = x,\n      y = y,\n      radius = radius,\n      n = n,\n      style = style(...)\n    )\n  }\n)\n\nThere’s a few things to note here:\n\nThe constructor function always ends with a call to new_object(), and the first argument is always the parent class (i.e., in this case the parent of circle is drawable).\nThe way I’ve cleaned up my arguments is by having four explicitly defined arguments for a circle (x, y, radius, and n), and then included the dots ... that are then passed onto style()\nI’ve surreptitiously changed the way that my default arguments are specified: the constructor function is now responsible for defining defaults.\n\nThe extra effort that goes into writing a custom constructor is in some ways a pain (especially because it does mean that any subclass of circle will also need a custom constructor), but in this case it’s worth it because I can create a blue circle in a natural way:\n\ncc &lt;- circle(radius = 2, fill = \"#7acbf5\")\ncc\n\n&lt;circle&gt;\n @ style : &lt;style&gt;\n .. @ color    : chr \"black\"\n .. @ fill     : chr \"#7acbf5\"\n .. @ linewidth: num 1\n @ points: &lt;points&gt;\n .. @ x: num [1:100] 2 2 1.98 1.96 1.94 ...\n .. @ y: num [1:100] 0 0.127 0.253 0.379 0.502 ...\n @ x     : num 0\n @ y     : num 0\n @ radius: num 2\n @ n     : int 100\n\n\n\n\nOther drawables\nLater in the post I’ll go on to define the blob and ribbon classes that are so much fun for artistic purposes, but we have a long way to go before we get to that. One thing I will do now, however, is define a general purpose shape class that isn’t very interesting, but handy to have as a fallback. The shape class has no interesting parameters: the user directly supplies the x and y coordinates that define the points property for the shape. This is very handy artistically in situations where you don’t really want to bother defining a shape parametrically, and instead just want to supply the vertices directly:\n\nshape &lt;- new_class(\n  name = \"shape\",\n  parent = drawable,\n  properties = list(\n    x = class_numeric,\n    y = class_numeric,\n    points = new_property(\n      class = points,\n      getter = function(self) {\n        points(x = self@x, y = self@y)\n      }\n    )\n  ),\n  validator = function(self) {\n    if (length(self@x) != length(self@y)) {\n      \"x and y must be the same length\"\n    }\n  },\n  constructor = function(x, y, ...) {\n    new_object(\n      drawable(),\n      x = x,\n      y = y,\n      style = style(...)\n    )\n  }\n)\n\nIt’s not interesting, but you can use it to manually define a pink triangle like this:\n\ntr &lt;- shape(\n  x = c(0, 2, 1), \n  y = c(sqrt(3), sqrt(3), 0),\n  fill = \"#eaacb8\"\n)\ntr\n\n&lt;shape&gt;\n @ style : &lt;style&gt;\n .. @ color    : chr \"black\"\n .. @ fill     : chr \"#eaacb8\"\n .. @ linewidth: num 1\n @ points: &lt;points&gt;\n .. @ x: num [1:3] 0 2 1\n .. @ y: num [1:3] 1.73 1.73 0\n @ x     : num [1:3] 0 2 1\n @ y     : num [1:3] 1.73 1.73 0\n\n\nHonestly, I’m only mentioning it now because in a moment I’m going to define drawing functions that I’ll apply to the circle object cc from before, and the tr object I’ve just created."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#act-ii-sketching-with-grid",
    "href": "posts/2024-02-25_s7/index.html#act-ii-sketching-with-grid",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Act II: Sketching with grid",
    "text": "Act II: Sketching with grid\n\nI am sitting in shock on my couch. That was… that was rape, right? If I were a cisgender woman that would clearly be rape, right? I said no, repeatedly. He forced his way in. Into my apartment. Into me. It happened, right? I mean, I know it happened, because it hurts where he forced me. So I know it happened, my body is telling me exactly what happened. But… that really did just happen, right?\n\nAt this stage in the development process I have a sensible collection of S7 classes that I can use to represent polygons in a systematic way. I have style and points classes that capture the “raw” characteristics of the polygons, and I have drawable and its child classes circle and shape (with blob and ribbon to come shortly) that allows me to construct drawable shapes in a convenient way.\nWhat I don’t have yet is a method for actually drawing these things.\n\nDrawing drawables\nThere are a lot of ways I could go about creating actual drawings from drawable objects, and I did spend a little time thinking about the pros and cons of different approaches. What I landed on in the end, however, was a strategy where I would define a new generic function called draw() and then supply methods for it that use the grid graphics system in R to do the rendering.\nTo construct a new generic function in S7, we use the new_generic() function. Here’s is how I’ve done that to create draw():\n\ndraw &lt;- new_generic(\"draw\", dispatch_args = \"object\")\n\nIn this code, the dispatch_args argument is used to indicate which arguments to the draw() function should be used in method dispatch. I’ve kept it simple in this case: the specific method that S7 will choose for draw() is based on the class of the object argument. For the moment, the only class for which I really need to supply a method is the drawable class (i.e., how should draw() handle the input when it is passed a single drawable object), but very shortly I’m going to define a sketch class that allows many drawable objects to be composed into a single image, so I’ll need a draw() method to handle sketch inputs as well.\nBut I am getting ahead of myself. To specify a method for an S7 generic function, we call the method() function and specify the generic for which the method is to be defined (i.e., draw()), and the class for which the method is defined (i.e., drawable). Here’s the code that I used:\n\nmethod(draw, drawable) &lt;- function(object, xlim = NULL, ylim = NULL, ...) {\n\n  # plotting area is a single viewport with equal-axis scaling\n  if (is.null(xlim)) xlim &lt;- range(object@points@x)\n  if (is.null(ylim)) ylim &lt;- range(object@points@x)\n  x_width &lt;- xlim[2] - xlim[1]\n  y_width &lt;- ylim[2] - ylim[1]\n  vp &lt;- grid::viewport(\n    xscale = xlim,\n    yscale = ylim,\n    width  = grid::unit(min(1, x_width / y_width), \"snpc\"),\n    height = grid::unit(min(1, y_width / x_width), \"snpc\"),\n  )\n\n  # shapes are always polygon grobs\n  grob &lt;- grid::polygonGrob(\n    x = object@points@x,\n    y = object@points@y,\n    gp = grid::gpar(\n      col = object@style@color,\n      fill = object@style@fill,\n      lwd = object@style@linewidth\n    ),\n    vp = vp,\n    default.units = \"native\"\n  )\n\n  # draw the grob\n  grid::grid.newpage()\n  grid::grid.draw(grob)\n}\n\nIf you are familiar with the grid package, you can immediately see that this is a very simple use of grid (e.g., compare it to ggplot2, which is very sophisticated in how it uses grid). All I’m doing here is defining a single plotting region (i.e., the viewport) and – because this is art rather than data visualisation – using \"snpc\" units to enforce a strong constraint that the x and y axes of this plot region will always be on the same scale, similar to how coord_equal() works in ggplot2. Our drawable object is then used to construct a single polygon grob: the internal points object contained within every drawable is used to define the vertices of the polygon, and the internal style object contained within every drawable is used to specify the graphical parameters associated with the polygon. See? I told you that all the effort we went to in imposing structure on the drawable class would come in handy. The draw() method is easy to write because the structure of the objects is aligned with the underlying functions we want those objects to support.\nSo here we go. At last we can draw simple shapes. Here’s our blue circle:\n\ndraw(cc)\n\n\n\n\n\n\n\n\nHere is our pink triangle:\n\ndraw(tr)\n\n\n\n\n\n\n\n\nNot exactly high-calibre art, but the foundations are in place for us to start building more interesting tools atop these classes and methods.\n\n\nMaking sketches\n\nEarly in transition, walking into the office in my academic department. “So you’re a woman now? Are you going to get the surgery? Or did you already have it? Are you looking for a husband?” Um, I was hoping to use the photocopier if that’s okay? I have a class handout to prepare. Wasn’t planning to have a conversation about my genital status actually, and I feel like if I were a cis woman you’d realise that your professional colleagues’ genitals are not an appropriate workplace conversation topic.\n\nAt last the time has come to define the sketch class that will allow us to compose multiple drawables into a single drawing. Happily for us this is very simple to do. My first pass at defining a sketch class looks like this:\n\nsketch &lt;- new_class(\n  name = \"sketch\",\n  properties = list(\n    shapes = new_property(class = class_list, default = list())\n  ),\n  validator = function(self) {\n    if (!all(purrr::map_lgl(self@shapes, \\(d) inherits(d, \"drawable\")))) {\n      \"shapes must be a list of drawable-classed objects\"\n    }\n  }\n)\n\nInternally it is very simple: it has one property shapes, which is defined to be a list of drawable objects. Notice that we have a validator function for the class that checks to see that every element of the shapes list inherits from the drawable class. Easy.\nDefining a draw() method for the sketch class is only slightly more complicated than the corresponding method was for a single drawable object. The only subtlety to it is determining what the “plot limits” should be if the user doesn’t manually specify xlim and ylim. Here, all I’ve done is set the default limits so that the plot includes all the constituent objects in the sketch:\n\nmethod(draw, sketch) &lt;- function(object, xlim = NULL, ylim = NULL, ...) {\n\n  # set default axis limits\n  if (is.null(xlim)) {\n    xlim &lt;- c(\n      min(purrr::map_dbl(object@shapes, \\(s, id) min(s@points@x))),\n      max(purrr::map_dbl(object@shapes, \\(s, id) max(s@points@x)))\n    )\n  }\n  if (is.null(ylim)) {\n    ylim &lt;- c(\n      min(purrr::map_dbl(object@shapes, \\(s) min(s@points@y))),\n      max(purrr::map_dbl(object@shapes, \\(s) max(s@points@y)))\n    )\n  }\n\n  # plotting area is a single viewport with equal-axis scaling\n  x_width &lt;- xlim[2] - xlim[1]\n  y_width &lt;- ylim[2] - ylim[1]\n  vp &lt;- grid::viewport(\n    xscale = xlim,\n    yscale = ylim,\n    width  = grid::unit(min(1, x_width / y_width), \"snpc\"),\n    height = grid::unit(min(1, y_width / x_width), \"snpc\"),\n  )\n\n  # draw the grobs\n  grid::grid.newpage()\n  for(s in object@shapes) {\n    grob &lt;- grid::polygonGrob(\n      x = s@points@x,\n      y = s@points@y,\n      gp = grid::gpar(\n        col = s@style@color,\n        fill = s@style@fill,\n        lwd = s@style@linewidth\n      ),\n      vp = vp,\n      default.units = \"native\"\n    )\n    grid::grid.draw(grob)\n  }\n}\n\nYou’ll notice that the drawing code isn’t very sophisticated here either: it’s a simple loop that draws the individual shape objects in the same order that they appear in the sketch. I’ve made no effort at all to make the code efficient. For now all I care about is that it works.\nYou can see it in action here, where I’ve created a very simple sketch called sk that includes the blue circle cc and pink triangle tr, and then drawn it:\n\nsk &lt;- sketch(list(cc, tr))\ndraw(sk)\n\n\n\n\n\n\n\n\n\n\n\nCreating art programmatically\nRecall earlier, back when I was talking about constructors, how I said that the primary reason for writing my custom constructor functions was user convenience, so that stylistic properties could be passed to circle() etc as “top level” arguments rather than being bundled within a call to style()? Well, that was true as far as it goes, but the convenience factor is a little bit more substantial than saving myself some keystrokes. By creating an API that exposes the stylistic and structural parameters as top level arguments, I can create a data frame that describes all the desired features of a sketch, like this:\n\nset.seed(1)\nvalues &lt;- tibble::tibble(\n  color  = sample(palette, size = 100L, replace = TRUE),\n  fill   = color,\n  x      = sample(1:6, size = 100L, replace = TRUE),\n  y      = sample(1:6, size = 100L, replace = TRUE),\n  radius = runif(100L, min = .1, max = .5)\n)\nvalues\n\n# A tibble: 100 × 5\n   color   fill        x     y radius\n   &lt;chr&gt;   &lt;chr&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;\n 1 #e50000 #e50000     3     5  0.162\n 2 #028121 #028121     2     5  0.239\n 3 #e50000 #e50000     2     1  0.364\n 4 #ff8d00 #ff8d00     5     5  0.225\n 5 #004cff #004cff     2     4  0.241\n 6 #ffee00 #ffee00     5     2  0.159\n 7 #770088 #770088     4     6  0.364\n 8 #ff8d00 #ff8d00     5     2  0.174\n 9 #ffee00 #ffee00     4     1  0.482\n10 #ffee00 #ffee00     6     5  0.459\n# ℹ 90 more rows\n\n\nBecause the column names in values match the corresponding arguments to circle(), including those that are passed down to style(), I can create a list of 100 circles to plot using a single call to purrr::pmap():\n\ncircles &lt;- purrr::pmap(values, circle)\n\nI’m not going to print this circles object because the output would be quite long, but the key thing is that it is a list of 100 objects that possess the drawable class, so I can pass it straight to sketch(), and then pass the corresponding sketch straight to draw(). So the process of constructing my plot is as simple as this:\n\ncircles |&gt; sketch() |&gt; draw()\n\n\n\n\n\n\n\n\nFinally we are starting to see some artistic payoff here, and some potential benefit to using a system like this rather than ggplot2. As much as I adore using ggplot2 for artistic purposes, it’s often clunky to do so precisely because ggplot2 is an extremely powerful tool that is designed for a different purposes. Art is not data visualisation (though arguably they are related), and the design goals of an artistic system are not always aligned with the design goals of a data visualisation tool."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#act-iii-ambient",
    "href": "posts/2024-02-25_s7/index.html#act-iii-ambient",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Act III: Ambient",
    "text": "Act III: Ambient\n\nI have become fearful now. Anxiety about travelling too far from home has become severe. I know my neighbourhood. I know where all the exits are. I know which bathrooms are safe. Travel to an unfamiliar part of Sydney makes me anxious. Travel to another city makes me panic. Travel to another country is terrifying.\n\nTo be entirely honest with you, dear reader, everything up to this point in the post has been an exercise in creating the infrastructure for artistic work using S7 and grid. The circle() and shape() functions are both handy little things for illustrating the basic ideas, while sketch() and draw() serve as the core vehicles for turning abstract descriptions into actual art. But there’s a limit to how much artistic work you can pull off using only circles, so the time has come to spread our wings and start writing more interesting drawable subclasses that can be used to create more intricate pieces.\nLet’s get to it, shall we?\n\nBlobs\nAt this point in the process of exploring my new system I’ve only written two fun classes that I’ve defined so far, the blob and ribbon objects that you saw at the start of the post. Let’s start with blobs.\nA blob object is essentially a circle with a non-constant radius, where pointwise variation in the radius is calculated using Perlin noise with the assistance of the ambient package. This is actually a type of object I’ve used in my art before. The Perlin Blobs series I created a few years ago uses exactly this trick. In the code below all I’m really doing is defining a blob() function (and corresponding S7 class) that incorporates this idea within the “sketches” framework that I’ve just built. Here’s the entire source code:\n\nblob &lt;- new_class(\n  name = \"blob\",\n  parent = drawable,\n  properties = list(\n    x          = class_numeric,\n    y          = class_numeric,\n    radius     = class_numeric,\n    range      = class_numeric,\n    n          = class_integer,\n    frequency  = class_numeric,\n    octaves    = class_integer,\n    seed       = class_integer,\n    points = new_property(\n      class = points,\n      getter = function(self) {\n        angle &lt;- seq(0, 2*pi, length.out = self@n)\n        pointwise_radius &lt;- ambient::fracture(\n          noise = ambient::gen_simplex,\n          fractal = ambient::fbm,\n          x = self@x + cos(angle) * self@radius,\n          y = self@y + sin(angle) * self@radius,\n          frequency = self@frequency,\n          seed = self@seed,\n          octaves = self@octaves\n        ) |&gt;\n          ambient::normalize(to = self@radius + c(-1, 1) * self@range)\n        points(\n          x = self@x + pointwise_radius * cos(angle),\n          y = self@y + pointwise_radius * sin(angle)\n        )\n      }\n    )\n  ),\n  constructor = function(x = 0,\n                         y = 0,\n                         radius = 1,\n                         range = 0.2,\n                         n = 100L,\n                         frequency = 1,\n                         octaves = 2L,\n                         seed = 1L,\n                         ...) {\n    new_object(\n      drawable(),\n      x = x,\n      y = y,\n      radius = radius,\n      range = range,\n      n = n,\n      frequency = frequency,\n      octaves = octaves,\n      seed = seed,\n      style = style(...)\n    )\n  },\n  validator = function(self) {\n    if (length(self@x) != 1) return(\"x must be length 1\")\n    if (length(self@y) != 1) return(\"y must be length 1\")\n    if (length(self@radius) != 1) return(\"radius must be length 1\")\n    if (length(self@range) != 1) return(\"range must be length 1\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (length(self@frequency) != 1) return(\"frequency must be length 1\")\n    if (length(self@octaves) != 1) return(\"octaves must be length 1\")\n    if (length(self@seed) != 1) return(\"seed must be length 1\")\n    if (self@radius &lt; 0) return(\"radius must be a non-negative number\")\n    if (self@range &lt; 0) return(\"range must be a non-negative number\")\n    if (self@frequency &lt; 0) return(\"frequency must be a non-negative number\")\n    if (self@n &lt; 1L) return(\"n must be a positive integer\")\n    if (self@octaves &lt; 1L) return(\"octaves must be a positive integer\")\n  }\n)\n\nThe code for blob() is a bit longer than the corresponding code for circle(), and all the interesting work is done within the computed property that constructs the points object. This post is already fairly long, so I won’t unpack all the detail, but I will note that if you read my blog post on the Splatter system this code probably seems familiar, at least to the extent that I’m using the tools from the ambient package in a fairly similar way.\nThe one new trick hidden in this code is the fact that the Perlin noise is computed at the actual coordinates where the object is to be drawn in the plot (see the x and y arguments to ambient::fracture(). This part is critical to producing the spatial autocorrelation, because any two blobs constructed using the same seed value will use the same underlying generator, and the distortions associated with those objects will vary smoothly in the actual coordinate space within which the are subsequently drawn.\nIn any case, here is a simple blob object:\n\nbb &lt;- blob(n = 500L)\ndraw(bb)\n\n\n\n\n\n\n\n\nSo, using the rainbow palette I defined at the top of the post, we can create a new piece that is somewhat similar in style to the blob-based art I showed at the very beginning:\n\nvalues &lt;- tibble::tibble(\n  x = cos(seq(0, pi * 21, length.out = 18)) * (1:18),\n  y = sin(seq(0, pi * 21, length.out = 18)) * (1:18),\n  radius = (5 + (1:18)) / 3,\n  range = radius * .2, \n  n = 500L,\n  frequency = .2, \n  fill = rep(palette, 3),\n  color = fill\n)\n\nblobs &lt;- purrr::pmap(values, blob)\nblobs |&gt; sketch() |&gt; draw()\n\n\n\n\n\n\n\n\nHonestly, I feel it could do with a bit of work to be interesting, but you get the idea.\n\n\nRibbons\n\nAt least once a week, my mind flashes back to the time early in transition when I was told that if I was ever caught using the women’s bathroom I’d be violently assaulted as punishment for the transgression. I find myself wondering how serious that threat was, if it was meant in earnest. The context for the threat is long gone. The fear it created has never quite disappeared.\n\nNext let’s take a look at ribbon(). Once you’ve wrapped your head around the Perlin noise trick used by blob(), you can see that ribbons are a fairly trivial variation on the theme. Ribbons are similar to blobs insofar as they use Perlin noise to describe pointwise variation along a path, but whereas blob() uses this trick to vary the radius of a “circle”, ribbon() starts with a linear segment and then constructs a symmetric polygon around it whose width at any given point is defined using the Perlin noise trick. The result is that ribbon objects end up looking rather similar to violin plots used to vidualise distributions.\n\nribbon &lt;- new_class(\n  name = \"ribbon\",\n  parent = drawable,\n  properties = list(\n    x          = class_numeric,\n    y          = class_numeric,\n    xend       = class_numeric,\n    yend       = class_numeric,\n    width      = class_numeric,\n    n          = class_integer,\n    frequency  = class_numeric,\n    octaves    = class_integer,\n    seed       = class_integer,\n    points = new_property(\n      class = points,\n      getter = function(self) {\n        x &lt;- seq(self@x, self@xend, length.out = self@n)\n        y &lt;- seq(self@y, self@yend, length.out = self@n)\n        displacement &lt;- ambient::fracture(\n          noise = ambient::gen_simplex,\n          fractal = ambient::fbm,\n          x = x,\n          y = y,\n          frequency = self@frequency,\n          seed = self@seed,\n          octaves = self@octaves\n        ) |&gt;\n          ambient::normalize(to = c(0, 1))\n        taper &lt;- sqrt(\n          seq(0, 1, length.out = self@n) * seq(1, 0, length.out = self@n)\n        )\n        width &lt;- displacement * taper * self@width\n        dx &lt;- self@xend - self@x\n        dy &lt;- self@yend - self@y\n        points(\n          x = c(x - width * dy, x[self@n:1L] + width[self@n:1L] * dy),\n          y = c(y + width * dx, y[self@n:1L] - width[self@n:1L] * dx)\n        )\n      }\n    )\n  ),\n  constructor = function(x = 0,\n                         y = 0,\n                         xend = 1,\n                         yend = 1,\n                         width = 0.2,\n                         n = 100L,\n                         frequency = 1,\n                         octaves = 2L,\n                         seed = 1L,\n                         ...) {\n    new_object(\n      drawable(),\n      x = x,\n      y = y,\n      xend = xend,\n      yend = yend,\n      width = width,\n      n = n,\n      frequency = frequency,\n      octaves = octaves,\n      seed = seed,\n      style = style(...)\n    )\n  },\n  validator = function(self) {\n    if (length(self@x) != 1) return(\"x must be length 1\")\n    if (length(self@y) != 1) return(\"y must be length 1\")\n    if (length(self@xend) != 1) return(\"xend must be length 1\")\n    if (length(self@yend) != 1) return(\"yend must be length 1\")\n    if (length(self@width) != 1) return(\"width must be length 1\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (length(self@frequency) != 1) return(\"frequency must be length 1\")\n    if (length(self@octaves) != 1) return(\"octaves must be length 1\")\n    if (length(self@seed) != 1) return(\"seed must be length 1\")\n    if (self@width &lt; 0) return(\"width must be a non-negative number\")\n    if (self@frequency &lt; 0) return(\"frequency must be a non-negative number\")\n    if (self@n &lt; 1L) return(\"n must be a positive integer\")\n    if (self@octaves &lt; 1L) return(\"octaves must be a positive integer\")\n  }\n)\n\nHere’s the ribbon class in action:\n\nrb &lt;- ribbon(n = 500L)\ndraw(rb)\n\n\n\n\n\n\n\n\nUsing the same general tricks as before, we can construct the image by defining a values data frame, passing it to purrr::pmap() to construct a list of ribbons, and then passing those ribbons to sketch() and draw(). Here’s the result:\n\nset.seed(101L)\nvalues &lt;- tibble::tibble(\n  x = rnorm(200L, sd = 1.5),\n  y = rnorm(200L, sd = 1.5),\n  xend = x + rnorm(200L, sd = .5),\n  yend = y,\n  width = 1,\n  n = 500L,\n  fill = sample(palette, 200L, replace = TRUE),\n  color = fill\n)\n\nribbons &lt;- purrr::pmap(values, ribbon)\nribbons |&gt; \n  sketch() |&gt; \n  draw(xlim = c(-2, 2), ylim = c(-2, 2))\n\n\n\n\n\n\n\n\nI’m quite fond of this one. Honestly I prefer it to most of the Mardi Gras graphic design work I’ve seen plastered all over Sydney lately."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#epilogue",
    "href": "posts/2024-02-25_s7/index.html#epilogue",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Epilogue",
    "text": "Epilogue\nSo we come to the end of another blog post and, unsurprisingly, I have ended up back where I started. I find myself thinking about the Mardi Gras festivities that are currently taking over Sydney. I do like Mardi Gras. It’s a celebration and a party, true, but there is value in a highly visible event that allow the queer community to take centre stage for a few short weeks every year. Besides, parties are fun.\nBut there is a side to it that I don’t love. Lately it has often felt to me that there is a pressure on us to tell only the happy and uplifting stories. Yes, we deserve to celebrate, but I can’t help thinking about the Marks Park Memorial. I visited it this morning, and it was not an uplifting or happy moment. It felt dark, sombre, and sad.\nAnd therein lies the part that makes me uneasy. I keep thinking about all those people who are not part of the queer community who insist on telling me that “everything is different now, everyone is so accepting now”. I keep wondering if the reason that people keep telling me this – despite the obvious falseness to the claim – is that the only thing they ever see are the parties. The celebrations are what they remember; the hate crimes are what they forget.\nQueer people don’t have those memory lapses. We remember the hate crimes, both past and present. We remember because those stories are still a part of our daily experience of the world. But they seem to have become invisible to those outside our community, even those who believe themselves to be allies. So it comes to pass that outsiders come to believe that the world is much kinder to us than it actually is.\nThe little slivers from my personal experience are not things I share lightly. The events they refer to were traumatic, and of course there are other like them I’ve chosen not to discuss. I have shared only a small slice of what my life is actually like. I chose to share these ones specifically because these are the ones I’m mostly recovered from. These memories do cause problems for me from time to time, but not as much as they used to.\nEven so, it is not pleasant to revisit this history. I do so here not because I want to, but because if we keep our histories secret then people start to believe the pleasant lies that they tell themselves about our world."
  },
  {
    "objectID": "posts/2024-02-25_s7/index.html#footnotes",
    "href": "posts/2024-02-25_s7/index.html#footnotes",
    "title": "Creating new generative art tools in R with grid, ambient, and S7",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLook, I know I’m hitting the reader over the head with the metaphor, but I’m not trying to be subtle here.↩︎\nFor folks outside of the R community: S3, S4, and S7 are all considered to be “functional” OOP systems, and they are broadly similar to function overloading in C++ and, I have recently learned, is also analogous to how methods are defined in Julia for functions of the same name but with different signatures. The functional style of OOP programming is one in which methods attach to generic functions (or to function names), as opposed to more traditional “encapsulated” OOP systems in which methods attach to objects. The encapsulated style is standard in Python, for example, and also appears in R via the R6 OOP system that I have used in other blog posts (e.g., here and here). I mention these tiresome details because I have learned to my detriment that if I don’t exhaustively list the various distinctions between OOP systems and how R adopts multiple styles, I will have to deal with tiresome people mansplaining to me how R is bad at OOP and how OOP in R is wrong and isn’t really OOP and honestly I’m asleep before I finish reading the complaint.↩︎\nA Slow Horses reference, in case you missed it.↩︎\nOr at least not without a lot of extra effort on the developer side.↩︎\nIn contrast S3 is so chaotically informal that it doesn’t have any real notion of an “accessor” operator. Some S3 classes are atomic objects internally (e.g., a Date is simply a double with some attributes) and have no accessor operator, whereas others are lists internally and as such use $ as the accessor.↩︎\nLike S3 and S4 and like the vast majority of R objects, S7 objects have copy-on-modify semantics. This is distinct to environments and R6 classes which have reference semantics. But let’s be honest, shall we? You either (a) already know this, (b) don’t care, or (c) don’t find this telegraphic explanation very helpful because it really isn’t very helpful. My aims in this post are pretty limited so really if you want to wrap your head around this stuff a better bet is to read Advanced R.↩︎\nI mean, at this point I’m effectively turning my circle into a pentagon, but of course I could always change n again later if I wanted a different number of points.↩︎\nLOL. Let’s be honest, there is always another way if you try hard enough but the point here is that you’re not supposed to modify a read-only property and S7 will throw up roadblocks if you try to do it.↩︎\nIdeally I’d like to make this an abstract class, in which users can’t directly create a new drawable. In principle this is achievable by setting abstract = TRUE in the call to new_class(), but what I’ve found so far is that creates problems when – as happens slightly later in the post – I define a custom constructor function for my “circle-subclassing-drawable” class. I imagine this will be dealt with at some point, but for now I’m just ignoring the idea of abstract classes.↩︎"
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html",
    "href": "posts/2024-12-18_art-from-code-1/index.html",
    "title": "Art from code I: Generative art with R",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the first post in that series."
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#introduction",
    "href": "posts/2024-12-18_art-from-code-1/index.html#introduction",
    "title": "Art from code I: Generative art with R",
    "section": "Introduction",
    "text": "Introduction\nOnce upon a time I was a professor.\nIt’s a dull story, and I won’t bore you with the details, but I mention it because at the start of almost every class I taught I’d have to go being by explaining the terminology. Want to teach a class on human learning? You better start out by saying what you think “learning” means. Want to teach people about computational models of cognition? The students would like you to start by giving a working definition of “computation”, “model” and “cognition”. It doesn’t really matter if the definitions aren’t good definitions (spoiler: they’re always terrible), but it helps people to start out from something that looks vaguely like stable ground to stand on.\nSo. If this is going to be a workshop on generative art in R, I should say something about what I mean by “R”, “art”, and “generative”. Of the three terms, R is the easiest one to define: it’s a programming language used primarily for statistical computing and data science. To anyone starting this workshop I don’t really have to give a lot more detail than that. We all know (roughly speaking) what R is: we wouldn’t be participating at rstudio::conf if we didn’t! For the current purposes all I’ll say is that I’m talking about R the way it is typically used in 2022: R doesn’t just mean “base R”. It also includes “tidyverse”. It includes the sprawling ecosystem of packages on CRAN and GitHub. Often that includes code written in other languages: thanks to packages like Rcpp and cpp11 it’s not at all unusual for computationally intensive subroutines to be farmed out to C++ code, and not written in “pure” R. Supplying bindings to libraries written in other languages is a well-established tradition in R: in this workshop we shall not be purists. If we can do a thing by working with R code, then we’re doing something with R even if other languages are doing some of the work for us.\nOkay, so that’s “R”. What about “art” – what counts as “art”? In this essay I will…\n…just kidding. I do not have the arrogance to pretend that I know what art is. Art is pretty (except when it isn’t). Art is created (except when it is found). Art is intentional (except when it is accidental). Art makes you think (sometimes, not always). Art relies on artist skill (except when you send GPT-3 generated text to DALL-E). Art is uniquely human (no it isn’t). Art… yeah, look, I have no idea what art is. Personally, I like to make pretty things from code that appeal to my aesthetic sensibilities, and that’s good enough for me.\nWhich brings me to the third word: “generative”. When we talk about “generative art”, what do we mean? The term isn’t particularly well defined, but what we typically (not always) mean is that generative art is computational artwork that incorporates some source of noise, disorder, or randomness (natural or artificial) into the artistic process itself. It’s a kind of perverse artistry: almost always we’re working with computers, one of the most precise and flexible tools that humans have devised, and yet we are deliberately making the products of the machine unpredictable, unknowable even to the artist.\nWe are writing code that makes unpredictable paintings."
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#art-is-theft",
    "href": "posts/2024-12-18_art-from-code-1/index.html#art-is-theft",
    "title": "Art from code I: Generative art with R",
    "section": "Art is theft",
    "text": "Art is theft\nOkay that’s quite enough preamble. Let’s stop talking and start making some art! Following the oft-repeated dictum that “art is theft” I want to begin with an act of wholesale thievery. I am going to co-opt tools that were created for serious data science purposes and repurpose them for art. If you’re in this room with me then you already know these tools by name: ggplot2 is an R package designed for data visualisation; dplyr is designed for data manipulation; tibble is designed for data representation. None of these packages were created for artists. They’re practical tools designed for statisticians, data scientists, analysts, and other knowledge workers.\nFuck it. Let’s steal them. Sorry Hadley.\n\nlibrary(ggplot2)\nlibrary(tibble)\n\nLet’s start with a data visualisation exercise that most R users have encountered at one time or another: creating a scatterplot. The ggplot2 package supplies the tediously-overused mpg data set that… I don’t know, it has something to do with cars, I think? I don’t drive and I am trying very hard to pretend I never learned anything about the internal combustion engine. The point here is that we have a data set. From the perspective of the generative artist our main concern is that it’s a table that contains some numbers and text organised in a table:\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model  displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4       1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4       1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4       2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4       2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4       2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4       2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4       3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 qu…   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 qu…   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 qu…   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nWe need not concern ourselves with why displ refers to engine displacement nor what that term even means. Nor do we have to care about how hwy represents highway mileage or how drv has something to do with gears. Really, this is not our concern. As generative artists we think of these as raw sources of structure and randomness.\nHere’s what I mean. As a data scientist, I might draw a plot like this:\n\nmpg |&gt;\n  ggplot(aes(displ, hwy, colour = drv)) + \n  geom_point()\n\n\n\n\n\n\n\n\nOn the x-axis I’ve plotted one column in the table, on the y-axis I’ve plotted another column, and each row in the table shows up as a dot. A third row of the table is used to supply the colour. Because ggplot2 is designed to support interpretation, the resulting plot has a collection of guides, legends and other labels. In the world of data visualisation, these things are essential for helping the viewer understand how the picture relates to the thing in the world (cars, apparently) that the data pertains to.\nAs an artist, I cannot be bothered with such dull practicalities. Let’s get rid of all of them, and – violating all sorts of data visualisation best practices – I’m going to vary the size of the dots in a way that doesn’t help anyone make sense of the data:\n\nmpg |&gt;\n  ggplot(aes(displ, hwy, colour = drv, size = cyl)) + \n  geom_point(show.legend = FALSE) + \n  theme_void() + \n  scale_color_brewer()\n\n\n\n\n\n\n\n\nOnce we strip away the practical purpose of the data visualisation, we’re left with something that isn’t quite so easy to interpret, but is… well, it’s kind of pretty, right? There’s an overall structure to the scatterplot. The colours, shapes, and positions of the data don’t necessarily tell an easily-understood story about the real world anymore but they’re quite pleasing to look at.\nViewing ggplot2 through this lens opens up a new world of possibilities. Inspired by Leland Wilkinson’s book on the subject, Hadley Wickham wrote the package to provide a grammar of graphics: it supplies a collection of transformation and composition rules that we can use to construct mappings between data (that represent a thing in the world) and images (that our visual systems can quickly interpret). Those rules are not arbitrary: they’re created to make our lives as data analysts easier. But we can repurpose them. The grammatical rules of human language did not evolve to serve the needs of poets, but the poets use them anyway. So too we as visual artists can (and will) reuse the grammar of graphics for artistic purposes. A few minor tweaks gives us this:\n\nmpg |&gt;\n  ggplot(aes(displ, hwy, colour = drv)) + \n  geom_point(show.legend = FALSE, size = 4) + \n  geom_point(show.legend = FALSE, size = 1, colour = \"#222222\") + \n  coord_polar() + \n  theme_void() + \n  scale_color_brewer()\n\n\n\n\n\n\n\n\nAnd let’s be honest. At this point the image has very little to do with cars. Sure, I’m using the mpg data to drive the process, but as far as I’m concerned it’s really just a source of raw numbers. As a generative artist these numbers are my raw materials, but I’m not too fussed about exactly how I came into possession of these numbers.\n\n\n\n\n\n\nExercise\n\n\n\nTry it yourself! Using nothing other than ggplot2 and the mpg data set, create your own artwork. Don’t take too long: 3 minutes at the absolute most! See what you can come up with in that time!\n\n\nI could just have easily created my own raw materials. As a statistical programming language R comes equipped with a very sophisticated collection of tools for generating pseudorandom numbers with various distributions. I can tap into this whenever I like but in a lot of cases I don’t need anything more fancy than the runif() function. Uniformly distributed random numbers are perfectly adequate for many purposes. Let’s make some:\n\nset.seed(1)\nn &lt;- 50\ndat &lt;- tibble(\n  x0 = runif(n),\n  y0 = runif(n),\n  x1 = x0 + runif(n, min = -.2, max = .2),\n  y1 = y0 + runif(n, min = -.2, max = .2),\n  shade = runif(n), \n  width = runif(n)\n)\ndat\n\n# A tibble: 50 × 6\n       x0     y0    x1      y1 shade width\n    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0.266  0.478  0.327  0.523  0.268 0.762\n 2 0.372  0.861  0.313  0.884  0.219 0.933\n 3 0.573  0.438  0.481  0.370  0.517 0.471\n 4 0.908  0.245  1.11   0.226  0.269 0.604\n 5 0.202  0.0707 0.255  0.0709 0.181 0.485\n 6 0.898  0.0995 0.784 -0.0282 0.519 0.109\n 7 0.945  0.316  0.796  0.328  0.563 0.248\n 8 0.661  0.519  0.652  0.349  0.129 0.499\n 9 0.629  0.662  0.799  0.573  0.256 0.373\n10 0.0618 0.407  0.101  0.292  0.718 0.935\n# ℹ 40 more rows\n\n\nYup. Those look like a bunch of numbers. They do not mean anything to me. They’re entirely deterministic – at least in the sense that the set.seed() command ensures that the pseudorandom number will always generate the same random numbers with this code – but the underlying generator is utterly without structure. As the outside observer I am entirely indifferent about whether I should use set.seed(1) or set.seed(234534). Both versions of this command will initialise the state of the random number generator in a way that ensures that runif() generates pure noise. Under the hood there are some fancy definitions of what we mean by “randomness” and “noise”, but this is not the place to talk about Martin-Löf randomness. For our purposes it is sufficient to agree that the output of a uniform random number generator is meaningless noise, no matter what seed value we supply.\nThe key point is this: the input is garbage. Garbage in…\n\ndat |&gt; \n  ggplot(aes(\n    x = x0,\n    y = y0,\n    xend = x1,\n    yend = y1,\n    colour = shade,\n    linewidth = width\n  )) +\n  geom_segment(show.legend = FALSE) +\n  coord_polar() +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_x_continuous(expand = c(0, 0)) + \n  scale_color_viridis_c() + \n  scale_linewidth(range = c(0, 10)) + \n  theme_void()\n\n\n\n\n\n\n\n\n…art out?\nNot that anyone has ever asked my opinion on the topic, but this is what I think generative art really is. An automated process that takes garbage as input and creates unpredictably delightful outputs – sometimes with a little helpful human oversight and curation – is a generative art system. It is fundamentally a process of making something from nothing. Art from the void. Treasure from trash. Signal from noise. You get the idea."
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#technique",
    "href": "posts/2024-12-18_art-from-code-1/index.html#technique",
    "title": "Art from code I: Generative art with R",
    "section": "Technique",
    "text": "Technique\nAt this point we have some sense of what our creative endeavour is all about, which leads naturally to questions about technique. One thing I’ve learned – about art, yes, but I think it holds quite generally – is that you can’t teach “personal style”. Everyone has their own tastes and preferences, and it makes no sense whatsoever to tell someone else who, what, or how they should love. I have no business telling you what is pleasing and what isn’t. I’m not arrogant enough to try.\nWhat I can do, however, is talk about the tools and coding practices that have made it easier for me to create the things that I think have aesthetic value. When art schools talk about teaching “technique”, this is what I think they mean. The point isn’t to dictate what you create, but to give you skills that will let you make a reality from the thing you envision. Fundamentally, this is a workshop on technique, in this sense of the term.\nLet’s start with a core principle: code reuse. The entire point of generative art is that we’re turning trash into treasure, and trash is not a scarce resource. If you can write code to create one beautiful thing, the same code should be reusable to create many beautiful things. This “reuse principle” means that the substantive work of writing generative art code is almost always a matter of writing functions. Functions are beautiful…\n…but they are opinionated. A function exists if (and only if) you intend to reuse it. In the ideal case, a function is a thing that makes sense on its own terms and – as a consequence – you might reuse it in many different contexts. Every scientific programming language since the dawn of time has included functions like exp() and log() because scientists will always reuse these functions. Exponents and logarithms are scientifically-reusable concepts.\nSo what counts as an artistically-reusable concept? Well, at a minimum, the definition of an artistic system is a reusable concept. Suppose I want to create many art pieces that are “in the same style” as the piece I created above. To do that, I could wrap my code in a function that I’ll call polar_art():\npolar_art &lt;- function(seed, n, palette) {\n  \n  # set the state of the random number generator\n  set.seed(seed)\n  \n  # data frame containing random values for \n  # aesthetics we might want to use in the art\n  dat &lt;- tibble(\n    x0 = runif(n),\n    y0 = runif(n),\n    x1 = x0 + runif(n, min = -.2, max = .2),\n    y1 = y0 + runif(n, min = -.2, max = .2),\n    shade = runif(n), \n    width = runif(n)\n  )\n  \n  # plot segments in various colours, using \n  # polar coordinates and a gradient palette\n  dat |&gt; \n    ggplot(aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      linewidth = width\n    )) +\n    geom_segment(show.legend = FALSE) +\n    coord_polar() +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_x_continuous(expand = c(0, 0)) + \n    scale_colour_gradientn(colours = palette) + \n    scale_linewidth(range = c(0, 10)) + \n    theme_void()\n}\n\n\n\nBecause I’ve written this thing down as a function, I’m now free to reuse it to create multiple pieces. Varying the seed argument creates new pieces that don’t differ in any systematic way from one another, whereas varying n and palette changes the number of segments plotted and the colour scheme used.\npolar_art(seed = 1, n = 500, palette = c(\"antiquewhite\", \"orange\", \"bisque\"))\npolar_art(seed = 1, n = 500, palette = c(\"red\", \"black\", \"white\"))\npolar_art(seed = 2, n = 50, palette = c(\"red\", \"black\", \"white\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nIn the materials there is a file called polar_art.R that contains a copy of the polar_art() function. Open it and use the polar_art() function to generate your own pieces. Try changing the seed, n, and the palette to create a variety of different pieces.\nCreate a new file called polar_art_02.R that contains the polar_art() function. In the new file, try modifying the polar_art() function itself to see if you can create your own new system.\nSomething to think about: Usually in data science we try to avoid naming our files my_file_version_1, my_file_version_2, etc, and instead we place files under version control using git. Yet here I am in an art context, apparently giving the advice to fall back on the old-fashioned system of naming files with version numbers. Why might I be doing that?"
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#colour",
    "href": "posts/2024-12-18_art-from-code-1/index.html#colour",
    "title": "Art from code I: Generative art with R",
    "section": "Colour",
    "text": "Colour\n\nlibrary(scales)\nlibrary(ggthemes)\n\nPicking a palette is always tricky, but can often be made simpler because R has so many packages that provides prespecified colour palettes. My usual approach is to use palettes defined by a few colours, and it’s easy to find sites online that make it easy to find colour combinations you like. One of my favourite sites is coolors.co, which you can browse for preselected palettes or use the tools to build your own. For example I might decide that this palette is the one I want to use. The site makes it easy to export the hex codes for each colour, so I can cut and paste to get this:\n\npal &lt;- c(\"#cdb4db\", \"#ffc8dd\", \"#ffafcc\", \"#bde0fe\", \"#a2d2ff\")\n\nThe show_col() function from the scales package is a nice way to quickly preview the colours:\n\nshow_col(pal)\n\n\n\n\n\n\n\n\nThough there are only five colours in this palette, the polar_art() function uses scales_colour_gradientn() to construct a continuous colour scale from them by linearly interpolating between them. You can do the same thing manually using colorRampPalette(). In the code below I’ve created a new function palette_fn() that will generate a vector of colours that linearly interpolates between the five input colours in pal:\n\npalette_fn &lt;- colorRampPalette(pal)\n\nIf I need 100 colours distributed along the spectrum defined by pal, all I need to do is this:\n\npalette_fn(100)\n\n  [1] \"#CDB4DB\" \"#CFB4DB\" \"#D1B5DB\" \"#D3B6DB\" \"#D5B7DB\" \"#D7B8DB\" \"#D9B8DB\"\n  [8] \"#DBB9DB\" \"#DDBADB\" \"#DFBBDB\" \"#E1BCDB\" \"#E3BCDB\" \"#E5BDDB\" \"#E7BEDC\"\n [15] \"#E9BFDC\" \"#EBC0DC\" \"#EDC0DC\" \"#EFC1DC\" \"#F1C2DC\" \"#F3C3DC\" \"#F5C4DC\"\n [22] \"#F7C4DC\" \"#F9C5DC\" \"#FBC6DC\" \"#FDC7DC\" \"#FFC7DC\" \"#FFC6DC\" \"#FFC5DB\"\n [29] \"#FFC4DA\" \"#FFC3DA\" \"#FFC2D9\" \"#FFC1D8\" \"#FFC0D8\" \"#FFBFD7\" \"#FFBED6\"\n [36] \"#FFBDD5\" \"#FFBCD5\" \"#FFBBD4\" \"#FFBAD3\" \"#FFB9D3\" \"#FFB8D2\" \"#FFB7D1\"\n [43] \"#FFB6D1\" \"#FFB5D0\" \"#FFB4CF\" \"#FFB3CF\" \"#FFB2CE\" \"#FFB1CD\" \"#FFB0CD\"\n [50] \"#FFAFCC\" \"#FDAFCD\" \"#FBB1CF\" \"#F8B3D1\" \"#F5B5D3\" \"#F3B7D5\" \"#F0B9D7\"\n [57] \"#EDBBD9\" \"#EBBDDB\" \"#E8BFDD\" \"#E5C1DF\" \"#E3C3E1\" \"#E0C5E3\" \"#DDC7E5\"\n [64] \"#DBC9E7\" \"#D8CBE9\" \"#D5CDEB\" \"#D3CFED\" \"#D0D1EF\" \"#CDD3F1\" \"#CBD5F3\"\n [71] \"#C8D7F5\" \"#C5D9F7\" \"#C3DBF9\" \"#C0DDFB\" \"#BDDFFD\" \"#BCDFFE\" \"#BBDFFE\"\n [78] \"#BADEFE\" \"#B8DDFE\" \"#B7DDFE\" \"#B6DCFE\" \"#B5DCFE\" \"#B4DBFE\" \"#B3DBFE\"\n [85] \"#B2DAFE\" \"#B1D9FE\" \"#B0D9FE\" \"#AFD8FE\" \"#AED8FE\" \"#ACD7FE\" \"#ABD7FE\"\n [92] \"#AAD6FE\" \"#A9D5FE\" \"#A8D5FE\" \"#A7D4FE\" \"#A6D4FE\" \"#A5D3FE\" \"#A4D3FE\"\n [99] \"#A3D2FE\" \"#A2D2FF\"\n\n\nHere’s what those colours look like as a smooth palette:\n\nimage(\n  x = matrix(1:100, ncol = 1), \n  col = palette_fn(100),\n  useRaster = TRUE,\n  axes = FALSE\n)\n\n\n\n\n\n\n\n\nIn this example, I took one set of colours from the web to define a palette, but there are many built-in palettes you can select from randomly as part of your generative process. For example, the ggthemes package contains a list called canva_palettes, which contains 150 palettes taken from canva.com. For example, here’s one of those palettes\n\ncanva_palettes[[101]]\n\n[1] \"#4abdac\" \"#fc4a1a\" \"#f7b733\" \"#dfdce3\"\n\nshow_col(canva_palettes[[101]])\n\n\n\n\n\n\n\n\nThe fact that we have a list containing 150 different palettes, it’s a simple matter to write a sample_canva() function that samples one of these palettes at random:\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nHere’s an example of me using it:\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 2))\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 3))\npolar_art(seed = 2, n = 100, palette = sample_canva(seed = 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that I’ve set it up in a way that allows me some degree of control over which elements of the image are allowed to vary. In all three images I used the same seed when calling polar_art(), so the random configuration of shapes is identical in all three cases. In contrast, I gave different seeds to the sample_canva() function, so the images have different palettes. The reverse is also possible, producing different configurations with the same colour scheme:\npolar_art(seed = 5, n = 100, palette = sample_canva(seed = 1))\npolar_art(seed = 6, n = 100, palette = sample_canva(seed = 1))\npolar_art(seed = 7, n = 100, palette = sample_canva(seed = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe ability to pick and choose when the randomness gets turned on and off is quite handy!\n\n\n\n\n\n\nExercises\n\n\n\n\nIn the materials there is a file called palette-generators.R that contains a copy of the sample_canva() function. Take a look and try calling the function a few times to see what kind of output it produces. You may find it handy to use show_col() to visualise the results.\nTry writing your own random palette generator. A simple (and occasionally useful) approach is to construct a palette consisting of distinct but otherwise randomly selected named colours in R. There is a set of 502 colour names generated by calling colours() with distinct = TRUE). Write a function called sample_named_colours() that takes n as an input argument, and then returns a sample of n of these colour. Try using it with the polar_art() function.\nThe sample_canva() function, as I’ve written it, preserves the original structure of the 150 palettes in ggthemes::canva_palettes, so that the 4 colours returned all belong to the same palette on the Canva website originally. Try breaking this constraint. If you call unlist(ggthemes::canva_palettes) you get a vector of 600 distinct colours. Write a palette generating function that samples colours randomly from that set of 600 colours."
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#composition",
    "href": "posts/2024-12-18_art-from-code-1/index.html#composition",
    "title": "Art from code I: Generative art with R",
    "section": "Composition",
    "text": "Composition\nThe polar_art() function I wrote earlier is nice, but it’s not very flexible. It allows some control over the palette and the number of segments to be plotted, but that’s all. It doesn’t let me use the full flexibility of ggplot2 to create artwork. For example, what if I wanted to create more pieces in a “similar” style to the ones I created earlier, but plot different geoms? Or perhaps I want to plot more than one data set as part of a single piece? The polar_art() function doesn’t allow that. The data generation and plot building is all handled internally. Perhaps there’s a case to be made that we should break this into smaller functions and see if that helps.\nLet’s start out by writing a “random tibble generator” function, sample_data(). This function will generate tibbles full of random numbers, but that’s all:\n\nsample_data &lt;- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat &lt;- tibble(\n    x0 = runif(n),\n    y0 = runif(n),\n    x1 = x0 + runif(n, min = -.2, max = .2),\n    y1 = y0 + runif(n, min = -.2, max = .2),\n    shade = runif(n), \n    size = runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\n\nNext, let’s create a styled_plot() function that takes a palette and (optionally) a data set as inputs, and sets up the mappings and the stylistic aspects to the plot. This function does a lot of the work in defining what kind of artwork is possible using this system, even though it doesn’t actually draw anything. For example, it specifies coord_polar() as the coordinate system, so any points or lines that get created will be shown in polar coordinates. It uses guide_none() to suppress legends, and theme_void() to suppress axes, axis titles and so on.\n\npolar_styled_plot &lt;- function(data = NULL, palette) {\n  ggplot(\n    data = data,\n    mapping = aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      size = size\n    )) + \n    coord_polar(clip = \"off\") +\n    scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) +\n    scale_x_continuous(\n      expand = c(0, 0), \n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) + \n    scale_colour_gradientn(colours = palette) + \n    scale_size(range = c(0, 10)) + \n    theme_void() + \n    guides(\n      colour = guide_none(),\n      size = guide_none(),\n      fill = guide_none(),\n      shape = guide_none()\n    )\n}\n\nThis structure gives a clean delineation of responsibility among the different functions. The sample_canva() function does the work of generating random palettes, sample_data() does the job of creating random data to drive the plot, polar_styled_plot() takes care of all the ggplot set up, and then you can pick and choose which geom you want to add. So we can write code like this that differs only in the choice of geom:\ndat &lt;- sample_data(n = 100, seed = 1) \npal &lt;- sample_canva(seed = 1)\n\npolar_styled_plot(data = dat, palette = pal) + geom_segment()\npolar_styled_plot(data = dat, palette = pal) + geom_path()\npolar_styled_plot(data = dat, palette = pal) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBecause the output of polar_styled_plot() is a ggplot that we can add layers to, and because each layer in a ggplot can supply its own data, we now have the ability to reuse these components in different ways. For that it’s helpful to load dplyr. Here’s an example where we generate two random data sets and feed those into four separate geoms:\n\nlibrary(dplyr)\n\ndat1 &lt;- sample_data(n = 2000, seed = 123) \ndat2 &lt;- sample_data(n = 100, seed = 456) |&gt;  \n  mutate(y0 = .3 + y0 * .6, y1 = .3)\n\npolar_styled_plot(palette = sample_canva(seed = 7)) + \n  geom_segment(\n    data = dat1 |&gt; mutate(size = size * 3)\n  ) + \n  geom_segment(\n    data = dat2 |&gt; mutate(size = size / 5), \n    lineend = \"round\", \n    colour = \"white\"\n  ) +\n  geom_segment(\n    data = dat2 |&gt; mutate(size = size / 40), \n    lineend = \"round\", \n    colour = \"#222222\"\n  ) +\n  geom_point(\n    data = dat2 |&gt; mutate(size = size * 2),\n    colour = \"#222222\"\n  )\n\n\n\n\n\n\n\n\nAnother example that three copies of the same random data set to produce a variation that has additional symmetries:\n\ndat &lt;- sample_data(n = 2000, seed = 123) |&gt;\n  mutate(y1 = y0, size = size / 2)\n\npolar_styled_plot(palette = sample_canva(seed = 456)) + \n  geom_segment(data = dat) + \n  geom_segment(data = dat |&gt; mutate(y1 = y1 - .2, y0 = y0 - .2)) +\n  geom_segment(data = dat |&gt; mutate(y1 = y1 - .4, y0 = y0 - .4))\n\n\n\n\n\n\n\n\nFinally, here’s a version that uses the linetype argument to geom_segment() to create a “choppier” version of these pieces:\n\ndat &lt;- sample_data(n = 1000, seed = 1) |&gt;\n  mutate(y1 = y0, size = size / 4)\n\npolar_styled_plot(palette = sample_canva(seed = 2)) + \n  geom_segment(data = dat, linetype = \"331311\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercises\n\n\n\n\nIn the materials there is a file called polar-styled-plots.R that contains a copy of the sample_canva(), sample_data() and polar_styled_plot() functions. Without modifying any of these three functions, explore how much flexibility you have to make different pieces in which (1) data are generated with sample_data(), (2) the plot is initialised by calling polar_styled_plot(), and (3) the piece is created by adding ggplot2 geoms. Data manipulation with dplyr is allowed!\nIn the examples above and the previous exercise you saw that the polar_styled_plot() function plays the role of defining an overarching “style” for possible art pieces, but it doesn’t completely constrain artistic freedom. Your task in this exercise is to try to write a my_styled_plot() that does something similar… but creates a different style that you can explore"
  },
  {
    "objectID": "posts/2024-12-18_art-from-code-1/index.html#materials",
    "href": "posts/2024-12-18_art-from-code-1/index.html#materials",
    "title": "Art from code I: Generative art with R",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\npalette-generators.R\n\n\n\n\n\n\nlibrary(ggthemes)\n\n# the original function from the first session\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\n# the extended function used in later sessions\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\n\n\n\n\n\n\n\n\n\npolar-art.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\n\npolar_art &lt;- function(seed, n, palette) {\n  \n  # set the state of the random number generator\n  set.seed(seed)\n  \n  # data frame containing random values for \n  # aesthetics we might want to use in the art\n  dat &lt;- tibble(\n    x0 = runif(n),\n    y0 = runif(n),\n    x1 = x0 + runif(n, min = -.2, max = .2),\n    y1 = y0 + runif(n, min = -.2, max = .2),\n    shade = runif(n), \n    size = runif(n)\n  )\n  \n  # plot segments in various colours, using \n  # polar coordinates and a gradient palette\n  dat |&gt; \n    ggplot(aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      size = size\n    )) +\n    geom_segment(show.legend = FALSE) +\n    coord_polar() +\n    scale_y_continuous(expand = c(0, 0)) +\n    scale_x_continuous(expand = c(0, 0)) + \n    scale_colour_gradientn(colours = palette) + \n    scale_size(range = c(0, 10)) + \n    theme_void()\n}\n\npolar_art(\n  seed = 2, \n  n = 50, \n  palette = c(\"red\", \"black\", \"white\")\n)\n\n\n\n\n\n\n\n\n\n\npolar-styled-plots.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(dplyr)\n\nsample_canva &lt;- function(seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]]\n}\n\nsample_data &lt;- function(seed = NULL, n = 100){\n  if(!is.null(seed)) set.seed(seed)\n  dat &lt;- tibble(\n    x0 = runif(n),\n    y0 = runif(n),\n    x1 = x0 + runif(n, min = -.2, max = .2),\n    y1 = y0 + runif(n, min = -.2, max = .2),\n    shade = runif(n), \n    size = runif(n),\n    shape = factor(sample(0:22, size = n, replace = TRUE))\n  )\n}\n\npolar_styled_plot &lt;- function(data = NULL, palette) {\n  ggplot(\n    data = data,\n    mapping = aes(\n      x = x0,\n      y = y0,\n      xend = x1,\n      yend = y1,\n      colour = shade,\n      size = size\n    )) + \n    coord_polar(clip = \"off\") +\n    scale_y_continuous(\n      expand = c(0, 0),\n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) +\n    scale_x_continuous(\n      expand = c(0, 0), \n      limits = c(0, 1), \n      oob = scales::oob_keep\n    ) + \n    scale_colour_gradientn(colours = palette) + \n    scale_size(range = c(0, 10)) + \n    theme_void() + \n    guides(\n      colour = guide_none(),\n      size = guide_none(),\n      fill = guide_none(),\n      shape = guide_none()\n    )\n}"
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html",
    "href": "posts/2024-12-22_art-from-code-5/index.html",
    "title": "Art from code V: Iterated function systems",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the fifth post in that series.\nlibrary(Rcpp)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(ggthemes)\nlibrary(tictoc)\nSo… iterated function systems. What are they?"
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#some-tiresome-formalism",
    "href": "posts/2024-12-22_art-from-code-5/index.html#some-tiresome-formalism",
    "title": "Art from code V: Iterated function systems",
    "section": "Some tiresome formalism",
    "text": "Some tiresome formalism\nOne of the joys of leaving academia is that I can stop pretending that I don’t get all my mathematical knowledge from Wikipedia, and as the entry for iterated function systems oh so helpfully informs us, an iterated function system is defined as a finite set of contractive maps on a complete metric space \\(X = (M, d)\\), formally denoted\n\\[\n\\left\\{f_i : X \\rightarrow X \\mid i = 1, 2, \\ldots N \\right\\}, N \\in \\mathcal{N}\n\\]\nwhere the function \\(f_i\\) is a contraction on \\(X\\) if there exists some real number \\(k\\) such that \\(d(f_i(x), f_i(y)) \\leq k \\ d(x,y)\\) for all \\(x \\in M\\) and \\(y \\in M\\).\nIf that weren’t impenetrable enough, Wikipedia continues to explain that\n\nHutchinson (1981) showed that, for the metric space \\({\\displaystyle \\mathbb {R} ^{n}}\\), or more generally, for a complete metric space \\(X\\), such a system of functions has a unique nonempty compact (closed and bounded) fixed set \\(S\\). One way of constructing a fixed set is to start with an initial nonempty closed and bounded set \\(S_0\\) and iterate the actions of the \\(f_i\\), taking \\(S_{n+1}\\) to be the union of the images of \\(S_n\\) under the \\(f_i\\); then taking \\(S\\) to be the closure of the union of the \\(S_n\\). Symbolically, the unique fixed (nonempty compact) set \\(S\\subseteq X\\) has the property\n\\[S = \\overline{\\bigcup_{i=1}^N f_i(S)}.\\]\nThe set \\(S\\) is thus the fixed set of the Hutchinson operator \\(F:2^{X}\\to 2^{X}\\) defined for \\(A\\subseteq X\\) via\n\\[F(A)={\\overline {\\bigcup _{i=1}^{N}f_{i}(A)}}.\\]\nThe existence and uniqueness of \\(S\\) is a consequence of the contraction mapping principle, as is the fact that\n\\[\\lim _{n\\to \\infty }F^{\\circ n}(A)=S\\]\nfor any nonempty compact set \\(A \\in X\\). (For contractive IFS this convergence takes place even for any nonempty closed bounded set \\(A\\)). Random elements arbitrarily close to \\(S\\) may be obtained by the “chaos game”\n\nI am entirely certain that you do not care.\nAs impressive as I find all this notation, I don’t find it helps me understand what an iterated function system actually does. What I do find helpful, however, is to play the chaos game, because that’s a concrete method we can use to simulate the behaviour of an IFS, and in practice that’s what our code will actually do!"
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#barnsley-fern-chaos-game",
    "href": "posts/2024-12-22_art-from-code-5/index.html#barnsley-fern-chaos-game",
    "title": "Art from code V: Iterated function systems",
    "section": "Barnsley fern chaos game",
    "text": "Barnsley fern chaos game\nWhen written as pseudocode, the chaos game is remarkably simple:\n\nChoose a set of starting values \\((x_0, y_0)\\)\nSet iteration number \\(i = 1\\)\nChoose a transformation function \\(f\\) to use on this iteration\nGet the next value by passing the current value to the function, i.e. \\((x_i, y_i) = f(x_{i-1}, y_{i-1})\\)\nUpdate iteration number \\(i = i + 1\\) and return to step 3; or finish\n\nI’ve written this on the assumption that the functions are defined over a two dimensional space with \\(x\\) and \\(y\\) coordinates, but it generalises naturally to any number of dimensions. When choosing a transformation function in step 3, you can sample uniformly at random, or impose a bias so that some transformation are applied more often than others.\nTo get a sense of how this works, let’s start with a classic example: the Barnsley fern. The Barnsley fern, like many iterated function systems I use for my art, is constructed from functions \\(f(x, y)\\) defined in two dimensons. Better yet, they’re all affine transformations so we can write any such function down using good old fashioned linear algebra, and compute everything using matrix multiplication and addition:\n\\[f(x,y)={\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}}{\\begin{bmatrix}x\\\\y\\end{bmatrix}}+{\\begin{bmatrix}e\\\\f\\end{bmatrix}}\\]\nThere are four such functions used to build the Barnsley fern, with coefficients shown below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(b\\)\n\\(c\\)\n\\(d\\)\n\\(e\\)\n\\(f\\)\nweight\ninterpretation\n\n\n\n\n\\(f_1(x, y)\\)\n0\n0\n0\n0.16\n0\n0\n0.01\nmakes the stem\n\n\n\\(ƒ_2(x, y)\\)\n0.85\n0.04\n−0.04\n0.85\n0\n1.60\n0.85\nmakes ever-smaller leaflets\n\n\n\\(ƒ_3(x, y)\\)\n0.20\n−0.26\n0.23\n0.22\n0\n1.60\n0.07\nmakes largest left-hand leaflet\n\n\n\\(ƒ_4(x, y)\\)\n−0.15\n0.28\n0.26\n0.24\n0\n0.44\n0.07\nmakes largest right-hand leaflet\n\n\n\nOkay, so let’s start by implementing the Barnsley fern transformation functions in R. The fern_transform() function below takes coord input as a two-element numeric vector, and an ind argument that specifies which of the four transformations to apply (this should be an integer between 1 and 4). The output is the next set of coord values to use in the chaos game:\n\nfern_transform &lt;- function(coord, ind) {\n  \n  # coefficients for the stem function f_1\n  if(ind == 1) {\n    mat &lt;- matrix(c(0, 0, 0, .16), 2, 2) # matrix to multiply\n    off &lt;- c(0, 0)                       # offset vector to add\n  }\n  \n  # coefficients for the small leaflet function f_2\n  if(ind == 2) {\n    mat &lt;- matrix(c(.85, -.04, .04, .85), 2, 2)\n    off &lt;- c(0, 1.6)                      \n  }\n  # coefficients for the right-side function f_3\n  if(ind == 3) {\n    mat &lt;- matrix(c(.2, .23, -.26, .22), 2, 2)\n    off &lt;- c(0, 1.6)                      \n  }\n  \n  # coefficients for the left-side function f_4\n  if(ind == 4) {\n    mat &lt;- matrix(c(-.15, .26, .28, .24), 2, 2)\n    off &lt;- c(0, .44)                     \n  }\n  \n  # return the affine transformed coords\n  coord &lt;- mat %*% coord + off\n  return(coord)\n}\n\nArmed with the fern_transform() function, we can write a fern_chaos() function that implements the chaos game for the Barnsley fern. The arguments to fern_chaos() specify the number of iterations over which the game should be played, and (optionally) a seed to control the state of the random number generator:\n\nfern_chaos &lt;- function(iterations = 10000, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  # which transformation to apply at each iteration\n  transform_index &lt;- sample(\n    x = 1:4, \n    size = iterations, \n    replace= TRUE, \n    prob = c(.01, .85, .07, .07)\n  )\n  \n  # initialise chaos game at the origin\n  start &lt;- matrix(c(0, 0))\n  \n  # helper function to collapse accumulated output\n  bind_to_column_matrix &lt;- function(lst) {\n    do.call(cbind, lst)\n  }\n  \n  # iterate until done!\n  coord_matrix &lt;- transform_index |&gt;\n    accumulate(fern_transform, .init = start) |&gt;\n    bind_to_column_matrix() \n  \n  # tidy the output, add extra columns, and return\n  coord_df &lt;- t(coord_matrix) |&gt; \n    as.data.frame() \n  names(coord_df) &lt;- c(\"x\", \"y\")\n  coord_df &lt;- coord_df |&gt;\n    as_tibble() |&gt;\n    mutate(\n      transform = c(0, transform_index),\n      iteration = row_number() - 1\n    )\n  return(coord_df)\n}\n\nThis function is a little fussier than it really needs to be. For example, if you compare my code to the base R version on Wikipedia you’ll see I spend extra effort tidying the results at the end: rather than returning a matrix of points, I’ve coerced it to a tibble that includes the coordinates as columns x and y, but in addition contains a column transform specifying which of the transformation functions was used to generate each point, and the iteration number as a unique identifier for each row. In any case, here’s the output:\n\nfern_dat &lt;- fern_chaos(seed = 1)\nfern_dat\n\n# A tibble: 10,001 × 4\n        x     y transform iteration\n    &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1  0     0             0         0\n 2  0     1.6           2         1\n 3  0.064 2.96          2         2\n 4  0.173 4.11          2         3\n 5 -1.03  2.54          3         4\n 6 -0.778 3.80          2         5\n 7 -1.14  2.26          3         6\n 8  0.804 0.684         4         7\n 9  0.711 2.15          2         8\n10  0.690 3.40          2         9\n# ℹ 9,991 more rows\n\n\nIt looks nicer as a plot though :)\n\nggplot(fern_dat, aes(x, y)) +\n  geom_point(size = 1, stroke = 0) +\n  coord_equal() +\n  theme_void()\n\n\n\n\n\n\n\n\nThe reason I went to the extra trouble of storing the transform column was so I could map it to the colour aesthetic in my plot. When I do this, I get this as the result: there’s a transformation function that defines the left leaf shape, another that defines the right leaf shape, and a third one that defines the stem shape. Finally, there’s a function that copies, shifts-up, and rotates its input in a way that produces the vertical symmetry in the output.\n\nggplot(fern_dat, aes(x, y, colour = factor(transform))) +\n  geom_point(size = 1, stroke = 0) +\n  coord_equal() +\n  theme_void() + \n  guides(colour = guide_legend(\n    title = \"transformation\", \n    override.aes = list(size = 5))\n  )\n\n\n\n\n\n\n\n\nIt’s rather more obvious now what each of the transformation functions does!\nAs we’ll see a little later, it can be very useful to plot your outputs this way sometimes: even if you’re planning to do something fancier with colour later, the ability to visualise which parts of your output are associated with a particular function is useful for diagnosing what your system is doing. My experience has been that iterated function systems are difficult to reason about just by looking at the code: the relationship between the code and the output is pretty opaque, so you have to rely on diagnostics like this when tweaking the output of your system.\nFor no particular reason, here’s our fern with the colour aesthetic mapped to the iteration number:\n\nggplot(fern_dat, aes(x, y, colour = iteration)) +\n  geom_point(size = 1, stroke = 0, show.legend = FALSE) +\n  coord_equal() +\n  theme_void() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the barnsley-fern.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#happy-accidents",
    "href": "posts/2024-12-22_art-from-code-5/index.html#happy-accidents",
    "title": "Art from code V: Iterated function systems",
    "section": "Happy accidents",
    "text": "Happy accidents\nIterated function systems can be a lot more elaborate than the Barnsley fern, often involving transformation functions that are constructed according to some fancypants compositional rules. For example, the fractal flame algorithm proposed by Scott Draves in 1992 (here’s the original article) specifies transformation functions \\(f_i()\\) – called “flame functions” – that are composed according to the rule:\n\\[\nf_i(\\mathbf{x}) = \\sum_j w_{ij} \\ g_j(\\mathbf{A}_i \\mathbf{x})\n\\]\nwhere\n\n\\(\\mathbf{A}_i\\) is a matrix that defines an affine transformation of the coordinates \\(\\mathbf{x}\\) associated with this specific flame function (i.e., each flame function \\(f_i()\\) has its own transformation \\(\\mathbf{A_i}\\), and in the two dimensional case \\(\\mathbf{x}\\) is just the points \\((x, y)\\));\nthe various \\(g_j()\\) functions are called “variant functions”, and these don’t have to be linear: they can be sinusoidal, or discontinuous, or whatever you like really; and\neach flame function is defined as a linear combination of the variant functions: the coefficient \\(w_{ij}\\) specifies the weight assigned to the \\(j\\)-th variant function by the \\(i\\)-th flame function.\n\nAdditionally, just as we saw with the Barnsley fern, the flame functions themselves can be weighted with a probability vector: a system can be defined in a way that has a bias for some flame functions over others.\nThis probably sounds a bit… intense, right?\nSo yeah. Um.\nWhen I first decided to try implementing the fractal flame algorithm I decided I wasn’t going to bother with fancypants weights \\(w_{ij}\\), so I… ignored them. But then – because I was tired and not really paying attention to the subscripts in Draves equations – I decided that my system was going to have one flame function for every possible combination of transformation matrix \\(\\mathbf{A}_i\\) and variant function \\(g_j()\\). What this meant is that the thing I actually coded was this. Given a set of variant functions \\(g_1, g_2, \\ldots, g_n\\) and some set of transformation matrices \\(\\mathbf{A}_1, \\mathbf{A}_2, \\ldots, \\mathbf{A}_m\\), I included every transformation function \\(f_{ij}(\\mathbf{x})\\) of the following form:\n\\[\nf_{ij}(\\mathbf{x}) = g_j(\\mathbf{A}_i \\mathbf{x})\n\\] When your transformation functions are composed in this way you can sample a random transformation \\(f_{ij}\\) by sampling the two components independently: sample a transformation matrix \\(\\mathbf{A}_i\\) and a variant function \\(g_j\\), and then you’re done. It ends up being a weird special case of the fractal flame algorithm, but it turns out you can make pretty things that way.\nOh well. Whatever.\nThe point of art isn’t to mindlessly copy what someone else has done, and if I’m being honest with myself the truth is that some of the best art I’ve created started with a coding error or a misinterpretation like this one. As Bob Ross famously said,\n\nThere are no mistakes, just happy accidents."
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#chaos-game-for-unboxing",
    "href": "posts/2024-12-22_art-from-code-5/index.html#chaos-game-for-unboxing",
    "title": "Art from code V: Iterated function systems",
    "section": "Chaos game for unboxing",
    "text": "Chaos game for unboxing\nEnough chitchat about my artistic process. Let’s actually implement a version of my Unboxing system. In this example, the coefficients that define the affine transformations \\(\\mathbf{A_i}\\) have been sampled uniformly at random, with values ranging from -1 to 1. There’s a layers input argument that specifies how many of these affine transformations to include (no I don’t know why I called it layers – it’s a bad name I think). Anyway, the code snippet below shows how this is implemented:\n\ncoeffs &lt;- array(\n  data = runif(9 * layers, min = -1, max = 1), \n  dim = c(3, 3, layers)\n)\n\nThe coefficients are stored in an array: coeffs[,,i] is the matrix of coefficients \\(\\mathbf{A_i}\\).\nThere are three variant functions \\(g_j\\) in this system: two of them are sinusoidal functions: one of them computes sin(x) and sin(y), and the other computes the same thing but multiplies the output by two. Both of these will produce wavy shapes. The other one is a rescaling function: it tends to shift points towards the top right corner. The code snippet below implements these variant functions:\n\nfuns &lt;- list(\n  function(point) point + (sum(point ^ 2)) ^ (1/3),\n  function(point) sin(point),\n  function(point) 2 * sin(point)\n)\n\nThe unboxer_base() function below implements the whole thing:\n\nunboxer_base &lt;- function(iterations, layers, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  # coefficients defining affine layer transforms, A_i\n  coeffs &lt;- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  # list of variant functions, g_j\n  funs &lt;- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  # updater function: apply the layer, then the function\n  # (the weirdness with point[3] is me treating colour as special)\n  update &lt;- function(point, layer, transform) {\n    f &lt;- funs[[transform]]\n    z &lt;- point[3]\n    point[3] &lt;- 1\n    point &lt;- f(point %*% coeffs[,,layer])\n    point[3] &lt;- (point[3] + z)/2\n    return(point)\n  }\n  \n  # initial point\n  point0 &lt;- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  # sample points\n  layer_ind &lt;- sample(layers, iterations, replace = TRUE)  \n  trans_ind &lt;- sample(length(funs), iterations, replace = TRUE)  \n  points &lt;- accumulate2(layer_ind, trans_ind, update, .init = point0)\n  \n  # tidy up, add columns, and return\n  points &lt;- cbind(\n    points,\n    c(0, layer_ind),\n    c(0, trans_ind)\n  )\n  return(points)\n}\n\nLet’s run this system for a few iterations, just so we can see what the output looks like:\n\nunboxer_base(10, layers = 5, seed = 333)\n\n             [,1]        [,2]       [,3] [,4] [,5]\n [1,] -0.88255069 -0.04718621  0.1969293    0    0\n [2,] -1.67203174  1.84627339 -0.7948247    5    3\n [3,]  0.36262158  1.58519247 -0.4479714    3    1\n [4,]  1.07641590  0.44778402 -0.1920135    1    1\n [5,]  0.96570274  0.34005575 -0.8206870    1    3\n [6,] -0.98567258 -0.99766695 -0.7158925    3    2\n [7,]  0.94597134  0.21844210 -0.3696898    4    2\n [8,] -0.02509324  0.35517269 -0.2772464    4    2\n [9,]  1.21601399  0.20495003 -0.9885079    1    3\n[10,]  1.22236642  3.28357073  1.2944046    2    1\n[11,] -0.93808971 -0.91680227  0.5876316    5    2\n\n\nAs you can see, this time around I’ve not gone to the effort of converting it to a tibble or making it pretty. This output is a matrix. The first column is the x-coordinate and the second column is the y-coordinate. The third column is a “z-coordinate” that we’ll map to the colour aesthetic later. Column four specifies the layer number (i.e., the value \\(i\\) specifying which affine matrix \\(\\mathbf{A}_i\\) was used), and column five specifies the variant function number (i.e., the value \\(j\\) specifying which variant function \\(g_j()\\) was used).\nIf we want to turn these numbers into art and attach colours to the points, we are going to need a palette function, so as usual I’ll insert my code to sample one of the canva palettes:\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nHaving done all that work, the rendering function in not very fancy: it’s just some ggplot2 code to create a scatter plot from the points and colour them using a canva palette:\nunbox_art &lt;- function(data, seed = NULL, size = 1) {\n  \n  # convert to data frame and sample a palette\n  data &lt;- data |&gt; as.data.frame() |&gt; as_tibble()\n  names(data) &lt;- c(\"x\", \"y\", \"c\", \"l\", \"t\")[1:ncol(data)]\n  shades &lt;- sample_canva2(seed)\n  \n  # render image as a scatter plot\n  ggplot(data, aes(x, y, colour = c)) +\n    geom_point(\n      size = size,\n      stroke = 0,\n      show.legend = FALSE\n    ) + \n    theme_void() + \n    coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) + \n    scale_colour_gradientn(colours = shades) + \n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    theme(panel.background = element_rect(\n      fill = shades[1], colour = shades[1]\n    ))\n}\n\n\n\nThe results can be very pretty, especially when you generate a large number of points and plot them with a very small marker size.\n\ntic()\nunboxer_base(1000000, layers = 3, seed = 66) |&gt; \n  unbox_art(seed = 66, size = .1)\n\n\n\n\n\n\n\ntoc()\n\n21.17 sec elapsed\n\n\nThe system is slow, but I’m usually willing to wait a bit for something pretty. (I’ll talk about how we can speed this up later)\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the unbox-base.R script.\n\n\nThe outputs from this system have a fairly consistent look and feel: a pair of nested boxes, with something “bursting” from the top right corner. The fine grained details vary a lot from output to output, and there are some systematic differences as a function of the number of layers. Here’s an example showing what happens when I ratchet up the number of layers from 2 to 9:\ntic()\nunboxer_base(100000, layers = 2, seed = 999) |&gt; unbox_art(seed = 2, size = .2)\nunboxer_base(100000, layers = 5, seed = 333) |&gt; unbox_art(seed = 2, size = .2) \nunboxer_base(100000, layers = 9, seed = 420) |&gt; unbox_art(seed = 2, size = .2)\ntoc() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.331 sec elapsed\n\n\n\nTo understand what’s going on in this system, I’ll go through the same exercise I did with the Barnsley fern. I’ll generate the data for a piece of art by calling unboxer_base(), and then plot it three ways. First I’ll show it as a pure black and white image to show the overall configuration of points, then I’ll break it down based on the components. Because each transformation function is defined in terms the affine component and the variant component, I’ll show two different versions of this. First, here’s the data:\n\ndat &lt;- unboxer_base(100000, layers = 2, seed = 999) |&gt; \n  as.data.frame() |&gt; \n  as_tibble()\n\nnames(dat) &lt;- c(\"x\", \"y\", \"c\", \"affine_layer\", \"variant_function\")\n\ndat &lt;- dat |&gt; \n  slice_tail(n = -1) |&gt; # remove initialisation point\n  mutate(\n    affine_layer = factor(affine_layer),\n    variant_function = factor(variant_function)\n  ) \n\ndat\n\n# A tibble: 100,000 × 5\n        x       y     c affine_layer variant_function\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;        &lt;fct&gt;           \n 1 -0.710 -0.979  0.577 1            2               \n 2  0.797 -0.211  1.32  1            1               \n 3 -1.71  -0.630  0.788 1            3               \n 4  0.323  0.554  0.614 2            1               \n 5 -0.116 -1.06   0.750 2            3               \n 6  0.549  0.0594 1.45  1            1               \n 7 -0.797 -0.334  0.651 1            2               \n 8 -0.636 -0.998  0.271 1            2               \n 9 -0.742 -0.978  0.368 1            2               \n10 -0.723 -0.963  0.398 1            2               \n# ℹ 99,990 more rows\n\n\nNow the plots:\nggplot(dat, aes(x, y)) +\n  geom_point(size = 1, stroke = 0, show.legend = FALSE) + \n  theme_void() + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) + \n  theme(panel.background = element_rect(fill = \"grey90\"))\nggplot(dat, aes(x, y, colour = variant_function)) +\n  geom_point(size = 1, stroke = 0) + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) +\n  scale_colour_brewer(palette = \"Set2\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 5))) +\n  theme_void() + \n  theme(\n    panel.background = element_rect(fill = \"grey90\"),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.2, 0.1)\n  )\nggplot(dat, aes(x, y, colour = affine_layer)) +\n  geom_point(size = 1, stroke = 0) + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) +\n  scale_colour_brewer(palette = \"Set1\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 5))) +\n  theme_void() + \n  theme(\n    panel.background = element_rect(fill = \"grey90\"),\n    legend.position = \"inside\",\n    legend.position.inside = c(0.2, 0.1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis gives you a sense of what’s going on here: in the middle panel you can see that the two “sinusoidal” components have the effect of creating the boxes, because sin(x) is constrained to lie between -1 and 1. The snaky, wavy patterns that you see in some the outputs are also related to these components, but I haven’t plotted the data in a way that makes this obvious.\nIn contrast, on the right you can see the effect of the affine transformations. Notice that the blue pattern kind of looks like a “squashed and rotated” version of the red pattern? That’s exactly what the affine transforms do. They create these distortions."
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#faster-chaos-with-rcpp",
    "href": "posts/2024-12-22_art-from-code-5/index.html#faster-chaos-with-rcpp",
    "title": "Art from code V: Iterated function systems",
    "section": "Faster chaos with Rcpp",
    "text": "Faster chaos with Rcpp\nWaiting 30 seconds (or whatever) for something pretty is kind of annoying, especially when you’re still developing the system and you just want to tinker with the settings to see what it does. It would be nice if we could speed this up, right? The easiest way to speed things up is to run fewer iterations and use larger plot sizes. I mean, this works perfectly fine…\ntic()\nunboxer_base(10000, layers = 2, seed = 999) |&gt; unbox_art(seed = 2, size = 1)\nunboxer_base(10000, layers = 5, seed = 333) |&gt; unbox_art(seed = 2, size = 1) \nunboxer_base(10000, layers = 9, seed = 420) |&gt; unbox_art(seed = 2, size = 1)\ntoc() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.61 sec elapsed\n\n\n\nIf you’re okay with a coarser grained output (which honestly does have a certain aesthetic appeal), or simply don’t want to mess around with C++ code, your problems are solved! Read no further!\nIf speed is a consideration – especially if the rendering times are interfering with the creative process – one possibility would be to write the slow parts of your code in C++, and then call it from R using the Rcpp package. To be honest, I’m not the best C++ coder myself and am only moderately comfortable with Rcpp, so I’m not going to attempt a tutorial here. Instead, what I’ll do is mention that rcpp.org has some excellent resources, and Advanced R also has a good chapter on Rewriting R code in C++ that you may find helpful. I’ll also show you what I did for this system, because sometimes it’s helpful to see C++ code that implements the same functions as the original R code. Let’s imagine I have a file called unbox-fast.cpp that includes the following:\n\n#include &lt;Rcpp.h&gt;\n#include &lt;iostream&gt;\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_rcpp(int iterations, int layers) {\n  \n  // variables\n  NumericMatrix pts(iterations, 3); \n  NumericMatrix cff(9, layers);\n  int r, f;\n  double x, y, z, s;\n  \n  // coefficients\n  for(int i = 0; i &lt; 9; i++) {\n    for(int j = 0; j &lt; layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // initial point\n  pts(0, 0) = R::runif(-1, 1);\n  pts(0, 1) = R::runif(-1, 1);\n  pts(0, 2) = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t &lt; iterations; t++) {\n    r = rand() % layers; // which transform to use?\n    f = rand() % 3;      // which function to use?\n    \n    // apply transformation\n    x = cff(0, r) * pts(t-1, 0) + cff(1, r) * pts(t-1, 1) + cff(2, r);\n    y = cff(3, r) * pts(t-1, 0) + cff(4, r) * pts(t-1, 1) + cff(5, r);\n    z = cff(6, r) * pts(t-1, 0) + cff(7, r) * pts(t-1, 1) + cff(8, r);\n    \n    // apply function\n    if(f == 0) {\n      s = pow(x*x + y*y + z*z, 1/3);\n      x = x + s;\n      y = y + s;\n      z = z + s;\n    } else if(f == 1) {\n      x = sin(x);\n      y = sin(y);\n      z = sin(z);\n    } else {\n      x = 2 * sin(x);\n      y = 2 * sin(y);\n      z = 2 * sin(z);\n    }\n    \n    // store new point\n    pts(t, 0) = x;\n    pts(t, 1) = y;\n    pts(t, 2) = (z + pts(t-1, 2))/2;\n  }\n  return pts;\n}\n\nWhen sourced from R in the “right” way, this will create a function unboxer_rcpp() that I can call from R. And when I say “sourced” from R what I really mean is if I did this:\n\nRcpp::sourceCpp(file = \"unbox-fast.cpp\")\n\nIf you’ve used Rcpp, this should seem familiar.\nIf you haven’t used Rcpp and are trying to make up your mind if it is worth the effort to learn, well, I’ll offer this comparison. Here’s the difference in speed for generating a hundred thousand data points in the original system unbox_base(), compared to the C++ implementation unbox_rcpp():\n\ntic(); set.seed(999); dat &lt;- unboxer_base(100000, layers = 2); toc()\ntic(); set.seed(999); dat &lt;- unboxer_rcpp(100000, layers = 2); toc() \n\n1.454 sec elapsed\n0.004 sec elapsed\n\n\nNot too bad, really :)\nWhen written in C++ we can generate 10 million data points extremely quickly. So much so that it’s outrageously fast to do it three times with different seeds and different numbers of layers:\n\ntic()\nset.seed(123); dat1 &lt;- unboxer_rcpp(10000000, layers = 2) \nset.seed(101); dat2 &lt;- unboxer_rcpp(10000000, layers = 5) \nset.seed(420); dat3 &lt;- unboxer_rcpp(10000000, layers = 9) \ntoc()\n\n1.566 sec elapsed\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe C++ code for this system is included in the unbox-fast.cpp script, and the code calling it from R to test the timing is included as the unbox-fast-test.R script.\n\n\nTransforming the data into plots, on the other hand, is a little slower. At this point the rendering code is the part that is causing slowness:\ntic()\ndat1 |&gt; unbox_art(seed = 123, size = .2)\ndat2 |&gt; unbox_art(seed = 101, size = .2)\ndat3 |&gt; unbox_art(seed = 420, size = .2)\ntoc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n203.938 sec elapsed\n\n\n\nOkay, so the data generation is fast now but producing the plots is still painfully slow. That’s a little unfortunate. Perhaps we can speed that up too? After all, ggplot2 has a lot of bells and whistles that we aren’t using in this plot. Maybe we can sidestep the issue…"
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#even-faster-chaos-with-raster-representation",
    "href": "posts/2024-12-22_art-from-code-5/index.html#even-faster-chaos-with-raster-representation",
    "title": "Art from code V: Iterated function systems",
    "section": "Even faster chaos with raster representation",
    "text": "Even faster chaos with raster representation\nBack in the era when I held an academic appointment, one of my research topics used to be human mental representation. When people have to make judgments, choices, or reason about something unfamiliar, we rely on our knowledge of the world to guide us. We have rich, structured knowledge from our past experience that we can bring to bear on new situations, which is super useful because in addition to being fabulous and insanely complicated things, neurons are slow and squishy things relative to machines. Honestly it’s a bit of a surprise that we can compute anything with these things, and borderline miraculous that we manage to think clever thoughts using them.\nAll of this is in service of a really basic comment: if your computing machine doesn’t store data in a sensible format, you’re going to find it really hard to do anything useful. But the converse is also true… if you represent information in the right way, you’ll be able to accomplish a lot. Over and over again, across a lot of different problems I used to study, I’d see a consistent pattern: people make sensible choices when we’re given information structured in the “right” way. But if you present the same information a different and counterintuitive way, people don’t know what to do with it and they make extremely poor choices. As a psychological researcher, it’s really easy to design studies that make people look stupid, and equally easy to design studies that make people look smart. Looking back, it strikes me that it’s almost criminally easy to “rig” the results of a study this way.\nAnyway.\nMy point here is that machines are kind of the same. If you want your image rendering to go faster, well, maybe you should store the data in a format that mirrors the output you want? I mean, at this point we’re storing a data frame with 10 millions coordinates, and then plotting circles in an abstract canvas that ggplot2 constructs with the help of the grid graphics system, and then… aren’t you tired already?\nIf you want a bitmap that stores pixel values at the end of your generative process, why not start with the data in exactly the same format at the beginning? Don’t draw circles-as-polygons-around-a-coordinate. Just store the damned pixel values from the outset.\nOkay, so here’s a slight reimagining of our Rcpp function that does exactly that. We store a matrix representing the bitmap from the very beginning. The output of this unboxer_grid() function is a square matrix with the number of rows and columns determined by the pixels input:\n\n#include &lt;Rcpp.h&gt;\n#include &lt;iostream&gt;\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_grid(int iterations, \n                           int layers,\n                           int pixels, \n                           double border) {\n  \n  // variables\n  NumericMatrix image(pixels, pixels); \n  NumericMatrix cff(9, layers);\n  int r, c, f, x_ind, y_ind;\n  double x, y, z, s;\n  \n  // set image matrix to zeros\n  for(int r = 0; r &lt; pixels; r++) {\n    for(int c = 0; c &lt; pixels; c++) {\n      image(c, r) = 0;\n    }\n  }\n  \n  // coefficients\n  for(int i = 0; i &lt; 9; i++) {\n    for(int j = 0; j &lt; layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // values for initial state\n  double x_old = R::runif(-1, 1);\n  double y_old = R::runif(-1, 1);\n  double z_old = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t &lt; iterations; t++) {\n    r = rand() % layers; // which transform to use?\n    f = rand() % 3;      // which function to use?\n    \n    // apply transformation\n    x = cff(0, r) * x_old + cff(1, r) * y_old + cff(2, r);\n    y = cff(3, r) * x_old + cff(4, r) * y_old + cff(5, r);\n    z = cff(6, r) * x_old + cff(7, r) * y_old + cff(8, r);\n    \n    // apply function\n    if(f == 0) {\n      s = pow(x*x + y*y + z*z, 1/3);\n      x = x + s;\n      y = y + s;\n      z = abs(z + s);\n    } else if(f == 1) {\n      x = sin(x);\n      y = sin(y);\n      z = sin(z) + 1;\n    } else {\n      x = 2 * sin(x);\n      y = 2 * sin(y);\n      z = 2 * (sin(z) + 1);\n    }\n    \n    // compute indices to be updated\n    x_ind = int (x * pixels / (2 * border)) + pixels / 2;\n    y_ind = int (y * pixels / (2 * border)) + pixels / 2;\n    \n    // store results if they fall within the range\n    if(x_ind &gt;= 0 & x_ind &lt; pixels) {\n      if(y_ind &gt;= 0 & y_ind &lt; pixels) {\n        image(x_ind, y_ind) = z;\n      }\n    }\n    \n    // move new to old\n    x_old = x;\n    y_old = y;\n    z_old = (z + z_old) / 2; \n  }\n  return image;\n}\n\nFrom a data generation perspective, there’s really not much difference between this version and the last one. They’re both fast. The C++ code to generate the image in a bitmap format isn’t faster or slower than the C++ code we wrote last time:\n\ntic()\nset.seed(123); mat1 &lt;- unboxer_grid(10000000, 2, 1000, 4) \nset.seed(101); mat2 &lt;- unboxer_grid(10000000, 5, 1000, 4) \nset.seed(420); mat3 &lt;- unboxer_grid(10000000, 9, 1000, 4)\ntoc()\n\n1.186 sec elapsed\n\n\nAh, but now look what happens when we generate an image from the data. Originally we were working with ggplot2, and we were forcing it to convert a large data frame to an image in a very very painful way. This time around, the data is already in the right format. It’s a bitmap that we can pass to image(). No heavy lifting required!\nraster_art &lt;- function(mat, seed = NULL, trim = .001) {\n  \n  zlim &lt;- quantile(mat, c(trim, 1 - trim))\n  mat[mat &lt; zlim[1]] &lt;- zlim[1]\n  mat[mat &gt; zlim[2]] &lt;- zlim[2]\n  \n  op &lt;- par(mar = c(0, 0, 0, 0))\n  image(\n    z = mat, \n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = sample_canva2(seed, n = 256)\n  )\n  par(op)\n}\n\ntic()\nraster_art(mat1, seed = 123)\nraster_art(mat2, seed = 101)\nraster_art(mat3, seed = 420)\ntoc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.983 sec elapsed\n\n\n\nOkay fine, this new version doesn’t handle shading in precisely the same way the original version did, but it’s still very pretty – and it’s soooooo much faster!\nHow fast is it? Fast enough that I’m perfectly willing to generate an image by playing the chaos game for 100 million iterations. Hell, it’s fast enough that I’ll generate six of them:\npretty_boxes &lt;- function(\n    seed,\n    iterations = 100000000, \n    layers = 5, \n    pixels = 4000, \n    background = \"black\",\n    border = 4,\n    trim = .001\n) {\n  \n  set.seed(seed)\n  \n  mat &lt;- unboxer_grid(\n    iterations = iterations, \n    layers = layers, \n    pixels = pixels, \n    border = border\n  )\n  \n  shades &lt;- c(background, sample_canva2(seed, n = 1023))\n  \n  zlim &lt;- quantile(mat, c(trim, 1 - trim))\n  mat[mat &lt; zlim[1]] &lt;- zlim[1]\n  mat[mat &gt; zlim[2]] &lt;- zlim[2]\n  \n  op &lt;- par(mar = c(0, 0, 0, 0))\n  image(\n    z = mat, \n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = shades\n  )\n  par(op)\n}\n\ntic()\npretty_boxes(286)\npretty_boxes(380)\npretty_boxes(100)\ntoc()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n34.924 sec elapsed\n\n\n\ntic()\npretty_boxes(222)\npretty_boxes(567)\npretty_boxes(890)\ntoc() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n39.075 sec elapsed\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nThe C++ code to generate the data for this system is included in the unbox-grid.cpp script, and plotting code is in the pretty-boxes.R script."
  },
  {
    "objectID": "posts/2024-12-22_art-from-code-5/index.html#materials",
    "href": "posts/2024-12-22_art-from-code-5/index.html#materials",
    "title": "Art from code V: Iterated function systems",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\nbarnsley-fern.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\n\nfern_transform &lt;- function(coord, ind) {\n  \n  # coefficients for the stem function f_1\n  if(ind == 1) {\n    mat &lt;- matrix(c(0, 0, 0, .16), 2, 2) # matrix to multiply\n    off &lt;- c(0, 0)                       # offset vector to add\n  }\n  \n  # coefficients for the small leaflet function f_2\n  if(ind == 2) {\n    mat &lt;- matrix(c(.85, -.04, .04, .85), 2, 2)\n    off &lt;- c(0, 1.6)                      \n  }\n  # coefficients for the right-side function f_3\n  if(ind == 3) {\n    mat &lt;- matrix(c(.2, .23, -.26, .22), 2, 2)\n    off &lt;- c(0, 1.6)                      \n  }\n  \n  # coefficients for the left-side function f_4\n  if(ind == 4) {\n    mat &lt;- matrix(c(-.15, .26, .28, .24), 2, 2)\n    off &lt;- c(0, .44)                     \n  }\n  \n  # return the affine transformed coords\n  coord &lt;- mat %*% coord + off\n  return(coord)\n}\n\nfern_chaos &lt;- function(iterations = 10000, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  # which transformation to apply at each iteration\n  transform_index &lt;- sample(\n    x = 1:4, \n    size = iterations, \n    replace= TRUE, \n    prob = c(.01, .85, .07, .07)\n  )\n  \n  # initialise chaos game at the origin\n  start &lt;- matrix(c(0, 0))\n  \n  # helper function to collapse accumulated output\n  bind_to_column_matrix &lt;- function(lst) {\n    do.call(cbind, lst)\n  }\n  \n  # iterate until done!\n  coord_matrix &lt;- transform_index |&gt;\n    accumulate(fern_transform, .init = start) |&gt;\n    bind_to_column_matrix() \n  \n  # tidy the output, add extra columns, and return\n  coord_df &lt;- t(coord_matrix) |&gt; \n    as.data.frame() \n  names(coord_df) &lt;- c(\"x\", \"y\")\n  coord_df &lt;- coord_df |&gt;\n    as_tibble() |&gt;\n    mutate(\n      transform = c(0, transform_index),\n      iteration = row_number() - 1\n    )\n  return(coord_df)\n}\n\nfern_dat &lt;- fern_chaos(seed = 1)\n\npic &lt;- ggplot(fern_dat, aes(x, y, colour = factor(transform))) +\n  geom_point(size = 1, stroke = 0) +\n  coord_equal() +\n  theme_void() + \n  guides(colour = guide_legend(\n    title = \"transformation\", \n    override.aes = list(size = 5))\n  )\n\nplot(pic)\n\n\n\n\n\n\n\n\n\n\npretty-boxes.R\n\n\n\n\n\n\nlibrary(tictoc)\nlibrary(Rcpp)\nlibrary(here)\n\nsourceCpp(file = here(\"materials\", \"unbox-grid.cpp\"))\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\npretty_boxes &lt;- function(\n    seed,\n    iterations = 100000000, \n    layers = 5, \n    pixels = 4000, \n    background = \"black\",\n    border = 4,\n    trim = .001\n) {\n  \n  set.seed(seed)\n  \n  mat &lt;- unboxer_grid(\n    iterations = iterations, \n    layers = layers, \n    pixels = pixels, \n    border = border\n  )\n  \n  shades &lt;- c(background, sample_canva2(seed, n = 1023))\n  \n  zlim &lt;- quantile(mat, c(trim, 1 - trim))\n  mat[mat &lt; zlim[1]] &lt;- zlim[1]\n  mat[mat &gt; zlim[2]] &lt;- zlim[2]\n  \n  op &lt;- par(mar = c(0, 0, 0, 0))\n  image(\n    z = mat, \n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = shades\n  )\n  par(op)\n}\n\ntic()\npretty_boxes(100, iterations = 10000000)\ntoc()\n\n\n\n\n\n\n\n\n\n\nunbox-base.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tictoc)\nlibrary(ggthemes)\nlibrary(here)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nfuns &lt;- list(\n  function(point) point + (sum(point ^ 2)) ^ (1/3),\n  function(point) sin(point),\n  function(point) 2 * sin(point)\n)\n\nunboxer_base &lt;- function(iterations, layers, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  # coefficients defining affine layer transforms, A_i\n  coeffs &lt;- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  # list of variant functions, g_j\n  funs &lt;- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  # updater function: apply the layer, then the function\n  # (the weirdness with point[3] is me treating colour as special)\n  update &lt;- function(point, layer, transform) {\n    f &lt;- funs[[transform]]\n    z &lt;- point[3]\n    point[3] &lt;- 1\n    point &lt;- f(point %*% coeffs[,,layer])\n    point[3] &lt;- (point[3] + z)/2\n    return(point)\n  }\n  \n  # initial point\n  point0 &lt;- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  # sample points\n  layer_ind &lt;- sample(layers, iterations, replace = TRUE)  \n  trans_ind &lt;- sample(length(funs), iterations, replace = TRUE)  \n  points &lt;- accumulate2(layer_ind, trans_ind, update, .init = point0)\n  \n  # tidy up, add columns, and return\n  points &lt;- matrix(unlist(points), ncol = 3, byrow = TRUE)\n  points &lt;- cbind(\n    points,\n    c(0, layer_ind),\n    c(0, trans_ind)\n  )\n  return(points)\n}\n\nunbox_art &lt;- function(data, seed = NULL, size = 1) {\n  \n  # convert to data frame and sample a palette\n  data &lt;- data |&gt; as.data.frame() |&gt; as_tibble()\n  names(data) &lt;- c(\"x\", \"y\", \"c\", \"l\", \"t\")[1:ncol(data)]\n  shades &lt;- sample_canva2(seed)\n  \n  # render image as a scatter plot\n  ggplot(data, aes(x, y, colour = c)) +\n    geom_point(\n      size = size,\n      stroke = 0,\n      show.legend = FALSE\n    ) + \n    theme_void() + \n    coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) + \n    scale_colour_gradientn(colours = shades) + \n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    theme(panel.background = element_rect(\n      fill = shades[1], colour = shades[1]\n    ))\n}\n\ntic()\n\nseed &lt;- 1234\nlayers &lt;- 5\n\npic &lt;- unboxer_base(1000000, layers = layers, seed = seed) |&gt; \n  unbox_art(seed = seed, size = .2) \nfname &lt;- paste0(\"unboxer-base-\", layers, \"-\", seed, \".png\")\n\nggsave(\n  filename = here(\"output\", fname), \n  plot = pic,\n  width = 2000,\n  height = 2000,\n  units = \"px\",\n  dpi = 300\n)\n\ntoc()\n\n\n\n\n\n\n\n\n\n\nunbox-fast-test.R\n\n\n\n\n\n\nlibrary(tictoc)\nlibrary(Rcpp)\nlibrary(here)\n\nsourceCpp(file = here(\"materials\", \"unbox-fast.cpp\"))\n\ntic() \nset.seed(999)\ndat &lt;- unboxer_rcpp(1000000, layers = 2)\ntoc() \n\n\n\n\n\n\n\n\n\n\nunbox-fast.cpp\n\n\n\n\n\n\n#include &lt;Rcpp.h&gt;\n#include &lt;iostream&gt;\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_rcpp(int iterations, int layers) {\n  \n  // variables\n  NumericMatrix pts(iterations, 3); \n  NumericMatrix cff(9, layers);\n  int r, f;\n  double x, y, z, s;\n  \n  // coefficients\n  for(int i = 0; i &lt; 9; i++) {\n    for(int j = 0; j &lt; layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // initial point\n  pts(0, 0) = R::runif(-1, 1);\n  pts(0, 1) = R::runif(-1, 1);\n  pts(0, 2) = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t &lt; iterations; t++) {\n    r = rand() % layers; // which transform to use?\n      f = rand() % 3;      // which function to use?\n        \n        // apply transformation\n      x = cff(0, r) * pts(t-1, 0) + cff(1, r) * pts(t-1, 1) + cff(2, r);\n      y = cff(3, r) * pts(t-1, 0) + cff(4, r) * pts(t-1, 1) + cff(5, r);\n      z = cff(6, r) * pts(t-1, 0) + cff(7, r) * pts(t-1, 1) + cff(8, r);\n      \n      // apply function\n      if(f == 0) {\n        s = pow(x*x + y*y + z*z, 1/3);\n        x = x + s;\n        y = y + s;\n        z = z + s;\n      } else if(f == 1) {\n        x = sin(x);\n        y = sin(y);\n        z = sin(z);\n      } else {\n        x = 2 * sin(x);\n        y = 2 * sin(y);\n        z = 2 * sin(z);\n      }\n      \n      // store new point\n      pts(t, 0) = x;\n      pts(t, 1) = y;\n      pts(t, 2) = (z + pts(t-1, 2))/2;\n  }\n  return pts;\n}\n\n\n\n\n\n\n\n\n\n\nunbox-grid.cpp\n\n\n\n\n\n\n#include &lt;Rcpp.h&gt;\n#include &lt;iostream&gt;\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_grid(int iterations, \n                           int layers,\n                           int pixels, \n                           double border) {\n  \n  // variables\n  NumericMatrix image(pixels, pixels); \n  NumericMatrix cff(9, layers);\n  int r, c, f, x_ind, y_ind;\n  double x, y, z, s;\n  \n  // set image matrix to zeros\n  for(int r = 0; r &lt; pixels; r++) {\n    for(int c = 0; c &lt; pixels; c++) {\n      image(c, r) = 0;\n    }\n  }\n  \n  // coefficients\n  for(int i = 0; i &lt; 9; i++) {\n    for(int j = 0; j &lt; layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // values for initial state\n  double x_old = R::runif(-1, 1);\n  double y_old = R::runif(-1, 1);\n  double z_old = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t &lt; iterations; t++) {\n    r = rand() % layers; // which transform to use?\n    f = rand() % 3;      // which function to use?\n    \n    // apply transformation\n    x = cff(0, r) * x_old + cff(1, r) * y_old + cff(2, r);\n    y = cff(3, r) * x_old + cff(4, r) * y_old + cff(5, r);\n    z = cff(6, r) * x_old + cff(7, r) * y_old + cff(8, r);\n    \n    // apply function\n    if(f == 0) {\n      s = pow(x*x + y*y + z*z, 1/3);\n      x = x + s;\n      y = y + s;\n      z = abs(z + s);\n    } else if(f == 1) {\n      x = sin(x);\n      y = sin(y);\n      z = sin(z) + 1;\n    } else {\n      x = 2 * sin(x);\n      y = 2 * sin(y);\n      z = 2 * (sin(z) + 1);\n    }\n    \n    // compute indices to be updated\n    x_ind = int (x * pixels / (2 * border)) + pixels / 2;\n    y_ind = int (y * pixels / (2 * border)) + pixels / 2;\n    \n    // store results if they fall within the range\n    if(x_ind &gt;= 0 & x_ind &lt; pixels) {\n      if(y_ind &gt;= 0 & y_ind &lt; pixels) {\n        image(x_ind, y_ind) = z;\n      }\n    }\n    \n    // move new to old\n    x_old = x;\n    y_old = y;\n    z_old = (z + z_old) / 2; \n  }\n  return image;\n}"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html",
    "title": "Binding Apache Arrow to R",
    "section": "",
    "text": "So I have a new job.\nIn my previous job as an academic, a large part of my work – my favourite part, if I’m honest – involved creating open access resources to help people use modern open source tools for data analysis. In my totally different role in developer relations at Voltron Data, a large part of my work involves, um … [checks job description] … creating open access resources to help people use modern open source tools for data analysis. Well okay then!\nI’d better get on that, I suppose?\nYes I have been binge watching The Magicians lately. My preemptive apologies to everyone for the gif spam. This is Eliot Waugh. All I can say at this point is that thanks to his magnificent performance I have developed a terribly awkward crush on Hale Appleman. Image via giphy, copyright syfy\nI’ve been in my current role for a little over a week (or had been when I started writing this post!), and today my first contribution to Apache Arrow was merged. It was very modest contribution: I wrote some code that determines whether any given year is a leap year. It precisely mirrors the behaviour of the leap_year() function in the lubridate package, except that it can be applied to Arrow data and it will behave itself when used in the context of a dplyr pipeline (more on that later). The code itself is not complicated, but it relies on a little magic and a deeper understanding of Arrow than I possessed two weeks ago.\nThis post is the story of how I learned Arrow magic. ✨ 🏹 ✨"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#why-am-i-writing-this",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#why-am-i-writing-this",
    "title": "Binding Apache Arrow to R",
    "section": "Why am I writing this?",
    "text": "Why am I writing this?\n\n\nThe danger of sublimated trauma is a major theme in our story      – The Great God Ember (The Magicians: Season 2, Episode 3)\n\nIt might seem peculiar that I’m writing such a long post about such a tiny contribution to an open source project. After all, it doesn’t actually take a lot of work to figure out how to detect leap years. You can do it in one line of R code:\n\n(year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n\nThis is a logical expression corresponding to the following rules. If the year is divisible by 4 then it is a leap year (e.g., 1996 was a leap year). But there’s an exception: if year is divisible by 100 then it isn’t a leap year (e.g., 1900 wasn’t a leap year). But there’s also an exception to the exception: if year is divisible by 400 then it is a leap year (e.g., 2000 was a leap year). Yes, the process of mapping the verbally stated rules onto a logical expression is kind of annoying, but it’s not conceptually difficult or unusual. There is no magic in leap year calculation, no mystery that needs unpacking and explaining.\n\n\nAll this assumes years are counted using the Gregorian calendar. There are, of course, other calendars\nThe magic comes in when you start thinking about what the arrow package actually does. It lets you write perfectly ordinary R code for data manipulation that returns perfectly ordinary R data structures, even though the data have never been loaded into R and all the computation is performed externally using Apache Arrow. The code you write with arrow looks and feels like regular R code, but almost none of the work is being done by R. This is deep magic, and it is this magic that needs to be demystified.\n\n\n\n\n\n\nTwo key moments in “The Magicians” when Julia Wicker discovers she can do magic, defying the expectations of others. One moment occurs at the start of Season 1 as a novice, after she had been told she failed the magic exams at Brakebills University; another moment occurs at the end of Season 2 after all magic has supposedly been turned off by the Old Gods or something. The parallel between the two moments is striking. Oh and Quentin Coldwater is in both scenes too I guess. Whatevs. Image via giphy, copyright syfy\n\n\n\n\n\nI have three reasons for wanting to unpack and explain this magic.\n\nThe first reason is personal: I’ve been a professional educator for over 15 years and it has become habit. The moment I learn a new thing my first impulse is to work out how to explain it to someone else.\nThe second reasons is professional: I work for Voltron Data now, and part of my job is to make an educational contribution to the open source Apache Arrow project. Arrow is a pretty cool project, but there’s very little value in magnificent software if you don’t help people learn how to take advantage of it!\nThe third reason is ethical: a readable tutorial/explanation lowers barriers to entry. I mean, let’s be honest: the only reason I was able to work up the courage to contribute to Apache Arrow is that I work for a company that is deeply invested in open source software and in the Arrow project specifically. I had colleagues and friends I could ask for advice. If I failed I knew they would be there to help me. I had a safety net.\n\nThe last of these is huuuuuuugely important from a community point of view. Not everyone has the safety net that I have, and it makes a big difference. In a former life I’ve been on the other side of this divide: I’ve been the person with no support, nobody to ask for help, and I’ve run afoul of capricious gatekeeping in the open source world. It is a deeply unpleasant experience, and one I would not wish upon anyone else. We lose good people when this happens, and I really don’t want that!\nThe quote from the beginning of this section, the one about the danger of sublimated trauma, is relevant here: if we want healthy user communities it is our obligation on the inside to provide safe environments and delightful experiences. Our job is to find and remove barriers to entry. We want to provide that “safety net” that ensures that even if you fall (because we all fall sometimes), you don’t get hurt. Failing safely at something can be a learning experience; suffering trauma, however, is almost never healthy. So yeah, this matters to me. I want to take what I’ve learned now that I’m on the inside and make that knowledge more widely accessible.\n\n\nEveryone deserves a safety net when first learning to walk the tightropes. It’s not a luxury, it’s a necessity\nBefore diving in, I should say something about the “assumed knowledge” for this post.\n\nI’ll do my best to explain R concepts as I go, but the post does assume that the reader is comfortable in R and knows how to use dplyr for data manipulation. If you need a refresher on these topics, I cannot recommend “R for data science” highly enough. It’s a fabulous resource!\nOn the Arrow side it would help a little if you have some vague idea of what Arrow is about. I will of course explain as I go, but if you’re looking for a post that starts at the very beginning, I wrote a post on “Getting started with Apache Arrow” that does exactly this and discusses a lot of the basics.\nFinally, a tiny R warning: later in the post I will do a little excursion into object oriented programming and metaprogramming in R, which will be familiar to some but not all readers. If you’re not comfortable with these topics, you should still be okay to skim those sections and still get the important parts of this post. It’s not essential to understand the main ideas.\n\n\n\n\n\n\n\nThe Great God Ember. Capricious, chaotic, and utterly unreliable unless what you’re looking for is a whimsical death. Pretty much the opposite of what we’d hope for in a healthy open source community really! He is, however, a very entertaining character. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#what-is-arrow",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#what-is-arrow",
    "title": "Binding Apache Arrow to R",
    "section": "What is Arrow?",
    "text": "What is Arrow?\n\nIn case you decided not to read the introductory “Getting started with Apache Arrow” post, here’s an extremely condensed version. Apache Arrow is a standard and open source library that represents tabular data efficiently in memory. More generally it refers to a collection of tools used to work with Arrow data. There are libraries supporting Arrow in many different programming languages, including C, C++, C#, Go, Java, JavaScript, Julia, MATLAB, Python, R, Ruby, and Rust. It’s pretty cool."
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-in-r",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-in-r",
    "title": "Binding Apache Arrow to R",
    "section": "Using Arrow in R",
    "text": "Using Arrow in R\n\nA fundamental thing to understand about the arrow package in R is that it doesn’t implement the Apache Arrow standard directly. In fact, it tries very hard not to do any of the heavy lifting itself. There’s a C++ library that does that in a super efficient way, and the job of the R package is to supply bindings that allow the R user to interact with that library using a familiar interface. The C++ library is called libarrow. Although the long term goal is to make the integration so seamless that you can use the arrow R package without ever needing to understand the C++ library, my experience has been that most people want to know something about what’s happening under the hood. It can be unsettling to find yourself programming with tools you don’t quite understand, so I’ll dig a little deeper in this post.\nLet’s start with the C++ library. The role of libarrow is to do all the heavy computational work. It implements all the Arrow standards for representing tabular data in memory, provides support for the Apache “Inter-Process Communication” (IPC) protocol that lets you efficiently transfer data from one application to another, and supplies various compute kernels that allow you to do some data wrangling when your data are represented as an Arrow table.1 It is, fundamentally, the engine that makes everything work.\nWhat about the R package? The role of arrow is to expose the functionality of libarrow to the R user, to make that functionality feel “natural” in R, and to make it easier for R users to write Arrow code that is smoothly interoperable with Arrow code written in other languages (e.g., Python). In order to give you the flexibility you need, the arrow package allows you to interact with libarrow at three different levels of abstraction:\n\nThere’s a heavily abstracted interface that uses the dplyr bindings supplied by arrow. This version strives to make libarrow almost completely invisible, hidden behind an interface that uses familiar R function names.\nThere’s a lightly abstracted interface you can access using the arrow_*() functions. This version exposes the libarrow functions without attempting to exactly mirror any particular R functions, and provides a little syntactic sugar to make your life easier.\nFinally, there’s a minimally abstracted interface using call_function(). This version provides a bare bones interface, without any of the syntactic sugar.\n\nOver the next few sections section I’ll talk about these three levels of abstraction. So let’s load the packages we’re going to need for this post and dive right in!\n\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(arrow)\n\n\n\n\n\n\n\nPenny Adiyodi in the Neitherlands, diving head first into a fountain that transports him to a new and magical world. I cannot stress enough that Penny does not, by and large, make good choices. Impulse control is a virtue, but not one that he possesses in abundance. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-dplyr-bindings",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-dplyr-bindings",
    "title": "Binding Apache Arrow to R",
    "section": "Using “arrowplyr” bindings",
    "text": "Using “arrowplyr” bindings\n\n\nI think I have some special fish magicks.     – Josh Hoberman (The Magicians: Season 4, Episode 13)\n\nWhen I wrote my Getting started with Apache Arrow post, I concluded with an illustration of how you can write dplyr code that will work smoothly in R even when the data themselves are stored in Arrow. Here’s a little recap of how that works, using a tiny data set I pulled from The Magicians Wikipedia page. Here’s what that data set looks like:\n\nmagicians &lt;- read_csv_arrow(\"magicians.csv\", as_data_frame = TRUE)\nmagicians\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# ℹ 55 more rows\n\n\nIn the code above I used the read_csv_arrow() function from the arrow package. If you’ve used the read_csv() function from readr this will seem very familiar: although Arrow C++ code is doing a lot of the work under the hood, the Arrow options have been chosen to mirror the familiar readr interface. The as_data_frame argument is specific to arrow though: when it is TRUE the data are imported into R as a data frame or tibble, and when it is FALSE the data are imported into Arrow. Strictly speaking I didn’t need to specify it in this example because TRUE is the default value. I ony included here so that I could draw attention to it.\nOkay, now that we have the data let’s start with a fairly typical data analysis process: computing summary variables. Perhaps I want to know the average popularity and ratings for each season of The Magicians, and extract the year in which the season aired. The dplyr package provides me with the tools I need to do this, using functions like mutate() to create new variables, group_by() to specify grouping variables, and summarise() to aggregate data within group:\n\nmagicians %&gt;% \n  mutate(year = year(air_date)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\n# A tibble: 5 × 4\n  season viewers rating  year\n   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\n\nAll of these computations take place within R. The magicians data set is stored in R, and all the calculations are done using this data structure.\nWhat can we do when the data are stored in Arrow? It turns out the code is almost identical, but the first thing I’ll need to do is load the data into Arrow. The simplest way to do this is to set as_data_frame = FALSE when calling arrow_read_csv()\n\narrowmagicks &lt;- read_csv_arrow(\"magicians.csv\", as_data_frame = FALSE)\narrowmagicks\n\nTable\n65 rows x 6 columns\n$season &lt;int64&gt;\n$episode &lt;int64&gt;\n$title &lt;string&gt;\n$air_date &lt;date32[day]&gt;\n$rating &lt;double&gt;\n$viewers &lt;double&gt;\n\n\n\n\nThe “arrowmagicks” variable name is a reference to the quote at the start of the section. For a while Josh was convinced he had been gifted with special magic because he had been a fish. It made sense at the time, I guess? It’s a weird show\nWhen I do this, two things happen. First, a data set is created outside of R in memory allocated to Arrow: all of the computations will be done on that data set. Second, the arrowmagicks variable is created inside R, which consists of a pointer to the actual data along with some handy metadata.\nThe most natural way to work with this data in R is to make sure that both the arrow and dplyr packages are loaded, and then write regular dplyr code. You can do this because the arrow package supplies methods for dplyr functions, and these methods will be called whenever the input data is an Arrow Table. I’ll refer to this data analyses that use this workflow as “arrowplyr pipelines”. Here’s an example of an arrowplyr pipeline:\n\n\nI’ve chosen not to boldface the “arrowplyr” terminology. arrow is a package and dplyr is a package, but arrowplyr isn’t. It’s simply a convenient fiction\n\narrowmagicks %&gt;% \n  mutate(year = year(air_date)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  )\n\nTable (query)\nseason: int64\nviewers: double\nrating: double\nyear: int64\n\nSee $.data for the source Arrow object\n\n\nIt looks like a regular dplyr pipeline, but because the input is arrowmagicks (an Arrow Table object), the effect of this is construct a query that can be passed to libarrow to be evaluated.\nIt’s important to realise that at this point, all we have done is define a query: no computations have been performed on the Arrow data. This is a deliberate choice for efficiency purposes: on the C++ side there are a lot of performance optimisations that are only possible because libarrow has access to the entire query before any computations are performed. As a consequence of this, you need to explicitly tell Arrow when you want to pull the trigger and execute the query.\n\n\nLater in the post I’ll talk about Arrow Expressions, the tool that powers this trickery\nThere are two ways to trigger query execution, one using the compute() function and the other using collect(). These two functions behave slightly differently and are useful for different purposes. The compute() function runs the query, but leaves the resulting data inside Arrow:\n\narrowmagicks %&gt;% \n  mutate(year = year(air_date)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %&gt;% \n  compute()\n\nTable\n5 rows x 4 columns\n$season &lt;int64&gt;\n$viewers &lt;double&gt;\n$rating &lt;double&gt;\n$year &lt;int64&gt;\n\n\nThis is useful whenever you’re creating an intermediate data set that you want to reuse in Arrow later, but don’t need to use this intermediate data structure inside R. If, however, you want the output to be pulled into R so that you can do R computation with it, use the collect() function:\n\narrowmagicks %&gt;% \n  mutate(year = year(air_date)) %&gt;% \n  group_by(season) %&gt;% \n  summarise(\n    viewers = mean(viewers),\n    rating = mean(rating), \n    year = max(year)\n  ) %&gt;% \n  collect()\n\n# A tibble: 5 × 4\n  season viewers rating  year\n   &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;\n1      1   0.776  0.308  2016\n2      2   0.788  0.323  2017\n3      3   0.696  0.269  2018\n4      4   0.541  0.2    2019\n5      5   0.353  0.111  2020\n\n\nThe nice thing for R users is that all of this feels like regular R code. Under the hood libarrow is doing all the serious computation, but at the R level the user really doesn’t need to worry too much about that. The arrowplyr toolkit works seamlessly and invisibly.\nIn our ideal world, the arrowplyr interface is all you would ever need to use. Internally, the arrow package would intercept all the R function calls you make, and replace them with an equivalent function that performs exactly the same computation using libarrow. You the user would never need to think about what’s happening under the hood.\nReal life, however, is filled with leaky abstractions, and arrowplyr is no exception. Because it’s a huge project that is under active development, there’s a lot of functionality being introduced. As an example, the current version of the package (v6.0.1) has limited support for tidyverse packages like lubridate and stringr. It’s awesome that this functionality is coming online, but because it’s happening so quickly there are gaps. The small contribution that I made today was to fill one of those gaps: currently, you can’t refer to the leap_year() function from lubridate in an arrowplyr pipeline. Well, technically you can, but whenever arrow encounters a function it doesn’t know how to execute in Arrow it throws a warning, pulls the data into R, and completes the query using native R code. Here’s what that looks like:\n\narrowmagicks %&gt;% \n  mutate(\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %&gt;% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                    &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;\n 1      1       1 Unauthorized Magic       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 The Source of Magic      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 Consequences of Advance… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 The World in the Walls   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 Mendings, Major and Min… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 Impractical Applications 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 The Mayakovsky Circumst… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 The Strangled Heart      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 The Writing Room         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 Homecoming               2016-03-21    0.3    0.78  2016 TRUE \n# ℹ 55 more rows\n\n\n\n\nThis is a bit of an oversimplification. The “warn and pull into R” behaviour shown here is what happens when the data is a Table object. If it is a Dataset object, arrow throws an error\nAn answer has been calculated, but the warning is there to tell you that the computations weren’t performed in Arrow. Realising that it doesn’t know how to interpret leap_year(), the arrow package has tried to “fail gracefully” and pulled everything back into R. The end result of all this is that the code executes as a regular dplyr pipeline and not as an arrowplyr one. It’s not the worst possible outcome, but it still makes me sad. 😭\n\n\n\n\n\n\nQuentin from timeline 40 talking to Alice from timeline 23. Communication across incommensurate universes is difficult. In the show it requires a Tesla Flexion. In Arrow, we use dplyr bindings. Image via giphy, copyright syfy”"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-functions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-arrow-functions",
    "title": "Binding Apache Arrow to R",
    "section": "Calling “arrow-prefix” functions",
    "text": "Calling “arrow-prefix” functions\n\nOkay, let’s dig a little deeper.\nIn the last section I talked about arrowplyr, a collection of dplyr bindings provided by the arrow package. These are designed to mimic their native R equivalents as seamlessly as possible to enable you to write familiar code. Internally, there’s quite a lot going on to make this magic work. In most cases, the arrow developers – which I guess includes me now! 🎉 – have rewritten the R functions that they mimic. We’ve done this in a way that the computations rely only the C++ compute functions provided by libarrow, thereby ensuring that the data never have to enter R. The arrowplyr interface is the way you’d usually interact with Arrow in R, but there are ways in which you can access the C++ compute functions a little more more directly. There are two different ways you can call these compute functions yourself. If you’re working within an arrowplyr pipeline it is (relatively!) straightforward, and that’s what I’ll talk about in this section. However, there is also a more direct method which I’ll discuss later in the post.\nTo see what compute functions are exposed by the C++ libarrow library, you can call list_compute_functions() from R:\n\nlist_compute_functions()\n\n  [1] \"abs\"                             \"abs_checked\"                    \n  [3] \"acos\"                            \"acos_checked\"                   \n  [5] \"add\"                             \"add_checked\"                    \n  [7] \"all\"                             \"and\"                            \n  [9] \"and_kleene\"                      \"and_not\"                        \n [11] \"and_not_kleene\"                  \"any\"                            \n [13] \"approximate_median\"              \"array_filter\"                   \n [15] \"array_sort_indices\"              \"array_take\"                     \n [17] \"ascii_capitalize\"                \"ascii_center\"                   \n [19] \"ascii_is_alnum\"                  \"ascii_is_alpha\"                 \n....\n\n\nThe actual output continues for quite a while: there are currently 251 compute functions, most of which are low level functions needed to perform basic computational operations.\nLet’s imagine you’re writing dplyr code to work with datetime data in a Table object like arrowmagicks. If you were working with native R data like magicians, you can do something like this:\n\nstart_date &lt;- as.Date(\"2015-12-16\")\n\nmagicians %&gt;% \n  mutate(days = air_date - start_date)\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers days \n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                          &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;drt&gt;\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92  0 d…\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11 40 d…\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9  47 d…\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75 54 d…\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75 61 d…\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65 68 d…\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7  75 d…\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67 82 d…\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71 89 d…\n10      1      10 Homecoming                     2016-03-21    0.3    0.78 96 d…\n# ℹ 55 more rows\n\n\nHere I’ve created a new days column that counts the number of days that have elapsed between the air_date for an episode and the start_date (December 16th, 2015) when the first episode of Season 1 aired. There are a lot of data analysis situations in which you might want to do something like this, but right now you can’t actually do this using the arrow dplyr bindings because temporal arithmetic is a work in progress. In the not-too-distant future users should be able to expect code like this to work seamlessly, but right now it doesn’t. If you try it right now, you get this error:\n\n\nImproving support for date/time calculations is one of the things I’m working on\n\narrowmagicks %&gt;% \n  mutate(days = air_date - start_date) %&gt;% \n  collect()\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers days \n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                          &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;drt&gt;\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92     …\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11 3456…\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9  4060…\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75 4665…\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75 5270…\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65 5875…\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7  6480…\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67 7084…\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71 7689…\n10      1      10 Homecoming                     2016-03-21    0.3    0.78 8294…\n# ℹ 55 more rows\n\n\nRight now, there are no general purpose arithmetic operations in arrow that allow you to subtract one date from another. However, because I chose this example rather carefully to find an edge case where the R package is missing some libarrow functionality, it turns out there is actually a days_between() function in libarrow that we could use to solve this problem, and it’s not too hard to use it. If you want to call one of the libarrow functions inside your dplyr pipeline, all you have to do is add an arrow_ prefix to the function name. For example, the C++ days_between() function becomes arrow_days_between() when called within the arrow dplyr pipeline:\n\narrowmagicks %&gt;% \n  mutate(days = arrow_days_between(start_date, air_date)) %&gt;% \n  collect()\n\n# A tibble: 65 × 7\n   season episode title                          air_date   rating viewers  days\n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                          &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n 1      1       1 Unauthorized Magic             2015-12-16    0.2    0.92     0\n 2      1       2 The Source of Magic            2016-01-25    0.4    1.11    40\n 3      1       3 Consequences of Advanced Spel… 2016-02-01    0.4    0.9     47\n 4      1       4 The World in the Walls         2016-02-08    0.3    0.75    54\n 5      1       5 Mendings, Major and Minor      2016-02-15    0.3    0.75    61\n 6      1       6 Impractical Applications       2016-02-22    0.3    0.65    68\n 7      1       7 The Mayakovsky Circumstance    2016-02-29    0.3    0.7     75\n 8      1       8 The Strangled Heart            2016-03-07    0.3    0.67    82\n 9      1       9 The Writing Room               2016-03-14    0.3    0.71    89\n10      1      10 Homecoming                     2016-03-21    0.3    0.78    96\n# ℹ 55 more rows\n\n\nNotice there’s no warning message here? That’s because the computations were done in Arrow and the data have not been pulled into R."
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#a-slightly-evil-digression",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#a-slightly-evil-digression",
    "title": "Binding Apache Arrow to R",
    "section": "A slightly-evil digression",
    "text": "A slightly-evil digression\n\n\nMarina, blatantly lying:     “Hi. I’m Marina. I’m here to help.”  Josh, missing important memories:     “So you’re like some powerful, benevolent White Witch?”  Marina, comically sincere:     “Uh-huh.”      – The Magicians: Season 4, Episode 2\n\n\n\nAt this point in the show everybody except the currently-amnesic main characters knows that Marina has no interest in helping anyone except Marina. I love Marina so much\nOkay, here’s a puzzle. In the previous section I used the arrow_days_between() function in the middle of a dplyr pipe to work around a current limitation in arrow. What happens if I try to call this function in another context?\n\ntoday &lt;- as.Date(\"2022-01-18\")\n\narrow_days_between(start_date, today)\n\nError in arrow_days_between(start_date, today): could not find function \"arrow_days_between\"\n\n\nIt turns out there is no R function called arrow_days_between(). This is … surprising, to say the least. I mean, it really does look like I used this function in the last section, doesn’t it? How does this work? The answer to this requires a slightly deeper understanding of what the dplyr bindings in arrow do, and it’s kind of a two-part answer.\n\nPart one: Object oriented programming\nLet’s consider the mutate() function. dplyr defines mutate() as an S3 generic function, which allows it to display “polymorphism”: it behaves differently depending on what kind of object is passed to the generic. When you pass a data frame to mutate(), the call is “dispatched” to the mutate.arrow_dplyr_query() methods supplied by (but not exported by) dplyr. The arrow package builds on this by supplying methods that apply for Arrow objects. Specifically, there are internal functions mutate.ArrowTabular(), mutate.Dataset(), and mutate.arrow_dplyr_query() that are used to provide mutate() functionality for Arrow data sets. In other words, the “top level” dplyr functions in arrow are S3 methods, and method dispatch is the mechanism that does the work.\n\n\nPart two: Metaprogramming\nNow let’s consider the leap_year() function that my contribution focused on. Not only is this not a generic function, it’s not even a dplyr function. It’s a regular function in the lubridate package. So how is it possible for arrow to mimic the behaviour of lubridate::leap_year() without messing up lubridate? This is where the dplyr binding part comes in. Let’s imagine that I’d written an actual function called arrowish_leap_year() that performs leap year calculations for Arrow data. If I’d done this inside the arrow package2 then I’d include a line like this to register a binding:\n\n\nI’ll show you how to write your own “arrowish” functions later in the post\n\nregister_binding(\"leap_year\", arrowish_leap_year)\n\nOnce the binding has been registered, whenever leap_year() is encountered within one of the arrow-supplied dplyr functions, R will substitute my arrowish_leap_year() function in place of the lubridate::leap_year() function that would normally be called. This is only possible because R has extremely sophisticated metaprogramming tools: you (the developer) can write functions that “capture” the code that the user input, and if necessary modify that code before R evaluates it. This is a very powerful tool for constructing domain-specific languages within R. The tidyverse uses it extensively, and the arrow package does too. The dplyr bindings inside arrow use metaprogramming tricks to modify the user input in such a way that – in this example – the user input is interpreted as if the user had called arrowish_leap_year() rather than leap_year().\n\n\nCooperative magic\nTaken together, these two pieces give us the answer to our puzzle. The call to arrow_days_between() works in my original example because that call was constructed within the context of an arrow-supplied mutate() function. The interpretation of this code isn’t performed by dplyr it is handled by arrow. Internally, arrow uses metaprogramming magic to ensure that arrow_days_between() is reinterpreted as a call to the libarrow days_between() function. But that metaprogramming magic doesn’t apply anywhere except the arrowplyr context. If you try to call arrow_days_between() from the R console or even in a regular dplyr pipeline, you get an error because technically speaking this function doesn’t exist.\n\n\n\n\n\n\nI guess there’s a connection between slightly-evil-Julia burning down the talking trees and my slightly-evil digression? Sort of. I mean the truth is just that I just love this scene and secretly wish I was her. Of all the characters Julia has the most personally transformative arc (in my opinion), in both good ways and bad. There’s a lot going on with her life, her person, and her body. I relate to that. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#using-call-function",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#using-call-function",
    "title": "Binding Apache Arrow to R",
    "section": "Calling libarrow directly",
    "text": "Calling libarrow directly\n\nThe weirdness of that digression leads naturally to a practical question. Given that the “arrow-prefix” function don’t actually exist in the usual sense of the term, and the corresponding bindings can only be called in an arrowplyr context, how the heck does an R developer call the libarrow functions directly? In everyday data analysis you wouldn’t want to do this very often, but from a programming perspective it matters: if you want to write your own functions that play nicely with arrowplyr pipelines, it’s very handy to know how to call libarrow directly.\nSo let’s strip back another level of abstraction!\nShould you ever find yourself wanting to call libarrow compute functions directly from R, call_function() will become your new best friend. It provides a very minimal interface that exposes the libarrow functions to R. The “bare bones” nature of this interface has advantages and disadvantages. The advantage is simplicity: your code doesn’t depend on any of the fancy bells and whistles. Those are fabulous from the user perspective, but from a developer point of view you usually want to keep it simple. The price you pay for this is that you must pass appropriate Arrow objects. You can’t pass a regular R object to a libarrow function and expect it to work. For example:\n\ncall_function(\"days_between\", start_date, today)\n\nError: Argument 1 is of class Date but it must be one of \"Array\", \"ChunkedArray\", \"RecordBatch\", \"Table\", or \"Scalar\"\n\n\nThis doesn’t work because start_date and today are R-native Date objects and do not refer to any data structures in Arrow. The libarrow functions expect to receive pointers to Arrow objects. To fix the previous example, all we need to do is create Arrow Scalars for each date. Here’s how we do that:\n\narrow_start_date &lt;- Scalar$create(start_date)\narrow_today &lt;- Scalar$create(today)\n\narrow_start_date\n\nScalar\n2015-12-16\n\n\nThe arrow_start_date and arrow_today variables are R data structures, but they’re only thin wrappers. The actual data are stored in Arrow, and the R objects are really just pointers to the Arrow data. These objects are suitable for passing to the libarrow days_between() function, and this works:\n\ncall_function(\"days_between\", arrow_start_date, arrow_today)\n\nScalar\n2225\n\n\nHuh. Apparently it took me over 2000 days to write a proper fangirl post about The Magicians. I’m really late to the pop culture party, aren’t I? Oh dear. I’m getting old.\n\n\n\n\n\n\nI’m getting lazier with these connections. Using a Library gif because I’m talking about the C++ library? I mean really, you’d think I’d be better than that wouldn’t you? But no. I am not. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#arrow-expressions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#arrow-expressions",
    "title": "Binding Apache Arrow to R",
    "section": "Arrow expressions",
    "text": "Arrow expressions\n\nThere’s one more foundational topic I should discuss before I can show you how to write arrowplyr-friendly functions, and that’s Arrow Expressions. When I introduced arrowplyr early in the post I noted that most of your code is used to specify a query, and that query doesn’t get evaluated until compute() or collect() is called. If you want to write code that plays nicely with this workflow, you need to ensure that your custom functions return an Arrow Expression.\nThe basic idea behind expressions is probably familiar to R users, since they are what powers the metaprogramming capabilities of the language and are used extensively throughout tidyverse as well as base R. In base R, the quote() function is used to capture a user expression and eval() is used to force it to evaluate. Here’s a simple example where I use quote() to “capture” some R code and prevent it from evaluating:\n\nhead_expr &lt;- quote(head(magicians, n = 3))\nhead_expr\n\nhead(magicians, n = 3)\n\n\nIf I wanted to be clever I could modify the code in head_expr before allowing R to pull the trigger on evaluating it. I could combine a lot of expressions together, change parts of the code as needed, and evaluate them wherever I wanted. As you might imagine, this is super useful for creating domain specific languages within R. But this isn’t a post about metaprogramming so let’s evaluate it now:\n\neval(head_expr)\n\n# A tibble: 3 × 6\n  season episode title                                 air_date   rating viewers\n   &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                 &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n1      1       1 Unauthorized Magic                    2015-12-16    0.2    0.92\n2      1       2 The Source of Magic                   2016-01-25    0.4    1.11\n3      1       3 Consequences of Advanced Spellcasting 2016-02-01    0.4    0.9 \n\n\nThe example above uses native R code. It’s not tied to Arrow in any sense. However, the arrow package provides a mechanism for doing something similar in an Arrow context. For example, here’s me creating a character string as an Arrow Scalar:\n\nfillory &lt;- Scalar$create(\"A world as intricate as filigree\")\nfillory\n\nScalar\nA world as intricate as filigree\n\n\nHere’s me creating the corresponding object within an Arrow Expression:\n\nfillory &lt;- Expression$scalar(\n  Scalar$create(\"A world as intricate as filigree\")\n)\nfillory\n\nExpression\n\"A world as intricate as filigree\"\n\n\nI suspect this would not seem particularly impressive on its own, but you can use the same idea to create function calls that can be evaluated later within the Arrow context:\n\nember &lt;- Expression$create(\"utf8_capitalize\", fillory)\nember\n\nExpression\nutf8_capitalize(\"A world as intricate as filigree\")\n\n\nSo close. We are so very close to the end now.\n\n\n\n\n\nOkay look, I’ll level with you. At this point there is absolutely no connection between the gifs and the content. This post is getting very long and my brain is fried. I need a short break to appreciate the beautiful people, and Kings Idri and Eliot are both very beautiful people. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#arrowish-functions",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#arrowish-functions",
    "title": "Binding Apache Arrow to R",
    "section": "Writing arrowplyr functions",
    "text": "Writing arrowplyr functions\nAt long last we have all the ingredients needed to write a function that can be used in an arrowplyr pipeline. Here’s a simple implementation of the base R toupper() function\n\narrowish_toupper &lt;- function(x) {\n  Expression$create(\"utf8_upper\", x)\n}\n\nAs it happens arrowplyr pipelines already support the toupper() function, so there really wasn’t a need for me to write this. However, at present they don’t support the lubridate leap_year() function, which was the purpose of my very small contribution today. An Arrow friendly version of leap_year() looks like this:\n\narrowish_leap_year &lt;- function(date) {\n   year &lt;- Expression$create(\"year\", date)\n  (year %% 4 == 0) & ((year %% 100 != 0) | (year %% 400 == 0))\n}\n\nBefore putting our functions into action, let’s see what happens when we try to write a simple data analysis pipeline without them:\n\narrowmagicks %&gt;% \n  mutate(\n    title = toupper(title),\n    year = year(air_date), \n    leap = leap_year(air_date)\n  ) %&gt;% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                    &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;\n 1      1       1 UNAUTHORIZED MAGIC       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OF MAGIC      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES OF ADVANCE… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN THE WALLS   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJOR AND MIN… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL APPLICATIONS 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKY CIRCUMST… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLED HEART      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING ROOM         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING               2016-03-21    0.3    0.78  2016 TRUE \n# ℹ 55 more rows\n\n\n\n\nThe internal arrow function that handles this is called “abandon_ship”. No, I don’t know why I felt the need to mention this`\nYes, it returns the correct answer, but only because arrow detected a function it doesn’t understand and has “abandoned ship”. It pulled the data into R and let dplyr do all the work. Now let’s see what happens when we use our functions instead:\n\narrowmagicks %&gt;% \n  mutate(\n    title = arrowish_toupper(title),\n    year = year(air_date),\n    leap = arrowish_leap_year(air_date)\n  ) %&gt;% \n  collect()\n\n# A tibble: 65 × 8\n   season episode title                    air_date   rating viewers  year leap \n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                    &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;lgl&gt;\n 1      1       1 UNAUTHORIZED MAGIC       2015-12-16    0.2    0.92  2015 FALSE\n 2      1       2 THE SOURCE OF MAGIC      2016-01-25    0.4    1.11  2016 TRUE \n 3      1       3 CONSEQUENCES OF ADVANCE… 2016-02-01    0.4    0.9   2016 TRUE \n 4      1       4 THE WORLD IN THE WALLS   2016-02-08    0.3    0.75  2016 TRUE \n 5      1       5 MENDINGS, MAJOR AND MIN… 2016-02-15    0.3    0.75  2016 TRUE \n 6      1       6 IMPRACTICAL APPLICATIONS 2016-02-22    0.3    0.65  2016 TRUE \n 7      1       7 THE MAYAKOVSKY CIRCUMST… 2016-02-29    0.3    0.7   2016 TRUE \n 8      1       8 THE STRANGLED HEART      2016-03-07    0.3    0.67  2016 TRUE \n 9      1       9 THE WRITING ROOM         2016-03-14    0.3    0.71  2016 TRUE \n10      1      10 HOMECOMING               2016-03-21    0.3    0.78  2016 TRUE \n# ℹ 55 more rows\n\n\nEverything works perfectly within Arrow. No ships are abandoned, the arrowplyr pipeline springs no leaks, and we all live happily ever after.\nSort of.\nI mean, we’re all still alive.\nThat has to count as a win, right? 🎉\n\n\n\n\n\n\nEliot and Margo applaud your success. They are the best characters, and you are also the best because you have made it to the end of a long and strange blog post. Image via giphy, copyright syfy"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#epilogue-wheres-the-rest-of-the-owl",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#epilogue-wheres-the-rest-of-the-owl",
    "title": "Binding Apache Arrow to R",
    "section": "Epilogue: Where’s the rest of the owl?",
    "text": "Epilogue: Where’s the rest of the owl?\n\n\n\nIn case you don’t know the reference: how to draw an owl\nThe story I’ve told in this post is a little incomplete. I’ve shown you how to write a function like arrowish_leap_year() that can slot into a dplyr pipeline and operate on an Arrow data structure. But I haven’t said anything about the precise workings of how register_binding() works, in part because the details of the metaprogramming magic is one of the mysteries I’m currently unpacking while I dig into the code base.\nBut that’s not the only thing I’ve left unsaid. I haven’t talked about unit tests, for example. I haven’t talked about the social/technical process of getting code merged into the Arrow repository. If you’ve made it to the end of this post and are curious about joining the Arrow developer community, these are things you need to know about. I’ll probably write something about those topics later on, but in the meantime here are some fabulous resources that might be handy:\n\nApache Arrow New Contributors Guide (thank you to Alenka Frim!)\nDevelopers Guide to Writing Bindings (thank you to Nic Crane!)\nApache Arrow R Cookbook (thank you to Nic Crane again)\n\nEnjoy! 🍰"
  },
  {
    "objectID": "posts/2022-01-18_binding-arrow-to-r/index.html#footnotes",
    "href": "posts/2022-01-18_binding-arrow-to-r/index.html#footnotes",
    "title": "Binding Apache Arrow to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI didn’t quite understand what “kernels” meant in this context until Nic Crane kindly explained it to me. The compute API contains a number of functions which are divided up into “kernels”, specialised functions designed to work on a specific data type. The C++ Arrow compute documentation explains this better.↩︎\nMy actual code didn’t bother to name my function. It’s just an anonymous function passed to register_binding().↩︎"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html",
    "title": "Using Amazon S3 with R",
    "section": "",
    "text": "I have a shameful confession to make, one that may shock and surprise you. Although I am an R user, data scientist, and developer of many years experience, I’ve never used Amazon Web Services.\nIt’s hard to believe, I know, but I’ve never spun up a virtual machine on “Amazon EC2” (…whatever that is), I don’t know what “AWS Lambda” is, and the only thing I know about “Amazon S3” is that fancy data science people use it to store data. Or something along those lines. Honestly, I really haven’t been paying attention. Every time people start talking about it my eyes glaze over and my impostor syndrome arrives to berate me. A true data scientist is born knowing how to spin up EC2 instances, and if baby doesn’t post her drawings on S3 then she’s already falling behind, etc etc. It’s terribly stressful.\nMy dark and terrible personal tragedy notwithstanding,1 I suspect my situation is not entirely uncommon. Back in my academic days, I knew very few people who used Amazon Web Services (a.k.a. AWS) for much of anything. It wasn’t needed, so it wasn’t knowledge that people acquired. Now that I’m working in an industry setting I’m finding that it’s so widely used that it’s almost assumed knowledge. Everyone knows this stuff, so there’s not a lot said about why you might care, or how to get started using these tools if you decide that you do care.\nToday I decided to do something about this, starting by teaching myself how to use Amazon’s Simple Storage Service (a.k.a S3). With the help of the aws.s3 package authored by Thomas Leeper and currently maintained by Simon Urbanek, it’s surprisingly easy to do.\nIn this post I’ll walk you through the process.\nlibrary(dplyr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(stringr)\nlibrary(tibble)\nlibrary(magick)\nlibrary(aws.s3)"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#what-is-s3-and-why-do-i-care",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#what-is-s3-and-why-do-i-care",
    "title": "Using Amazon S3 with R",
    "section": "What is S3 and why do I care?",
    "text": "What is S3 and why do I care?\nLet’s get started. As with everything else in life, the place to start is asking yourself if you even care. I mean, if we don’t care what S3 is or what it does, why even bother? Just let your eyes glaze over as the nerd keeps talking and wonder if there’s anything good on TV…\nStill here? Cool.\nFrom the user perspective, Amazon’s “Simple Storage Service” isn’t particularly complicated. It’s just a remote storage system that you can dump files into, kind of like a programmable Dropbox. Each file (and its accompanying metadata) is stored as an object, and collections of objects are grouped together into a bucket. If you want to store files on S3, all you need to do is open an account, create a new bucket, and upload your files. It’s exactly that boring, and the only reason anyone cares (as far as I know) is that Amazon designed this to work at scale and it’s fairly easy to write scripts that allow you to control the whole thing programmatically. Which is actually a pretty handy service, now that I think about it!"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#downloading-public-data-from-s3",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#downloading-public-data-from-s3",
    "title": "Using Amazon S3 with R",
    "section": "Downloading public data from S3",
    "text": "Downloading public data from S3\nThe first thing to understand about S3 is that there’s many different ways of using it. Very often, you’re not interested in storing your own data on S3. You might just want to download data that someone else has stored, and if that data has been made publicly accessible, then you don’t even need an Amazon Web Services (AWS) account at all. You can download to your hearts content. For a data scientist it’s a fun way to start, because you get to dive straight into playing with data insted of fiddling about with accounts and credentials and all those dull things.\nSo let’s find a public data set to play with. While browsing through the registry of open data sets listed on the S3 website I came across the National Herbarium of NSW data set. As described on the website:\n\nThe National Herbarium of New South Wales is one of the most significant scientific, cultural and historical botanical resources in the Southern hemisphere. The 1.43 million preserved plant specimens have been captured as high-resolution images and the biodiversity metadata associated with each of the images captured in digital form. Botanical specimens date from year 1770 to today, and form voucher collections that document the distribution and diversity of the world’s flora through time, particularly that of NSW, Austalia and the Pacific. The data is used in biodiversity assessment, systematic botanical research, ecosystem conservation and policy development. The data is used by scientists, students and the public.\n\nAs an example, here’s one of the images stored in the data set, of a plant specimen collected quite close to where I currently live, albeit quite a long time ago:\n\n\n\n\n\n\n\n\n\n\n\nSo yeah, I love this data set already and I want to play with it. But how do I do that? I’ve never done anything with S3 before and it’s all a bit new to me. Well, on the right hand side of the listing page for the National Herbarium data, there’s a section that contains the following metadata:\nDescription\nHerbarium Collection Image files\n\nResource type\nS3 Bucket\n\nAmazon Resource Name (ARN)\narn:aws:s3:::herbariumnsw-pds\n\nAWS Region\nap-southeast-2\nUsing this information, I can get started. I know what the data is (an S3 bucket), I know where the data is (in the \"ap-southeast-2\" region), and on top of that I know the name of the data (\"herbariumnsw-pds\"). This should be enough for me to find what I’m looking for!\n\nFinding the bucket\nOkay, so let’s see if we can find this bucket using R code. The aws.s3 package contains a handy function called bucket_exists(), which returns TRUE when it finds an S3 bucket at the specified location (and using whatever credentials you currently have available), and FALSE when it does not. That seems relatively easy. We know the name of our bucket, specified more precisely as \"s3://herbariumnsw-pds/\", and we can verify that it exists. And of course when we do this it turns out that there…\n\nbucket_exists(\"s3://herbariumnsw-pds/\")\n\n\n\n[1] FALSE\n\n\n…isn’t? Wait, what????\nI’ve made a very common mistake here, and forgotten to specify the region. S3 is very picky about regions and you need to tell it explicitly which one to use. The National Herbarium is an Australian institution and the data are stored in Amazon’s Sydney data center. In Amazon parlance, that’s the \"ap-southeast-2\" region, but unless you’ve done something to set a different default (more on that later), everything you do will probably default to the \"us-east-1\" region. To override this default, we can explicitly specify the region that bucket_exists() should look in. So now let’s try that again:\n\nbucket_exists(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\"\n)\n\n\n\n[1] TRUE\n\n\nMuch better!\n\n\nOkay, okay, I lied…\nOne more thing. If you’ve been following along at home and trying out these commands, you’ve probably noticed that the output you’re getting is a little more verbose than simply returning TRUE or FALSE. The actual output comes with a lot of additional metadata, stored as attributes. I didn’t really want to clutter the output by showing all that stuff, so the examples above secretly removed the attributes before printing the results. What you’ll actually see is something like this:\n\nbucket_exists(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\"\n)\n\n[1] TRUE\nattr(,\"x-amz-id-2\")\n[1] \"cr7uaPSaKx4B5BJNRCDIU+Cpns0menZBjxjT5OltIViGFUlStJxSqI5rT1lZfN3ASVz+p4XMalE=\"\nattr(,\"x-amz-request-id\")\n[1] \"AE47G6MWJA7NRMYJ\"\nattr(,\"date\")\n[1] \"Fri, 22 Apr 2022 09:04:11 GMT\"\nattr(,\"x-amz-bucket-region\")\n[1] \"ap-southeast-2\"\nattr(,\"x-amz-access-point-alias\")\n[1] \"false\"\nattr(,\"content-type\")\n[1] \"application/xml\"\nattr(,\"server\")\n[1] \"AmazonS3\"\n\n\nIf you stare at this long enough this metadata all starts to make sense, especially after you’ve been playing around with S3 for a while. There’s a timestamp, there’s some information about which region the data came from, and so on. Nothing particularly special or interesting here, so let’s move on to something more fun.\n\n\nListing bucket contents\nAt this point in the journey we’ve located the bucket, but we have no idea what it contains. To get a list of the bucket contents, the get_bucket_df() function from aws.s3 is our friend. The National Herbarium data set contains a lot of objects, so I’ll be “frugal” and restrict myself to merely downloading max = 20000 records:\n\nherbarium_files &lt;- get_bucket_df(\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\", \n  max = 20000\n) %&gt;% \n  as_tibble()\n\nNow that we’ve downloaded a list of the bucket contents, let’s have a look and see what we’ve got:\n\nherbarium_files\n\n# A tibble: 20,000 × 8\n   Key                        LastM…¹ ETag  Size  Owner…² Owner…³ Stora…⁴ Bucket\n   &lt;chr&gt;                      &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; \n 1 ReadMe.txt                 2020-0… \"\\\"5… 2729  97c09b… herbar… STANDA… herba…\n 2 dwca-nsw_avh-v1.0.zip      2019-1… \"\\\"2… 8231… 97c09b… herbar… STANDA… herba…\n 3 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"3… 33    &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 4 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"5… 433   &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 5 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"5… 33    &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 6 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"f… 433   &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 7 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"b… 33    &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 8 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"4… 433   &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n 9 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"f… 33    &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n10 herbariumnsw-pds/PublicDa… 2021-0… \"\\\"6… 433   &lt;NA&gt;    &lt;NA&gt;    STANDA… herba…\n# … with 19,990 more rows, and abbreviated variable names ¹​LastModified,\n#   ²​Owner_ID, ³​Owner_DisplayName, ⁴​StorageClass\n\n\nWonderful! The very first object in the bucket happens to be a file called ReadMe.txt. Perhaps I should download this marvelous object and perhaps even read it?\n\n\nDownloading files\nOkay then. We are now at the step where we want to download a specific object from the bucket, and save it locally as a file. To do this we use the save_object() function. As before, we specify the bucket and the region, but we’ll also need to specify which object should be downloaded, and the file path to which it should be saved. Here’s how that works for the Read Me file:\n\nsave_object(\n  object = \"ReadMe.txt\",\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\",\n  file = \"herbarium/ReadMe.txt\"\n)\n\n[1] \"herbarium/ReadMe.txt\"\n\n\nOnce again this works and so off I go, reading the Read Me in search of further clues.\nAs you might hope, the Read Me file does in fact tell us something about how the National Herbarium dataset is organised. In particular, one line in the Read Me informs me that there’s a file storing all the metadata, encoded as a zipped csv file:\n\nA zipped csv containing the biocollections metadata for the images is available as a DarwinCore Archive at: https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/dwca-nsw_avh-v1.0.zip\n\nThis sounds like a good place to start, doesn’t it? Once again, I’ll use save_object() and try to download the metadata file dwca-nsw_avh-v1.0.zip:\n\nsave_object(\n  object = \"dwca-nsw_avh-v1.0.zip\",\n  bucket = \"s3://herbariumnsw-pds/\", \n  region = \"ap-southeast-2\",\n  file = \"herbarium/dwca-nsw_avh-v1.0.zip\"\n) \n\n[1] \"herbarium/dwca-nsw_avh-v1.0.zip\"\n\n\nSuccess!\nI now have a copy of the 79MB zip file on my laptop, and after decompressing the file it turns out I have a 402MB file called occurrence.txt that contains the metadata. As it turns out, the metadata aren’t stored in comma-separated value format, they’re stored in tab-separated value format. Still, that’s fine: the read_tsv() function from the readr package can handle it:\n\nherbarium &lt;- read_tsv(\"herbarium/dwca-nsw_avh-v1.0/occurrence.txt\")\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 725507 Columns: 74\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (38): id, type, institutionCode, collectionCode, basisOfRecord, occurre...\ndbl   (7): minimumElevationInMeters, maximumElevationInMeters, minimumDepthI...\nlgl  (28): lifeStage, associatedSequences, associatedTaxa, previousIdentific...\ndttm  (1): modified\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nherbarium\n\n# A tibble: 725,507 × 74\n   id     type  modified            institutionCode collectionCode basisOfRecord\n   &lt;chr&gt;  &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;        \n 1 NSW:N… Phys… 2013-11-28 11:56:00 NSW             NSW            PreservedSpe…\n 2 NSW:N… Phys… 2012-08-09 15:47:00 NSW             NSW            PreservedSpe…\n 3 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 4 NSW:N… Phys… 2018-07-24 10:06:00 NSW             NSW            PreservedSpe…\n 5 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 6 NSW:N… Phys… 2013-07-24 15:16:00 NSW             NSW            PreservedSpe…\n 7 NSW:N… Phys… 2015-03-13 15:51:00 NSW             NSW            PreservedSpe…\n 8 NSW:N… Phys… 2010-12-01 14:25:00 NSW             NSW            PreservedSpe…\n 9 NSW:N… Phys… 2018-01-24 16:49:00 NSW             NSW            PreservedSpe…\n10 NSW:N… Phys… 2018-07-24 10:05:00 NSW             NSW            PreservedSpe…\n# … with 725,497 more rows, and 68 more variables: occurrenceID &lt;chr&gt;,\n#   catalogNumber &lt;chr&gt;, occurrenceRemarks &lt;chr&gt;, recordNumber &lt;chr&gt;,\n#   recordedBy &lt;chr&gt;, lifeStage &lt;lgl&gt;, reproductiveCondition &lt;chr&gt;,\n#   establishmentMeans &lt;chr&gt;, occurrenceStatus &lt;chr&gt;, preparations &lt;chr&gt;,\n#   associatedSequences &lt;lgl&gt;, associatedTaxa &lt;lgl&gt;,\n#   previousIdentifications &lt;lgl&gt;, eventDate &lt;chr&gt;, verbatimEventDate &lt;chr&gt;,\n#   habitat &lt;chr&gt;, eventRemarks &lt;lgl&gt;, continent &lt;lgl&gt;, waterBody &lt;lgl&gt;, …\n\n\nThere’s quite a lot of interesting information stored in the 74 columns of the herbarium data, but I won’t dive very deep into it in this post. I will mention, however, that if you find yourself following along at home you’ll likely discover that there is a small proportion of the 725507 rows that cause problems for read_tsv(), likely because they contain additional tab characters that mess up the parsing slightly. In real life I’d want to look into this, but this is a blog post. Nothing here is real and nobody is watching, right?\n\n\nWrangling the data\nNow that I have some data, I can do a little poking around to see what’s in it. Exploring a new data set is always fun, but this isn’t really a post about data wrangling, so I’ll keep this brief. A quick look suggests that (unsurprisingly) there are a lot of records corresponding to samples collected in Australia, and a disproportionate number of those come from New South Wales:\n\nherbarium %&gt;% \n  filter(country == \"Australia\") %&gt;% \n  count(stateProvince)\n\n# A tibble: 10 × 2\n   stateProvince                     n\n   &lt;chr&gt;                         &lt;int&gt;\n 1 Australian Capital Territory     52\n 2 External Territories           1549\n 3 New South Wales              394439\n 4 Northern Territory            28922\n 5 Queensland                    89016\n 6 South Australia               20206\n 7 Tasmania                      23994\n 8 Victoria                      40984\n 9 Western Australia             80447\n10 &lt;NA&gt;                           4287\n\n\nThat’s nice, but doesn’t immediately suggest a fun example for me to continue this post. On a whim, I decide to name search my neighbourhood. I live in Newtown (in Sydney), so I’m going to find the subset of images in the National Herbarium data whose locality matches the string \"Newtown\":\n\nnewtowners &lt;- herbarium %&gt;% \n  filter(\n    country == \"Australia\", \n    locality %&gt;% str_detect(\"Newtown\")\n  )\n\nYeah, no. This is misleading.\nFrom a data science point of view I’m being extremely sloppy here. If my intention had been to find only plants from my neighbourhood, I would also be wise to filter by recorded longitude and latitude where available, and I would certainly want to exclude cases listed as coming from another Australian state. “Newtown” is not an uncommon name, and – to the surprise of nobody – it turns out that there are several different locations called “Newtown” in different parts of Australia. Fortunately for me, I really don’t care! All I wanted was a query that would return around 20-30 results, so this is fine for my purposes.\nNow that we’ve got a subset of records, let’s pull out the catalog numbers:\n\nnewtowners %&gt;% \n  pull(catalogNumber)\n\n [1] \"NSW 395530\"  \"NSW 461895\"  \"NSW 650052\"  \"NSW 1055313\" \"NSW 1056305\"\n [6] \"NSW 29246\"   \"NSW 36860\"   \"NSW 39618\"   \"NSW 687458\"  \"NSW 121207\" \n[11] \"NSW 214616\"  \"NSW 306564\"  \"NSW 307215\"  \"NSW 389387\"  \"NSW 395529\" \n[16] \"NSW 402973\"  \"NSW 403188\"  \"NSW 404127\"  \"NSW 421494\"  \"NSW 446243\" \n[21] \"NSW 570557\"  \"NSW 702035\"  \"NSW 676197\"  \"NSW 776212\"  \"NSW 777249\" \n[26] \"NSW 739455\"  \"NSW 751830\" \n\n\nThe Read Me file had something useful to say about these numbers. Specifically, the catalog numbers are used as the basis of the file naming convention for images stored in the bucket:\n\nImage data are organized by NSW specimen barcode number. For example, the file for Dodonaea lobulata recorded on 1968-09-07 = NSW 041500 can be accessed via the URI https://herbariumnsw-pds.s3-ap-southeast-2.amazonaws.com/images/NSW041500.jp2\n\nHm. I wonder if I can write code to extract these images?\n\n\n\n\n\n\n\n\n\n\n\n\n\nScripting the download\nOkay, now I want to pull the images for these records. First, I’m going to construct the paths. I am not going to download the jp2 files because they’re about 100MB each. Multiplying that number by the number of records gives… well, it gives a big enough number that I think I’ve worked out why the National Herbarium dataset is on S3 and not on a laptop in a damp basement somewhere!\nIn any case, for a lot of the records there’s a jpg file that is considerably smaller in size, so I’m going to try to download those. Based on the barcodes I’ve got, these are the files I’m expecting to find:\n\nobjects &lt;- newtowners %&gt;%\n  pull(catalogNumber) %&gt;% \n  str_remove_all(\" \") %&gt;% \n  str_c(\"images/\", ., \".jpg\")\n\nobjects\n\n [1] \"images/NSW395530.jpg\"  \"images/NSW461895.jpg\"  \"images/NSW650052.jpg\" \n [4] \"images/NSW1055313.jpg\" \"images/NSW1056305.jpg\" \"images/NSW29246.jpg\"  \n [7] \"images/NSW36860.jpg\"   \"images/NSW39618.jpg\"   \"images/NSW687458.jpg\" \n[10] \"images/NSW121207.jpg\"  \"images/NSW214616.jpg\"  \"images/NSW306564.jpg\" \n[13] \"images/NSW307215.jpg\"  \"images/NSW389387.jpg\"  \"images/NSW395529.jpg\" \n[16] \"images/NSW402973.jpg\"  \"images/NSW403188.jpg\"  \"images/NSW404127.jpg\" \n[19] \"images/NSW421494.jpg\"  \"images/NSW446243.jpg\"  \"images/NSW570557.jpg\" \n[22] \"images/NSW702035.jpg\"  \"images/NSW676197.jpg\"  \"images/NSW776212.jpg\" \n[25] \"images/NSW777249.jpg\"  \"images/NSW739455.jpg\"  \"images/NSW751830.jpg\" \n\n\nThis all seems pretty reasonable, but there’s a nuance here that is worth pointing out. When you look at the output above, it’s tempting to think that \"images\" must be a subfolder within the S3 bucket. That intuition isn’t correct: each S3 bucket is a flat datastore. It doesn’t contain any subfolders: the \"/\" is treated as part of the object name, nothing more. It can be convenient to name objects this way, though, because it makes it a little easier to organise them into subfolders later on if you want to move them onto a more traditional hierarchical file system.\nAnyway…\nSince I’m going to try downloading objects that may or may not actually exist (i.e., I’m not certain if all these records actually have jpg files), I’m going to start out by writing a helper function save_herbarium_image() that does three things:\n\nFirst, it uses the object_exists() function to check if an object with that name exists in this bucket. The object_exists() function works similarly to the bucket_exists() function I used earlier: the only difference is that I also specify the object name.\nSecond, if the object exists, it downloads the file and stores it locally, in the \"herbarium\" subfolder in the folder that contains this blog post.\nThird, it returns information to the user. If the object exists and was successfully downloaded, it returns a character string specifying the location of the saved file. If the object doesn’t exist, it returns NA.\n\nHere’s the code:\n\nsave_herbarium_image &lt;- function(file) {\n  \n  # if object doesn't exist in bucket, return NA\n  ok &lt;- object_exists(\n    object = file,\n    bucket = \"s3://herbariumnsw-pds/\", \n    region = \"ap-southeast-2\"\n  )\n  if(!ok) return(NA_character_)\n  \n  # if object exists, save it and return file path\n  save_object(\n      object = file,\n      bucket = \"s3://herbariumnsw-pds/\", \n      region = \"ap-southeast-2\",\n      file = paste0(\"herbarium/\", file)\n  )\n}\n\nAnd here it is applied to the first file:\n\nobjects[1] %&gt;% \n  save_herbarium_image()\n\n[1] \"herbarium/images/NSW395530.jpg\"\n\n\nThat seemed to work well when applied to a single file, so I’ll use the functional programming tools from purrr to vectorise the operation. More precisely, I’ll use map_chr() to iterate over all of the objects, applying the save_herbarium_image() function to each one, and collecting the return values from all these function calls into a character vector:\n\nobjects %&gt;% \n  map_chr(save_herbarium_image)\n\nClient error: (404) Not Found\nClient error: (404) Not Found\nClient error: (404) Not Found\nClient error: (404) Not Found\n\n\n [1] \"herbarium/images/NSW395530.jpg\"  \"herbarium/images/NSW461895.jpg\" \n [3] \"herbarium/images/NSW650052.jpg\"  \"herbarium/images/NSW1055313.jpg\"\n [5] NA                                \"herbarium/images/NSW29246.jpg\"  \n [7] \"herbarium/images/NSW36860.jpg\"   \"herbarium/images/NSW39618.jpg\"  \n [9] \"herbarium/images/NSW687458.jpg\"  \"herbarium/images/NSW121207.jpg\" \n[11] \"herbarium/images/NSW214616.jpg\"  \"herbarium/images/NSW306564.jpg\" \n[13] \"herbarium/images/NSW307215.jpg\"  \"herbarium/images/NSW389387.jpg\" \n[15] NA                                \"herbarium/images/NSW402973.jpg\" \n[17] \"herbarium/images/NSW403188.jpg\"  \"herbarium/images/NSW404127.jpg\" \n[19] NA                                \"herbarium/images/NSW446243.jpg\" \n[21] \"herbarium/images/NSW570557.jpg\"  \"herbarium/images/NSW702035.jpg\" \n[23] \"herbarium/images/NSW676197.jpg\"  \"herbarium/images/NSW776212.jpg\" \n[25] \"herbarium/images/NSW777249.jpg\"  \"herbarium/images/NSW739455.jpg\" \n[27] NA                               \n\n\nDid it work? Well, kind of. Notice there are some missing values in the output. In those cases the object doesn’t exist in this bucket, and when that happens the save_herbarium_image() function bails and doesn’t try to download anything. But in most cases images it worked.\n\n\nA minor irritant appears!\nAt this point, I’d like to start displaying the images. It’s nice to have pretty pictures in a blog post, don’t you think? Like, maybe what I could do is include some of those images in this post. One problem though is that the files stored in the National Herbarium dataset are high resolution images and as consequence even the jpg files are usually about 7MB each. That’s a bit excessive, so I think what I’ll do is write a little helper function that reads in each image, resizes it to something smaller, and then saves that smaller file.\nIf I want to do this within R, the magick package is my friend. It’s extremely well suited to this kind of image manipulation task. This post isn’t about the magick package, so I’m not going to explain this part of the code,2 but suffice it to say that this helper function solves the problem:\n\nshrink_herbarium_image &lt;- function(file) {\n  on.exit(gc())\n  img_from &lt;- file.path(\"herbarium\", \"images\", file)\n  img_to &lt;- file.path(\"herbarium\", \"tiny_images\", file)\n  image_read(img_from) %&gt;% \n    image_resize(geometry_size_pixels(width = 1000)) %&gt;% \n    image_write(img_to)\n}\n\nNow that I have this function, I can iterate over every image stored in my local images folder, shrink it, and save the small version to the tiny_images folder:\n\nlist.files(\"herbarium/images\") %&gt;% \n  map_chr(shrink_herbarium_image)\n\n [1] \"herbarium/tiny_images/NSW1055313.jpg\"\n [2] \"herbarium/tiny_images/NSW121207.jpg\" \n [3] \"herbarium/tiny_images/NSW214616.jpg\" \n [4] \"herbarium/tiny_images/NSW29246.jpg\"  \n [5] \"herbarium/tiny_images/NSW306564.jpg\" \n [6] \"herbarium/tiny_images/NSW307215.jpg\" \n [7] \"herbarium/tiny_images/NSW36860.jpg\"  \n [8] \"herbarium/tiny_images/NSW389387.jpg\" \n [9] \"herbarium/tiny_images/NSW395530.jpg\" \n[10] \"herbarium/tiny_images/NSW39618.jpg\"  \n[11] \"herbarium/tiny_images/NSW402973.jpg\" \n[12] \"herbarium/tiny_images/NSW403188.jpg\" \n[13] \"herbarium/tiny_images/NSW404127.jpg\" \n[14] \"herbarium/tiny_images/NSW446243.jpg\" \n[15] \"herbarium/tiny_images/NSW461895.jpg\" \n[16] \"herbarium/tiny_images/NSW570557.jpg\" \n[17] \"herbarium/tiny_images/NSW650052.jpg\" \n[18] \"herbarium/tiny_images/NSW676197.jpg\" \n[19] \"herbarium/tiny_images/NSW687458.jpg\" \n[20] \"herbarium/tiny_images/NSW702035.jpg\" \n[21] \"herbarium/tiny_images/NSW739455.jpg\" \n[22] \"herbarium/tiny_images/NSW776212.jpg\" \n[23] \"herbarium/tiny_images/NSW777249.jpg\" \n\n\nThe output here is a character vector containing names for the created files. That’s nice as a way of checking that everything worked, but I want pretty pictures! So here’s the contents of the tiny_images folder, but shown as the actual images rather than file names:3\n\n\n\nusing 'image-only' layout\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgress!"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#accounts-and-credentials",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#accounts-and-credentials",
    "title": "Using Amazon S3 with R",
    "section": "Accounts and credentials",
    "text": "Accounts and credentials\nAt this point it is starting to dawn on me that it would be kind of neat to create my own S3 bucket and store the tiny images there. I could make the tiny images public and then display them in this post. The National Herbarium data is released under a Creative Commons By-Attribution licence, so I’m allowed to use the images that way as long as I properly acknowledge the source… which I think is fairly well covered in this post already!\nThe task I’m going to set for myself later in this post is to do exactly that, and use tools from the aws.s3 package to do everything in R. However, I can’t do any of that unless I have an AWS account of my very own. The time has come for me to do that.\n\nCreating the account\nSigning up for the account turns out to be pretty easy. All I had to do was visit https://aws.amazon.com/s3/ and click on the “Create an AWS Account” button shown in the image below:\n\n\n\n\n\n\n\n\n\n\n\nThis then led me through a pretty standard sign up process. I had to provide an email address for the “root user” (i.e., me!), specify a password, and so on. I didn’t sign up for anything that cost money. The free tier allows you 5GB of storage for 12 months, which is fairly convenient for “playing around” purposes, and that’s all I’m intending to do here.\n\n\nCreating credentials\nThe next step is to create an access key, so that R can interact with S3 using my credentials. At this point a little care is needed. It is possible to create access credentials for the root user, but that’s not a good idea. The root user has access to every AWS service, not just S3, and it’s a bad idea to give R access to any credentials that have those permissions. What I’ll do here is create an an “IAM user” – where “IAM” stands for “Identity and Access Management” – that only has access to my S3 storage, and the credentials I supply to R will be associated with that user. Here’s how I did that. First, I went over to the IAM console here:\nhttps://us-east-1.console.aws.amazon.com/iamv2/home#/users\nOn this screen there’s an “add users” button that I dutifully click…\n\n\n\n\n\n\n\n\n\n\nFrom here it’s mostly a matter of following prompts. The screenshot below shows me part way through the creation process. The IAM user has its own username, and it will be allowed programmatic access using an access key:\n\n\n\n\n\n\n\n\n\n\nWhen I get to the next screen it asks me to set the permissions associated with this user. I click on “attach existing policies directly”, and then type “S3” into the search box. It comes up with a list of permission policies associated with S3 and I select the one I want:\n\n\n\n\n\n\n\n\n\n\nThe third screen is boring. It asks for tags. I don’t give it any. I move onto the fourth screen, which turns out to be a review screen:\n\n\n\n\n\n\n\n\n\n\nHaving decided I am happy with these settings, I click on the “next” button that isn’t actually shown in these screenshots (it’s at the bottom of the page) and it takes me to a final screen that gives me the access key ID and the secret access key:\n\n\n\n\n\n\n\n\n\nThese are the two pieces of information I need to let R access to my S3 storage.\n\n\nStoring your AWS credentials in R\nThere are several ways of storing these credentials in R. The easiest is to add the credentials to your .Renviron file, which you can conveniently open with the edit_r_environ() function from the usethis package. To get access to the account, the following lines need to be added to your .Renviron file:\nAWS_ACCESS_KEY_ID=&lt;your access key id&gt;\nAWS_SECRET_ACCESS_KEY=&lt;your secret key&gt;\nHowever, if you’re going to be using the same AWS region all the time (e.g., you’re in Sydney so you tend to use \"ap-southeast-2\" rather than \"us-east-1\"), you might as well add a third line that sets your default region. That way, you won’t need to bother manually specifying the region argument every time you want to interact with S3: the aws.s3 package will use your default. So for me, the relevant lines ended up looking like this:\nAWS_ACCESS_KEY_ID=&lt;my access key id&gt;\nAWS_SECRET_ACCESS_KEY=&lt;my secret key&gt;\nAWS_DEFAULT_REGION=ap-southeast-2\nAfter restarting R, these new settings will apply."
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#manipulating-your-s3-storage-from-r",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#manipulating-your-s3-storage-from-r",
    "title": "Using Amazon S3 with R",
    "section": "Manipulating your S3 storage from R",
    "text": "Manipulating your S3 storage from R\nNow that I have an AWS account and credentials, I can start using the aws.s3 package for more than just downloading files. I can create my own buckets, put objects into those buckets, control the access settings for those objects, and a good deal more besides. So let’s give that a go, shall we?\n\nCreating a new bucket\nThe function to create a new bucket is called put_bucket() and now that my credentials are set up it’s almost comically easy to use. If I want to create a bucket called \"tiny-herbs\", this is what I do:\n\nput_bucket(\"tiny-herbs\")\n\n\n\n[1] TRUE\n\n\nThat seems too easy? I am skeptical. I’m convinced that something must have gone wrong, so my first impulse is to use bucket_exists() to verify that it worked. Okay, so… does this new bucket exist?\n\nbucket_exists(\"s3://tiny-herbs/\") \n\n[1] TRUE\nattr(,\"x-amz-id-2\")\n[1] \"4B8L2PauxADAbOSyFra5ra/OHwObxniV89yWTPe44PJ0TRjIOdsNQdHl1El1t0/39aSQOyJS1FE=\"\nattr(,\"x-amz-request-id\")\n[1] \"GH35D6V4G9W945BY\"\nattr(,\"date\")\n[1] \"Tue, 23 Aug 2022 03:05:15 GMT\"\nattr(,\"x-amz-bucket-region\")\n[1] \"ap-southeast-2\"\nattr(,\"x-amz-access-point-alias\")\n[1] \"false\"\nattr(,\"content-type\")\n[1] \"application/xml\"\nattr(,\"server\")\n[1] \"AmazonS3\"\n\n\nIt does, and notice that both put_bucket() and bucket_exists() have respected my default region setting. When I called put_bucket(), the aws.s3 package supplied the region from my default and so the bucket was created in Sydney (i.e., “ap-southeast-2”), and it did the same again when I used bucket_exists() to look for the buckets.\nSo what’s in the bucket? Just like I did with the National Herbarium bucket, I can use the get_bucket_df() function to inspect the contents of my bucket:\n\nget_bucket_df(\"s3://tiny-herbs/\") %&gt;% \n  as_tibble()\n\n\n\n# A tibble: 0 × 8\n# … with 8 variables: Key &lt;chr&gt;, LastModified &lt;chr&gt;, ETag &lt;chr&gt;, Size &lt;chr&gt;,\n#   Owner_ID &lt;chr&gt;, Owner_DisplayName &lt;chr&gt;, StorageClass &lt;chr&gt;, Bucket &lt;chr&gt;\n\n\nHm. Well, yes. Of course it’s empty: I haven’t put any objects in it yet. Maybe I should do that? It does seem like a good idea!\nBut first…\n\n\nManaging access control\nOne thing though… is this private or public? This is governed by the Access Control List (ACL) settings. By default, S3 buckets are set to private. You can read and write to them, but no-one else has any access at all. Let’s soften that slightly, and allow anyone to read from the “tiny-herbs” bucket. I could have done that from the beginning by setting acl = \"public-read\" when I called put_bucket(). However, because I “forgot” to do that earlier, I’ll change it now using put_acl()\n\nput_acl(\n  bucket = \"s3://tiny-herbs/\",\n  acl = \"public-read\"\n)\n\n[1] TRUE\n\n\nNow everyone has read access to the bucket.4\n\n\nAdding objects to your bucket\nTo put an object inside my new bucket, the function I need is put_object(). When calling it, I need to specify the local path to the file that I want to upload, the name that the object will be assigned when it is added to the bucket, and of course the bucket itself. This time around, I’ll also explicitly set acl = \"public-read\" to ensure that – while only I have write access – everyone has read access and can download the object if they want to. Because I’m going to call this repeatedly, I’ll wrap all this in a helper function called put_tiny_image():\n\nput_tiny_image &lt;- function(file) {\n  put_object(\n    file = file.path(\"herbarium\", \"tiny_images\", file),\n    object = file, \n    bucket = \"s3://tiny-herbs/\",\n    acl = \"public-read\"\n  )\n}\n\nTo see this in action, let’s create a vector that lists the names of all the tiny images, and then apply the put_tiny_image() function to the first one:\n\ntiny_images &lt;- list.files(\"herbarium/tiny_images\")\ntiny_images[1] %&gt;% \n  put_tiny_image()\n\n[1] TRUE\n\n\nOkay that seems to work, so once again I’ll use purrr to iterate over all the tiny_images, uploading them one by one into my newly-created bucket:\n\ntiny_images %&gt;% \n  map_lgl(put_tiny_image)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nThat looks pretty good! I’m seeing nothing but TRUE values in the output so it looks like I’ve successfully uploaded all the tiny images. Now that I’ve done this, I can try calling get_bucket_df() again to inspect the current contents of the bucket:\n\nget_bucket_df(\"s3://tiny-herbs/\") %&gt;% \n  as_tibble()\n\n# A tibble: 23 × 8\n   Key            LastModified        ETag  Size  Owner…¹ Owner…² Stora…³ Bucket\n   &lt;chr&gt;          &lt;chr&gt;               &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; \n 1 NSW1055313.jpg 2022-08-23T03:05:1… \"\\\"c… 1494… b8b231… djnava… STANDA… tiny-…\n 2 NSW121207.jpg  2022-04-22T09:08:0… \"\\\"6… 1562… b8b231… djnava… STANDA… tiny-…\n 3 NSW214616.jpg  2022-04-22T09:08:0… \"\\\"f… 2041… b8b231… djnava… STANDA… tiny-…\n 4 NSW29246.jpg   2022-04-22T09:08:0… \"\\\"e… 1043… b8b231… djnava… STANDA… tiny-…\n 5 NSW306564.jpg  2022-04-22T09:08:0… \"\\\"d… 1819… b8b231… djnava… STANDA… tiny-…\n 6 NSW307215.jpg  2022-04-22T09:08:0… \"\\\"e… 1684… b8b231… djnava… STANDA… tiny-…\n 7 NSW36860.jpg   2022-04-22T09:08:1… \"\\\"f… 1962… b8b231… djnava… STANDA… tiny-…\n 8 NSW389387.jpg  2022-04-22T09:08:1… \"\\\"d… 1240… b8b231… djnava… STANDA… tiny-…\n 9 NSW395530.jpg  2022-04-22T09:08:1… \"\\\"3… 1435… b8b231… djnava… STANDA… tiny-…\n10 NSW39618.jpg   2022-04-22T09:08:1… \"\\\"a… 1028… b8b231… djnava… STANDA… tiny-…\n# … with 13 more rows, and abbreviated variable names ¹​Owner_ID,\n#   ²​Owner_DisplayName, ³​StorageClass\n\n\nYay! It’s done!\n\n\nURLs for objects in public buckets\nOne last thing. Because the \"tiny-herbs\" bucket is public, the objects it contains each have their own URL. To make my life a little easier, I wrote a helper function that constructs these URL:\n\ntiny_herb_url &lt;- function(object, \n                          bucket = \"tiny-herbs\",\n                          region = \"ap-southeast-2\") {\n  paste0(\n    \"https://\", bucket, \".\", \"s3-\", \n    region, \".amazonaws.com\", \"/\", object\n  )\n}\n\nFor example, here’s one of the URLs associated with the \"tiny-herbs\" bucket:\n\ntiny_herb_url(\"NSW121207.jpg\")\n\n[1] \"https://tiny-herbs.s3-ap-southeast-2.amazonaws.com/NSW121207.jpg\"\n\n\nThe images I’ve been showing throughout this post aren’t the original ones from the National Herbarium data set. Rather, they’re the smaller files I stored in the \"tiny-herbs\" bucket, and the code I’ve been using to display the images throughout the post looks like this:\n\ntiny_herb_url(\"NSW121207.jpg\") %&gt;% \n  knitr::include_graphics()"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#wrapping-up",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#wrapping-up",
    "title": "Using Amazon S3 with R",
    "section": "Wrapping up",
    "text": "Wrapping up\nAt the end of all this you might have all kinds of questions. Questions like, “Danielle, what’s wrong with you?” and “Danielle, is this even your job? Aren’t you supposed to be working on Apache Arrow?” While I could write an entire novel trying to answer the first one, I think I’ll skip over it and move straight onto the second on, because that’s more interesting and doesn’t require anyone to get a therapist.\nAlthough this isn’t a post about Apache Arrow – and so is not directly related to the work I do every day – of the reasons I found myself looking into S3 in the first place is that Arrow is a tool designed to let data scientists work with very large data sets, and S3 is a tool designed to make it easy to store very large data sets. These two things go well together, so much so that the arrow R package has its own support for S3 data storage, and many of the data sets that new arrow users encounter are stored on S3. From an educational perspective (sorry – I used to be an academic and I can’t help myself) it’s really difficult for people when they need to learn lots of things at the same time. Trying to learn how Arrow works is really hard when you’re still confused about S3. When I started learning Arrow I didn’t know anything about S3, and it was extremely frustrating to have to learn Arrow concepts with all this confusing S3 stuff floating around.\nHence… this post. My main goal here was to talk about S3 as a topic in its own right, and how tools like aws.s3 allow R users to write code that interacts with S3 data storage. But it’s very handy background knowledge to have if you’re planning to use arrow later on.\nOn top of all that, the aws.s3 package has a lot more functionality that I haven’t talked about here. You can use it to copy objects from one bucket to another, and delete objects and buckets that you control. You can use it to add tagging metadata, you can use it to configure your S3 bucket as a website (yes, even with all that painful cross-origin resource sharing configuration stuff), and a good deal more besides. It’s a really nice package and I’m glad I took the time to learn it!"
  },
  {
    "objectID": "posts/2022-03-17_using-aws-s3-in-r/index.html#footnotes",
    "href": "posts/2022-03-17_using-aws-s3-in-r/index.html#footnotes",
    "title": "Using Amazon S3 with R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s deeply important to me that you read this knowing that I was singing Tragedy by Steps at the time I wrote this, complete with dramatic hand gestures↩︎\nI will, yet again, sigh in frustration that I have to include the on.exit(gc()) line. My limited understanding is as follows. The magick package provides wrappers to the C++ ImageMagick library, and none of the image manipulation is actually done in R. The objects that get “loaded” are just pointers, and exiting the shrink_herbarium_image() function doesn’t necessarily cause R to release memory. So whenever I’m iterating over many images, R doesn’t release old images from the magick resource cache unless I trigger the garbage collection with gc(). This feels inelegant but I haven’t had time to find a better solution.↩︎\nI’m using the bs4cards package to display the images in this layout. Oh, and if you click on any image you’ll see a higher resolution version of that image.↩︎\nYou can specify different ACL settings for each object, if you want to. The put_acl() function also has an object argument that allows you to control the setting for a single object in a bucket.↩︎"
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html",
    "href": "posts/2023-06-12_pins-and-needles/index.html",
    "title": "Pins and needles",
    "section": "",
    "text": "A few days ago I made a huge mistake. I was working on a project that, quite rightly, I’d placed under version control. It lived within a git repository, and that git repository was hosted remotely on GitHub. That’s not the mistake: version control is a good thing. The mistake arrived when the project generated a large output files (about 300Mb in size), and because it’s not a good idea to have large files under version control with git,1 I added that file to .gitignore. Again, not a mistake per se, but you can see the shape of the mistake already: the large file doesn’t exist anywhere except as an untracked file in my local git repo. Bad things can happen in this situation, and lo and behold a few hours later when I messed up something else and unthinkingly called git reset --hard to revert to an earlier state, the file – which took about two hours to generate in the first place – vanished.\nFuuuuuuuuuuck. I’m an idiot.\nOkay, that’s not true. I’m not an idiot. But I what I am (or was), is a person who doesn’t have a workflow in place to manage data files that don’t really belong in a git repository, and I paid the price for that huge tiny mistake. Lesson learned.\nThis is the story of how I learned how to stop worrying and love pins. It’s not a very thorough or detailed post. I’m trying to follow Julia Evans’ advice on blogging and allow myself to tell personal stories, even if those stories are incomplete or imperfect. Not every post needs to be a goddamn monograph."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#storing-data-with-pins",
    "href": "posts/2023-06-12_pins-and-needles/index.html#storing-data-with-pins",
    "title": "Pins and needles",
    "section": "Storing data with pins",
    "text": "Storing data with pins\nAs with everything in data science, mine is a problem that can be solved in many different ways. In this instance I’m working within R, which influences how I think about this. Even within R there are multiple solutions, but I’m lazy and chose to use the pins package for this because I know the developers and it was the first option that came to mind. I don’t really want to write a full tutorial on the pins package because I’m fairly new to it myself, but in case you don’t already know pins, here’s a very quick summary:\n\nA pin is a serialised copy of an R object saved to disk along with accompanying metadata. The most obvious kind of object to store in this way is a data frame, but the general approach is broader. You can choose what format the object is stored in. For general R objects you can write to the native rds format or the qs format. Lists can be saved as json. For rectangular objects like data frames you also have the options of writing to a csv file, a parquet file, or the arrow IPC file format.\nPins live within a board, which refers to the file storage location. This can be a folder on your local machine, or a bucket on a cloud storage provider (AWS, Azure, GCS, etc). When reading or writing a pin, you read or write to the board. Each board is associated with a manifest, a yaml file called _pins.yaml that lives in the root directory of the board and lists all the pins stored in the board.\n\nThere’s more nuance to pins than this: pins can be versioned and given tags, for instance. But I’m not going to bother with all that here. Instead, I’ll dive straight in and illustrate a very simple workflow using pins that solves about 90% of my problems with almost no effort…"
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#using-pins-with-dropbox-personal-storage",
    "href": "posts/2023-06-12_pins-and-needles/index.html#using-pins-with-dropbox-personal-storage",
    "title": "Pins and needles",
    "section": "Using pins with Dropbox personal storage",
    "text": "Using pins with Dropbox personal storage\nOkay, let’s have a look at ways to solve my problem using pins. I am by nature a lazy person who looks for the easiest solutions to my problems. In particular, I habitually look for solutions that align nicely with my existing setup. In this case, my first thought was to use Dropbox. For many years now I have organised my “personal” files2 on my local machine by keeping my git repos in ~/GitHub and keeping my files that aren’t under version control (but should still be backed up) in ~/Dropbox. Viewed from that perspective, the solution I’m looking for is something where I can keep my git repos in ~/GitHub, but have the large data files stored in ~/Dropbox.\nThis workflow is very easy to set up with pins. It’s not what I do now (more on that momentarily) but it was the first thing I tried and it works beautifully for personal use. All I have to do is create a Dropbox folder that I’ll use to store my pins (say, ~/Dropbox/Pins), and then create a board in that folder. First, let’s make sure I have the folder:\n\nfs::dir_create(\"~/Dropbox/Pins\")\n\nNow I’ll load the pins package and create a board in that folder.\n\nlibrary(pins)\nboard_dropbox &lt;- board_folder(\"~/Dropbox/Pins\")\nboard_dropbox\n\nPin board &lt;pins_board_folder&gt;\nPath: '~/Dropbox/Pins'\nCache size: 0\n\n\nLet’s suppose I want to pin a copy of the mpg data from the ggplot2 package as a csv file. I’d do that by calling pin_write():\n\npin_write(\n  board = board_dropbox,\n  x = ggplot2::mpg,\n  name = \"mpg\",\n  type = \"csv\"\n)\n\nCreating new version '20230613T004154Z-26aa0'\nWriting to pin 'mpg'\n\n\nThe pin_write() function allows you to supply more information and metadata than this, but – again – I’m not going to bother with all that here. The main thing for my purposes is that pin_write() has indeed written some files to disk:\n\nfs::dir_ls(\"~/Dropbox/Pins\", recurse = TRUE)\n\n/home/danielle/Dropbox/Pins/mpg\n/home/danielle/Dropbox/Pins/mpg/20230613T004154Z-26aa0\n/home/danielle/Dropbox/Pins/mpg/20230613T004154Z-26aa0/data.txt\n/home/danielle/Dropbox/Pins/mpg/20230613T004154Z-26aa0/mpg.csv\n\n\nOkay that’s easy enough. The next step is to check if I can read the data back to R. That’s very easy to do with pin_read(). When I pass the board (board_dropbox) and the name of the pin (\"mpg\") to this function, it returns the data:\n\npin_read(board_dropbox, \"mpg\") |&gt; tibble::as_tibble()\n\n# A tibble: 234 × 11\n   manufacturer model  displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4       1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4       1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4       2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4       2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4       2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4       2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4       3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 qu…   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 qu…   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 qu…   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nYay!\nThere’s quite a bit going on under the hood, and I’m glossing over all sorts of details related to metadata, versioning and so on. There’s a reason why pins doesn’t simply dump the csv file into the root folder and call it a day. The extra stuff serves a purpose. But let’s not worry about that for now. What matters is that it works.\nUnfortunately, my job is not quite finished. What I haven’t done yet is create a manifest file that summarises the content of the board. Fortunately that’s also easy to do:\n\nwrite_board_manifest(board_dropbox)\n\nManifest file written to root folder of board, as `_pins.yaml`\n\n\nThis creates a _pins.yaml file that contains the relevant summary. The pins package doesn’t automate this part of the process, so you need to call write_board_manifest() every time you modify the pins stored in the board. But that’s pretty easy to do.\nAll in all, quite painless, and the really nice thing about it is that this dead-simple workflow is perfectly sufficient for my immediate needs. I can now use my ~/Dropbox/Pins folder as the place to keep my big data files without messing up my git workflow at all. The pins package provides a suite of tools that allow me to manipulate pins, but this isn’t the post for that. All I care about right now is convincing myself that yes, this actually will address my immediate concern.\nYay again! Problem solved.\nWell, sort of."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#making-a-huge-tiny-mistake",
    "href": "posts/2023-06-12_pins-and-needles/index.html#making-a-huge-tiny-mistake",
    "title": "Pins and needles",
    "section": "Making a huge tiny mistake…",
    "text": "Making a huge tiny mistake…\nThe shortcoming of this approach is that it only works for me. Although my ~/Dropbox/Pins folder is backed up, thereby ensuring I don’t lose my data files, it’s not shared with anyone else. One solution to this would of course be to share the folder. Dropbox supports that, and this would work perfectly well for small-group collaboration. If I’m working with a bunch of people on a project and we need shared copies of a data file, I could share the folder with them and everything would work for them too. But it’s not a great solution to the problem, and it’s not ideal if I want to create a public repository of data files that anyone can access. Yes, you can do that with Dropbox, but it’s not really what the service is supposed to be used for. To my mind, a better approach in that situation is to create a file storage bucket with Amazon AWS S3, or Google Cloud Storage, or whatever. If I then make that bucket publicly readable, anyone can access my pins. That seems like a much better plan to me.\nUnfortunately, it’s at this point I made a tiiiiiny mistake. I decided to use Google Cloud Storage to host my buckets because that’s the cloud provider I’m most familiar with… but unbeknownst to me at the time it turns out that support for GCS buckets in the pins package is a work in progress. It mostly works but there’s a few features still missing, and I had to resort to a truly awful hack in order to get what I wanted.\nBut I’m getting ahead of myself. Let’s start out by creating the buckets and see what happens…"
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#creating-and-configuring-the-storage-buckets",
    "href": "posts/2023-06-12_pins-and-needles/index.html#creating-and-configuring-the-storage-buckets",
    "title": "Pins and needles",
    "section": "Creating and configuring the storage buckets",
    "text": "Creating and configuring the storage buckets\nOver the years I’ve done quite a bit of work using Google Cloud Platform. Small scale stuff, generally, but nevertheless I’ve had enough experience with it that I’m fairly comfortable with the gcloud command line tool. So naturally, the first thing I did was create my buckets from the terminal.3 This isn’t supposed to be a gcloud tutorial, but in case you’re interested there’s some relevant documentation that explains what I did. Everything in google cloud takes place within a project, specified by project ID (in this case pins-389407). For my own purposes I have two buckets in my pins project: djnavarro-pins is where I store publicly accessible data, and djnavarro-private-pins is where I keep private pins. Rather than set defaults, I have a tendency to do everything explicitly, so the --project flag is set in each command, as is the --location flag used to specify that I want my data to be stored in Sydney (also known as australia-southeast1). Anyway, here’s the command to create the bucket:\ngcloud storage buckets create gs://djnavarro-pins/ \\\n  --project pins-389407 \\\n  --location australia-southeast1 \\\n  --uniform-bucket-level-access\nThe --uniform-bucket-level-access flag is used to indicate that I’m not doing fancy file-specific access control. I’m too lazy or simple-minded for that: I want one bucket to be public, and another bucket to be private. By default the buckets are private, and to make all files in the bucket publicly readable (relevant documentation) the command I want is this:\ngcloud storage buckets add-iam-policy-binding gs://djnavarro-pins/ \\\n  --member=allUsers \\\n  --role=roles/storage.objectViewer\nCool. Now I have a machine gun. Um, sorry. Now I have a public GCS bucket. Yes. That’s what I meant. Let’s use it to store my pins."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#writing-pins-to-a-board-on-google-cloud-storage",
    "href": "posts/2023-06-12_pins-and-needles/index.html#writing-pins-to-a-board-on-google-cloud-storage",
    "title": "Pins and needles",
    "section": "Writing pins to a board on Google Cloud Storage",
    "text": "Writing pins to a board on Google Cloud Storage\nFor the moment I’m going to look at this solely from my perspective: as the owner of this public bucket, how can I use it to store pins? I’m not going to concern myself with how other people can access it right now. I’ll get to that at the end of the post.\nLet’s say I want to pin the diamonds data from the ggplot2 package as a csv file. In order to write anything to my bucket, I need to authenticate with google. I find this step clunky but doable. The first thing I need to do is obtain an authentication token, which I can obtain with the help of the gargle package:\n\nlibrary(gargle)\nscope &lt;- \"https://www.googleapis.com/auth/cloud-platform\"\ntoken &lt;- token_fetch(scopes = scope)\n\nThis workflow is designed for interactive use so there’s a confirmation process to follow. Once that’s done I can authenticate with the googleCloudStorageR package by passing the token to the gcs_auth() function. Once that’s done I can list the contents of my bucket with gcs_list_objects():\n\nlibrary(googleCloudStorageR)\ngcs_auth(token = token)\ngcs_list_objects(bucket = \"gs://djnavarro-pins\")\n\n\n                                                      name      size             updated\n1                                               _pins.yaml 161 bytes 2023-06-12 07:33:04\n2              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11\n3       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11\n4             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05\n5     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26\n6           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44\n7 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44\n\nAs you can see from this file listing, there are already a few pins stored in this bucket. That’s because I’d been using it in my previous post, which was indeed the project in which I’d made my huge mistake.\nThe googleCloudStorageR package has quite a bit of functionality built into it, and I can read and write whatever I want to the bucket using this interface. However, what I really want to do is abstract over the low level processes using the pins package. So, very similar to what I did with my local dropbox folder, I’ll use the board_gcs() function to treat the gs://djnavarro-pins bucket as a board:\n\nlibrary(pins)\nboard &lt;- board_gcs(\"gs://djnavarro-pins\")\n\nBecause I’ve authenticated and because I have write access to the bucket, I can write my pins directly to the board using pin_write(). The command for a board stored as a GCS bucket is essentially identical to the command I used with my local folder earlier:\n\npin_write(\n  board, \n  ggplot2::diamonds, \n  name = \"diamonds\", \n  type = \"csv\"\n)\n\nUnder the hood the work is done by googleCloudStorageR, and when I run this command it whines a little about needing predefinedAcl = \"bucketLevel\", but it’s smart enough to get the job done anyway. In principle what I should be doing to prevent the warning messages is pass this argument to pin_write() via the dots, but unfortunately pin_write() appears to be a bit trigger happy and it throws an error if I do that. As far as I can tell what’s happening is that pins is incorrectly guessing that the argument is misspelled, so for the time being I’ll just have to live with googleCloudStorageR grumbling at me.\nWhat really matters here is that it works. We can verify that the files have been written as follows:\n\ngcs_list_objects(bucket = \"gs://djnavarro-pins\")\n\n\n                                                      name      size             updated\n1                                               _pins.yaml 161 bytes 2023-06-12 07:33:04\n2                 diamonds/20230612T073405Z-c9e9b/data.txt 189 bytes 2023-06-12 07:34:05\n3             diamonds/20230612T073405Z-c9e9b/diamonds.csv    2.6 Mb 2023-06-12 07:34:14\n4              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11\n5       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11\n6             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05\n7     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26\n8           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44\n9 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44\n\nBrilliant! It does work! The files for the diamonds pin have been written. Everything is fine. Except… it isn’t. Notice that the _pins.yaml manifest file is still 161 bytes in size. It hasn’t been update to add an entry for the diamonds data, which we can confirm by downloading the file directly using googleCloudStorageR::gcs_get_object():\n\ngcs_get_object(\"gs://djnavarro-pins/_pins.yaml\") |&gt; \n  yaml::as.yaml() |&gt;\n  cat()\n\n✔ Downloaded and parsed _pins.yaml into R object of class: character\n|\n  warfpk_data:\n  - warfpk_data/20230610T142554Z-b8888/\n  warfpk_draws:\n  - warfpk_draws/20230610T142202Z-5bd80/\n  warfpk_summary:\n  - warfpk_summary/20230610T083635Z-340c1/\nOkay, fair enough. That’s not at all surprising, because I haven’t called write_board_manifest() to update the manifest file yet. Unfortunately, this is the moment at which I had to resort to a terrible horrible no good very bad hack.4\nThis is the “needles” part of my “pins and needles” story."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#the-terrible-horrible-no-good-very-bad-hack",
    "href": "posts/2023-06-12_pins-and-needles/index.html#the-terrible-horrible-no-good-very-bad-hack",
    "title": "Pins and needles",
    "section": "The terrible horrible no good very bad hack",
    "text": "The terrible horrible no good very bad hack\nThere’s a bit of a painful thing that follows because pins doesn’t currently have working pin_list() method for google cloud storage. This in turn means that I can’t currently use write_board_manifest() to write a manifest file for my board, because it relies on the pin_list() function to do part of the work. That’s a huge pain. A little browsing on github reassures me that the developers are well aware of the problem, and addressing this is indeed on the to-do list. Awesome. I’m a patient person, and happy to wait for it to be addressed. Unfortunately patience doesn’t solve my problem in the here-and-now, so while I’m waiting I decided to put together some helper functions that are good enough for my immediate needs. This pin_list_gcs() function returns a character vector with the names of my stored pins:\n\npin_list_gcs &lt;- function(board, ...) {\n  googleCloudStorageR::gcs_list_objects(bucket = board$bucket)$name |&gt; \n    grep(pattern = \"/\", x = _, value = TRUE) |&gt; \n    gsub(pattern = \"/.*\", replacement = \"\", x = _) |&gt;\n    unique()\n}\n\nNow a little bit of evil, in which I do the thing you should never ever do. I’m going to mess with the internals of the pins package and overwrite its internal pins_list.pins_board_gcs() function.\n\nunlockBinding(as.symbol(\"pin_list.pins_board_gcs\"), getNamespace(\"pins\"))\nassignInNamespace(\"pin_list.pins_board_gcs\", pin_list_gcs, \"pins\")\n\nI am a terrible person.\nBut you knew that already, so let’s move on.\nMy next step is to write an S3 method that allows me to write a manifest file for my :\n\nwrite_board_manifest_yaml.pins_board_gcs &lt;- function(board, manifest, ...) {\n  temp_file &lt;- withr::local_tempfile()\n  yaml::write_yaml(manifest, file = temp_file)\n  googleCloudStorageR::gcs_upload(\n    file = temp_file, \n    bucket = board$bucket, \n    type = \"text/yaml\",\n    name = \"_pins.yaml\"\n  )\n}\n\nNow this works:\n\nwrite_board_manifest_yaml(board, manifest = pins:::make_manifest(board))\n\nWe can verify that we’ve written the updated file. For reasons that escape me, this won’t work unless you start a new session and re-authenticate in the process5. If you don’t do that you’ll just get the old version of the manifest file. I have no idea why. Anyway here’s the result:\n\ngcs_get_object(\"gs://djnavarro-pins/_pins.yaml\") |&gt; \n  yaml::as.yaml() |&gt;\n  cat()\n\n✔ Downloaded and parsed _pins.yaml into R object of class: character\n|\n  diamonds:\n  - diamonds/20230612T071111Z-c9e9b/\n  warfpk_data:\n  - warfpk_data/20230610T142554Z-b8888/\n  warfpk_draws:\n  - warfpk_draws/20230610T142202Z-5bd80/\n  warfpk_summary:\n  - warfpk_summary/20230610T083635Z-340c1/\nSimilarly, if we now list the contents of the bucket we can see that it’s all been updated:\n\ngcs_list_objects(bucket = \"gs://djnavarro-pins\")\n\n\n                                                      name      size             updated\n1                                               _pins.yaml 206 bytes 2023-06-12 07:39:53\n2                 diamonds/20230612T073405Z-c9e9b/data.txt 189 bytes 2023-06-12 07:34:05\n3             diamonds/20230612T073405Z-c9e9b/diamonds.csv    2.6 Mb 2023-06-12 07:34:14\n4              warfpk_data/20230610T142554Z-b8888/data.txt 190 bytes 2023-06-10 14:26:11\n5       warfpk_data/20230610T142554Z-b8888/warfpk_data.csv      8 Kb 2023-06-10 14:26:11\n6             warfpk_draws/20230610T142202Z-5bd80/data.txt 200 bytes 2023-06-10 14:23:05\n7     warfpk_draws/20230610T142202Z-5bd80/warfpk_draws.csv  281.4 Mb 2023-06-10 14:25:26\n8           warfpk_summary/20230610T083635Z-340c1/data.txt 200 bytes 2023-06-10 08:38:44\n9 warfpk_summary/20230610T083635Z-340c1/warfpk_summary.csv    1.2 Mb 2023-06-10 08:38:44\n\nYup, it works. But I’m under no illusions. This is not a good workflow. Messing around with the internals of someone else’s package is… bad. Very bad. I do not recommend anyone do this. This is purely a temporary fix I’m adopting for the time being while GCS support in pins is incomplete.\nOkay, whatever. Let’s move on, shall we? Ugly hack notwithstanding, I now have a workflow that allows me to use a public GCS bucket to store my pins. It’s good enough for now, so I’ll now take a look at how other people can read pins from this bucket."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#the-read-only-workflow",
    "href": "posts/2023-06-12_pins-and-needles/index.html#the-read-only-workflow",
    "title": "Pins and needles",
    "section": "The read-only workflow",
    "text": "The read-only workflow\nThe nice thing about cloud storage services like AWS S3 and GCS is that they provide public, human readable URLs for shared files. My djnavarro-pins bucket, for instance, is publicly available at https://storage.googleapis.com/djnavarro-pins/. Any R users who wants to use one of my pins can do so using the pins::board_url() function. We can create a read-only board by passing it the path to the manifest file, like this:\n\nread_only_board &lt;- board_url(\n  \"https://storage.googleapis.com/djnavarro-pins/_pins.yaml\"\n)\n\nSince this other person isn’t me and doesn’t know what pins are stored here, they might want to use the pin_search() function to get a listing of all the pins, like this:\n\npin_search(read_only_board)\n\n# A tibble: 4 × 6\n  name           type  title        created             file_size meta      \n  &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;        &lt;dttm&gt;              &lt;fs::byt&gt; &lt;list&gt;    \n1 diamonds       csv   diamonds: a… 2023-06-12 17:34:05     2.64M &lt;pins_met&gt;\n2 warfpk_data    csv   warfpk_data… 2023-06-11 00:25:54     8.01K &lt;pins_met&gt;\n3 warfpk_draws   csv   warfpk_draw… 2023-06-11 00:22:02   281.39M &lt;pins_met&gt;\n4 warfpk_summary csv   warfpk_summ… 2023-06-10 18:36:35     1.15M &lt;pins_met&gt;\n\n\nCool. Now that we know what pins are available, we can read the data stored in the “diamonds” pin directly into R by calling pin_read():\n\npin_read(read_only_board, \"diamonds\") |&gt;\n  tibble::as_tibble()\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\nA nice thing about using pin_read() rather than downloading the file directly is that the pins package automatically creates a local cache of previously-downloaded pins. You don’t have to download the same file over and over when re-running your code. I’m not going to go into those details here, but I will mention that it also provides functions that let you manage this cache without much difficulty. From this perspective, everything “just works”. It’s really quite nice."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#epilogue",
    "href": "posts/2023-06-12_pins-and-needles/index.html#epilogue",
    "title": "Pins and needles",
    "section": "Epilogue",
    "text": "Epilogue\nOn the whole I’m very happy with pins. It solves a real problem I have, and the interface is nice. I’ll probably use it for other things in the future. The specific issue I ran into with GCS boards is an annoyance, but one that I’m pretty certain will vanish as GCS support in pins matures. No big deal.\nThat being said, because I am very uncomfortable with the fact that I’ve published my terrible horrible no good very bad hack to the internet, I will say it again… as a general long-term strategy, my workflow is terrible and I have no intention whatsoever of relying on it. Nobody else should rely on my terrible horrible no good very bad hack. I implemented it as a hotfix, nothing else. Under no circumstances is it wise to rely on a method that fucks around with the internals of a package.\nAnyways… that’s about all I had to say really."
  },
  {
    "objectID": "posts/2023-06-12_pins-and-needles/index.html#footnotes",
    "href": "posts/2023-06-12_pins-and-needles/index.html#footnotes",
    "title": "Pins and needles",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIndeed, GitHub won’t let you push a file that large to the remote repository unless you use large file storage, which is not a thing I want to do.↩︎\nIn this case what I really mean by “personal” isn’t that the files are sensitive in any way, just that I’m not trying to share them with anyone else. My workflow is designed to allow me to reproduce anything I’ve done in the past if my laptop mysteriously catches fire. At this point I’m not at all concerned about whether anyone else can reproduce it.↩︎\nAs a disclaimer: I am not a fan of so-called “command line bullshittery”. There’s nothing special about working from the command line, and I feel obliged to point out that you don’t actually need to use the gcloud CLI to do any of this. The google cloud console lets you do the exact same thing with a GUI. No big deal. If that approach feels easier to you, go for it!↩︎\nSome hacks are like that. Even in Australia.↩︎\nActually, it does still work even without re-authentication if you explicitly specify the generation argument to gcs_get_object() but whatever.↩︎"
  },
  {
    "objectID": "posts/2025-01-01_schools-of-magic/index.html",
    "href": "posts/2025-01-01_schools-of-magic/index.html",
    "title": "The schools of magic",
    "section": "",
    "text": "In the dying days of 2024 I found myself doing Tidy Tuesday again. I really like playing around with data wrangling and visualisation in these exercises, and I wish I were able to find the time to do it more often. But life so often gets in the way, and I suppose I should simply be happy that I get to do it sometimes. Anyway, one of the recent Tidy Tuesday data sets is related to Dungeons & Dragons, which I used to play when I was a teenager, and my children are pestering me to play again now that they’re old enough to get into it. So I decided to play around with this data set as part of a totally unrelated side project (for a different post!) and I ended up posting these two images to social media:\nA couple of people emailed me asking for the code for these, and I haven’t gotten around to replying to them yet. Part of the reason I didn’t reply initially was that the code was tangled up with the code for the other side project, which made it a little awkward to explain without going down a whole other rabbit hole.\nSo now I find myself rewriting the code in a slightly more palatable form and posting it to this blog. I mean, why not? It’s the first day of a new year, and I have nothing better to do this morning.\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(readr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(ggrepel)\nlibrary(legendry)"
  },
  {
    "objectID": "posts/2025-01-01_schools-of-magic/index.html#the-spells-data",
    "href": "posts/2025-01-01_schools-of-magic/index.html#the-spells-data",
    "title": "The schools of magic",
    "section": "The spells data",
    "text": "The spells data\nThe spells data that I’m using here comes from the TidyTuesday D&D Spells data set. The data set was compiled by Jon Harmon, and originates in the recently released Dungeons & Dragons Free Rules. If you’ve played D&D before, this should be quite familiar:\n\nspells &lt;- read_csv(\"./spells.csv\", show_col_types = FALSE)\nprint(spells)\n\n# A tibble: 314 × 27\n   name      level school bard  cleric druid paladin ranger sorcerer warlock\n   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;lgl&gt; &lt;lgl&gt;  &lt;lgl&gt; &lt;lgl&gt;   &lt;lgl&gt;  &lt;lgl&gt;    &lt;lgl&gt;  \n 1 Acid Spl…     0 evoca… FALSE FALSE  FALSE FALSE   FALSE  TRUE     FALSE  \n 2 Aid           2 abjur… TRUE  TRUE   TRUE  TRUE    TRUE   FALSE    FALSE  \n 3 Alarm         1 abjur… FALSE FALSE  FALSE FALSE   TRUE   FALSE    FALSE  \n 4 Alter Se…     2 trans… FALSE FALSE  FALSE FALSE   FALSE  TRUE     FALSE  \n 5 Animal F…     1 encha… TRUE  FALSE  TRUE  FALSE   TRUE   FALSE    FALSE  \n 6 Animal M…     2 encha… TRUE  FALSE  TRUE  FALSE   TRUE   FALSE    FALSE  \n 7 Animal S…     8 trans… FALSE FALSE  TRUE  FALSE   FALSE  FALSE    FALSE  \n 8 Animate …     3 necro… FALSE TRUE   FALSE FALSE   FALSE  FALSE    FALSE  \n 9 Animate …     5 trans… TRUE  FALSE  FALSE FALSE   FALSE  TRUE     FALSE  \n10 Antilife…     5 abjur… FALSE FALSE  TRUE  FALSE   FALSE  FALSE    FALSE  \n# ℹ 304 more rows\n# ℹ 17 more variables: wizard &lt;lgl&gt;, casting_time &lt;chr&gt;, action &lt;lgl&gt;,\n#   bonus_action &lt;lgl&gt;, reaction &lt;lgl&gt;, ritual &lt;lgl&gt;,\n#   casting_time_long &lt;chr&gt;, trigger &lt;chr&gt;, range &lt;chr&gt;, range_type &lt;chr&gt;,\n#   verbal_component &lt;lgl&gt;, somatic_component &lt;lgl&gt;,\n#   material_component &lt;lgl&gt;, material_component_details &lt;chr&gt;,\n#   duration &lt;chr&gt;, concentration &lt;lgl&gt;, description &lt;chr&gt;\n\n\nIf you don’t already know what you’re looking at, here’s a quick summary of the columns that I’m actually using for these plots:\n\nname: the name of the spell, obviously\nlevel: the spell difficulty level, with level 0 spells (cantrips) being the easiest and level 9 being the most difficult\nschool: which of the nine schools of magic (e.g., enhantment, illusion, necromancy, etc) does this spell belong to?\nbard, cleric, druid, etc: is this spell learnable by characters that belong to this class?\ndescription: the text for the spell description\n\nThe values stored in most of these variables are pretty straightforward, but it’s probably useful to look at the description variable in particular since that one is a long string. Here’s a few spell descriptions:\n\ndescribe_spell &lt;- function(spell = NULL) {\n  if (is.null(spell)) spell &lt;- sample(spells$name, 1L)\n  spells |&gt; \n    filter(name == spell) |&gt; \n    pull(description) |&gt; \n    str_wrap(width = 70) |&gt; \n    cat()\n}\n\ndescribe_spell(\"Acid Splash\")\n\nYou create an acidic bubble at a point within range, where it explodes\nin a 5-foot-radius Sphere. Each creature in that Sphere must succeed\non a Dexterity saving throw or take 1d6 Acid damage. Cantrip Upgrade.\nThe damage increases by 1d6 when you reach levels 5 (2d6), 11 (3d6),\nand 17 (4d6).\n\ndescribe_spell(\"Fireball\")\n\nA bright streak flashes from you to a point you choose within range\nand then blossoms with a low roar into a fiery explosion. Each\ncreature in a 20-foot-radius Sphere centered on that point makes a\nDexterity saving throw, taking 8d6 Fire damage on a failed save or\nhalf as much damage on a successful one. Flammable objects in the area\nthat aren’t being worn or carried start burning. Using a Higher-Level\nSpell Slot. The damage increases by 1d6 for each spell slot level\nabove 3.\n\ndescribe_spell(\"Prismatic Spray\")\n\nEight rays of light flash from you in a 60-foot Cone. Each creature\nin the Cone makes a Dexterity saving throw. For each target, roll 1d8\nto determine which color ray affects it, consulting the Prismatic Rays\ntable. Prismatic Rays 1d8 Ray 1 Red. Failed Save: 12d6 Fire damage.\nSuccessful Save: Half as much damage. 2 Orange. Failed Save: 12d6 Acid\ndamage. Successful Save: Half as much damage. 3 Yellow. Failed Save:\n12d6 Lightning damage. Successful Save: Half as much damage. 4 Green.\nFailed Save: 12d6 Poison damage. Successful Save: Half as much damage.\n5 Blue. Failed Save: 12d6 Cold damage. Successful Save: Half as much\ndamage. 6 Indigo. Failed Save: The target has the Restrained condition\nand makes a Constitution saving throw at the end of each of its turns.\nIf it successfully saves three times, the condition ends. If it fails\nthree times, it has the Petrified condition until it is freed by an\neffect like the Greater Restoration spell. The successes and failures\nneedn’t be consecutive; keep track of both until the target collects\nthree of a kind. 7 Violet. Failed Save: The target has the Blinded\ncondition and makes a Wisdom saving throw at the start of your next\nturn. On a successful save, the condition ends. On a failed save,\nthe condition ends, and the creature teleports to another plane of\nexistence (DM’s choice). 8 Special. The target is struck by two rays.\nRoll twice, rerolling any 8.\n\n\n\n\n\n\n\nD&D Boxed Set: Basic Rules"
  },
  {
    "objectID": "posts/2025-01-01_schools-of-magic/index.html#the-spell-dice-plot",
    "href": "posts/2025-01-01_schools-of-magic/index.html#the-spell-dice-plot",
    "title": "The schools of magic",
    "section": "The spell dice plot",
    "text": "The spell dice plot\nReading these spell descriptions I had the thought that it would be interesting to explore the distribution of dice rolls mentioned in the spell descriptions. In the description for “Acid Splash”, for example, it refers to a 1d6 roll, a 2d6 roll, a 3d6 roll, and a 4d6 roll. “Fireball” mentions 8d6 and 1d6 in the text. What does the distribution of these dice rolls look like.\n\nData wrangling\nTo start with let’s think about what information we want to extract from the spell descriptions, and how we want to do it. Using str_extract_all() and a bit of regular expression sorcery, we can pull this information out of a description. Here’s what we get for the three spells I listed above:\n\nspells |&gt; \n  filter(name %in% c(\"Acid Splash\", \"Fireball\", \"Prismatic Spray\")) |&gt; \n  pull(description) |&gt; \n  str_extract_all(\"\\\\b\\\\d+d\\\\d+\\\\b\")\n\n[[1]]\n[1] \"1d6\" \"1d6\" \"2d6\" \"3d6\" \"4d6\"\n\n[[2]]\n[1] \"8d6\" \"1d6\"\n\n[[3]]\n[1] \"1d8\"  \"1d8\"  \"12d6\" \"12d6\" \"12d6\" \"12d6\" \"12d6\"\n\n\nThis list highlights a decision we need to make about counting tokens: in the “Prismatic Spray” description, there are five separate mentions of a 12d6 dice roll. Do we want to count all five of these, or to we want to consider unique mentions only? After a bit of thought I ended up going with the latter, so the extraction code would look like this:\n\nspells |&gt; \n  filter(name %in% c(\"Acid Splash\", \"Fireball\", \"Prismatic Spray\")) |&gt; \n  pull(description) |&gt; \n  str_extract_all(\"\\\\b\\\\d+d\\\\d+\\\\b\") |&gt; \n  map(unique)\n\n[[1]]\n[1] \"1d6\" \"2d6\" \"3d6\" \"4d6\"\n\n[[2]]\n[1] \"8d6\" \"1d6\"\n\n[[3]]\n[1] \"1d8\"  \"12d6\"\n\n\nDoing this within a data frame produces a dice_txt list column:\n\nspells |&gt;\n  select(name, level, description) |&gt;\n  mutate(\n    dice_txt = description |&gt; \n      str_extract_all(\"\\\\b\\\\d+d\\\\d+\\\\b\") |&gt; \n      map(unique)\n  )\n\n# A tibble: 314 × 4\n   name              level description                              dice_txt\n   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                                    &lt;list&gt;  \n 1 Acid Splash           0 \"You create an acidic bubble at a point… &lt;chr&gt;   \n 2 Aid                   2 \"Choose up to three creatures within ra… &lt;chr&gt;   \n 3 Alarm                 1 \"You set an alarm against intrusion. Ch… &lt;chr&gt;   \n 4 Alter Self            2 \"You alter your physical form. Choose o… &lt;chr&gt;   \n 5 Animal Friendship     1 \"Target a Beast that you can see within… &lt;chr&gt;   \n 6 Animal Messenger      2 \"A Tiny Beast of your choice that you c… &lt;chr&gt;   \n 7 Animal Shapes         8 \"Choose any number of willing creatures… &lt;chr&gt;   \n 8 Animate Dead          3 \"Choose a pile of bones or a corpse of … &lt;chr&gt;   \n 9 Animate Objects       5 \"Objects animate at your command. Choos… &lt;chr&gt;   \n10 Antilife Shell        5 \"An aura extends from you in a 10-foot … &lt;chr&gt;   \n# ℹ 304 more rows\n\n\nList columns aren’t the easiest thing to visualise, so I’ll use the unnest_longer() function to produce a tidy data set that has one row per unique dice roll mention:\n\nspells |&gt;\n  select(name, level, description) |&gt;\n  mutate(\n    dice_txt = description |&gt; \n      str_extract_all(\"\\\\b\\\\d+d\\\\d+\\\\b\") |&gt; \n      map(unique)\n  ) |&gt; \n  unnest_longer(\n    col = \"dice_txt\",\n    values_to = \"dice_txt\",\n    indices_to = \"position\"\n  )\n\n# A tibble: 236 × 5\n   name            level description                       dice_txt position\n   &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;                             &lt;chr&gt;       &lt;int&gt;\n 1 Acid Splash         0 \"You create an acidic bubble at … 1d6             1\n 2 Acid Splash         0 \"You create an acidic bubble at … 2d6             2\n 3 Acid Splash         0 \"You create an acidic bubble at … 3d6             3\n 4 Acid Splash         0 \"You create an acidic bubble at … 4d6             4\n 5 Alter Self          2 \"You alter your physical form. C… 1d6             1\n 6 Animate Objects     5 \"Objects animate at your command… 1d4             1\n 7 Animate Objects     5 \"Objects animate at your command… 1d6             2\n 8 Animate Objects     5 \"Objects animate at your command… 1d12            3\n 9 Animate Objects     5 \"Objects animate at your command… 2d6             4\n10 Animate Objects     5 \"Objects animate at your command… 2d12            5\n# ℹ 226 more rows\n\n\nWith a little more data wrangling, we can carve up a dice_txt value like “3d6” into the number of dice to be rolled (i.e. die_num is 3), the number of sides on the die to be rolled (i.e., die_die is 6), and the average value that you’d get from rolling these dice (i.e., die_val is 10.5). Here’s the final processed data:\n\ndice_dat &lt;- spells |&gt;\n  select(name, level, description) |&gt;\n  mutate(\n    dice_txt = description |&gt; \n      str_extract_all(\"\\\\b\\\\d+d\\\\d+\\\\b\") |&gt; \n      map(unique)\n  ) |&gt; \n  unnest_longer(\n    col = \"dice_txt\",\n    values_to = \"dice_txt\",\n    indices_to = \"position\"\n  ) |&gt;\n  mutate(\n    dice_num = dice_txt |&gt; str_extract(\"\\\\d+(?=d)\") |&gt; as.numeric(),\n    dice_die = dice_txt |&gt; str_extract(\"(?&lt;=d)\\\\d+\") |&gt; as.numeric(),\n    dice_val = dice_num * (dice_die + 1)/2,\n    dice_txt = factor(dice_txt) |&gt; fct_reorder(dice_val)\n  )\n\nprint(dice_dat)\n\n# A tibble: 236 × 8\n   name       level description dice_txt position dice_num dice_die dice_val\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;       &lt;fct&gt;       &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 Acid Spla…     0 \"You creat… 1d6             1        1        6      3.5\n 2 Acid Spla…     0 \"You creat… 2d6             2        2        6      7  \n 3 Acid Spla…     0 \"You creat… 3d6             3        3        6     10.5\n 4 Acid Spla…     0 \"You creat… 4d6             4        4        6     14  \n 5 Alter Self     2 \"You alter… 1d6             1        1        6      3.5\n 6 Animate O…     5 \"Objects a… 1d4             1        1        4      2.5\n 7 Animate O…     5 \"Objects a… 1d6             2        1        6      3.5\n 8 Animate O…     5 \"Objects a… 1d12            3        1       12      6.5\n 9 Animate O…     5 \"Objects a… 2d6             4        2        6      7  \n10 Animate O…     5 \"Objects a… 2d12            5        2       12     13  \n# ℹ 226 more rows\n\n\nNotice that this processed version of the data codes dice_txt as a factor, and the levels are arranged by increasing dice_val. That’s the mechanism by which the die rolls will be ordered sensibly in the plot.\n\n\n\n\n\nD&D Boxed Set: Expert Rules\n\n\n\n\nMaking the plot\nA basic version of the plot we want to create looks like this:\n\nggplot(dice_dat, aes(dice_txt, fill = factor(level))) + geom_bar()\n\n\n\n\n\n\n\n\nThere’s a lot of tinkering required to make it look pretty, but this is essentially the data we want to display and the format we want it to be displayed in. The full code for the visualisation looks like this:\n\npalette &lt;- hcl.colors(n = 10, palette = \"PuOr\")\n\nlabs &lt;- dice_dat |&gt;\n  summarise(\n    dice_txt = first(dice_txt),\n    count = n(),\n    .by = dice_txt\n  )\n\npic &lt;- ggplot(\n  data = dice_dat,\n  mapping = aes(\n    x = dice_txt,\n    fill = factor(level)\n  )\n) +\n  geom_bar(color = \"#222\") +\n  geom_label_repel(\n    data = labs,\n    mapping = aes(\n      x = dice_txt,\n      y = count,\n      label = dice_txt\n    ),\n    size = 3,\n    direction = \"y\",\n    seed = 1,\n    nudge_y = 4,\n    color = \"#ccc\",\n    fill = \"#222\",\n    arrow = NULL,\n    inherit.aes = FALSE\n  ) +\n  scale_fill_manual(\n    name = \"Spell level\",\n    values = palette\n  ) +\n  scale_x_discrete(\n    name = \"Increasing average outcome \\u27a1\",\n    breaks = NULL,\n    expand = expansion(.05)\n  ) +\n  scale_y_continuous(name = NULL) +\n  labs(title = \"Dice rolls in D&D spell descriptions by spell level\") +\n  theme_void() +\n  theme(\n    plot.background = element_rect(fill = \"#222\"),\n    text = element_text(color = \"#ccc\"),\n    axis.text = element_text(color = \"#ccc\"),\n    axis.title = element_text(color = \"#ccc\"),\n    plot.margin = unit(c(1, 1, 1, 1), units = \"cm\"),\n    legend.position = \"inside\",\n    legend.position.inside = c(.3, .825),\n    legend.direction = \"horizontal\",\n    legend.title.position = \"top\",\n    legend.byrow = TRUE\n  )\n\nplot(pic)\n\n\n\n\n\n\n\n\nVery pretty.\nThere’s a small mystery that this plot leads to. As a general pattern, the low-level spells tend to mention die rolls with lower average value when compared to higher-level spells (i.e., there’s lots of brown and orange bars on the left, and mostly blueish/purple bars on the right). That makes sense, especially because the dice rolls mentioned in spell descriptions are usually describing the amount of damage the spell does.\nSo what’s the story with this one weird outlier, the level 0 spell that mentions a 12d6 roll? To answer this we need to work out which spell it is:\n\ndice_dat |&gt; \n  filter(level == 0 & dice_txt == \"12d6\") |&gt; \n  pull(name)\n\n[1] \"Druidcraft\"\n\n\nNow let’s look at the description:\n\ndescribe_spell(\"Druidcraft\")\n\nWhispering to the spirits of nature, you create one of the following\neffects within range. Weather Sensor. You create a Tiny, harmless\nsensory effect that predicts what the weather will be at your location\nfor the next 24 hours. The effect might manifest as a golden orb for\nclear skies, a cloud for rain, falling snowflakes for snow, and so on.\nThis effect persists for 1 round. Bloom. You instantly make a flower\nblossom, a seed pod open, or a leaf bud bloom. Sensory Effect. You\ncreate a harmless sensory effect, such as falling leaves, spectral\ndancing fairies, a gentle breeze, the sound of an animal, or the\nfaint odor of skunk. The effect must fit in a 5-foot Cube. Fire Play.\nYou light or snuff out a candle, a torch, or a campfire. Spells (E)\nEarthquake Level 8 Transmutation (Cleric, Druid, Sorcerer) Casting\nTime: Action Range: 500 feet Components: V, S, M (a fractured rock)\nDuration: Concentration, up to 1 minute Choose a point on the ground\nthat you can see within range. For the duration, an intense tremor\nrips through the ground in a 100-foot-radius circle centered on that\npoint. The ground there is Difficult Terrain. When you cast this spell\nand at the end of each of your turns for the duration, each creature\non the ground in the area makes a Dexterity saving throw. On a failed\nsave, a creature has the Prone condition, and its Concentration is\nbroken. You can also cause the effects below. Fissures. A total of 1d6\nfissures open in the spell’s area at the end of the turn you cast it.\nYou choose the fissures’ locations, which can’t be under structures.\nEach fissure is 1d10 × 10 feet deep and 10 feet wide, and it extends\nfrom one edge of the spell’s area to another edge. A creature in the\nsame space as a fissure must succeed on a Dexterity saving throw or\nfall in. A creature that successfully saves moves with the fissure’s\nedge as it opens. Structures. The tremor deals 50 Bludgeoning damage\nto any structure in contact with the ground in the area when you cast\nthe spell and at the end of each of your turns until the spell ends.\nIf a structure drops to 0 Hit Points, it collapses. A creature within\na distance from a collapsing structure equal to half the structure’s\nheight makes a Dexterity saving throw. On a failed save, the creature\ntakes 12d6 Bludgeoning damage, has the Prone condition, and is buried\nin the rubble, requiring a DC 20 Strength (Athletics) check as an\naction to escape. On a successful save, the creature takes half as\nmuch damage only.\n\n\nAh… now it makes more sense: it’s a parsing error from when the data set was constructed. The stats and description for “Earthquake” (an 8th level spell) has been appended to the description for “Druidcraft”, and the 12d6 roll in question isn’t for “Druidcraft” it’s for “Earthquake”.\nMystery solved.\n\n\n\n\n\nD&D Boxed Set: Companion Rules"
  },
  {
    "objectID": "posts/2025-01-01_schools-of-magic/index.html#the-schools-of-magic",
    "href": "posts/2025-01-01_schools-of-magic/index.html#the-schools-of-magic",
    "title": "The schools of magic",
    "section": "The schools of magic",
    "text": "The schools of magic\nOkay, so that’s the story behind the first plot. What about the second one, the one that looks like a heatmap with some dendrograms? The data wrangling for that one one is a little more elaborate, because we have to construct data for the heatmap and data for the dendrograms.\n\nData for the heatmap\nTo produce data for the heatmap, we select the relevant columns: i.e., those corresponding to the character classes, the school variable that denotes the school of magic for the spell, and the name variable because I like having an id column in my data. We then use pivot_longer() to arrange this data set in long form:\n\nspells_long &lt;- spells |&gt;\n  select(name, school, bard:wizard) |&gt;\n  pivot_longer(\n    cols = bard:wizard,\n    names_to = \"class\",\n    values_to = \"castable\"\n  ) \n\nprint(spells_long)\n\n# A tibble: 2,512 × 4\n   name        school     class    castable\n   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    &lt;lgl&gt;   \n 1 Acid Splash evocation  bard     FALSE   \n 2 Acid Splash evocation  cleric   FALSE   \n 3 Acid Splash evocation  druid    FALSE   \n 4 Acid Splash evocation  paladin  FALSE   \n 5 Acid Splash evocation  ranger   FALSE   \n 6 Acid Splash evocation  sorcerer TRUE    \n 7 Acid Splash evocation  warlock  FALSE   \n 8 Acid Splash evocation  wizard   TRUE    \n 9 Aid         abjuration bard     TRUE    \n10 Aid         abjuration cleric   TRUE    \n# ℹ 2,502 more rows\n\n\nNow we have a tidy data set with one row per “observation”, in the sense that it specifies whether a spell of a specific name (which belongs to a specific school), is in fact castable by members of a particular character class. We can summarise this by aggregating over the specific spells, and count the number of castable spells for each combination of magic school and character class:\n\ndat &lt;- spells_long |&gt;\n  summarise(\n    count = sum(castable),\n    .by = c(\"school\", \"class\")\n  ) |&gt;\n  mutate(\n    school = str_to_title(school),\n    class  = str_to_title(class)\n  )\n\nprint(dat)\n\n# A tibble: 64 × 3\n   school     class    count\n   &lt;chr&gt;      &lt;chr&gt;    &lt;int&gt;\n 1 Evocation  Bard         7\n 2 Evocation  Cleric      12\n 3 Evocation  Druid       17\n 4 Evocation  Paladin      3\n 5 Evocation  Ranger       3\n 6 Evocation  Sorcerer    30\n 7 Evocation  Warlock      4\n 8 Evocation  Wizard      30\n 9 Abjuration Bard        16\n10 Abjuration Cleric      33\n# ℹ 54 more rows\n\n\nThis dat data frame is suitable for plotting as a heat map with geom_tile(), so let’s now move to stage two of the data wrangling.\n\n\nDissimilarity data for the dendrograms\nThe data structure that we need at this step is slightly more complicated, because what we want to display on each axis is a hierarchical clustering, of the sort typically produced by hclust(). In a distant, distant past I actually wrote my PhD thesis on clustering and scaling tools used to represent item (dis)similarities, and as such I’m acutely aware that these tools are extremely sensitive to the way you define similarity (or dissimilarity, or distance, or association, or whatever…). So I’ll be a little careful here, because if you do this in a thoughtless way you get stupid answers.\nBefore I begin, I’ll quickly define a boring function that I’ll used when printing matrices. It’s not a very good function, but it works for the purposes I need it for in this post:\n\nprint_truncated &lt;- function(x) {\n  if (inherits(x, \"matrix\")) {\n    rownames(x) &lt;- str_trunc(rownames(x), width = 6, ellipsis = \".\")\n    colnames(x) &lt;- str_trunc(colnames(x), width = 6, ellipsis = \".\")\n  }\n  if (inherits(x, \"dist\")) {\n    attr(x, \"Labels\") &lt;- str_trunc(attr(x, \"Labels\"), width = 6, ellipsis = \".\")\n  }\n  print(round(x, digits = 3))\n}\n\nOkay, now let’s get to work on the data wrangling. We’ll start by reorganising the dat data frame into a matrix form. The mat matrix below contains the exact same information as the data frame: each cell in the matrix represents the number of castable spells for a specific combination of class and school.\n\nmat &lt;- dat |&gt;\n  pivot_wider(\n    names_from = \"school\",\n    values_from = \"count\"\n  ) |&gt;\n  as.data.frame()\n\nrownames(mat) &lt;- mat$class\nmat$class &lt;- NULL\nmat &lt;- as.matrix(mat)\n\nprint_truncated(mat)\n\n       Evoca. Abjur. Trans. Encha. Necro. Divin. Illus. Conju.\nBard        7     16     18     28      5     18     22      8\nCleric     12     33     13      8     14     17      1     11\nDruid      17     17     33      9      7     14      2     21\nPalad.      3     16      3      5      3      5      0      2\nRanger      3     11     13      3      1      9      1      7\nSorce.     30      7     33     13      9      8     14     19\nWarlo.      4      8      6     12     10      9     11      9\nWizard     30     22     41     15     18     19     26     24\n\n\nIn this matrix we have a measure of “affinity”, in the sense that larger values indicate a higher affinity between a class and a school. The tricky part here is that some classes are simply better at spellwork than others: clerics and wizards can both cast lots of spells; paladins and rangers cannot cast many. The kind of similarity that I have in mind here is not the boring “clerics and wizards are similar because they can both cast lots of spells” kind. What I really want to say is something like “paladins and clerics are similar because abjuration is the strongest school for both classes”. The same applies when thinking about the schools of magic: there are lots of transmutation spells and lots of abjuration spells. That doesn’t really make those schools similar, not in the sense I care about.\nWhat all this amounts to is an acknowledgement that we need to correct for overall prevalance, or – to frame it in probabilistic terms – to describe classes in terms of a “distribution over schools” and describe schools in terms of a “distribution over classes”. That gives us the following two matrices:\n\nclass_distro  &lt;- mat / replicate(ncol(mat), rowSums(mat))\nschool_distro &lt;- t(mat) / (replicate(nrow(mat), colSums(mat)))\n\nThe class_distro matrix is the one that describes classes as a distribution over schools, and you can see in the printout here that when described in this fashion the paladin row and the cleric row do look rather similar to each other:\n\nprint_truncated(class_distro)\n\n       Evoca. Abjur. Trans. Encha. Necro. Divin. Illus. Conju.\nBard    0.057  0.131  0.148  0.230  0.041  0.148  0.180  0.066\nCleric  0.110  0.303  0.119  0.073  0.128  0.156  0.009  0.101\nDruid   0.142  0.142  0.275  0.075  0.058  0.117  0.017  0.175\nPalad.  0.081  0.432  0.081  0.135  0.081  0.135  0.000  0.054\nRanger  0.062  0.229  0.271  0.062  0.021  0.188  0.021  0.146\nSorce.  0.226  0.053  0.248  0.098  0.068  0.060  0.105  0.143\nWarlo.  0.058  0.116  0.087  0.174  0.145  0.130  0.159  0.130\nWizard  0.154  0.113  0.210  0.077  0.092  0.097  0.133  0.123\n\n\nA similar phenomenon is observed in the school_distro matrix, where you can see that the rows for abjuration and divination are quite similar despite the fact that there are a lot more abjuration spells than divination spells:\n\nprint_truncated(school_distro)\n\n        Bard Cleric Druid Palad. Ranger Sorce. Warlo. Wizard\nEvoca. 0.066  0.113 0.160  0.028  0.028  0.283  0.038  0.283\nAbjur. 0.123  0.254 0.131  0.123  0.085  0.054  0.062  0.169\nTrans. 0.112  0.081 0.206  0.019  0.081  0.206  0.038  0.256\nEncha. 0.301  0.086 0.097  0.054  0.032  0.140  0.129  0.161\nNecro. 0.075  0.209 0.104  0.045  0.015  0.134  0.149  0.269\nDivin. 0.182  0.172 0.141  0.051  0.091  0.081  0.091  0.192\nIllus. 0.286  0.013 0.026  0.000  0.013  0.182  0.143  0.338\nConju. 0.079  0.109 0.208  0.020  0.069  0.188  0.089  0.238\n\n\nWe are now in a position to convert both of these to distance/distance matrices. Notwithstanding the fact that it’s probably not the ideal way to describe similarity between distributions, I’ll call dist() using the default Euclidean distance measure. I mean, sure, I could probably do something fancy with Jensen-Shannon divergence here, but in my experience the metric you use to measure distributional similarity is far less important than the manner in which you construct the distributions from raw features in the first place, so I’m not going to sweat this one. Here’s our measure of class dissimilarity:\n\nclass_dissim  &lt;- dist(class_distro)\nprint_truncated(class_dissim)\n\n        Bard Cleric Druid Palad. Ranger Sorce. Warlo.\nCleric 0.309                                         \nDruid  0.296  0.251                                  \nPalad. 0.373  0.167 0.381                            \nRanger 0.294  0.213 0.146  0.313                     \nSorce. 0.286  0.342 0.168  0.468  0.292              \nWarlo. 0.151  0.270 0.288  0.371  0.312  0.279       \nWizard 0.218  0.259 0.152  0.389  0.228  0.118  0.196\n\n\nHere’s our measure of school dissimilarity:\n\nschool_dissim &lt;- dist(school_distro)\nprint_truncated(school_dissim)\n\n       Evoca. Abjur. Trans. Encha. Necro. Divin. Illus.\nAbjur.  0.320                                          \nTrans.  0.122  0.279                                   \nEncha.  0.323  0.284  0.270                            \nNecro.  0.218  0.200  0.226  0.281                     \nDivin.  0.271  0.133  0.203  0.181  0.179              \nIllus.  0.319  0.409  0.301  0.217  0.313  0.303       \nConju.  0.134  0.251  0.073  0.273  0.178  0.184  0.319\n\n\n\n\n\n\n\nD&D Boxed Set: Master Rules\n\n\n\n\nHierarchical clustering for the dendrograms\nAfter all that effort in constructing the dissimilarity matrices, the hierarchical clustering is something of an anticlimax. The only substantive choice we need to make here is whether to use single-link, complete-link, average-link, or some other method for agglomeration. This does matter somewhat, at least in my experience, but I’m also feeling lazy so I’m going to go with average-link because it feels appropriate to me in this context:\n\nclusters &lt;- list(\n  class = hclust(class_dissim, method = \"average\"),\n  school = hclust(school_dissim, method = \"average\")\n)\nprint(clusters)\n\n$class\n\nCall:\nhclust(d = class_dissim, method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 8 \n\n\n$school\n\nCall:\nhclust(d = school_dissim, method = \"average\")\n\nCluster method   : average \nDistance         : euclidean \nNumber of objects: 8 \n\n\n\n\nPlotting the heatmap\nConstructing the plot can also be considered a two-part process. In the first stage, we constrict a base plot object that uses geom_tile() to display the class/school affinities data (i.e., dat), and add various stylistic features to make it look pretty:\n\nbase &lt;- ggplot(dat, aes(school, class, fill = count)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"RdPu\") +\n  labs(\n    x = \"The Schools of Magic\",\n    y = \"The Classes of Character\",\n    fill = \"Number of Learnable Spells\"\n  ) +\n  coord_equal() +\n  theme(\n    plot.background = element_rect(\n      fill = \"#222\", \n      color = \"#222\"\n    ),\n    plot.margin = unit(c(2, 2, 2, 2), units = \"cm\"),\n    text = element_text(color = \"#ccc\", size = 14),\n    axis.text = element_text(color = \"#ccc\"),\n    axis.title = element_text(color = \"#ccc\"),\n    axis.ticks = element_line(color = \"#ccc\"),\n    legend.position = \"bottom\",\n    legend.background = element_rect(\n      fill = \"#222\", \n      color = \"#222\"\n    )\n  )\n\nplot(base)\n\n\n\n\n\n\n\n\nIn this form, though, you can’t really see which schools are similar to each other and nor can you see how the classes are related in terms of their spell-casting affinities. What we really want to do is reorder the rows and columns so that the most similar schools are placed in adjacent columns, and the most similar classes are placed in adjacent rows.\n\n\nAdding the dendrograms\nUntil recently I’d never found a tool for doing this in R that I found satisfying, but with the release of the legendry package by Teun van den Brand (which has a lot of tools for working with plot legends and axes that I’m slowly learning…) this has changed. If we pass a hierarchical clustering to the scale_*_dendro() functions, the rows/columns are reordered appropriately, and the dendrograms themselves are shown alongside the axes:\n\npic &lt;- base +\n  scale_x_dendro(\n    clust = clusters$school,\n    guide = guide_axis_dendro(n.dodge = 2),\n    expand = expansion(0, 0),\n    position = \"top\"\n  ) +\n  scale_y_dendro(\n    clust = clusters$class,\n    expand = expansion(0, 0)\n  )\n\nplot(pic)\n\n\n\n\n\n\n\n\nSo much nicer!\nTo any D&D player, the plot is immediately interpretable: wizards and sorcerers are very similar spellcasting classes, and the spellcasting abilities of paladins are basically “clerics, but not very good at it”. The same dynamic is in play with regards to druids and rangers, in the sense that they’re both nature focused spellcasters but rangers aren’t very good at it. The grouping of bards and warlocks surprised me a little, until it was pointed out to me that they both rely heavily on charisma in their spellcasting, so there is a kind of connection there.\nOn the schools side, the plot is similarly interpretable: enchantment and illusion are closely related schools, as are abjuration and divination. Necromancy feels a little bit like the darker cousin of abjuration so yeah, that tracks too. Transmutation, conjuration, and evocation are all kinda related, so you get a clustering there too.\nThere are some limitations to hierarchical clustering, of course, and you can see a little bit of that coming through in the plot. By design, I constructed the dissimilarities so that they’d ignore the “primary spellcaster vs secondary spellcaster” distinction, so the overall brightness of adjacent rows and columns varies wildly. But to capture that in a clustering solution while also capturing the “stylistic” similarities I’ve plotted here, you’d need to use an overlapping clustering tool rather than a hierarchical one, and those are inherently trickier to work with, and I wouldn’t be able to draw the pretty dendrograms either!\n\n\n\n\n\nD&D Boxed Set: Immortals Rules"
  },
  {
    "objectID": "posts/2025-01-01_schools-of-magic/index.html#epilogue",
    "href": "posts/2025-01-01_schools-of-magic/index.html#epilogue",
    "title": "The schools of magic",
    "section": "Epilogue",
    "text": "Epilogue\nOne weirdly enjoyable game I discovered when writing this post is calling the describe_spell() helper function without specifying the actual spell to describe, which produces the description of a randomly selected spell. Trying to identify the spell name from the description is kind of fun:\n\ndescribe_spell()\n\nYou create a floating, spectral force that resembles a weapon of your\nchoice and lasts for the duration. The force appears within range in\na space of your choice, and you can immediately make one melee spell\nattack against one creature within 5 feet of the force. On a hit, the\ntarget takes Force damage equal to 1d8 plus your spellcasting ability\nmodifier. As a Bonus Action on your later turns, you can move the\nforce up to 20 feet and repeat the attack against a creature within 5\nfeet of it. Using a Higher-Level Spell Slot. The damage increases by\n1d8 for every slot level above 2.\n\n\nThis one?\n\ndescribe_spell()\n\nYou create an illusion of an object, a creature, or some other visible\nphenomenon within range that activates when a specific trigger occurs.\nThe illusion is imperceptible until then. It must be no larger than a\n30-foot Cube, and you decide when you cast the spell how the illusion\nbehaves and what sounds it makes. This scripted performance can last\nup to 5 minutes. When the trigger you specify occurs, the illusion\nsprings into existence and performs in the manner you described. Once\nthe illusion finishes performing, it disappears and remains dormant\nfor 10 minutes, after which the illusion can be activated again. The\ntrigger can be as general or as detailed as you like, though it must\nbe based on visual or audible phenomena that occur within 30 feet\nof the area. For example, you could create an illusion of yourself\nto appear and warn off others who attempt to open a trapped door.\nPhysical interaction with the image reveals it to be illusory, since\nthings can pass through it. A creature that takes the Study action\nto examine the image can determine that it is an illusion with a\nsuccessful Intelligence (Investigation) check against your spell save\nDC. If a creature discerns the illusion for what it is, the creature\ncan see through the image, and any noise it makes sounds hollow to the\ncreature.\n\n\nOr this one?\n\ndescribe_spell()\n\nYou receive an omen from an otherworldly entity about the results of a\ncourse of action that you plan to take within the next 30 minutes. The\nDM chooses the omen from the Omens table. Omens Omen For Results That\nWill Be... Weal Good Woe Bad Weal and woe Good and bad Indifference\nNeither good nor bad The spell doesn’t account for circumstances, such\nas other spells, that might change the results. If you cast the spell\nmore than once before finishing a Long Rest, there is a cumulative\n25 percent chance for each casting after the first that you get no\nanswer.\n\n\nEnjoy!"
  },
  {
    "objectID": "posts/2023-06-16_tabulizer/index.html",
    "href": "posts/2023-06-16_tabulizer/index.html",
    "title": "Extracting tables from pdf files with tabulizer",
    "section": "",
    "text": "In the last post I talked about something very grim, and to be honest it’s not at all what wanted to be writing about yesterday. My intention when I woke up yesterday was to write about tabulizer, an R package you can use to extract tables from a pdf document. This isn’t my favourite of data wrangling tasks: pdf is not a very good format in which to store data, but it’s awfully common to find yourself in a situation where the data you want to work with exists only as a table in a pdf document. Because this is a thing that happens, it’s nice to have tools that make it a little easier."
  },
  {
    "objectID": "posts/2023-06-16_tabulizer/index.html#to-extract-a-table-we-must-first-create-the-universe",
    "href": "posts/2023-06-16_tabulizer/index.html#to-extract-a-table-we-must-first-create-the-universe",
    "title": "Extracting tables from pdf files with tabulizer",
    "section": "To extract a table, we must first create the universe",
    "text": "To extract a table, we must first create the universe\nThe tabulizer package works by supplying bindings to tabula-java, a java library for extracting tables from pdfs. So if you want tabulizer to work in R you need a working installation of Java, and you need to have the rJava package to provide the R-to-Java bindings.\nMy experience in the past has been that getting all this setup can be a bit finicky. Happily for me, I’m on Ubuntu and Andrew Collier has a blog post that walks you through the process step by step. Following his guide, my first step was to install the Java runtime environment and the Java development kit:\nsudo apt-get install -y default-jre default-jdk\nThis worked smoothly, so I moved onto the next step and ensured that R knows where to find Java:\nsudo R CMD javareconf\nOnly now is it possible to install the rJava package:\ninstall.packages(\"rJava\")\nAndrew’s post suggests that you need to restart RStudio after doing this, so I did that too. Having done so, I could finally install the tabulizer package itself:\nremotes::install_github(c(\"ropensci/tabulizerjars\", \"ropensci/tabulizer\"))"
  },
  {
    "objectID": "posts/2023-06-16_tabulizer/index.html#let-there-be-tables",
    "href": "posts/2023-06-16_tabulizer/index.html#let-there-be-tables",
    "title": "Extracting tables from pdf files with tabulizer",
    "section": "Let there be tables",
    "text": "Let there be tables\nNow that I have the tabulizer package installed, I’ll load it along with the other packages I’ll be using in this post:\n\nlibrary(tabulizer)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(janitor)\n\nTo check that it works, I’ll need a pdf file to work with. As a convenience, the tabulizer package comes with a bundled “data.pdf” file that we can use for this purpose:\n\npdf_data &lt;- system.file(\"examples\", \"data.pdf\", package = \"tabulizer\")\n\nI’ve embedded a copy of the “data.pdf” file in this post, and as you can see it’s very simple test case (by design). The file contains four tables, and only those four tables:\n \n\nUnable to display PDF file. Download instead.\n\n\n\nWe can use this data file as a way to check that the package works and does what we expect. The workhorse function in the package is extract_tables(). We pass it the path to the pdf file as the first argument, and use the various other arguments to provide details about how the file should be processed. In this case, the only other argument I’ll specify is output = \"data.frame\", which tells the extract_tables() function to return a list of data frames rather than a list of matrices (the default behaviour). Let’s see if it’s working:\n\npdf_tables &lt;- pdf_data |&gt; \n  extract_tables(output = \"data.frame\") |&gt;\n  map(as_tibble)\n\npdf_tables\n\n[[1]]\n# A tibble: 31 × 10\n     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear\n   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1  21       6  160    110  3.9   2.62  16.5     0     1     4\n 2  21       6  160    110  3.9   2.88  17.0     0     1     4\n 3  22.8     4  108     93  3.85  2.32  18.6     1     1     4\n 4  21.4     6  258    110  3.08  3.22  19.4     1     0     3\n 5  18.7     8  360    175  3.15  3.44  17.0     0     0     3\n 6  18.1     6  225    105  2.76  3.46  20.2     1     0     3\n 7  14.3     8  360    245  3.21  3.57  15.8     0     0     3\n 8  24.4     4  147.    62  3.69  3.19  20       1     0     4\n 9  22.8     4  141.    95  3.92  3.15  22.9     1     0     4\n10  19.2     6  168.   123  3.92  3.44  18.3     1     0     4\n# ℹ 21 more rows\n\n[[2]]\n# A tibble: 6 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n6          5.4         3.9          1.7         0.4 setosa \n\n[[3]]\n# A tibble: 6 × 6\n      X Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n  &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;    \n1   145          6.7         3.3          5.7         2.5 virginica\n2   146          6.7         3            5.2         2.3 virginica\n3   147          6.3         2.5          5           1.9 virginica\n4   148          6.5         3            5.2         2   virginica\n5   149          6.2         3.4          5.4         2.3 virginica\n6   150          5.9         3            5.1         1.8 virginica\n\n[[4]]\n# A tibble: 14 × 1\n   supp \n   &lt;chr&gt;\n 1 VC   \n 2 VC   \n 3 VC   \n 4 VC   \n 5 VC   \n 6 VC   \n 7 VC   \n 8 VC   \n 9 VC   \n10 VC   \n11 VC   \n12 VC   \n13 VC   \n14 VC   \n\n\nThat looks nice. With very little effort we’ve extracted all four tables from the pdf file, and returned a list of tibbles containing the data. Yay! 🎉"
  },
  {
    "objectID": "posts/2023-06-16_tabulizer/index.html#wild-caught-pdf-files-are-trickier-to-work-with",
    "href": "posts/2023-06-16_tabulizer/index.html#wild-caught-pdf-files-are-trickier-to-work-with",
    "title": "Extracting tables from pdf files with tabulizer",
    "section": "Wild caught pdf files are trickier to work with",
    "text": "Wild caught pdf files are trickier to work with\nOkay, let’s try a harder example. One of the two reports I referred to in yesterdays blog post is a survey of LGBTQ people conducted by Data For Progress. Unlike the test file, it contains additional text that is not part of any table, and the tables within the report have a lot of fancier formatting that isn’t present in the test file. I’ve cached a local copy of the pdf file as “dfp_lgbtq_survey.pdf”, and you can take a look yourself to see what we’re working with this time:\n \n\nUnable to display PDF file. Download instead.\n\n\n\nThe data I used in that post comes from question 4, so I’ll try to extract the data for that table from the pdf file. This turns out to be a little harder to do. My first attempt tried to automatically pull all the tables from the second page by setting pages = 2, and this is what happened:\n\npdf_file &lt;- \"dfp_lgbtq_survey.pdf\"\nextract_tables(pdf_file, pages = 2)\n\n[[1]]\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11]\n[1,] \"79\" \"85\" \"82\" \"70\" \"87\" \"89\" \"77\" \"79\" \"83\" \"69\"  \"80\" \n[2,] \"17\" \"12\" \"13\" \"29\" \"7\"  \"6\"  \"19\" \"14\" \"15\" \"27\"  \"17\" \n\n[[2]]\n     [,1]       [,2]      [,3]       [,4]       [,5]    [,6]     [,7]  \n[1,] \"Response\" \"Topline\" \"African\"  \"or\"       \"White\" \"Female\" \"Male\"\n[2,] \"\"         \"\"        \"\"         \"\"         \"\"      \"\"       \"\"    \n[3,] \"\"         \"\"        \"American\" \"Latino/a​\" \"\"      \"\"       \"\"    \n     [,8]     [,9]          [,10]         [,11] [,12] [,13] [,14] [,15]\n[1,] \"\"       \"\"            \"identify as\" \"\"    \"\"    \"\"    \"\"    \"65+\"\n[2,] \"binary\" \"transgender\" \"\"            \"24\"  \"39\"  \"54\"  \"64\"  \"\"   \n[3,] \"\"       \"\"            \"transgender\" \"\"    \"\"    \"\"    \"\"    \"\"   \n\n\nOkay, that’s definitely not the data we want. To make this work we’re going to have to give extract_tables() a little more information. One way to do this is to explicitly specify the area of the pdf file that contains the table to be extracted. To that end, it’s helpful to first call the get_page_dims() function, which gives us the dimensions of each page in the pdf document:\n\nget_page_dims(pdf_file)\n\n[[1]]\n[1] 595 842\n\n[[2]]\n[1] 595 842\n\n[[3]]\n[1] 595 842\n\n\nNow that we have the dimensions for each page we can specify a rectangular region as a vector containing the top, left, bottom and right coordinates of the rectangle:\n\nregion &lt;- c(250, 0, 450, 595)\n\nThe command we want looks like this:\n\nmat &lt;- extract_tables(\n  file = pdf_file, \n  pages = 2, \n  guess = FALSE,\n  area = list(region)\n)[[1]]\n\nThis time around, in addition to setting pages = 2, we’ve set guess = FALSE in order to stop extract_tables() from trying to automatically detect regions containing tabular data, and also passed a list of regions (in this case just the one region) as the area argument, thereby telling extract_tables() to look in that specific part of the document.\nLet’s take a look at the result:\n\nmat\n\n\n\n      [,1]                                [,2]      [,3]       [,4]       [,5]    [,6]     [,7]   [,8]     [,9]            [,10]         [,11] [,12] [,13] [,14] [,15]\n [1,] \"\"                                  \"\"        \"Black or\" \"Hispanic\" \"\"      \"\"       \"\"     \"\"       \"\"              \"Does not\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n [2,] \"\"                                  \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"Non-\"   \"Identifies as\" \"\"            \"18-\" \"25-\" \"40-\" \"55-\" \"\"   \n [3,] \"Response\"                          \"Topline\" \"African\"  \"or\"       \"White\" \"Female\" \"Male\" \"\"       \"\"              \"identify as\" \"\"    \"\"    \"\"    \"\"    \"65+\"\n [4,] \"\"                                  \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"binary\" \"transgender\"   \"\"            \"24\"  \"39\"  \"54\"  \"64\"  \"\"   \n [5,] \"\"                                  \"\"        \"American\" \"Latino/​a\" \"\"      \"\"       \"\"     \"\"       \"\"              \"transgender\" \"\"    \"\"    \"\"    \"\"    \"\"   \n [6,] \"Yes, I have considered moving\"     \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n [7,] \"\"                                  \"27\"      \"24\"       \"28\"       \"27\"    \"26\"     \"20\"   \"44\"     \"43\"            \"24\"          \"41\"  \"28\"  \"18\"  \"17\"  \"17\" \n [8,] \"out of my community or state\"      \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n [9,] \"No, I have not considered\"         \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n[10,] \"moving out of my community or\"     \"61\"      \"57\"       \"59\"       \"64\"    \"60\"     \"75\"   \"37\"     \"40\"            \"65\"          \"43\"  \"60\"  \"74\"  \"80\"  \"70\" \n[11,] \"state\"                             \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n[12,] \"I have already moved out of my\"    \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n[13,] \"community or state as a result of\" \"5\"       \"6\"        \"4\"        \"3\"     \"3\"      \"3\"    \"13\"     \"8\"             \"4\"           \"8\"   \"4\"   \"2\"   \"1\"   \"9\"  \n[14,] \"anti-LGBTQ+ legislation\"           \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"\"       \"\"              \"\"            \"\"    \"\"    \"\"    \"\"    \"\"   \n[15,] \"Not sure\"                          \"7\"       \"14\"       \"8\"        \"6\"     \"11\"     \"3\"    \"6\"      \"8\"             \"7\"           \"9\"   \"8\"   \"7\"   \"2\"   \"4\"  \n[16,] \"Weighted N\"                        \"1,036\"   \"93\"       \"217\"      \"632\"   \"426\"    \"368\"  \"135\"    \"166\"           \"870\"         \"249\" \"425\" \"186\" \"93\"  \"83\" \n\n\nIt’s not quite organised the way we want, but it’s definitely the right data.\nEven better, you don’t actually have to do all this messing about trying to figure out the precise region containing the table. If you have the Shiny and miniUI packages installed, you can work interactively using a command like this:\n\nextract_areas(\"dfp_lgbtq_survey.pdf\", pages = 2)\n\nAfter using click and drag to select the region of the page containing the table, R returns the same data contained in the mat matrix shown earlier."
  },
  {
    "objectID": "posts/2023-06-16_tabulizer/index.html#cleaning-the-table",
    "href": "posts/2023-06-16_tabulizer/index.html#cleaning-the-table",
    "title": "Extracting tables from pdf files with tabulizer",
    "section": "Cleaning the table",
    "text": "Cleaning the table\nOnce we have the data in this matrix form, it’s slightly tedious to wrangle it into the format we want, but it’s not conceptually difficult once we have a few helper functions to make our lives easier. The first step is to split the matrix into a list of matrices, each of which contains the data that should belong in a single row of the final data set. The row_split() function below takes a matrix as input, and splits it up into a list of matrices specified by the list argument rows, where each element of rows is a vector containing the indices of the rows that should be included in the relevant element of the output:\n\nrow_split &lt;- function(x, rows) {\n  lapply(rows, \\(r) {\n    if(length(r) == 1) return(matrix(x[r, ], nrow = 1))\n    x[r, ]\n  })\n}\ngroups &lt;- list(1:5, 6:8, 9:11, 12:14, 15, 16)\n\nmat_split &lt;- row_split(mat, rows = groups)\nmat_split\n\n\n\n[[1]]\n     [,1]       [,2]      [,3]       [,4]       [,5]    [,6]     [,7]   [,8]     [,9]            [,10]         [,11] [,12] [,13] [,14] [,15]\n[1,] \"\"         \"\"        \"Black or\" \"Hispanic\" \"\"      \"\"       \"\"     \"\"       \"\"              \"Does not\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n[2,] \"\"         \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"Non-\"   \"Identifies as\" \"\"            \"18-\" \"25-\" \"40-\" \"55-\" \"\"   \n[3,] \"Response\" \"Topline\" \"African\"  \"or\"       \"White\" \"Female\" \"Male\" \"\"       \"\"              \"identify as\" \"\"    \"\"    \"\"    \"\"    \"65+\"\n[4,] \"\"         \"\"        \"\"         \"\"         \"\"      \"\"       \"\"     \"binary\" \"transgender\"   \"\"            \"24\"  \"39\"  \"54\"  \"64\"  \"\"   \n[5,] \"\"         \"\"        \"American\" \"Latino/​a\" \"\"      \"\"       \"\"     \"\"       \"\"              \"transgender\" \"\"    \"\"    \"\"    \"\"    \"\"   \n\n[[2]]\n     [,1]                            [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] \"Yes, I have considered moving\" \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n[2,] \"\"                              \"27\" \"24\" \"28\" \"27\" \"26\" \"20\" \"44\" \"43\" \"24\"  \"41\"  \"28\"  \"18\"  \"17\"  \"17\" \n[3,] \"out of my community or state\"  \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n\n[[3]]\n     [,1]                            [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] \"No, I have not considered\"     \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n[2,] \"moving out of my community or\" \"61\" \"57\" \"59\" \"64\" \"60\" \"75\" \"37\" \"40\" \"65\"  \"43\"  \"60\"  \"74\"  \"80\"  \"70\" \n[3,] \"state\"                         \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n\n[[4]]\n     [,1]                                [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] \"I have already moved out of my\"    \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n[2,] \"community or state as a result of\" \"5\"  \"6\"  \"4\"  \"3\"  \"3\"  \"3\"  \"13\" \"8\"  \"4\"   \"8\"   \"4\"   \"2\"   \"1\"   \"9\"  \n[3,] \"anti-LGBTQ+ legislation\"           \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"   \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n\n[[5]]\n     [,1]       [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] \"Not sure\" \"7\"  \"14\" \"8\"  \"6\"  \"11\" \"3\"  \"6\"  \"8\"  \"7\"   \"9\"   \"8\"   \"7\"   \"2\"   \"4\"  \n\n[[6]]\n     [,1]         [,2]    [,3] [,4]  [,5]  [,6]  [,7]  [,8]  [,9]  [,10] [,11] [,12] [,13] [,14] [,15]\n[1,] \"Weighted N\" \"1,036\" \"93\" \"217\" \"632\" \"426\" \"368\" \"135\" \"166\" \"870\" \"249\" \"425\" \"186\" \"93\"  \"83\" \n\n\nThe second helper function is col_paste() which takes a matrix with one or more rows as input and collapses it to a vector by pasting the contents of all cells in the same column together:\n\ncol_paste &lt;- function(x, ...) {\n  apply(x, 2, \\(y) {as.vector(paste(y, ...))})\n}\n\nTo illustrate the idea, let’s take mat_split[[1]], a five-row matrix that contains the data that should eventually become our column names, and convert it to a character vector using col_paste()\n\ncol_paste(mat_split[[1]], collapse = \" \")\n\n [1] \"  Response  \"                      \n [2] \"  Topline  \"                       \n [3] \"Black or  African  American\"       \n [4] \"Hispanic  or  Latino/​a\"            \n [5] \"  White  \"                         \n [6] \"  Female  \"                        \n [7] \"  Male  \"                          \n [8] \" Non-  binary \"                    \n [9] \" Identifies as  transgender \"      \n[10] \"Does not  identify as  transgender\"\n[11] \" 18-  24 \"                         \n[12] \" 25-  39 \"                         \n[13] \" 40-  54 \"                         \n[14] \" 55-  64 \"                         \n[15] \"  65+  \"                           \n\n\nFinally, we can use the row_combine() function below that takes a list of vectors and combines them into a matrix.\n\nrow_combine &lt;- function(x, ...) {\n  matrix(unlist(x), nrow = length(x), byrow = TRUE)\n}\n\nEquipped with these helpers, the following pipeline takes the raw output mat and converts it into a tibble dat containing the data in the format we want it to be:\n\ndat &lt;- mat |&gt; \n  row_split(groups) |&gt;                         # split into list of matrices\n  map(\\(x) {col_paste(x, collapse = \" \")}) |&gt;  # paste into character vector\n  row_combine() |&gt;                             # combine vectors into one matrix\n  as_tibble(.name_repair = \"minimal\") |&gt;       # convert to tibble\n  row_to_names(row_number = 1) |&gt;              # use first row as names\n  clean_names() |&gt;                             # clean the names\n  rename(                                      # shorten some names\n    \"black\" = \"black_or_african_american\",\n    \"hispanic\" = \"hispanic_or_latino_a\", \n    \"trans\" = \"identifies_as_transgender\",\n    \"not_trans\" = \"does_not_identify_as_transgender\"\n  ) |&gt;\n  mutate(across(!response, \\(x) {as.numeric(gsub(\",\", \"\", x))})) # numeric data\n\ndat\n\n\n\n# A tibble: 5 × 15\n  response                        topline black hispanic white female  male non_binary trans not_trans x18_24 x25_39 x40_54 x55_64   x65\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 Yes, I have considered moving …      27    24       28    27     26    20         44    43        24     41     28     18     17    17\n2 No, I have not considered movi…      61    57       59    64     60    75         37    40        65     43     60     74     80    70\n3 I have already moved out of my…       5     6        4     3      3     3         13     8         4      8      4      2      1     9\n4 Not sure                              7    14        8     6     11     3          6     8         7      9      8      7      2     4\n5 Weighted N                         1036    93      217   632    426   368        135   166       870    249    425    186     93    83\n\n\nEt voilà!"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html",
    "href": "posts/2023-03-13_shattered-landscapes/index.html",
    "title": "Shattered landscapes",
    "section": "",
    "text": "Update: I made a little shiny app based on this post: djnavarro.shinyapps.io/shattered-landscapes (source code). It doesn’t implement the complete system because rayshader causes out of memory problems for my free tier shinyapps account, but in other respects it’s the same thing.\nIn the last few weeks I’ve been tinkering with a generative art system I ended up calling Broken Lands. It creates maps of bizarre and impossible landscapes in R, using the ambient package to generate the topography, and rayshader to render shadows cast by a hypothetical light source. It creates images like these:\nTo my eye, at least, these images are both beautiful and tragic. I cannot help but interpret them as coastal landscapes in an alien geography of some kind, a land that has suffered some cataclysm like the Doom of Valyria or the Fall of Istar. The contours feel too contorted to be the result of any terrestrial process, and – again, by my interpretation – there’s a tension between the smoothness of the individual contours and the jagged, chaotic structure of the landscape overall.\nBut what would I know? I wrote the code that makes the system work, but I don’t have a monopoly of interpretation of the images. Death of the author and all that. Barthes would call me the “scriptor” rather than the author, I suppose, which honestly feels about right for generative art. So yeah. The pieces are what they are, quite separate from the artist and from the process by which the system was constructed.\nThat said, if you’re familiar with the R ecosystem you can probably take an educated guess about how I made these pieces. As mentioned at the start, the spatial noise patterns are created using the ambient package and the shadows and three-dimensional look are provided by rayshader. I wrote about both of these packages in my workshop on generative art in R workshop (specifically: ambient art, rayshader art), and those tutorials provide a lot of clues about how these pieces are made. But there are also some details I haven’t talked about before, and in any case it’s always fun to write about art.\nseed &lt;- 14"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#starting-simple",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#starting-simple",
    "title": "Shattered landscapes",
    "section": "Starting simple",
    "text": "Starting simple\nI’ll start by building a simple system that doesn’t go very far beyond what I covered in the Art From Code workshop. It’s built using three functions. There’s a new_grid() function used to define a grid of x and y coordinates, a generate_simplex() function used to create spatial noise patterns on such a grid, and a render() function used to create an image. First, the new_grid() function:\n\nnew_grid &lt;- function(n = 1000) {\n  ambient::long_grid(\n    x = seq(0, 1, length.out = n),\n    y = seq(0, 1, length.out = n)\n  )\n}\n\nnew_grid()\n\n# A tibble: 1,000,000 × 2\n       x       y\n   &lt;dbl&gt;   &lt;dbl&gt;\n 1     0 0      \n 2     0 0.00100\n 3     0 0.00200\n 4     0 0.00300\n 5     0 0.00400\n 6     0 0.00501\n 7     0 0.00601\n 8     0 0.00701\n 9     0 0.00801\n10     0 0.00901\n# ℹ 999,990 more rows\n\n\nThe output appears to be a tibble that contains x and y coordinates.2 This defines the spatial locations that we’ll use to create the image, but we’ll need to assign colours to each of those locations.\n\n\nPainting a canvas with spatial noise\nIn order to do this, we’ll write a function called generate_simplex() that generates interesting patterns of spatial noise:\n\ngenerate_simplex &lt;- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  ambient::fracture(\n    noise = ambient::gen_simplex,\n    fractal = ambient::billow,\n    octaves = 10,\n    freq_init = .02,\n    frequency = ~ . * 2,\n    gain_init = 1,\n    gain = ~ . * .8,\n    x = x,\n    y = y\n  )\n}\n\nThe particular choices I’ve made here came about from trial and error. I played around with a lot of different settings when creating generative art in this style, and these were things I liked. I’m not going to dive into the details here: you can find out more by reading the tutorial on spatial noise art I linked to earlier. For the current post, all I want to highlight is that we can use this function to add a new column to the canvas that defines our artwork:\n\ncanvas &lt;- new_grid() |&gt; \n  dplyr::mutate(paint = generate_simplex(x, y, seed = seed))\n\ncanvas\n\n# A tibble: 1,000,000 × 3\n       x       y paint\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     0 0       -3.96\n 2     0 0.00100 -3.95\n 3     0 0.00200 -3.95\n 4     0 0.00300 -3.94\n 5     0 0.00400 -3.93\n 6     0 0.00501 -3.92\n 7     0 0.00601 -3.91\n 8     0 0.00701 -3.91\n 9     0 0.00801 -3.90\n10     0 0.00901 -3.89\n# ℹ 999,990 more rows\n\n\nThis canvas object is structured like a lookup table: it’s a data frame with columns specifying x and y coordinates, and it contains a third column that specifies the colour of “paint” that needs to be applied at each coordinate. However, it’s a very structured data frame because the x and y values form a grid. This makes straightforward to flip from this format to a “bitmap” matrix format:3\n\nbitmap &lt;- canvas |&gt; as.array(value = paint)\nbitmap[1:6, 1:6]\n\n      x\ny           [,1]      [,2]      [,3]      [,4]      [,5]      [,6]\n  [1,] -3.963129 -3.931507 -3.899957 -3.868550 -3.837357 -3.806447\n  [2,] -3.954987 -3.932781 -3.901242 -3.869845 -3.838662 -3.807762\n  [3,] -3.946846 -3.924674 -3.902573 -3.871211 -3.840061 -3.809194\n  [4,] -3.938708 -3.916594 -3.894550 -3.872644 -3.841552 -3.810740\n  [5,] -3.930575 -3.908541 -3.886577 -3.864752 -3.843132 -3.812398\n  [6,] -3.922448 -3.900516 -3.878655 -3.856932 -3.835414 -3.814166\n\n\nA grid of numbers isn’t very pretty to look at, but we will need to create this matrix representation before passing the data to rayshader later. But I’m getting ahead of myself. For now, we can use the image() function to render an image from matrix-formatted data:\n\ncanvas |&gt; \n  as.array(value = paint) |&gt;\n  image(axes = FALSE, asp = 1, useRaster = TRUE)\n\n\n\n\n\n\n\n\nWe’re a long way from our goal, but at least we now have an output that looks like art rather than a matrix of numbers. It’s a start!\n\n\n\nCasting shadows across the landscape\nThe next step in the process is to define a render() function that will take an “elevation” matrix as input, but instead of drawing a “heat map” like image() does, it renders it as a three-dimensional topographic map with shadows cast by a hypothetical light source. This is surprisingly easy to do using rayshader. Here’s the function I’ll use in this post:\n\nrender &lt;- function(mat, shades = NULL, zscale = .005) {\n  if(is.null(shades)) {\n    n &lt;- length(unique(mat))\n    shades &lt;- hcl.colors(n, \"YlOrRd\", rev = TRUE)\n  }\n  rayshader::height_shade(\n    heightmap = mat,\n    texture = shades\n  ) |&gt;\n    rayshader::add_shadow(\n      shadowmap = rayshader::ray_shade(\n        heightmap = mat,\n        sunaltitude = 50,\n        sunangle = 80,\n        multicore = TRUE,\n        zscale = zscale\n      ),\n      max_darken = .2\n    ) |&gt;\n    rayshader::plot_map()\n}\n\nI’m not going to go into the specifics: you can find out more by reading the tutorial on rayshader art I linked to earlier. For this post, I’m simply going to show you what it does. Taking the canvas data as input, we first use as.array() to switch from a “data frame style” representation to a “matrix style” representation, and then pass the matrix to render():\n\ncanvas |&gt;\n  as.array(value = paint) |&gt;\n  render()\n\n\n\n\n\n\n\n\nAgain, still a long way from our desired goal, but we are making progress. Thanks to rayshader, we have output that looks like a shaded topographic map.\n\n\n\nMaking islands from the landscape\nAt this point we have the ability to generate landscapes, but the images just look like a bunch of hills. They don’t have the “coastal” feeling that the original images did. We can create islands by setting a “sea level”. You can do this in a sophisticated way in rayshader using detect_water() and add_water(), but that’s overkill for our purposes. All we really want to do is imagine setting a sea level such that about half the image is “water” and half the image is “land”. To do that we just calculate the median value in the original data:\n\nsea_level &lt;- median(canvas$paint)\n\nFrom there it’s an exercise in using dplyr. Using mutate() we create a new “islands” column whose value is equal to the original value or the sea level, whichever is higher:\n\ncanvas |&gt; \n  dplyr::mutate(\n    islands = dplyr::if_else(\n      condition = paint &lt; sea_level,\n      true = sea_level, \n      false = paint\n    )\n  ) |&gt;\n  as.array(value = islands) |&gt;\n  render()\n\n\n\n\n\n\n\n\nEt voilà! We have a generative art system that creates fictitious topographic maps of coastal islands. It’s still not quite the same thing as the original, but it’s kind of a nice system in itself. If you want to play with it, the complete source code for generating this image is included in the islands.R script accompanying this post.\n\n\n\nTweaking the spatial noise generator\nIf you do end up playing around, a really useful way to create variations on this system is to modify the function that generates the spatial noise patterns. For example, this generate_fancy_noise() function is awfully similar to the noise generator I used in the Broken Lands series:\n\ngenerate_fancy_noise &lt;- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  z &lt;- ambient::fracture(\n    noise = ambient::gen_worley,\n    fractal = ambient::billow,\n    octaves = 8,\n    freq_init = .1,\n    frequency = ~ . * 2,\n    gain_init = 3,\n    gain = ~ . * .5,\n    value = \"distance2\",\n    x = x,\n    y = y\n  )\n  ambient::fracture(\n    noise = ambient::gen_simplex,\n    fractal = ambient::billow,\n    octaves = 10,\n    freq_init = .02,\n    frequency = ~ . * 2,\n    gain_init = 1,\n    gain = ~ . * .8,\n    x = x + z,\n    y = y + z\n  )\n}\n\nHere it is in action:\n\nnew_grid() |&gt;\n  dplyr::mutate(\n    height = generate_fancy_noise(x, y, seed = seed),\n    islands = dplyr::if_else(\n      condition = height &lt; median(height),\n      true = median(height),\n      false = height\n    )\n  ) |&gt;\n  as.array(value = islands) |&gt;\n  render(zscale = .01)\n\n\n\n\n\n\n\n\nVery pretty. I suspect that had I isolated this particular noise generator earlier in the artistic process – rather than figuring out in hindsight that this was what I’d been using all along – I might have stopped here, and not bothered with any of the other tricks that I used.4 But of course this is a post-mortem deconstruction, not a description of the bizarrely tangled artistic process I actually followed, so there are more layers to come…"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#queering-geography",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#queering-geography",
    "title": "Shattered landscapes",
    "section": "Queering geography",
    "text": "Queering geography\nThe final image in the last section captures something about the overall structure of the Broken Lands images, but it feels wrong in the particulars. It’s too smooth, too fluid, too… natural. It doesn’t have the same feel as the originals. I don’t have the same feeling of alienness that the original pieces have. Where does that not-quite-real feeling come from?\nThe answer to this involves every generative artists favourite trick: curl fields. If you’ve read the tutorial articles I linked to earlier, you’ve encountered these before so I won’t repeat myself by explaining yet again what a curl field is. What I’ll do instead is write a generate_curl() function that takes the original grid of coordinates (in the “base” space) and transforms them to a new set of points (in an “embedding” space) using a curl transformation:5\n\ngenerate_curl &lt;- function(x, y, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  ambient::curl_noise(\n    generator = ambient::fracture,\n    noise = ambient::gen_simplex,\n    fractal = ambient::fbm,\n    octaves = 3,\n    frequency = ~ . * 2,\n    freq_init = .3,\n    gain_init = 1,\n    gain = ~ . * .5,\n    x = x,\n    y = y\n  )\n}\n\nHere’s what happens when we apply this function:\n\ngrid &lt;- new_grid()\ncoords &lt;- generate_curl(grid$x, grid$y, seed = seed)\nhead(coords)\n\n             x        y\n1 1.417494e-07 2.625000\n2 4.275159e-05 2.624981\n3 1.705772e-04 2.624924\n4 3.836068e-04 2.624829\n5 6.818209e-04 2.624697\n6 1.065192e-03 2.624526\n\n\nThe code here is slightly unpleasant, yes, but I’ll do it in a slightly cleaner way in a moment. What matters right now is the fact that the coords data frame is a transformed version of the grid data. The original (x,y) coordinates in the base space have been transformed to some (x,y) coordinates in some new space.\nA slightly cleaner way of doing this – keeping both the original coordinates and the transformed values – would be as follows:\n\ncanvas &lt;- grid |&gt;\n  dplyr::mutate(\n    curl_x = coords$x,\n    curl_y = coords$y\n  )\n\ncanvas\n\n# A tibble: 1,000,000 × 4\n       x       y      curl_x curl_y\n   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1     0 0       0.000000142   2.62\n 2     0 0.00100 0.0000428     2.62\n 3     0 0.00200 0.000171      2.62\n 4     0 0.00300 0.000384      2.62\n 5     0 0.00400 0.000682      2.62\n 6     0 0.00501 0.00107       2.62\n 7     0 0.00601 0.00153       2.62\n 8     0 0.00701 0.00209       2.62\n 9     0 0.00801 0.00273       2.62\n10     0 0.00901 0.00345       2.62\n# ℹ 999,990 more rows\n\n\nOkay that’s nice, but what exactly are these “curl transformed” values? What do they look like? Fair question. Here’s a plot showing what has happened to our nice rectangular grid after the transformation…\n\n\n\n\n\n\n\n\n\nThis image has an evocative feel, right? Like I’ve taken a regular square sheet of fabric and folded or transformed it in some strange way to create an “embedded” manifold? Well, yeah. That’s precisely what I’ve done.\nOur noise operations will be specified on this transformed/embedded manifold, but – to reveal the ending slightly too soon – the final image will be defined on the base space. The code below shows how to apply the noise operations in the embedding space:\n\ncanvas &lt;- canvas |&gt;\n  dplyr::mutate(\n    height = generate_fancy_noise(curl_x, curl_y, seed = seed),\n    islands = dplyr::if_else(\n      condition = height &lt; median(height),\n      true = median(height),\n      false = height\n    )\n  )\n\ncanvas\n\n# A tibble: 1,000,000 × 6\n       x       y      curl_x curl_y height islands\n   &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     0 0       0.000000142   2.62 -0.676  -0.676\n 2     0 0.00100 0.0000428     2.62 -0.675  -0.675\n 3     0 0.00200 0.000171      2.62 -0.674  -0.674\n 4     0 0.00300 0.000384      2.62 -0.672  -0.672\n 5     0 0.00400 0.000682      2.62 -0.670  -0.670\n 6     0 0.00501 0.00107       2.62 -0.667  -0.667\n 7     0 0.00601 0.00153       2.62 -0.664  -0.664\n 8     0 0.00701 0.00209       2.62 -0.662  -0.662\n 9     0 0.00801 0.00273       2.62 -0.660  -0.660\n10     0 0.00901 0.00345       2.62 -0.658  -0.658\n# ℹ 999,990 more rows\n\n\nJust to give you a sense of what that looks like in the embedding space, here’s what happens when we redraw the “manifold” plot from above, with each point coloured using the value of the “islands” variable:\n\n\n\n\n\n\n\n\n\nYou can sort of see what’s going on here. We have a spatial noise pattern that generated the topography that I showed in the last section, but it’s defined on the embedding space. Our base space is like a rectangular fabric that has been laid and folded over and over onto this embedding space, and then we’ve spray painted this pattern onto the fabric.6 7 8 When we unfold the spray painted fabric and lay it flat again, this is what we get:\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  image(axes = FALSE, asp = 1, useRaster = TRUE)\n\n\n\n\n\n\n\n\nIt’s a bit like tie-dyeing I guess? That’s what it feels like to me. I’m taking something regular, scrunching it up in a strange way, and then applying the colours to the scrunched up object before unfolding it.\nIn any case, we can use our render() function to add shadows with rayshader:\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  render(zscale = .05)\n\n\n\n\n\n\n\n\nOkay now that feels like an alien geography to me! It still doesn’t look at all like our final images, but it has the right feel to it. Yes, it’s still a geography of sorts, but it feels stretched and twisted in an unnatural way. It feels… well, it feels painful. Nothing like this can occur naturally without the action of some catastrophic process. That’s what it feels like to me. The “brokenness” of the original images is created by this transformation: natural-ish patterns imposed on a twisted space create bizarre and alien patterns when those contortions are unfolded. It feels weird… it feels strange… it feels queer.9"
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#artistic-trickery",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#artistic-trickery",
    "title": "Shattered landscapes",
    "section": "Artistic trickery",
    "text": "Artistic trickery\nThe last image in the previous section doesn’t look all that much like the Broken Lands pieces, but – perhaps surprisingly – we’re weirdly close to creating something that really does look like those pieces. There are no deep insights left to explore. From here on out, it’s really just a matter of applying a few artistic tricks. To be precise, there are three little tricks left to document.\n\n\nBe discre[et|te]\nThe first trick is discretisation. So far we’ve been creating images in which the “elevation” of the landscapes vary smoothly. The Broken Lands images don’t do that. Instead, there is a distinct feeling that the lands are terraced. In the original pieces there’s that same unnatural terracing that that you see in open cut mining.10 Creating that look in this system is not difficult. First, I’ll define a discretise() function that takes a continuously-varying vector as input, cuts it into n distinct levels that vary in value between 0 and 1, and returns the discretised values:\n\ndiscretise &lt;- function(x, n) {\n  round(ambient::normalise(x) * n) / n\n}\n\nHere’s an example in which 100 normally distributed numbers are sliced into 5 levels:\n\ndiscretise(rnorm(100), 5)\n\n  [1] 0.6 0.8 0.2 0.4 0.6 0.4 0.6 0.4 0.8 0.6 0.4 0.2 0.2 0.4 0.6 0.6 0.6\n [18] 0.4 0.6 0.4 0.8 0.4 0.4 0.6 0.6 0.4 0.2 0.0 0.4 0.6 0.6 0.4 0.0 0.8\n [35] 0.2 0.2 0.4 0.6 0.4 0.4 0.8 0.4 0.4 0.4 0.4 0.2 0.4 0.2 0.6 0.4 0.8\n [52] 0.6 0.6 0.4 0.6 0.6 0.6 0.8 0.4 0.6 0.4 0.4 0.4 0.8 0.6 0.4 0.4 0.8\n [69] 0.4 0.2 0.4 0.2 0.6 0.2 0.2 0.8 0.4 0.4 0.6 0.4 0.6 0.4 0.8 0.6 0.4\n [86] 0.0 0.6 0.4 1.0 0.4 0.6 0.4 0.8 0.0 0.8 0.4 0.8 0.8 0.2 0.6\n\n\nTo create a discretised version of our alien landscapes, all we have to do is liberally pepper our original code with a few calls to discretise(). Here’s an example:\n\ngrid &lt;- new_grid() \ncoords &lt;- generate_curl(grid$x, grid$y, seed = seed)\n\ncanvas &lt;- grid |&gt; \n  dplyr::mutate(\n    curl_x = coords$x |&gt; discretise(50), \n    curl_y = coords$y |&gt; discretise(50),\n    height = generate_fancy_noise(curl_x, curl_y, seed = seed) |&gt; \n      discretise(50),\n    islands = dplyr::if_else(\n      condition = height &lt; median(height),\n      true = median(height),\n      false = height\n    )\n) \n\ncanvas\n\n# A tibble: 1,000,000 × 6\n       x       y curl_x curl_y height islands\n   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1     0 0         0.32      1   0.72    0.72\n 2     0 0.00100   0.32      1   0.72    0.72\n 3     0 0.00200   0.32      1   0.72    0.72\n 4     0 0.00300   0.32      1   0.72    0.72\n 5     0 0.00400   0.32      1   0.72    0.72\n 6     0 0.00501   0.32      1   0.72    0.72\n 7     0 0.00601   0.32      1   0.72    0.72\n 8     0 0.00701   0.32      1   0.72    0.72\n 9     0 0.00801   0.32      1   0.72    0.72\n10     0 0.00901   0.32      1   0.72    0.72\n# ℹ 999,990 more rows\n\n\nIf we plot this as a heatmap, the discrete levels are immediately obvious:\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  image(axes = FALSE, asp = 1, useRaster = TRUE) \n\n\n\n\n\n\n\n\nThis terracing has the effect of levelling out some of the more bizarre features of the alien landscape we plotted earlier. Here’s what we get when we pass this terraced landscape to our render() function:\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  render(zscale = .01) \n\n\n\n\n\n\n\n\nAh, yes. Now we have something that feels closer to the Broken Lands pieces. The twists and contortions of the alien landscape are preserved, but they have now been forced onto a flatter, controlled geometry. The chaos of the alien land has been tamed. This is a domesticated variant. Safe for children and capitalists alike.\n\n\n\nBe smooth\nAt some level I appreciate the stark feel of the previous piece, but even I am not enough of a masochist11 to truly enjoy the brutality of what I just did. All those intricate alien swirls have been flattened and erased so crudely that we are left with something a little too minimal for my tastes.\nAnd so to the second artistic sleight-of-hand: some of the starkness of the last piece can be ameliorated if we apply noise processes in both the embedding space (i.e., noise is applied to curl_x and curl_y) and in the base space (i.e., to x and y). The code for that might look a little like this:\n\ngrid &lt;- new_grid() \ncoords &lt;- generate_curl(grid$x, grid$y, seed = seed)\n\ncanvas &lt;- grid |&gt; \n  dplyr::mutate(\n    curl_x = coords$x |&gt; discretise(50), \n    curl_y = coords$y |&gt; discretise(50),\n    noise_curl = generate_fancy_noise(curl_x, curl_y, seed = seed),\n    noise_base = generate_simplex(x, y, seed = seed),\n    height = (noise_curl + noise_base) |&gt; discretise(50),\n    islands = dplyr::if_else(\n      condition = height &lt; median(height),\n      true = median(height),\n      false = height\n    )\n) \n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  render(zscale = .01) \n\n\n\n\n\n\n\n\nThere is no principle to this. No deep underlying logic. It is simply an attempt to paper over the cracks, to smooth out some of the raw, sharp edges that were left over when we discretised in the first step.\nThere is probably a life metaphor here, but I choose not to look too closely.\n\n\n\nBe chaotic\nThe final layer of trickery involves the colour palette. Throughout this post I’ve used the default “yellow and red” palette that image() uses to create heat map images, but the render() function I wrote at the beginning lets you choose your own colour scheme. For instance, let’s say I want the land to vary smoothly along a “teal and green” colour palette, while having the water stay white (or thereabouts). It’s surprisingly straightforward to do this, by passing a hand crafted vector of colours to render():\n\nshades &lt;- hcl.colors(50, \"TealGrn\")\nshades[1] &lt;- \"#ffffff\"\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  render(shades = shades) \n\n\n\n\n\n\n\n\nThis is so very, very close to the style of imagery in the original Broken Lands series. The only thing missing is a slight feeling of chaos to the colours. If you scroll back up to the top of the post you’ll notice that the original images don’t quite adhere to the smoothly-varying-shades feel of a proper topographic map. The reason for this is that I shuffled the colour palette, so each “level” in the discrete map has a randomly sampled colour from the palette. Here’s some code that does precisely that:\n\ngenerate_shades &lt;- function(palette = \"TealGrn\", n = 50, seed = NULL) {\n  if(!is.null(seed)) {\n    set.seed(seed)\n  }\n  shades &lt;- hcl.colors(n, palette)\n  shades &lt;- sample(shades)\n  shades[1] &lt;- \"#ffffff\"\n  shades  \n}\n\ncanvas |&gt; \n  as.array(value = islands) |&gt;\n  render(shades = generate_shades(seed = seed)) \n\n\n\n\n\n\n\n\n… and there it is. This version of the system isn’t precisely equivalent to the original, but it mirrors it in every respect that matters to me. The magic is all laid bare. There are no artistic secrets left in this system. Everything you need to know about these images is committed to text. Documented. Described. Codified.\nI love the originals no less now that the magic is revealed. There is no art in secrecy."
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#goodnight-sweet-dreams",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#goodnight-sweet-dreams",
    "title": "Shattered landscapes",
    "section": "Goodnight, sweet dreams",
    "text": "Goodnight, sweet dreams\n\nOne day we’re gonna wake up laughing  Put on your dancing shoes  You won’t believe the tales I tell  That time, Danielle, ain’t mine to choose  Danielle, Danielle, Danielle      – Tex Perkins (and others, but whatever…)12\n\nMuch like the Broken Lands system itself,13 this post has a peculiar genesis. If you read the strange year post I wrote a few months ago, you’d be unsurprised to hear that I am attempting to square a few circles right now. Something broke – rather badly – and I’m trying to work out how to put the pieces together even knowing that the shattered parts can’t go back together in the shape they were before. Aspects to my life that were once central to my sense of self are scattered, and there are little slivers of glass laid everywhere – when I attempt to pick up one of the pieces from the floor I get cut deeply by those tiny transparent needles.\nThis post is one of those attempts. One of the pieces I need to pick up is my writing. The little cataclysm of 2022 broke my writing. I didn’t become a bad writer, or at least I don’t think I did. I hope I didn’t! Rather, I lost my sense of ownership over my writing. I felt like I was writing for others rather than myself, as if my blog were a product to be optimised rather than a thing I write because I love writing. It’s been some work trying to reconnect with the joy of writing for my own purposes.\nAnd so to the point…\nI wrote this post because I loved creating the artwork, and it’s written the way it is written because it makes me happy to write again. That’s it. It’s something I wrote because I want to own my words again. There’s no “take home message”. There’s no “call to action”. You can love it, or hate it, or ignore it. That’s okay. I didn’t write it for you – I wrote it for me."
  },
  {
    "objectID": "posts/2023-03-13_shattered-landscapes/index.html#footnotes",
    "href": "posts/2023-03-13_shattered-landscapes/index.html#footnotes",
    "title": "Shattered landscapes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso Lev Grossman I guess, but honestly Hale Appleman’s delivery is so good that I cannot possibly attribute the quote to anyone except Eliot.↩︎\nIt’s actually a slightly different kind of object called a “long grid” but for now I’ll treat it like a tibble↩︎\nOkay that’s not quite correct. The as.array() method works here because we created this object by calling ambient::long_grid() rather than tidyr::expand_grid() or the expand.grid() function in base R. When you call expand_grid() all you’re doing is creating a regular data frame (or tibble), and you can redefine the x and y values however you like. With a long grid object, however, you’re creating a somewhat different data structure that is required to form a grid. You can’t modify the x and y values the same way you would for a tibble, because those values are part of the underlying data structure. That seems like an odd restriction the first time you encounter it, but the fact that the grid itself is immutable is what makes it trivially easy to call as.array() in this context.↩︎\nEaster egg for the three people who know the reference: I rather suspect that what I’ve done in this section is create the artistic version of the “linear ballistic accumulator” model of human choice behaviour, where my original system was a full fledged diffusion model. Happily for all concerned, neither Scott nor Roger are going to read this post.↩︎\nFine. Yes, if you look closely at the code you can see I’m doing more than simply applying a curl field. But please… allow me some latitude here. This is a blog post not a dissertation.↩︎\nPresumably using some magic spray paint that coats every layer of the folded fabric, not just the topmost layer!↩︎\nOkay yes the metaphor is a bit strained, but it’s the best I can think of when what I’m actually doing is constructing a surjective mapping from the base space to the embedding space, adding noise to the mapped values, and then using “magic” to pull back to the base space because I’m a good girl who doesn’t throw away the original values when she performs a many-to-one trick.↩︎\nYes I’m aware that the “many to one” jokes write themselves at this point but as I mentioned previously I am a good girl so I shan’t continue this line of thought.↩︎\nThere is an obvious metaphor for the queer experience of living in a world defined by cisheteronormativity here, but I’ll let you flesh out the rest of the mapping on your own. You’re a clever person, you know how to read between the lines, right?↩︎\nI grew up in a small mining town, four hours drive from the nearest city (sort of: technically it was the town founded around the refinery, and the mines were about an hour away). It was traumatic, and there are not enough drugs in the world to make me elaborate.↩︎\nWhich, let’s be frank, is saying quite a lot.↩︎\nA note for foreigners: if you are an Australian male-attracted person of a certain age, you immediately understand. It doesn’t matter if you’re a bisexual, a straight woman, or a gay man. It doesn’t matter if you got hooked by Beasts of Bourbon, or by Cruel Sea, or by the solo acts. There’s nothing specific to it. There are no invariants to uncover except Tex Perkins.↩︎\nThe very earliest version of the system appeared as a throwaway piece in the quarantine moods series, then became frozen in the ice floes system, re-emerged as the fractured terrain section in the Art From Code workshop, and then eventually became its own thing.↩︎"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html",
    "href": "posts/2021-04-18_pretty-little-clis/index.html",
    "title": "Pretty little CLIs",
    "section": "",
    "text": "Anytime you write R code whose output needs to be understood by a human being, it is an act of kindness to spend a little time making sure that the output shown to the human being properly communicates with that human. As a consequence of this, you often find yourself needing to write information to the R console, just to cater to those precious human sensibilities. Perhaps the simplest way to do this is to use the cat() function. It’s a simple tool and it gets the job done in most cases.\nFor example, consider the use case for the antagonist character “A” from Pretty Little Liars, whose stalking and threats were delivered mostly via text message. Had she used R to craft her threatening text messages, she could have written code like this:\nwait &lt;- function(seconds = 2) {\n  Sys.sleep(seconds)\n}\n\nsend_cat_threat &lt;- function() {\n  cat(\"Dead girls walking.\\n\"); wait()\n  cat(\"--A.\\n\")\n}\nEquipped with a function that specifies her threat, complete with a dramatic pause for effect, she’s ready to go. When her unwitting victim does something to trigger the send_cat_threat() function, a two part message is displayed on the console. The first part shows up immediately\nDead girls walking.\nand after a two second delay, her call sign is revealed\nDead girls walking.\n--A.\nIt’s not too difficult to imagine what this message might look like at the R console, but where’s the fun in that? Thanks to the asciicast package (Csárdi et al. 2019), there’s no need to leave anything to the imagination, and we can see the malevolent message in screencast form:\nUsing cat() to craft messages works perfectly well for simple text communication, but sometimes you want something that looks a little fancier. After all, if the big picture plan here is to impersonate a dead teenager and terrorise her friends - and for some reason you’ve chosen R to do so - you might as well put a little effort into the details, right?"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#meet-the-cli-package",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#meet-the-cli-package",
    "title": "Pretty little CLIs",
    "section": "Meet the cli package",
    "text": "Meet the cli package\nOne thing I love about the R community is that if you search long enough you’ll find that someone else has already written a package that solves the problem you’re facing. If your problem is “how to craft nicely formatted messages” then you’ll be delighted to learn that many wonderful things become possible if you have the cli package (Csárdi 2021a) as your talented assistant. To craft a beautiful command line interface (CLI) of her very own, the first thing A will need to do is load the package:\n\nlibrary(cli)\n\nOnce this is done, it is a very trivial task for A to write the same threatening text message using cli_text()…\n\nsend_cli_threat &lt;- function() {\n  cli_text(\"Dead girls walking.\"); wait()\n  cli_text(\"--A.\")\n}\nsend_cli_threat()\n\n\n\n\n\n\n\n\n\n\n…which is nice and all, but it doesn’t make much of a case for using cli. Stalking and threatening is busy work, and I’d imagine that A would want a more compelling justification before deciding to switch her evil workflow. However - much like A herself - the R console has many dark secrets, and fancier tricks than this are possible once you know how to expose them using cli."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#using-the-status-bar",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#using-the-status-bar",
    "title": "Pretty little CLIs",
    "section": "Using the status bar",
    "text": "Using the status bar\nOne piece of magic that I have wondered about for a long time is how fancy progress bars work: often when you’re doing something that takes a long time, you’ll see an ASCII progress bar rendered in text on the screen, which suddenly vanishes once the process is complete. How exactly does this work? Normally you can’t “unprint” a message from the console, so how is it possible for the progress bar to update without leaving an ugly trail of earlier messages behind it?\nWhile teaching myself cli, I found the answer. The most recent line of text generated at the terminal is speciall. It’s called the status bar: the state of the status bar can be manipulated, and the cli package provides a neat toolkit for doing so. So let’s say I were trying to convince A to switch to the cli tools. Right now, she’s writing a function that will send a four-part message, using cli_text() because I’ve at least convinced her to try the new tools:\n\nmessage_scroll &lt;- function() {\n  cli_text(\"You found my bracelet.\"); wait()\n  cli_text(\"Now come find me.\"); wait()\n  cli_text(\"Good luck bitches.\"); wait()\n  cli_text(\"-A\"); wait()\n}\nmessage_scroll()\n\nWhen her victim triggers this message the lines will appear on screen, one after the other with an appropriate dramatic pause separating them. The victim might see something that looks like this:\n\n\n\n\n\n\n\n\n\nThe problem – when viewed from an evil point of view – is that this message stays on screen after delivery.1 The victim has time to think about it, take a screenshot to show her friends, that kind of thing. Wouldn’t the gaslighting be so much more effective if she were to send the message piece by piece, each part disappearing as the next one appears, only to have the whole thing vanish without a trace and leaving the victim wondering if she imagined the whole thing? This is where the status bar comes in handy. Here’s how it would work:\n\nmessage_inline &lt;- function() {\n  id &lt;- cli_status(\"\")\n  cli_status_update(id, \"You found my bracelet.\"); wait()\n  cli_status_update(id, \"Now come find me.\"); wait()\n  cli_status_update(id, \"Good luck bitches.\"); wait()\n  cli_status_update(id, \"-A\"); wait()\n  cli_status_clear(id)\n}\n\nThe first line in this function uses cli_status() to create a blank message on the status bar, and returns an identifier that refers to the status bar. The next four lines all use cli_status_update() to overwrite the current state of the status bar, and then pause dramatically for two seconds. In a final act of malice, the last line in the function clears the status bar using cli_status_clear(), leaving nothing except a blank space behind. So what the victim sees is something more like this:\n\nmessage_inline()\n\n\n\n\n\n\n\n\n\n\n\n\nThis message was sent to Aria in episode 10 of season one. I’m sure it is deeply important to everyone that I mention this."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#creating-spinners",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#creating-spinners",
    "title": "Pretty little CLIs",
    "section": "Creating spinners",
    "text": "Creating spinners\nThe ability to control the status bar opens up a world of new possibilities. Progress bars are one such possibility, but the progress package (Csárdi and FitzJohn 2019) already does this nicely, and in any case I suspect that A might be more intrigued by the possibility of spinners, since they just spin and spin and give the victim no clue about when the process is going to end. Much more appealing when the developer doesn’t know (or doesn’t want to reveal) when the wait will end. The cli package has a nice makes_spinner function that serves this purpose. Here’s an example:\n\nspinny &lt;- make_spinner(\n  which = \"dots2\",\n  template = \"{spin} It's not over until I say it is.\"\n)\n\nThe which argument is used to choose how the spinner would look, and the template argument is used to define how the “spinny bit” is placed relative to the rest of the text. The spinny object includes functions to update the state of the spinner (in this case spinny$spin() would be that function), and a function to clear the spinner from the status bar. So here’s how A might define a function that uses a spinner to keep the victim in suspense…\n\ntheatrics &lt;- function(which) {\n  \n  # define the spinner\n  spinny &lt;- make_spinner(\n    which = which,\n    template = \"{spin} It's not over until I say it is.\"\n  )\n  \n  # update the spinner 100 times\n  for(i in 1:100) {\n    spinny$spin()\n    wait(.05)\n  }\n  \n  # clear the spinner from the status bar\n  spinny$finish()\n  \n  # send the final part of the message\n  cli_alert_success(\"Sleep tight while you still can, bitches. -A\")\n}\n\nHere’s what happens:\n\ntheatrics(\"dots2\")\n\n\n\n\n\n\n\n\n\n\n\n\nThis message was sent to all four of the liars in the final episode of season one. I don’t think A used a spinner though, which feels like a missed opportunity to me\nSetting which = \"dots2\" is only one possibility. There are quite a lot of different spinner types that come bundled with the cli package, and I’d imagine A would want to look around to see which one suits her needs. Personally, I’m a fan of hearts:\n\ntheatrics(\"hearts\")\n\n\n\n\n\n\n\n\n\n\nTo see the full list use the list_spinners() function:\n\nlist_spinners()\n\n [1] \"dots\"                \"dots2\"               \"dots3\"              \n [4] \"dots4\"               \"dots5\"               \"dots6\"              \n [7] \"dots7\"               \"dots8\"               \"dots9\"              \n[10] \"dots10\"              \"dots11\"              \"dots12\"             \n[13] \"dots13\"              \"dots8Bit\"            \"sand\"               \n[16] \"line\"                \"line2\"               \"pipe\"               \n[19] \"simpleDots\"          \"simpleDotsScrolling\" \"star\"               \n[22] \"star2\"               \"flip\"                \"hamburger\"          \n[25] \"growVertical\"        \"growHorizontal\"      \"balloon\"            \n[28] \"balloon2\"            \"noise\"               \"bounce\"             \n[31] \"boxBounce\"           \"boxBounce2\"          \"triangle\"           \n[34] \"arc\"                 \"circle\"              \"squareCorners\"      \n[37] \"circleQuarters\"      \"circleHalves\"        \"squish\"             \n[40] \"toggle\"              \"toggle2\"             \"toggle3\"            \n[43] \"toggle4\"             \"toggle5\"             \"toggle6\"            \n[46] \"toggle7\"             \"toggle8\"             \"toggle9\"            \n[49] \"toggle10\"            \"toggle11\"            \"toggle12\"           \n[52] \"toggle13\"            \"arrow\"               \"arrow2\"             \n[55] \"arrow3\"              \"bouncingBar\"         \"bouncingBall\"       \n[58] \"smiley\"              \"monkey\"              \"hearts\"             \n[61] \"clock\"               \"earth\"               \"material\"           \n[64] \"moon\"                \"runner\"              \"pong\"               \n[67] \"shark\"               \"dqpb\"                \"weather\"            \n[70] \"christmas\"           \"grenade\"             \"point\"              \n[73] \"layer\"               \"betaWave\"            \"fingerDance\"        \n[76] \"fistBump\"            \"soccerHeader\"        \"mindblown\"          \n[79] \"speaker\"             \"orangePulse\"         \"bluePulse\"          \n[82] \"orangeBluePulse\"     \"timeTravel\"          \"aesthetic\"          \n[85] \"growVeriticalDotsLR\" \"growVeriticalDotsRL\" \"growVeriticalDotsLL\"\n[88] \"growVeriticalDotsRR\""
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#showing-cli-messages-in-r-markdown",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#showing-cli-messages-in-r-markdown",
    "title": "Pretty little CLIs",
    "section": "Showing cli messages in R markdown",
    "text": "Showing cli messages in R markdown\nThroughout this post I’ve relied on asciicast to display screencasts of the R console as animated SVG files, rather than what I might normally do and rely on regular R markdown code chunks to do the work. There’s a reason for this: the R console is a terminal, and its behaviour doesn’t always translate nicely to HTML. Part of the magic of the rmarkdown package (Xie, Allaire, and Grolemund 2018) is that most of the time it is able to capture terminal output and translate it seamlessly into HTML, and we mere mortal users never notice how clever this is. However, when dealing with cli output, we run into cases where this breaks down and the law of leaky abstractions comes into play: text generated at the R console does not follow the same rules as text inserted into an HTML document, and R Markdown sometimes needs a little help when transforming one to the other.\nAn important thing to remember about cli is that the text it produces is a message, so its visibility in R Markdown depends on the chunk option for messages. As long as the message option is set to TRUE, R Markdown will include them as part of the output.2 In the simplest case, R Markdown works nicely, so as long as all A wants to do is send an unformatted threat within an R Markdown document, then this works:\n\ncli_text(\"I'm still here bitches, and I know everything. -A\")\n\nI'm still here bitches, and I know everything. -A\n\n\nHowever, the moment A tries to use any fancy formatting, things will go haywire for her. For example, suppose she wanted to send the message above as a simple “alert” message using cli_alert(), which uses fancy symbols and colours in the output. It is at this point that the cracks in the R Markdown pipeline start to leak. In this case, the leak would result in the document failing to knit and an error message complaining about\nPCDATA invalid Char value\nIntuitively she might guess that somewhere in the R Markdown pipeline, an invalid or malformed character has been created.3 The reason this happens is that the colours and symbols used by cli, and supported in the R console, rely on ANSI escape codes, but those escape codes aren’t recognised in HTML and – apparently – they can wreak havoc when R markdown writes those characters into the HTML document. ANSI colours in R are usually generated with the help of the crayon package (Csárdi 2021b), and per the issue #24 thread that I encounter on a semi-regular basis, it can be tricky to manage the process of translating these to HTML via R Markdown.\nSolving this issue requires A to jump through a few hoops. It’s annoying I know, but no-one ever said that running an unhinged stalking campaign via text messages was easy, right? Her first task is to make sure that the R Markdown document turns on crayon support:\n\noptions(crayon.enabled = TRUE)\n\nThis isn’t the whole solution, however, because while that tells R Markdown to stop ignoring all the ANSI stuff, it doesn’t necessarily allow it to render ANSI sequences properly. To fix this she needs to specify the knit hooks that explicitly tell R Markdown what to do. She can do this with the help of the fansi package (Gaslam 2021), which contains an obscurely-named function sgr_to_html() that translates a subset of the ANSI control sequences to HTML, and strips out all the others. Using this, she can write an ansi_aware_handler() function that will take an input string x and return HTML output appropriate for the R Markdown context:\n\nansi_aware_handler &lt;- function(x, options) {\n  paste0(\n    \"&lt;pre class=\\\"r-output\\\"&gt;&lt;code&gt;\",\n    fansi::sgr_to_html(x = x, warn = FALSE, term.cap = \"256\"),\n    \"&lt;/code&gt;&lt;/pre&gt;\"\n  )\n}\n\nFrom there, it’s relatively easy. All she needs to do is tell knitr (Xie 2021) to use this function whenever it needs to handle output. Just for good measure she might do the same for messages, errors, and warnings:\n\nknitr::knit_hooks$set(\n  output = ansi_aware_handler, \n  message = ansi_aware_handler, \n  warning = ansi_aware_handler,\n  error = ansi_aware_handler\n)\n\nAt long last she is done.4 Her campaign of bullying and cruelty can continue:\n\ncli_alert(\"I'm still here bitches, and I know everything. -A\")\n→ I'm still here bitches, and I know everything. -A\n\n\n\n\nThis message was sent in the pilot episode. Yes, the quotes I’ve used are all from season one: I’ve just started a rewatch of the show, so the early episodes are quite fresh in my memory!"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#writing-longer-messages",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#writing-longer-messages",
    "title": "Pretty little CLIs",
    "section": "Writing longer messages",
    "text": "Writing longer messages\nUp to this point the threatening messages that A has been sending have been short, only one line long. In several cases the messages have been cleverly constructed so that the same line (the status bar) is used to display multiple pieces of text, but ultimately it’s still one line messaging. A needs to take a little care when she wants to branch out. Conceptually, a message should correspond to “one semantically meaningful bundle of information” that might be split over several lines. However, as far as R is concerned, each call to cli_text() creates a distinct message. To see how this might cause A some grief, here’s the letter that she sent to Aria’s mother announcing the infidelity of Aria’s father:\n\nsend_cruel_letter_piecewise &lt;- function() {\n  cli_text('Your husband, Byron, is involved with another woman')\n  cli_text('and when I say involved I mean in a \"romantic\" way.')\n  cli_text('This is not something recent. It started before your')\n  cli_text('family went away to Iceland and from the look of')\n  cli_text('things, it may be starting up again now that you\\'re')\n  cli_text('back. I know this is hard to hear, but it is the')\n  cli_text('truth. If you don\\'t believe this about your husband,')\n  cli_text('ask your daughter. She knows all about it.')\n  cli_text('Sincerely,')\n  cli_text('A')\n}\n\nsend_cruel_letter_piecewise()\nYour husband, Byron, is involved with another woman\n\nand when I say involved I mean in a \"romantic\" way.\n\nThis is not something recent. It started before your\n\nfamily went away to Iceland and from the look of\n\nthings, it may be starting up again now that you're\n\nback. I know this is hard to hear, but it is the\n\ntruth. If you don't believe this about your husband,\n\nask your daughter. She knows all about it.\n\nSincerely,\n\nA\n\n\nThis is not an ideal implementation. What A wants to send is one message spanning 10 lines not 10 separate one-line messages, but it’s the latter that she has actually implemented here. This is where the cli() function is handy: to takes an expression as input and collects all the constituent parts into a single message. This version of the function now sends a single message:\n\nsend_cruel_letter_singly &lt;- function() {\n  cli({\n    cli_text('Your husband, Byron, is involved with another woman')\n    cli_text('and when I say involved I mean in a \"romantic\" way.')\n    cli_text('This is not something recent. It started before your')\n    cli_text('family went away to Iceland and from the look of')\n    cli_text('things, it may be starting up again now that you\\'re')\n    cli_text('back. I know this is hard to hear, but it is the')\n    cli_text('truth. If you don\\'t believe this about your husband,')\n    cli_text('ask your daughter. She knows all about it.')\n    cli_text('Sincerely,')\n    cli_text('A')\n  })\n}\n\nsend_cruel_letter_singly()\nYour husband, Byron, is involved with another woman\nand when I say involved I mean in a \"romantic\" way.\nThis is not something recent. It started before your\nfamily went away to Iceland and from the look of\nthings, it may be starting up again now that you're\nback. I know this is hard to hear, but it is the\ntruth. If you don't believe this about your husband,\nask your daughter. She knows all about it.\nSincerely,\nA\n\n\n\n\nThe letter was sent to Ella in episode four season one. Even on a rewatch I’m finding it impossible to imagine Holly Marie Combs as anyone other than Piper from Charmed and I keep expecting “Ella” to stop time and, idk, shave off her husbands eyebrows or something?\nMuch nicer. As every would-be tormenter knows, it’s important to pay attention to the details."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#creating-structured-messages",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#creating-structured-messages",
    "title": "Pretty little CLIs",
    "section": "Creating structured messages",
    "text": "Creating structured messages\nWriting long messages when sending a threatening letter is a simple enough thing, but at some point A will likely find herself wanting to add some structure to these missives. Lists are nice. Stalkers like keeping lists, I hear. With that in mind, a nice property of cli is that it allows you to separate style from structure using an HTML-like syntax. Top level headings are specified using cli_h1(), and second level headings are produced by cli_h2(). Unordered lists are produced using cli_ul() and ordered lists by cli_ol(). This make it easy to write structured messages to the R console:\n\ncli({\n  cli_h1(\"Characters\")\n  cli_h2(\"The Liars\")\n  cli_ul(c(\n    \"Alison DiLaurentis\",\n    \"Spencer Hastings\",\n    \"Aria Montgomery\",\n    \"Hanna Marin\",\n    \"Emily Fields\"\n  ))\n  cli_h2(\"The A-Team\")\n  cli_ul(c(\n    \"Mona Vanderwaal\",\n    \"Lucas Gottesman\",\n    \"Melissa Hastings\"\n  ))\n})\n\n── Characters ──────────────────────────────────────────────────────────────────\n\n── The Liars ──\n\n• Alison DiLaurentis\n• Spencer Hastings\n• Aria Montgomery\n• Hanna Marin\n• Emily Fields\n\n── The A-Team ──\n\n• Mona Vanderwaal\n• Lucas Gottesman\n• Melissa Hastings\n\n\nBetter yet, the cli package has a whole swathe of other utilities that follow this same HTML-like naming scheme, making it possible to send elaborate and disturbing messages in so many different ways."
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#epilogue",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#epilogue",
    "title": "Pretty little CLIs",
    "section": "Epilogue",
    "text": "Epilogue\nThere is a lot more to the cli package that I haven’t talked about in this post. I’ve not talked about how to modify the themes, how to create custom cli “apps” that use different themes or send output to different connections. I’ve not talked about how to use conditional logic within a cli call, displaying different messages depending on whether a process succeeds or fails. Those will have to remain secret for now, because this post is quite long enough already and quite frankly I’m still learning myself. Besides, these powers would no doubt would be put to terrible purposes in an R-themed Pretty Little Liars spinoff show, and I’m not entirely sure that all secrets need sharing…\n\ncli(\n  cli_blockquote(\n    quote = \"Friends share secrets, that's what keeps us close\",\n    citation = \"Alison\"\n  )\n)\n\n    “Friends share secrets, that's what keeps us close”\n    — Alison"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#last-updated",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#last-updated",
    "title": "Pretty little CLIs",
    "section": "Last updated",
    "text": "Last updated\n\n2023-05-27 18:21:57.217467"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#details",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#details",
    "title": "Pretty little CLIs",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2021-04-18_pretty-little-clis/index.html#footnotes",
    "href": "posts/2021-04-18_pretty-little-clis/index.html#footnotes",
    "title": "Pretty little CLIs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, it does disappear in this screencast, but that’s just the screencast. If it were the R console it would stay on screen the whole time.↩︎\nSomewhat counterintuitively, although cli emits messages that can be suppressed by suppressMessages(), they don’t behave precisely the same as the messages produced by message(). The default handler for base R messages sends the output to the stderr() connection and so they are often shown as the dreaded “red text” that users learn to fear. To avoid this, the default behaviour in cli sends messages to the stdout() connection, thereby avoiding this issue. However, cli does allow you to control this behaviour: see the start_app() and stop_app() functions for more information.↩︎\nAs an aside, if you’re running a site with an RSS feed it may also write malformed characters into the index.xml file as well as any generated .html file. When I encountered this problem I found that even when I “fixed” my .Rmd file the document wouldn’t re-knit, because of the problems with the xml file. Eventually I realised that I could solve the problem by deleting the index.xml file for the RSS feed and then knitting again. Sigh↩︎\nNote that there is also the fansi::set_knit_hooks() function which will set the hooks in a more user-friendly way. I don’t think there’s any reason not to use it: the only reason I didn’t is that I found it convenient to write things from scratch here so that I understood what was happening.↩︎"
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html",
    "href": "posts/2023-08-28_rxode2/index.html",
    "title": "Pharmacometric simulation with rxode2",
    "section": "",
    "text": "Hello, yes, this is another pharmacometrics post. There have been quite a few of these lately as I try to bring myself up to speed on a new discipline. This one is about the rxode2 package, a pharmacometric simulation tool and the successor to the widely-used RxODE package.1 Although the original RxODE package is now archived on CRAN, the syntax for rxode2 is very similar, and as far as I can tell it’s fairly (fully?) backward-compatible with the older package."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#installation",
    "href": "posts/2023-08-28_rxode2/index.html#installation",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Installation",
    "text": "Installation\nAs with other packages for pharmacometric simulation such as mrgsolve, models defined with rxode2 need to be compiled before they are run, and so when you install the package you need the appropriate build tools. There are some implications to this. The package is on CRAN, so you can install it with:\ninstall.packages(\"rxode2\")\nHowever, like most R packages that allow you to compile C/C++/Fortran/Rust/Your-Favourite-Language-Here code, it relies heavily on system dependencies that you may or may not have, and managing the build tools is an OS-specific thing. I’m running Ubuntu 22.04, and (for reasons that don’t bear mentioning) I recently did a “factory reset”2 and did a fresh install of Ubuntu. So, yeah, I didn’t have everything I needed. Yes, I did have the gcc compiler installed, but that’s not the only system dependency you have to care about. In my case, I was missing gfortran, libblas, and liblapack. As a consequence, when I tried to run the example code on the package website, all I got was a long stream of error messages. In order to get started, I had to do this:\nsudo apt install gfortran libblas-dev liblapack-dev liblapack-doc\nThat worked for me, but I make no promises that it will work for you. Caveat emptor and all that.3 But let’s not stand on installation formalities when there are simulations to run. It is time to load some packages and dive once more into the abyss…\n\nlibrary(rxode2)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(microbenchmark)"
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#the-rxode2-mini-language",
    "href": "posts/2023-08-28_rxode2/index.html#the-rxode2-mini-language",
    "title": "Pharmacometric simulation with rxode2",
    "section": "The rxode2 mini-language",
    "text": "The rxode2 mini-language\n\nI don’t understand  You claiming I’m a handful when you show up all empty-handed  The way you say you love me like you’ve just been reprimanded  ’Cause I know you like mind games      – BANKS\n\n\n\n\nThe story begins with a little commentary on the slippery nature of R as a programming language. It’s not exactly news to many people at this point, but R is famous4 for the extremely widespread use of metaprogramming as a tool for implementing domain-specific languages within R itself.5 As a consequence of this, the same code can have different meaning when called in different contexts. It’s both a curse and a blessing: one the one hand it makes R very flexible in a way that is convenient for analysts, but on the other hand it can be a bit confusing to people from a more conventional programming background who don’t expect R to work this way.\nThe use of domain-specific languages in pharmacometric modelling is not uncommon: for instance, in my previous post about mrgsolve, I talked about the mini-language used to specify models in that package. Not surprisingly, rxode2 has its own mini-language with it’s own custom syntax. In mrgsolve, you can specify a model by writing the code for it in a separate file, or passing it as a string within R. You can do that with rxode2 too, but rxode2 also allows you to pass the model specification as a code block: a collection of statements enclosed in curly braces and treated as a single expression. Here’s an example of how that works:\n\nmod &lt;- rxode2({\n  # initial values for all four \"compartments\"\n  depot(0) = 0;\n  central(0) = 0;\n  peripheral(0) = 0;\n  auc(0) = 0;\n  \n  # drug concentrations\n  CP = central / VC;    # central compartment concentration\n  PP = peripheral / VP; # peripheral compartment concentration\n  \n  # differential equations\n  d/dt(depot)       = -(KA * depot);\n  d/dt(central)     =  (KA * depot) - (Q * CP) + (Q * PP) - (CL * CP);\n  d/dt(peripheral)  =  (Q * CP) - (Q * PP);\n  d/dt(auc)         =  CP;\n})\n\nIf you don’t look at it too closely you might think this is regular R code, but… it isn’t. The code contained within the braces is captured by the rxode2() function, and then interpreted according to the rules of the mini-language. We’ll need to take a moment to unpack the mini-language itself, but that can wait.\nLet’s start by looking at this as a pharmacometrician might. Notice that although this is a two-compartment model in pharmacometric terms, from the perspective of rxode2 there are four “compartments” that define the state of the system. In addition to the usual two compartments (central and peripheral), there is an extravascular depot compartment used to model drug intake. For instance, for an orally-administered drug the depot compartment would be the gut.6 The depot compartment is “real” in the sense that it is loosely intended to correspond to something in the physical system that we’re modelling. By convention we don’t consider it to be one of the pharmacokinetic compartments, but it’s still a real thing. In contrast, the auc “compartment” has no physical analog at all. It’s included so that the model keeps track of the accumulated drug exposure.7 As I’m quickly coming to learn, this is a very handy trick when running pharmacometric simulations.\nNow that we’ve looked at it as an analyst, let’s look at it as a programmer. The syntax within the rxode2 model specification is not “real” R code. The statements enclosed within the curly braces look vaguely R-like, but if you tried to evaluate these expressions outside the context of the rxode2() function, you’d get errors. Thanks to the magic of non-standard evaluation in R, the rxode2() function is able capture the code before it is evaluated, and prevents R from evaluating it the way it normally would. Instead of following the regular rules of R, it follows the syntax provided by the rxode2 mini-language. This mini-language is similar to R in some ways:\n\nAssignment statements can use = or &lt;- as the assignment operator.8\nComments are specified using the hash (#) character\nSemi-colon characters (;) are optional, and specify the end of a line\n\nHowever, there are specialised statements used in the mini-language that don’t exist in regular R code. For example, there are two kinds of special statements I’ve used in this code:\n\nTime-derivative statements (i.e., the ones that have something like d/dt(central) on the left hand side) are used to specify the differential equations in the ODE system.\nInitial-condition statements (i.e., the ones where I set something like central(0) on the left hand side) are used to specify the initial state of the ODE system.\n\nYou can check the rxode2 syntax page for more information about the mini-language and what other kinds of special statements exist."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#the-rxode2-model-object",
    "href": "posts/2023-08-28_rxode2/index.html#the-rxode2-model-object",
    "title": "Pharmacometric simulation with rxode2",
    "section": "The rxode2 model object",
    "text": "The rxode2 model object\nIn the previous section I used the rxode2() function to specify a pretty standard two-compartment pharmacokinetic model, and assigned the resulting model object to a boringly-named variable called mod.9 The model object is the primary vehicle for interfacing with the compiled code from R, so it’s helpful to take a look at it:\n\nmod\nrxode2 2.0.13 model named rx_7b738a16dd646d432336a380787bd163 model (✔ ready). \nx$state: depot, central, peripheral, auc\nx$params: VC, VP, KA, Q, CL\nx$lhs: CP, PP\n\n\nAgain, there are a few things to unpack in this output:\n\nThe first line of the output has some technical information about the model. It tells us what version of rxode2 was used to build the model, gives us the name of the built model (see below), and tells us that it’s ready to use.10\nThe second line tells us about mod$state, which in this case are the four “compartment” variables that comprise the state vector for the underlying ODE system.\nThe third line tells us about mod$params, the list of parameters that need to be passed to the model as input to the simulation\nThe fourth line tells us about mod$lhs, the list of additional defined variables that are created by the model and whose value will be recorded in the output.\n\nLike many R packages that generate compiled code, rxode2 manages the compiled object for you. The long unintelligible “name” assigned to our model gives us the hint we need to find the compiled objects. Within the R session temp directory, the rxode2 package has created an “rxode2” subfolder.11 And indeed, if I take a peek at the contents of this folder, I find something with an identical name:\n\nfs::dir_ls(fs::path(tempdir(), \"rxode2\"))\n/tmp/RtmpC0i8oK/rxode2/018374b1ca9115fbc3be9f765f32e47f.md5\n/tmp/RtmpC0i8oK/rxode2/rx_7b738a16dd646d432336a380787bd163__.rxd\n\n\nOkay, makes sense."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#event-tables",
    "href": "posts/2023-08-28_rxode2/index.html#event-tables",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Event tables",
    "text": "Event tables\nEvent tables (also called event schedules) are the primary way the user specifies things that happen in the simulation. These mostly consist of two kinds of event: dosing events, where the drug is administered, and observation events, where the state of the system is measured. In the rxode2 package these are specified with the et() function, and you can use the pipe operator to build up complex event schedules. I’ll take my example from the rxode2 documentation, and walk through it slowly. One nice thing about the event schedules in rxode2 is that you can specify units, so we’ll start with an event table that doesn’t contain any actual events, but specifies the units in which those events will be expressed:\n\nevents &lt;- et(amountUnits = \"mg\", timeUnits = \"hours\")\nevents\n── EventTable with 0 records ──\n0 dosing records (see x$get.dosing(); add with add.dosing or et)\n0 observation times (see x$get.sampling(); add with add.sampling or et)\n\n\nThe output here isn’t super exciting, since there are no actual events encoded here. But it does let me mention one nice little feature of rxode2: the print methods are generally quite informative, and have nice little “nudges” like the ones you can see above that can help new (or even experienced) users work out what they might need to do next.\nAnyway, let’s add some dosing events, shall we? Let’s assume an initial dose of amt = 10000 (in milligrams) is administered at time = 0, and repeated for an additional 9 times at 12 hour intervals (i.e., addl = 9, ii = 12). In the interests of being explicit, I’ll set cmt = \"depot\" to be clear about which compartment the dose is administered to.\n\nevents &lt;- events |&gt; \n  et(time = 0, amt = 10000, addl = 9, ii = 12, cmt = \"depot\")\n\nevents\n── EventTable with 1 records ──\n1 dosing records (see x$get.dosing(); add with add.dosing or et)\n0 observation times (see x$get.sampling(); add with add.sampling or et)\nmultiple doses in `addl` columns, expand with x$expand(); or etExpand(x)\n── First part of x: ──\n# A tibble: 1 × 6\n   time cmt     amt    ii  addl evid        \n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;evid&gt;      \n1     0 depot 10000    12     9 1:Dose (Add)\n\n\nThis format for an event table – where time, amt, addl, and ii are used to specify a sequence of regularly spaced dosing events in a single row – will seem quite familiar to anyone in the field, and since I’ve talked about this notation in previous posts, I’ll not bore people by explaining it yet again.\nMoving along, let’s also assume that after 120 hours has passed (time = 120) the dosing schedule changes: the dose drops to amt = 2000 milligrams, the interdose interval is increased slightly to ii = 14 hours, and this dosing regime is maintained for addl = 4 additional doses (i.e., 5 in total). So now we have this:\n\nevents &lt;- events |&gt; \n  et(time = 120, amt = 2000, addl = 4, ii = 14, cmt = \"depot\")\n\nevents\n── EventTable with 2 records ──\n2 dosing records (see x$get.dosing(); add with add.dosing or et)\n0 observation times (see x$get.sampling(); add with add.sampling or et)\nmultiple doses in `addl` columns, expand with x$expand(); or etExpand(x)\n── First part of x: ──\n# A tibble: 2 × 6\n   time cmt     amt    ii  addl evid        \n  &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;evid&gt;      \n1     0 depot 10000    12     9 1:Dose (Add)\n2   120 depot  2000    14     4 1:Dose (Add)\n\n\nNow that we have specified all the dosing events, we need to add the “observation” events. In a real study, observation times would be the times at which we take a real-world measurement of some kind, but in the context of the simulation it’s just a set of times at which the state of the system is computed. Let’s compute the state of the system for the first 300 hours:\n\nevents &lt;- events |&gt; et(time = 0:300)\nevents \n── EventTable with 303 records ──\n2 dosing records (see x$get.dosing(); add with add.dosing or et)\n301 observation times (see x$get.sampling(); add with add.sampling or et)\nmultiple doses in `addl` columns, expand with x$expand(); or etExpand(x)\n── First part of x: ──\n# A tibble: 303 × 6\n    time cmt     amt    ii  addl evid         \n   &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;evid&gt;       \n 1     0 (obs)    NA    NA    NA 0:Observation\n 2     0 depot 10000    12     9 1:Dose (Add) \n 3     1 (obs)    NA    NA    NA 0:Observation\n 4     2 (obs)    NA    NA    NA 0:Observation\n 5     3 (obs)    NA    NA    NA 0:Observation\n 6     4 (obs)    NA    NA    NA 0:Observation\n 7     5 (obs)    NA    NA    NA 0:Observation\n 8     6 (obs)    NA    NA    NA 0:Observation\n 9     7 (obs)    NA    NA    NA 0:Observation\n10     8 (obs)    NA    NA    NA 0:Observation\n# ℹ 293 more rows\n\n\nAnd now we’re done. We have a complete events table that can be used in our simulation. Admittedly, I went through that awfully slowly. The whole thing could have been bundled into a single pipeline like this:\n\nevents &lt;- et(amountUnits = \"mg\", timeUnits = \"hours\") |&gt;\n  et(time = 0, amt = 10000, addl = 9, ii = 12, cmt = \"depot\") |&gt;\n  et(time = 120, amt = 2000, addl = 4, ii = 14, cmt = \"depot\") |&gt;\n  et(time = 0:300)"
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#simulating-one-subject",
    "href": "posts/2023-08-28_rxode2/index.html#simulating-one-subject",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Simulating one subject",
    "text": "Simulating one subject\n\nI was alone, falling free\nTrying my best not to forget\nWhat happened to us\nWhat happened to me\n   – Placebo12\n\nWe’re now almost at a point where we can run a simple simulation using the model specified via the mod object, and the events table in events. The only thing we haven’t done yet is specify pharmacokinetic parameters that need to be passed to the model as input. To keep things simple, I’ll simulate only a single subject, and so the input parameters will be passed as a table with one row corresponding to our lone subject, and one column per parameter that needs to be specified. If we look at the model spec we can see that requires all of the following to be given:\n\nelimination clearance (CL)\nabsorption rate constant (KA)\nintercompartmental clearance (Q)\nvolume of distribution for the central compartment (VC)\nvolume of distribution for the peripheral compartment (VP)\n\nIndeed, if we take a look at mod$params we see the same listing:\n\nmod$params\n\n[1] \"VC\" \"VP\" \"KA\" \"Q\"  \"CL\"\n\n\nOkay, so let’s put together a one-row data frame params containing all these parameters for a single simulated subject:\n\nparams &lt;- tibble(\n  KA = 0.294,\n  CL = 18.6,\n  VC = 40.2,\n  VP = 297,\n  Q = 10.5\n)\nparams\n# A tibble: 1 × 5\n     KA    CL    VC    VP     Q\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.294  18.6  40.2   297  10.5\n\n\nNow that we have our parameters, we’re ready to go. There are several ways you can call the solver and run the simulation (documentation here), but I’m currently quite partial to calling solve(),13 like so:\n\nout &lt;- solve(mod, params, events)\n\nWhen we print out, we get a fairly detailed description of the simulation that includes information about the parameters and the initial state:\n\nout\n── Solved rxode2 object ──\n── Parameters (x$params): ──\n     VC      VP      KA       Q      CL \n 40.200 297.000   0.294  10.500  18.600 \n── Initial Conditions (x$inits): ──\n     depot    central peripheral        auc \n         0          0          0          0 \n── First part of data (object): ──\n# A tibble: 301 × 7\n   time    CP    PP  depot central peripheral   auc\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1     0   0   0     10000       0          0    0  \n2     1  44.4 0.920  7453.   1784.       273.  26.4\n3     2  54.9 2.67   5554.   2206.       794.  77.7\n4     3  51.9 4.46   4140.   2087.      1324. 132. \n5     4  44.5 5.98   3085.   1789.      1776. 180. \n6     5  36.5 7.18   2299.   1467.      2132. 221. \n# ℹ 295 more rows\n\n\nExtremely pretty print method notwithstanding, under the hood it’s nothing fancy. It’s a regular data frame with a few extra classes and some metadata, which means we can pass it straight to ggplot without any coercion, and draw a pretty picture:\n\nggplot(out, aes(time, CP)) + \n  geom_area(linewidth = 1) + \n  ylab(\"concentration\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nYep, that looks about right."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#simulating-multiple-subjects",
    "href": "posts/2023-08-28_rxode2/index.html#simulating-multiple-subjects",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Simulating multiple subjects",
    "text": "Simulating multiple subjects\n\nI like big boys, itty bitty boys\nMississippi boys, inner city boys\nI like the pretty boys with the bow tie\nGet your nails did, let it blow dry\nI like a big beard, I like a clean face\nI don’t discriminate, come and get a taste\nFrom the playboys to the gay boys\nGo and slay, boys, you my fave boys\n    –Lizzo\n\nThe previous example shows how to simulate a single subject. However, the world is full of lots of different people with different characteristics, so in a more realistic simulation scenario we would want to simulate many people with different parameter values. In order to accommodate this, the parameter table now has multiple rows:\n\nparams &lt;- tibble(\n  KA = rnorm(20, mean = 0.294, sd = 0.03),\n  CL = rnorm(20, mean = 18.6, sd = 2),\n  VC = rnorm(20, mean = 40.2, sd = 2),\n  VP = rnorm(20, mean = 297, sd = 10),\n  Q = rnorm(20, mean = 10.5, sd = 1)\n)\nparams\n# A tibble: 20 × 5\n      KA    CL    VC    VP     Q\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0.275  20.4  39.9  321.  9.93\n 2 0.300  20.2  39.7  297. 10.4 \n 3 0.269  18.7  41.6  304. 11.7 \n 4 0.342  14.6  41.3  297.  8.98\n 5 0.304  19.8  38.8  290. 11.1 \n 6 0.269  18.5  38.8  299. 10.8 \n 7 0.309  18.3  40.9  279. 11.6 \n 8 0.316  15.7  41.7  312. 10.2 \n 9 0.311  17.6  40.0  299. 10.9 \n10 0.285  19.4  42.0  319. 10.8 \n11 0.339  21.3  41.0  302.  9.96\n12 0.306  18.4  39.0  290. 11.7 \n13 0.275  19.4  40.9  303. 11.7 \n14 0.228  18.5  37.9  288. 11.2 \n15 0.328  15.8  43.1  284. 12.1 \n16 0.293  17.8  44.2  300. 11.1 \n17 0.294  17.8  39.5  293.  9.22\n18 0.322  18.5  38.1  297.  9.93\n19 0.319  20.8  41.3  298.  9.28\n20 0.312  20.1  39.9  291. 10.0 \n\n\nThe command to run the simulation remains unchanged:\n\nout &lt;- solve(mod, params, events)\n\nI’ll show you the out object in a moment, but it’s probably easier to understand it if we start with a plot:\n\nout |&gt;\n  dplyr::mutate(sim.id = paste(\"Subject\", sim.id)) |&gt;\n  ggplot(aes(time, CP, fill = sim.id)) + \n  geom_area(linewidth = 1, show.legend = FALSE) + \n  facet_wrap(~ sim.id, nrow = 5) + \n  labs(x = \"Time\", y = \"Concentration\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nAs you can see, all 20 subjects have qualitatively similar profiles, but there are noticeable differences in the details. Not surprisingly really. I didn’t build in very much variability into the simulation, and I didn’t even try to incorporate an appropriate covariance structure among the parameters (that’s a topic for another post).14 The main thing that matters here is that we can see that the variation exists.\nAnyway, let’s have a look at the table of results out produced by our simulation. As you probably guessed from the ggplot2 code, there’s a column called sim.id that stores the subject identifier, and there are 20 times as many rows as last time, but it’s essentially the same:\n\nout\n── Solved rxode2 object ──\n── Parameters (x$params): ──\n# A tibble: 20 × 6\n   sim.id    VC    VP    KA     Q    CL\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1      1  39.9  321. 0.275  9.93  20.4\n 2      2  39.7  297. 0.300 10.4   20.2\n 3      3  41.6  304. 0.269 11.7   18.7\n 4      4  41.3  297. 0.342  8.98  14.6\n 5      5  38.8  290. 0.304 11.1   19.8\n 6      6  38.8  299. 0.269 10.8   18.5\n 7      7  40.9  279. 0.309 11.6   18.3\n 8      8  41.7  312. 0.316 10.2   15.7\n 9      9  40.0  299. 0.311 10.9   17.6\n10     10  42.0  319. 0.285 10.8   19.4\n11     11  41.0  302. 0.339  9.96  21.3\n12     12  39.0  290. 0.306 11.7   18.4\n13     13  40.9  303. 0.275 11.7   19.4\n14     14  37.9  288. 0.228 11.2   18.5\n15     15  43.1  284. 0.328 12.1   15.8\n16     16  44.2  300. 0.293 11.1   17.8\n17     17  39.5  293. 0.294  9.22  17.8\n18     18  38.1  297. 0.322  9.93  18.5\n19     19  41.3  298. 0.319  9.28  20.8\n20     20  39.9  291. 0.312 10.0   20.1\n── Initial Conditions (x$inits): ──\n     depot    central peripheral        auc \n         0          0          0          0 \n\nSimulation without uncertainty in parameters, omega, or sigma matricies\n\n── First part of data (object): ──\n# A tibble: 6,020 × 8\n  sim.id  time    CP    PP  depot central peripheral   auc\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1      1     0   0   0     10000       0          0    0  \n2      1     1  41.6 0.757  7594.   1657.       243.  24.7\n3      1     2  51.2 2.20   5767.   2041.       705.  72.7\n4      1     3  48.4 3.66   4380.   1928.      1176. 123. \n5      1     4  41.5 4.93   3326.   1656.      1581. 168. \n6      1     5  34.2 5.93   2526.   1362.      1902. 206. \n# ℹ 6,014 more rows\n\n\nAs we’ve seen throughout the post, the print method has lots of nice touches. It shows the simulation parameters as well as the simulation results, and has a very gentle message reminding me I haven’t incorporated measurement error, random effects, or parameter uncertainty. Which… I mean, I intentionally left those things out, but actually I do appreciate the clear statement of what wasn’t done here."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#performance-considerations",
    "href": "posts/2023-08-28_rxode2/index.html#performance-considerations",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Performance considerations",
    "text": "Performance considerations\n\nAin’t no use in trying to slow me down\n’Cause you’re running with the fastest girl in town\nAin’t you baby?\n   – Miranda Lambert\n\nFor small simulations like the ones I’m running in this post, you really don’t need to care much about performance. However, when you start running larger simulations it starts to matter a lot. To that end there’s a nice article on speeding up rxode2 in the package documentation which I’ve already found extremely useful at work when doing a little bit of code profiling on analysis code. Since this does matter a fair bit in practice, I’ll walk through the same ideas here.\nLet’s define a few functions that run the simulations in different ways. First, I’ll start with a solve_loop() function that deliberately strips out any form of multi-threading. Each row in params is passed as a separate call to solve(), nested inside a for loop:\n\nsolve_loop &lt;- function() {\n  for(i in 1:nrow(params)) solve(mod, params[i, ], events)\n}\n\nThis is our baseline case. It’s designed to make life as difficult as possible for rxode2 by enforcing single threaded execution within R. We can improve on this considerably by passing the entire params data frame, allowing rxode2 to run the simulations in parallel. I haven’t looked under the hood to work out exactly how rxode2 manages the parallelism15 Here are three functions that explicitly request 1, 2 or 4 cores/threads:\n\nsolve_thread_1 &lt;- function() solve(mod, params, events, cores = 1)\nsolve_thread_2 &lt;- function() solve(mod, params, events, cores = 2)\nsolve_thread_4 &lt;- function() solve(mod, params, events, cores = 4)\n\nFrom experience, I’ve learned that there’s almost never anything to be gained by trying to execute more than four resource-hogging threads simultaneously on my laptop, so I’ll be sensible and won’t try anything more than that. Let’s take a look at the difference in performance for each of these functions:\n\nbench &lt;- microbenchmark(\n  solve_loop(),\n  solve_thread_1(),\n  solve_thread_2(),\n  solve_thread_4()\n)\nbench\n\nUnit: milliseconds\n             expr       min        lq      mean    median       uq       max neval\n     solve_loop() 58.630598 62.227245 68.912034 64.780701 68.78793 168.11848   100\n solve_thread_1()  8.515051  8.867159  9.635308  9.126248  9.45503  26.11143   100\n solve_thread_2()  7.497051  7.720607  8.476758  7.968412  8.44852  15.94618   100\n solve_thread_4()  6.538996  7.063265  7.761950  7.160678  7.54286  17.27997   100\n\n\nYou can see from looking at the table that there’s a big drop in performance when we force rxode2 to simulate each subject one at a time within a loop: solve_loop() is much, much slower than any of the others. Increasing the number of threads from one to four helps a fair bit too, but not to the same dramatic extent. This is even more apparent when we visualise the results:\n\nautoplot(bench)\n\n\n\n\n\n\n\n\nAdmittedly, the time scale here is such that it doesn’t really matter much, but for more realistic examples I’ve played with the speed-up seems to be pretty similar and it can make a big difference to the performance of analysis code."
  },
  {
    "objectID": "posts/2023-08-28_rxode2/index.html#footnotes",
    "href": "posts/2023-08-28_rxode2/index.html#footnotes",
    "title": "Pharmacometric simulation with rxode2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs an aside: when getting started, I found it a little easier to look at the rxode2 user manual than to work from the pkgdown site. As far as I can tell it’s essentially the same material, but the manual organises it in a linear fashion that makes it a little clearer to new users because you get a better sense of the order in which to read things.↩︎\nDoes that term even make sense for a linux machine? It’s not like the thing shipped with linux in the first place. Whatever.↩︎\nI haven’t extensively checked the dependencies on other operating systems, but from what I can tell a Windows install requires RTools.↩︎\nOr notorious, depending on your perspective↩︎\nMetaprogramming in R relies on the fact that R adopts a lazy evaluation model for code execution. This allows the programmer to capture user code passed to a function before it is evaluated, modify the code as desired, and indeed prevent it being evaluated at all. R is hardly the only language to adopt this approach, but it does put it in contrast to languages like Python that adopt an eager evaluation approach.↩︎\nIn this post I’m assuming the drug has bioavailability of \\(F = 1\\), but that’s not true generally, so you’d have to model this explicitly by scaling the drug amount that passes from the gut to the central compartment in the ODE equations.↩︎\nIn essence, the value of auc that accrues is a numerical estimate of the time-integral of drug concentration. This “area under the curve” measure is one of several different measures used to assess drug exposure. I talked a lot about the AUC measure in my post on non-compartmental analysis.↩︎\nThe rxode2 mini-language also allows you to use ~ for this purpose, but I’m not going to do that here. For this post, I’ve chose to use = as a way of reminding myself that my model specification isn’t “normal” R code.↩︎\nModel objects in rxode2 have S3 class “rxode2”.↩︎\nYou can customise this name if you care deeply about such things. As noted in the roxde2() documentation, there is a modName argument that you can use for this purpose. Because this name is used throughout the C compilation process, it must start with a letter and contain only alphanumeric ASCII characters.↩︎\nYes, you can customise this too, by specifying the wd argument to rxode2().↩︎\nI actually feel bad about referencing “Meds” in this post, because let’s face it “The sex, and the drugs, and the complications” would be a fucking magnificent title for a blog post about PKPD models with covariates. Oh who am I kidding? I’m absolutely going to write a post with that title.↩︎\nExperienced R users would not be surprised to discover that solve() is an S3 generic defined in the base package, and equally unsurprised to note that rxode2 defines a method for “rxode2” objects such as mod. It somehow makes me happy to see solve() used this way.↩︎\nNote to future-Danielle: there is a nice discussion of this in the rxode2 context specifically, in the article on population simulation.↩︎\nIs it purely multi-threading we’re talking about? Do we care deeply about the multi-thread/multi-core distinction? Does SIMD come into play? Most importantly, does the author really want to be bothered writing a deep dive on these topics when the audience consists almost entirely of people who (a) already understand these topics or (b) do not care about these topics? The answer to that last one is no. No she does not.↩︎"
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html",
    "href": "posts/2024-01-26_splatter/index.html",
    "title": "Splatter",
    "section": "",
    "text": "Over the last few weeks I’ve been posting generative art pieces on mastodon from a generative art system I’ve called splatter. It creates pieces like these:\nThe splatter series has a lot in common with other generative art systems I’ve built in the past, and in many respects is a spiritual descendant of both curled and water colours.1 However, the system has developed enough of its own “look and feel” by now that I think of splatter as its own thing, deserving a writeup of its very own."
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#the-basic-idea",
    "href": "posts/2024-01-26_splatter/index.html#the-basic-idea",
    "title": "Splatter",
    "section": "The basic idea",
    "text": "The basic idea\nThe core logic of splatter is pretty simple three-step process:\n\nCreate a “base” image that provides the large-scale structure to the piece (left panel below).\nUsing the “base” data as input, create a “flowing” version of the same image (middle panel below). We do this by placing a particle at every cell in the grid, defining a vector field over the grid that describes how each particle moves, and then tracing out the paths of all the particles over many iterations.\nTaking the “flowing” data as input, apply a randomly generated palette and create a prettily-coloured version of the image (right panel below).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ll unpack these steps below, showing the actual code used to create these images and walking through some of the logic and design considerations that goes into building a system like this one.2"
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#creating-a-base-image",
    "href": "posts/2024-01-26_splatter/index.html#creating-a-base-image",
    "title": "Splatter",
    "section": "Creating a base image",
    "text": "Creating a base image\nFirst let’s take a look at the base images. As you can see from the one shown in the left panel above, these aren’t super-complicated things and they could be generated in many different ways. Somewhat arbitrarily, I decided that for this system I’d build them using via simple cellular automaton implemented in C++ and called from R via Rcpp. There’s no principled reason for this, I just happened to have suitable code lying around from a previous generative art system called stoneskip, which produces pieces like these:3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat said, the implementation used in splatter is different from the stoneskip system. The C++ code below defines an automaton() function that is exposed to R via Rcpp, and can be used to create the base images:4 5\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// wrap position to grid\nint wrap(int pos, int size) {\n  if(pos &lt; 0) pos = pos + size;\n  if(pos &gt;= size) pos = pos - size;\n  return pos;\n}\n\n// automaton run function\n// [[Rcpp::export]]\nNumericMatrix automaton(int n_rows, int n_cols, int iterations, int max_span) {\n  \n  int source_row = 0;\n  int source_col = 0;\n  int span_row = 0;\n  int span_col = 0;\n  int row = 0;\n  int col = 0;\n  int r = 0;\n  int c = 0;\n  double source_val = 0;\n  \n  NumericMatrix grid(n_rows, n_cols);\n  for (int row = 0; row &lt; n_rows; row++) {\n    for (int col = 0; col &lt; n_cols; col++) {\n      grid(row, col) = R::runif(0, 1);\n    }\n  }\n  \n  for (int it = 0; it &lt; iterations; it++) {\n    source_row = floor(R::runif(0, n_rows));\n    source_col = floor(R::runif(0, n_cols));\n    source_val = grid(source_row, source_col);\n    span_row = floor(R::runif(0, max_span));\n    span_col = floor(R::runif(0, max_span));\n    row = source_row - span_row;\n    col = source_col - span_col;\n    do {\n      c = wrap(col, n_cols);\n      do {\n        r = wrap(row, n_rows);\n        grid(r, c) = (grid(r, c) + source_val) / 2;\n        row++;\n      } while (row &lt; source_row + span_row);\n      col++;\n    } while (col &lt; source_col + span_col);\n  }\n  \n  return grid;\n}\n\nUltimately this algorithm is very simple. It’s approximately this:\n\nCreate a grid matrix and populate every element with a random number.\nOn every iteration:\n\nPick a random cell in grid to be the “source cell” and treat the value currently assigned to that cell to be the “source value”.\nDefine a rectangular “neighbourhood” around that cell (with a random width and height).\nFor every cell in the neighbourhood, update the value to be the average of its current value and the source value.\n\nReturn grid.\n\nIf you ran it long enough it would eventually produce a completely homogeneous grid, but if you run it for a modest amount of time you end up with somewhat smooth blobbish shapes.\nIn the normal course of events this code would live in .cpp file and then I’d call Rcpp::sourceCpp() from R to trigger compilation and linking to R, but in the context of this blog post I don’t have to because I’m using the Rcpp language engine which automatically takes care of that within the R session that is used to render the post. So we can skip that step, and go straight to the part where we define a base_data() function in R that constructs a data frame with columns specifying row and column indices, and using the automaton() function to assign a value to the corresponding cell:\n\nbase_data &lt;- function(seed, rows, cols, iterations, span) {\n  set.seed(seed)\n  tidyr::expand_grid(\n    x = seq(0, 1, length.out = cols),\n    y = seq(0, 1, length.out = rows),\n    z = 1,\n    iter = 0\n  ) |&gt;\n    dplyr::mutate(\n      id = dplyr::row_number(),\n      value = t(automaton(rows, cols, iterations, span)) |&gt;\n        as.vector() |&gt;\n        ambient::normalise()\n    )\n}\n\nThe primary intention in this function is to create a data frame that has columns x and y that specify coordinates for every element in a grid, and a value column that is populated by calling the automaton() function that Rcpp exposes to the R session.6 Here it is in action:\n\nbase &lt;- base_data(\n  seed = 789,\n  rows = 100,\n  cols = 100,\n  iterations = 400000,\n  span = 5\n)\n\nbase\n\n# A tibble: 10,000 × 6\n       x      y     z  iter    id value\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     0 0          1     0     1 0.850\n 2     0 0.0101     1     0     2 0.801\n 3     0 0.0202     1     0     3 0.818\n 4     0 0.0303     1     0     4 0.814\n 5     0 0.0404     1     0     5 0.736\n 6     0 0.0505     1     0     6 0.704\n 7     0 0.0606     1     0     7 0.704\n 8     0 0.0707     1     0     8 0.740\n 9     0 0.0808     1     0     9 0.640\n10     0 0.0909     1     0    10 0.595\n# ℹ 9,990 more rows\n\n\nYou can see from the output that there are a few other columns in the output. These are added because this base data is used as the initial state for a set of particles that we’re going to place within a vector field and then trace their paths. The logic for including each of these:\n\nWe might want to use a vector field that moves points in three dimensions, so there’s also a z column (which is set to a constant value in this case). I’m not really using this feature in the splatter system, but I’ve done it in the past in other systems. One handy trick if you want to give a system a feeling of “mild inhomogeneity” – with different points appearing to move according to similar but not identical flow fields – is to displace each point by a small amount in the z coordinate, and then use a three-dimensional flow field to move the points in three dimensions, while only plotting the x and y values. You can also use the “hidden z coordinate” trick as a way of smoothly changing the colour.\nSimilarly, because this base data is used as the initial state for an iterative process, there is also a column iter that records the iteration number (set to 0 for the base data). One the one hand this is nice for housekeeping purposes (e.g., filtering the data to find the subset corresponding to a particular iteration), but later on we’ll actually use this in the plots.\nFinally, we have a id column that assigns each element of the grid a unique number, which can come in handy later on if we want to trace out the path followed by a single particle.\n\nAll that being said, at this point in the development of our system the things we’re most interested in are the x and y coordinates of each cell in our grid, and the value that we use to provide an initial colour to that location. At this point we don’t actually have a genuine palette to map value onto an actual colour, so for now we’ll just use the ggplot2 default:\n\nbase |&gt;\n  ggplot2::ggplot(ggplot2::aes(\n    x = x, \n    y = y, \n    fill = value\n  )) +\n  ggplot2::geom_raster(show.legend = FALSE) +\n  ggplot2::coord_equal() +\n  ggplot2::theme_void() + \n  ggplot2::scale_x_continuous(expand = c(0, 0)) +\n  ggplot2::scale_y_continuous(expand = c(0, 0))\n\n\n\n\n\n\n\n\nThis is our base image. It’s not super exciting or even particularly aesthetically pleasing, but that’s not the point. What I’m looking for in the base image is something that varies fairly smoothly to create an overall “shape” for the final image, and with enough random variations and irregularities to produce interesting textures in the final image. When building a system like this I’ve found that it’s important to have something that plays this role. They do a huge amount of work in defining the “composition” of the final art work, and if you don’t put a bit of thought into what this part of the system looks like you can end up with pieces that don’t have global structure, and feel a bit bland.\nThat being said, I don’t think there’s anything very special about the automaton() function I have used in the splatter system. It was a pretty arbitrary choice, and I’m entirely certain that you could swap it out for any number of alternative algorithms and end up with fabulous pieces."
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#creating-the-flow",
    "href": "posts/2024-01-26_splatter/index.html#creating-the-flow",
    "title": "Splatter",
    "section": "Creating the flow",
    "text": "Creating the flow\nNow that we have a base image to provide a bit of global structure, we’re going to need a vector field that we can use to create a sense of flow in the piece. This part is by far the most elaborate part of splatter, and it relies heavily on the ambient package. I’ll start by writing a function curl_step() that defines a vector field that specifies, for every point defined by x, y, and z coordinates, how quickly and in what direction a particle located at that point is moving. When passed a data frame data that contains columns x, y, and z, it returns a new data frame with updated coordinates derived by moving each point a small distance defined by the relevant element in the vector field.7 Additionally, since our input data frame has an iter column corresponding to the iteration number (recall earlier our “base image” has iter = 0 for every cell), the output data frame records the updated iteration number.\n\ncurl_step &lt;- function(data, \n                      iteration, \n                      scale, \n                      octaves, \n                      seed) {\n  \n  noise_data &lt;- ambient::curl_noise(\n    x = data$x,\n    y = data$y,\n    z = data$z,\n    seed = seed,\n    generator = ambient::fracture,\n    noise = ambient::gen_simplex,\n    fractal = ambient::ridged,\n    octaves = octaves\n  )\n  data$iter &lt;- iteration\n  data$x &lt;- data$x + noise_data$x * scale\n  data$y &lt;- data$y + noise_data$y * scale\n  data$z &lt;- data$z + noise_data$z * scale\n  data\n}\n\nThere’s quite a lot going on internally here, and I’m not going to dive deeply into the ambient package in this post. I’ll provide a visual illustration of what happens when curl_step() is called – it’s a bit lower in the post after I define the curl_loop() function – but if you’re interested in a more detailed explanation of the logic underpinning this code, I’ll refer you to the notes I wrote for my art from code workshop I gave a couple of years ago. The notes have a section discussing curl fields and how they are generated using ambient::curl_noise(), along with sections discussing how ambient::fracture() can be used to create more elaborate and visually interesting flow fields with fractals.\nIn any case let’s move onto the next step, in which I’ll define a function curl_loop()that uses purrr::accumulate() to iteratively apply the curl_step() function. It starts with an initial state provided by the base image, and then treats the output of every call to curl_step() as the input to the next call. All the intermediate results are stored, resulting in a list of data frames (one per iteration) that is then collapsed into one large data frame that traces the paths of all points across many iterations.\n\ncurl_loop &lt;- function(data, \n                      seed, \n                      iterations, \n                      scale, \n                      octaves) {\n  states &lt;- purrr::accumulate(\n    .x = 1:iterations,\n    .f = curl_step,\n    .init = data,\n    scale = scale,\n    octaves = octaves,\n    seed = seed\n  )\n  dplyr::bind_rows(states)\n}\n\nNow that we have these functions, let’s apply them to create a data frame called flow that takes the grid of points defined in our base data frame and repeatedly moves each of them along paths defined by our vector field:\n\nflow &lt;- base |&gt; \n  curl_loop(\n    seed = 100,\n    iterations = 99,\n    scale = .0002,\n    octaves = 5\n  )\n\nflow\n\n# A tibble: 1,000,000 × 6\n       x      y     z  iter    id value\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n 1     0 0          1     0     1 0.850\n 2     0 0.0101     1     0     2 0.801\n 3     0 0.0202     1     0     3 0.818\n 4     0 0.0303     1     0     4 0.814\n 5     0 0.0404     1     0     5 0.736\n 6     0 0.0505     1     0     6 0.704\n 7     0 0.0606     1     0     7 0.704\n 8     0 0.0707     1     0     8 0.740\n 9     0 0.0808     1     0     9 0.640\n10     0 0.0909     1     0    10 0.595\n# ℹ 999,990 more rows\n\n\nThis flow data frame provides all the structural elements required to draw the plot. Here’s what it looks like when we create an image using the default shades-of-blue palette in ggplot2:\n\npic &lt;- flow |&gt;\n  ggplot2::ggplot(ggplot2::aes(\n    x = x,\n    y = y,\n    colour = value,\n    size = -iter\n  )) +\n  ggplot2::geom_point(\n    stroke = 0, \n    show.legend = FALSE\n  ) + \n  ggplot2::coord_equal(\n    xlim = c(.05, .95),\n    ylim = c(.05, .95)\n  ) + \n  ggplot2::scale_size(range = c(0, 6)) +  \n  ggplot2::theme_void()\n\npic\n\n\n\n\n\n\n\n\nLooking at this plot you get a visceral sense of motion and flow, which is of course the intention, but it’s hard to get a sense of how each of the “cells” in our original base image is moving. Again this is by design. You’re not supposed to be able to see that in the final image. But, for the purposes of unpacking it, here’s an animated version created using gganimate which traces out the pattern of movement for every cell:\n\npic + gganimate::transition_time(iter)\n\n\n\n\n\n\nIn this animation you can see how the plot begins with 10000 “particles” laid out on a regular grid (i.e., there’s one particle for each cell in the original base data), and then iteratively “moves” each of those particles along a path defined by the vector field (created by repeatedly calling curl_step()). You can also see that each particle is gradually shrinking in size over time, a feature that is also used in the splatter plots.\nTo unpack it a little further, we can use the flow data to create a visual representation of what the vector field itself looks like:\n\nflow |&gt; \n  dplyr::filter(iter &lt; 2) |&gt;\n  ggplot2::ggplot(ggplot2::aes(\n    x = x,\n    y = y,\n    group = id\n  )) + \n  ggplot2::geom_path(\n    arrow = grid::arrow(\n      length = grid::unit(.008, \"npc\"),\n      ends = \"last\",\n      type = \"open\"\n    )\n  ) + \n  ggplot2::coord_equal(\n    xlim = c(.2, .5),\n    ylim = c(.2, .5)\n  ) +\n  ggplot2::theme_void()\n\n\n\n\n\n\n\n\nIn this plot, what I’ve done is taken the locations of each of the particles at iteration 0 (because at time 0 the points all sit on a regular grid) and drawn a little arrow that shows where each of them move to when curl_step() is applied. This gives us a rough visualisation of what the vector field looks like. The field itself doesn’t change, but of course once you start moving the particles they no longer sit on a nice grid. To illustrate both of these facts, here’s the same plot but this time I show the step from iteration 10 to iteration 11:\n\n\n\n\n\n\n\n\n\nAt any given location the arrow is still the same (because the vector field itself is invariant), but it’s not a nice clean grid in this version because the particles will naturally tend to bunch up as you move them through the field.8\nIn any case, I think that at this point we have a fairly decent sense of how the splatter pieces are shaped by the underlying vector field, so it’s time to move on…"
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#choosing-a-palette",
    "href": "posts/2024-01-26_splatter/index.html#choosing-a-palette",
    "title": "Splatter",
    "section": "Choosing a palette",
    "text": "Choosing a palette\nThe final part of the splatter system is the paletting system. At this point I confess I started to get a little bit lazy, and created a very simple paletting system that doesn’t always work. Basically, the system is hard coded with a list of 100 colours and it randomly samples 4 of them to define a linear gradient palette that interpolates between those four shades. Here’s a sample_palette() function that implements this:\n\nsample_palette &lt;- function(seed, size) {\n  set.seed(seed)\n  cols &lt;- c(\n    \"#de9151\", \"#f34213\", \"#2e2e3a\", \"#bc5d2e\", \"#bbb8b2\",\n    \"#a63446\", \"#fbfef9\", \"#0c6291\", \"#000004\", \"#7e1946\",\n    \"#ffffff\", \"#ffcad4\", \"#b0d0d3\", \"#c08497\", \"#f7af9d\",\n    \"#aa8f66\", \"#ed9b40\", \"#ffeedb\", \"#61c9a8\", \"#ba3b46\",\n    \"#241023\", \"#6b0504\", \"#a3320b\", \"#d5e68d\", \"#47a025\",\n    \"#64113f\", \"#de4d86\", \"#f29ca3\", \"#f7cacd\", \"#84e6f8\",\n    \"#660000\", \"#990033\", \"#5f021f\", \"#8c001a\", \"#ff9000\",\n    \"#c9cba3\", \"#ffe1a8\", \"#e26d5c\", \"#723d46\", \"#472d30\",\n    \"#0e7c7b\", \"#17bebb\", \"#d4f4dd\", \"#d62246\", \"#4b1d3f\",\n    \"#0a0908\", \"#49111c\", \"#f2f4f3\", \"#a9927d\", \"#5e503f\",\n    \"#020202\", \"#0d324d\", \"#7f5a83\", \"#a188a6\", \"#9da2ab\",\n    \"#c2c1c2\", \"#42213d\", \"#683257\", \"#bd4089\", \"#f51aa4\",\n    \"#820263\", \"#d90368\", \"#eadeda\", \"#2e294e\", \"#ffd400\",\n    \"#f4e409\", \"#eeba0b\", \"#c36f09\", \"#a63c06\", \"#710000\",\n    \"#d9d0de\", \"#bc8da0\", \"#a04668\", \"#ab4967\", \"#0c1713\",\n    \"#012622\", \"#003b36\", \"#ece5f0\", \"#e98a15\", \"#59114d\",\n    \"#3c1518\", \"#69140e\", \"#a44200\", \"#d58936\", \"#fffb46\",\n    \"#6e0d25\", \"#ffffb3\", \"#dcab6b\", \"#774e24\", \"#6a381f\",\n    \"#bcabae\", \"#0f0f0f\", \"#2d2e2e\", \"#716969\", \"#fbfbfb\",\n    \"#2b4162\", \"#385f71\", \"#f5f0f6\", \"#d7b377\", \"#8f754f\"\n  )\n  sample(cols, size = size)\n}\n\nHere’s a few random palettes generated using this function:\nscales::show_col(sample_palette(seed = 100, size = 4))\nscales::show_col(sample_palette(seed = 123, size = 4))\nscales::show_col(sample_palette(seed = 666, size = 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt doesn’t always produce satisfying colour schemes, but it succeeds often enough for the system to be workable. And now that we have a paletting system we can write a make_plot() function that applies the palette using ggplot2::scale_colour_gradientn():\n\nmake_plot &lt;- function(data, seed) {\n  palette &lt;- sample_palette(seed = seed, size = 4)\n  data |&gt;\n    ggplot2::ggplot(ggplot2::aes(\n      x = x,\n      y = y,\n      colour = value,\n      size = -iter\n    )) +\n    ggplot2::geom_point(\n      stroke = 0, \n      show.legend = FALSE\n    ) + \n    ggplot2::coord_equal(\n      xlim = c(.05, .95),\n      ylim = c(.05, .95)\n    ) + \n    ggplot2::scale_size(range = c(0, 6)) +\n    ggplot2::scale_colour_gradientn(colours = palette) +\n    ggplot2::theme_void()\n}\n\nmake_plot(flow, seed = 123)\n\n\n\n\n\n\n\n\nAt this point, we have our final image."
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#exploring-the-system",
    "href": "posts/2024-01-26_splatter/index.html#exploring-the-system",
    "title": "Splatter",
    "section": "Exploring the system",
    "text": "Exploring the system\nIn order to play around with system, it’s helpful to wrap the whole generative process from beginning to end in a splatter() function that generates images. It takes a single argument seed, which can either be a scalar seed value that is used for all components to the system, or it can be a three-element vector that supplies separate seeds for the base data, the vector field, and the palette:\n\nsplatter &lt;- function(seed) {\n  stopifnot(length(seed) %in% c(1, 3))\n  if(length(seed) == 1) seed &lt;- rep(seed, 3)\n  base_data(\n    seed = seed[1],\n    rows = 100,\n    cols = 100,\n    iterations = 400000,\n    span = 5\n  ) |&gt;\n  curl_loop(\n    seed = seed[2],\n    iterations = 99,\n    scale = .0002,\n    octaves = 5\n  ) |&gt;\n  make_plot(seed = seed[3])\n}\n\nThe ability to set the seed separately for each of the components is useful for illustrating the role played by each one. First let’s look at what happens when we hold the base image and the vector field constant, but vary the palette:\nsplatter(seed = c(101, 102, 100))\nsplatter(seed = c(101, 102, 123))\nsplatter(seed = c(101, 102, 666))\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you’d expect, these are all structurally the same image: same shapes, same flowing look, but with very different colour schemes, and of course this drastically changes the feel of each piece.\nAlternatively, we can keep the base image and palette constant, but use a different vector field each time. Looking at the images below, you can see how this changes the “fine grain” of the image. The effect is much subtler, but it’s definitely noticeable. It’s almost as if someone were trying to paint the same piece all three times, but the brush strokes are different every time:\nsplatter(seed = c(101, 123, 666))\nsplatter(seed = c(101, 456, 666))\nsplatter(seed = c(101, 789, 666))\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can vary the base image but keep the vector field and palette the same:\nsplatter(seed = c(789, 100, 123))\nsplatter(seed = c(456, 100, 123))\nsplatter(seed = c(123, 100, 123))\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis, to my mind, is the strangest effect of all. Varying the base image changes the overall distribution of, as you’d expect, but when you look very closely at the images you can see that they all have the same “brush strokes”. The swirling circular pattern in the centre-left of image (which was very evident in the animated image) is present in all three images: on the left image it shows up as a pattern of red and orange strokes curling around one another, but on the right that same swirl is now mostly pink and black. For the image in the middle the swirl is harder to see because it’s almost entirely painted in orange, with just a little bit of black."
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#the-artist-statement",
    "href": "posts/2024-01-26_splatter/index.html#the-artist-statement",
    "title": "Splatter",
    "section": "The artist statement",
    "text": "The artist statement\nSo that’s everything, I guess. I wanted to write something about the splatter pieces because I’ve enjoyed building the system, and I really like the look and feel of these pieces. They’re naturalistic enough to feel organic, and artificial enough to feel like generative art unashamedly being generative art. Splatter is not trying to look like painted artwork, it is deliberately its own thing.\nThe ethos that underpins splatter is one I’m fond of artistically, and one that cuts a little deeply personally. So much of my everyday existence is spent worrying about “passing” for something I’m not,9 and it’s not a great feeling. Generative art shouldn’t need to “pass”. It shouldn’t be viewed as a second-class artistic citizen that is merely a simulacrum of other kinds of artwork. It can be its own thing, with its own coherent and meaningful standards for artistic merit. Are the “splatter” pieces aesthetically appealing? Sometimes they are. Do they evoke emotions in the audience? Well, they do for me. Does the artist have some underlying intention behind the artwork that makes a claim about the world we live in? At the risk of making the subtext in this paragraph obvious, yes she does. Splatter isn’t a completely arbitrary thing, it was designed to walk that fine line between “pretending to be something else” and “being honest about what it truly is”.\nAs anyone who has lived in a closet can attest, that is a treacherous tightrope to walk."
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#source-code",
    "href": "posts/2024-01-26_splatter/index.html#source-code",
    "title": "Splatter",
    "section": "Source code",
    "text": "Source code\n\n\n\n\nC++ code for the splatter system\n\nautomaton.cpp\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// wrap position to grid\nint wrap(int pos, int size) {\n  if(pos &lt; 0) pos = pos + size;\n  if(pos &gt;= size) pos = pos - size;\n  return pos;\n}\n\n// automaton run function\n// [[Rcpp::export]]\nNumericMatrix automaton(int n_rows, int n_cols, int iterations, int max_span) {\n  \n  int source_row = 0;\n  int source_col = 0;\n  int span_row = 0;\n  int span_col = 0;\n  int row = 0;\n  int col = 0;\n  int r = 0;\n  int c = 0;\n  double source_val = 0;\n  \n  NumericMatrix grid(n_rows, n_cols);\n  for (int row = 0; row &lt; n_rows; row++) {\n    for (int col = 0; col &lt; n_cols; col++) {\n      grid(row, col) = R::runif(0, 1);\n    }\n  }\n  \n  for (int it = 0; it &lt; iterations; it++) {\n    source_row = floor(R::runif(0, n_rows));\n    source_col = floor(R::runif(0, n_cols));\n    source_val = grid(source_row, source_col);\n    span_row = floor(R::runif(0, max_span));\n    span_col = floor(R::runif(0, max_span));\n    row = source_row - span_row;\n    col = source_col - span_col;\n    do {\n      c = wrap(col, n_cols);\n      do {\n        r = wrap(row, n_rows);\n        grid(r, c) = (grid(r, c) + source_val) / 2;\n        row++;\n      } while (row &lt; source_row + span_row);\n      col++;\n    } while (col &lt; source_col + span_col);\n  }\n  \n  return grid;\n}\n\n\n\n\n\n\nR code for the splatter system\n\nsplatter.R\n\nRcpp::sourceCpp(\"automaton.cpp\")\n\nbase_data &lt;- function(seed, rows, cols, iterations, span) {\n  set.seed(seed)\n  tidyr::expand_grid(\n    x = seq(0, 1, length.out = cols),\n    y = seq(0, 1, length.out = rows),\n    z = 1,\n    iter = 0\n  ) |&gt;\n    dplyr::mutate(\n      id = dplyr::row_number(),\n      value = t(automaton(rows, cols, iterations, span)) |&gt;\n        as.vector() |&gt;\n        ambient::normalise()\n    )\n}\n\ncurl_step &lt;- function(data, \n                      iteration, \n                      scale, \n                      octaves, \n                      seed) {\n  \n  noise_data &lt;- ambient::curl_noise(\n    x = data$x,\n    y = data$y,\n    z = data$z,\n    seed = seed,\n    generator = ambient::fracture,\n    noise = ambient::gen_simplex,\n    fractal = ambient::ridged,\n    octaves = octaves\n  )\n  data$iter &lt;- iteration\n  data$x &lt;- data$x + noise_data$x * scale\n  data$y &lt;- data$y + noise_data$y * scale\n  data$z &lt;- data$z + noise_data$z * scale\n  data\n}\n\ncurl_loop &lt;- function(data, \n                      seed, \n                      iterations, \n                      scale, \n                      octaves) {\n  states &lt;- purrr::accumulate(\n    .x = 1:iterations,\n    .f = curl_step,\n    .init = data,\n    scale = scale,\n    octaves = octaves,\n    seed = seed\n  )\n  dplyr::bind_rows(states)\n}\n\nsample_palette &lt;- function(seed, size) {\n  set.seed(seed)\n  cols &lt;- c(\n    \"#de9151\", \"#f34213\", \"#2e2e3a\", \"#bc5d2e\", \"#bbb8b2\",\n    \"#a63446\", \"#fbfef9\", \"#0c6291\", \"#000004\", \"#7e1946\",\n    \"#ffffff\", \"#ffcad4\", \"#b0d0d3\", \"#c08497\", \"#f7af9d\",\n    \"#aa8f66\", \"#ed9b40\", \"#ffeedb\", \"#61c9a8\", \"#ba3b46\",\n    \"#241023\", \"#6b0504\", \"#a3320b\", \"#d5e68d\", \"#47a025\",\n    \"#64113f\", \"#de4d86\", \"#f29ca3\", \"#f7cacd\", \"#84e6f8\",\n    \"#660000\", \"#990033\", \"#5f021f\", \"#8c001a\", \"#ff9000\",\n    \"#c9cba3\", \"#ffe1a8\", \"#e26d5c\", \"#723d46\", \"#472d30\",\n    \"#0e7c7b\", \"#17bebb\", \"#d4f4dd\", \"#d62246\", \"#4b1d3f\",\n    \"#0a0908\", \"#49111c\", \"#f2f4f3\", \"#a9927d\", \"#5e503f\",\n    \"#020202\", \"#0d324d\", \"#7f5a83\", \"#a188a6\", \"#9da2ab\",\n    \"#c2c1c2\", \"#42213d\", \"#683257\", \"#bd4089\", \"#f51aa4\",\n    \"#820263\", \"#d90368\", \"#eadeda\", \"#2e294e\", \"#ffd400\",\n    \"#f4e409\", \"#eeba0b\", \"#c36f09\", \"#a63c06\", \"#710000\",\n    \"#d9d0de\", \"#bc8da0\", \"#a04668\", \"#ab4967\", \"#0c1713\",\n    \"#012622\", \"#003b36\", \"#ece5f0\", \"#e98a15\", \"#59114d\",\n    \"#3c1518\", \"#69140e\", \"#a44200\", \"#d58936\", \"#fffb46\",\n    \"#6e0d25\", \"#ffffb3\", \"#dcab6b\", \"#774e24\", \"#6a381f\",\n    \"#bcabae\", \"#0f0f0f\", \"#2d2e2e\", \"#716969\", \"#fbfbfb\",\n    \"#2b4162\", \"#385f71\", \"#f5f0f6\", \"#d7b377\", \"#8f754f\"\n  )\n  sample(cols, size = size)\n}\n\nmake_plot &lt;- function(data, seed) {\n  palette &lt;- sample_palette(seed = seed, size = 4)\n  data |&gt;\n    ggplot2::ggplot(ggplot2::aes(\n      x = x,\n      y = y,\n      colour = value,\n      size = -iter\n    )) +\n    ggplot2::geom_point(\n      stroke = 0, \n      show.legend = FALSE\n    ) + \n    ggplot2::coord_equal(\n      xlim = c(.05, .95),\n      ylim = c(.05, .95)\n    ) + \n    ggplot2::scale_size(range = c(0, 6)) +\n    ggplot2::scale_colour_gradientn(colours = palette) +\n    ggplot2::theme_void()\n}\n\nsplatter &lt;- function(seed) {\n  stopifnot(length(seed) %in% c(1, 3))\n  if(length(seed) == 1) seed &lt;- rep(seed, 3)\n  base_data(\n    seed = seed[1],\n    rows = 100,\n    cols = 100,\n    iterations = 400000,\n    span = 5\n  ) |&gt;\n  curl_loop(\n    seed = seed[2],\n    iterations = 99,\n    scale = .0002,\n    octaves = 5\n  ) |&gt;\n  make_plot(seed = seed[3])\n}"
  },
  {
    "objectID": "posts/2024-01-26_splatter/index.html#footnotes",
    "href": "posts/2024-01-26_splatter/index.html#footnotes",
    "title": "Splatter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this post for an old write up of the water colours system.↩︎\nOkay fine. It’s not the actual source code for the original system, because the original system is an absolute mess of things I decided to modify on a whim and is not particularly pleasant. Instead, the code here (which is reproduced in full in the “source code” appendix) is a cleaner version that is almost equivalent to the original. And to be honest, if I were starting from scratch I’d be using this cleaned up version and not the bizarre mess that is the original source code.↩︎\nI should also mention that this code is very loosely based on the “stepping-stone automaton” that I discovered a few years ago when looking at Antonio Sánchez Chinchón’s watercolour art, and was the inspiration for my stoneskip system which I adapted to implement the automaton() function.↩︎\nIn complete honesty this code is slightly different to the version I implemented in the original splatter code. In the original version I was a little sloppy about managing the random seed, and in several places I was using the C++ native rand() function for some of the random number generation. That’s problematic in the generative art context because I want to exercise control over the RNG seed from R using the usual set.seed() function, so I need my C++ code to consistently call the random number generators in R using R::runif()↩︎\nAt some level I’m mildly amused that I’m writing a C++ function that I can call from R, and said C++ function reaches back up to R to invoke a random number generator that is implemented in C.↩︎\nIn the code here I’m using as.vector() to “flatten” the matrix to a vector. To control for any restriction of range that might have happened due to repeated averaging inside the C++ function, I use ambient::normalise() to ensure that the smallest value in this vector is always 0, and the largest value is always 1.↩︎\nYes I do know this is the worst explanation of fields ever. Hush.↩︎\nIt is of course also true to note that the paths that emerge when you do this are only a piecewise-linear approximation to the continuous-flow paths. When you take a series of discrete linear steps like this, errors are introduced at each step. But this is generative art not real analysis class so I’m not at all bothered by that.↩︎\nI probably don’t have to spell it out, right? By now everyone who reads my blog knows what I am, I should imagine.↩︎"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html",
    "title": "Unpacking Arrow Datasets",
    "section": "",
    "text": "Hello again lovely people. I am, once again, blogging about Apache Arrow and I’m not even sorry. Oh well.\nIn an earlier post I wrote about Tables and other in-memory data structures that Arrow uses to represent data objects. That meant the bulk of the post was focused on Record Batch and Table objects and the constituent objects used to define columns in one of these things (Arrays and Chunked Arrays).\nWhat I didn’t really talk about in that post was Datasets, which are used to represent data (typically larger-than-memory data) that are stored on-disk rather than in-memory. Okay, fine, yeah. Technically I did include a section on Datasets at the end of the post, but I was a bit evasive. I gave an example showing how to use Datasets, but I really didn’t talk much about what they are.\nI had a very good reason for this, dear reader, and that reason is this: when I wrote that post I had no f**king idea whatsoever how Datasets worked. I knew how to use them, but if you’d asked me questions about how the magic works I couldn’t have told you.1\nSince that time I’ve learned a few things, and because I’m an annoying person I’m going to tell you about them.2\nlibrary(arrow, warn.conflicts = FALSE)\nlibrary(dplyr, warn.conflicts = FALSE)"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#quick-recap-record-batches-and-tables",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#quick-recap-record-batches-and-tables",
    "title": "Unpacking Arrow Datasets",
    "section": "Quick recap: Record Batches and Tables",
    "text": "Quick recap: Record Batches and Tables\nAt this point I’ve written quite a few posts about Arrow, and it’s not necessarily a good idea for me to assume that you’ve had the misfortune to read all3 of them. So here’s a quick recap of some of the key Arrow data structures that I’ve talked about in other posts…\nLet’s start with Record Batches. A Record Batch is tabular data structure comprised of named Arrays,4 and an accompanying Schema5 that specifies the name and data type associated with each Array. We can create one manually using record_batch()\n\nrb &lt;- record_batch(\n  strs = c(\"hello\", \"amazing\", \"and\", \"cruel\", \"world\"), \n  ints = c(1L, NA, 2L, 4L, 8L),\n  dbls = c(1.1, 3.2, 0.2, NA, 11)\n)\nglimpse(rb)\n\nRecordBatch\n5 rows x 3 columns\n$ strs &lt;string&gt; \"hello\", \"amazing\", \"and\", \"cruel\", \"world\"\n$ ints  &lt;int32&gt; 1, NA, 2, 4, 8\n$ dbls &lt;double&gt; 1.1, 3.2, 0.2, NA, 11.0\n\n\nThis is a Record Batch containing 5 rows and 3 columns. The command rb[1:3, 1:2] extracts the first three rows and the first two columns:\n\nglimpse(rb[1:3, 1:2])\n\nRecordBatch\n3 rows x 2 columns\n$ strs &lt;string&gt; \"hello\", \"amazing\", \"and\"\n$ ints  &lt;int32&gt; 1, NA, 2\n\n\nThe structure of a Record Batch is shown below. In addition to the three Arrays specifying the columns, it includes an explicit Schema object containing relevant metadata:\n\nRecord Batches are a fundamental unit for data interchange in Arrow, but are not typically used for data analysis. The reason for this is that the constituent Arrays that store columns in a Record Batch are immutable: they cannot be modified or extended without creating a new object.6 When data arrive sequentially Record Batches can be inconvenient, because you can’t concatenate them. For that reason Tables are usually more practical…\nSo let’s turn to Tables next. From the user perspective a Table is very similar to a Record Batch but the constituent parts are Chunked Arrays. Chunked Arrays are flexible wrappers enclosing one or more Arrays.7 This makes it possible to concatenate tables. To quickly illustrate this, let’s first convert the rb Record Batch to a Table using arrow_table():\n\ndf1 &lt;- arrow_table(rb)\n\nNow we create a second Table with the same column names and types, again using arrow_table():\n\ndf2 &lt;- arrow_table(\n  strs = c(\"I\", \"love\", \"you\"), \n  ints = c(5L, 0L, 0L),\n  dbls = c(7.1, -0.1, 2)\n)\n\nWe can concatenate these using concat_tables():\n\ndf &lt;- concat_tables(df1, df2)\nglimpse(df)\n\nTable\n8 rows x 3 columns\n$ strs &lt;string&gt; \"hello\", \"amazing\", \"and\", \"cruel\", \"world\", \"I\", \"love\", \"you\"\n$ ints  &lt;int32&gt; 1, NA, 2, 4, 8, 5, 0, 0\n$ dbls &lt;double&gt; 1.1, 3.2, 0.2, NA, 11.0, 7.1, -0.1, 2.0\n\n\nThe structure of this Table object is similar to the structure of the Record Batch object I showed earlier, but the columns are Chunked Arrays rather than simple Arrays:\n You can see this if we print out a single column:\n\ndf$strs\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"hello\",\n    \"amazing\",\n    \"and\",\n    \"cruel\",\n    \"world\"\n  ],\n  [\n    \"I\",\n    \"love\",\n    \"you\"\n  ]\n]\n\n\nThere’s a visual separation there between the different chunks, used to indicated where the boundaries between individual Arrays are. In practice though you actually don’t have to care about this because it’s not semantically meaningful. It’s there for purely technical reasons.\nBut all this is background. So let’s move on, shall we?"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#so-datasets",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#so-datasets",
    "title": "Unpacking Arrow Datasets",
    "section": "So… Datasets?",
    "text": "So… Datasets?\nOkay, what about Datasets? Like Record Batch and Table objects, a Dataset is used to represent tabular data. At an abstract level, a Dataset can be viewed as an object comprised of rows and columns, and just like Record Batches and Tables, it contains an explicit Schema that specifies the name and data type associated with each column.\nHowever, where Tables and Record Batches are data explicitly represented in-memory, a Dataset is not. Instead, a Dataset is an abstraction that refers to data stored on-disk in one or more files. Reading the data takes place only as needed, and only when a query is executed against the data. In this respect Arrow Datasets are a very different kind of object to Arrow Tables, but the arrow package is written in a way that the dplyr commands used to analyze Tables can also be applied to Datasets."
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-is-a-dataset-on-disk",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-is-a-dataset-on-disk",
    "title": "Unpacking Arrow Datasets",
    "section": "What is a Dataset on-disk?",
    "text": "What is a Dataset on-disk?\nReduced to its simplest form, the on-disk structure of a Dataset is simply a collection of data files, each storing one subset of the data. These subsets are sometimes referred to as “fragments”, and the partitioning process is sometimes referred to as “sharding”. To illustrate how this works, I’ll write a multi-file dataset to disk manually, without using any of the Arrow Dataset functionality to do the work. I’ll keep it deliberately simple and use three small data frames, each containing one subset of the data we want to store:\n\ndf_a &lt;- data.frame(id = 1:5, value = rnorm(5), subset = \"a\")\ndf_b &lt;- data.frame(id = 6:10, value = rnorm(5), subset = \"b\")\ndf_c &lt;- data.frame(id = 11:15, value = rnorm(5), subset = \"c\")\n\nOur intention is that each of the data frames should be stored in a separate data file. As you can see, this is a quite structured partitioning: all data where subset = \"a\" belong to one file, all data where subset = \"b\" belong to another file, and all data where subset = \"c\" belong to the third file.8\nThe first step is to define and create a folder that will hold all the files:\n\nds_dir &lt;- \"mini-dataset\"\ndir.create(ds_dir)\n\nThe next step is to manually create a “Hive-style”9 folder structure:\n\nds_dir_a &lt;- file.path(ds_dir, \"subset=a\")\nds_dir_b &lt;- file.path(ds_dir, \"subset=b\")\nds_dir_c &lt;- file.path(ds_dir, \"subset=c\")\n\ndir.create(ds_dir_a)\ndir.create(ds_dir_b)\ndir.create(ds_dir_c)\n\nNotice that we have named each folder in a “key=value” format that exactly describes the subset of data that will be written into that folder. This naming structure is the essence of Hive-style partitions.\nNow that we have the folders, we’ll use write_parquet() to create a single parquet file10 for each of the three subsets:\n\nwrite_parquet(df_a, file.path(ds_dir_a, \"part-0.parquet\"))\nwrite_parquet(df_b, file.path(ds_dir_b, \"part-0.parquet\"))\nwrite_parquet(df_c, file.path(ds_dir_c, \"part-0.parquet\"))\n\nIf I’d wanted to, I could have further subdivided the dataset. A folder can contain multiple files (part-0.parquet, part-1.parquet, etc) if we would like it to, though there’s no point whatsoever in doing that with such a tiny dataset. Similarly, there is no requirement to name the files part-0.parquet this way at all: it would have been fine to call these files subset-a.parquet, subset-b.parquet, and subset-c.parquet if I’d wanted to do that. I only chose part-0.parquet because that’s the default filename that the write_dataset() function in the arrow package generates!\nAlong the same lines, it isn’t necessary to use Hive-style partitions to use Arrow Datasets. The default behaviour of write_dataset() is to construct Hive-style partitions, and the default in open_dataset() is to look for Hive-style partitions, but it isn’t required.\nIn any case, I’ve created an on-disk parquet Dataset using Hive-style partitioning. My Dataset is defined by these files:\n\nlist.files(ds_dir, recursive = TRUE)\n\n[1] \"subset=a/part-0.parquet\" \"subset=b/part-0.parquet\"\n[3] \"subset=c/part-0.parquet\"\n\n\nThis is exciting, right? I mean, I’m excited. How could anyone not be completely enthralled by this thrilling exposition?\nAaaaanyway…. to verify that everything has worked, I’ll now try to open the data with open_dataset() and call glimpse() to inspect its contents:\n\nds &lt;- open_dataset(ds_dir)\nglimpse(ds)\n\nFileSystemDataset with 3 Parquet files\n15 rows x 3 columns\n$ id      &lt;int32&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n$ value  &lt;double&gt; -0.08458607, 0.84040013, -0.46348277, -0.55083500, 0.73604043,…\n$ subset &lt;string&gt; \"a\", \"a\", \"a\", \"a\", \"a\", \"b\", \"b\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c…\nCall `print()` for full schema details\n\n\nAs you can see, the ds Dataset object aggregates the three separate data files. In fact, in this particular case the Dataset is so small that values from all three files appear in the output of glimpse().\nNow, it’s pretty obvious that I wouldn’t use this workflow in my everyday life. Manually writing individual files like this is tiresome, especially when the exact same dataset can be created with the following command:\n\nds |&gt; \n  group_by(subset) |&gt;\n  write_dataset(\"mini-dataset\")\n\nAs an aside, even if ds happens to refer to an on-disk Dataset that is larger than memory, and you’re just wanting to rewrite it with a different file structure, this pipeline should still work without any risk of an out-of-memory error. This is thanks to the Dataset backpressure functionality11 in which the reader will back off and slow down if the writer has fallen too far behind and the memory cache is filling up. Or something like that. Look, I almost managed to make myself care about the details, okay?"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#whats-stored-in-memory-by-the-dataset",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#whats-stored-in-memory-by-the-dataset",
    "title": "Unpacking Arrow Datasets",
    "section": "What’s stored in-memory by the Dataset?",
    "text": "What’s stored in-memory by the Dataset?\nAssuming I have any readers left at this point in the post, I know what you’re all thinking:\n\nYes okay Danielle that’s fine, I get it, a Dataset is just a bunch of files on disk. But actually I already knew that. There has to be something in-memory though right? What’s that thing? Tell me about that.\n\nFirst off, rude. I was getting to it! Second, yes you are totally right. Sorry. So okay, in the last section I created this the ds object. Like most objects created by the arrow package, it’s an R6 object with a bunch of fields and methods that are used to wrap bindings to the corresponding Arrow C++ dark magic… sorry, um, methods. Anyway, for our purposes there are two things of importance: the ds object has an active binding specifying the Schema of the Dataset, and another one specifying the paths to all the files. That’s pretty much it. Paths to these files are stored in an active binding ds$files:\n\nds$files \n\n[1] \"/home/danielle/GitHub/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=a/part-0.parquet\"\n[2] \"/home/danielle/GitHub/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=b/part-0.parquet\"\n[3] \"/home/danielle/GitHub/quarto-blog/posts/2022-11-30_unpacking-arrow-datasets/mini-dataset/subset=c/part-0.parquet\"\n\n\nThe Schema is stored as ds$schema:\n\nds$schema\n\nSchema\nid: int32\nvalue: double\nsubset: string\n\nSee $metadata for additional Schema metadata\n\n\nBy default this Schema is inferred by open_dataset() by inspecting the first file only, though it is possible to construct a unified schema after inspecting all files. To do this, set unify_schemas = TRUE when calling open_dataset(). It is also possible to use the schema argument to open_dataset() to specify the Schema explicitly (see the schema() function for details).\nIn any case, in most situations I think it’s reasonable to use this as the mental model of what the ds object contains:"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#how-does-a-dataset-query-work",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#how-does-a-dataset-query-work",
    "title": "Unpacking Arrow Datasets",
    "section": "How does a Dataset query work?",
    "text": "How does a Dataset query work?\nHm. So if the Dataset object12 is essentially nothing more than a Schema and a list of files, what happens at the time a query has to be evaluated? At some point the data (or at least some of it) do have to be read into memory in order to perform the compute operations! I mean, consider the following dplyr pipeline:\n\nds |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(new_value = round(100 * value)) |&gt;\n  select(id, subset, new_value) |&gt;\n  collect()\n\n  id subset new_value\n1 12      c        29\n2 13      c        42\n3 15      c         7\n4  2      a        84\n5  5      a        74\n\n\nAt some point in making this happen, data are loaded and computations are performed. At the user level we don’t really think about it much: the dplyr bindings supplied by the arrow package provide us with an abstraction layer for Datasets that completely mask this aspect of the process. That’s super cool because honestly I don’t care enough to spend my time on that sort of thing, but I also find myself curious… what happens when we strip the abstraction away? How would we do this analysis without these abstractions?\nWhen querying a Dataset, we need a strategy for reading data: this is coordinated by a Scanner object constructed for the specific Dataset and the specific query. When analyzing a Dataset using the dplyr interface you never need to construct a Scanner manually, but for explanatory purposes I’ll create one:\n\nscan &lt;- Scanner$create(dataset = ds)\n\nCalling the ToTable() method will materialise the Dataset (on-disk) as a Table (in-memory):13\n\nscan$ToTable()\n\nTable\n15 rows x 3 columns\n$id &lt;int32&gt;\n$value &lt;double&gt;\n$subset &lt;string&gt;\n\nSee $metadata for additional Schema metadata\n\n\nYou can see that this has returned 15 rows (i.e., the whole dataset). If we want to reproduce the behaviour of the dplyr pipeline using the low-level Dataset interface by creating a new scan by specifying the filter and projection arguments to Scanner$create(). The filter argument is used to modify the rows that are returned by the Scanner, and the projection argument is used to modify the columns. These arguments take Arrow Expressions as inputs, which is yet another topic I’ll try to write more about one of these days.\nAnyway, the scanner defined below mimics the dplyr pipeline shown above,\n\nscan &lt;- Scanner$create(\n  dataset = ds, \n  filter = Expression$field_ref(\"value\") &gt; 0,\n  projection = list(\n    id = Expression$field_ref(\"id\"),\n    subset = Expression$field_ref(\"subset\"),\n    new_value = Expression$create(\"round\", 100 * Expression$field_ref(\"value\"))\n  )\n)\n\nWe can check this by calling scan$ToTable() and then converting the result to a data frame so that we get a pretty print out:\n\nscan$ToTable() |&gt; as.data.frame()\n\n  id subset new_value\n1  2      a        84\n2  5      a        74\n3 12      c        29\n4 13      c        42\n5 15      c         7\n\n\nYep, that looks about right.\nWe can dig a little deeper though. To get a better sense of what happens when the query executes, what I’ll call scan$ScanBatches(). Much like the ToTable() method, the ScanBatches() method executes the query separately against each of the files, but it returns a list of Record Batches, one for each file. If we convert each one of those Record Batches to a data frame individually, we get this as a result:\n\nscan$ScanBatches() |&gt; lapply(as.data.frame)\n\n[[1]]\n  id subset new_value\n1  2      a        84\n2  5      a        74\n\n[[2]]\n[1] id        subset    new_value\n&lt;0 rows&gt; (or 0-length row.names)\n\n[[3]]\n  id subset new_value\n1 12      c        29\n2 13      c        42\n3 15      c         7\n\n\nThis version of the result helps you see each part of the Dataset at work in the query. When you pass a query to a Dataset, each file is processed in a separate thread14 and Record Batches will be added as they are returned. The key point here is that Datasets have no notion of row order: if you want the results returned in a particular order you must sort them explicitly.\nA second point to make about the the scanning process is that under the hood, Arrow keeps track of memory usage and doesn’t try to read too many files at once. It will also make use of whatever information it has about the file contents to avoid reading files that it doesn’t have to read: if I filter on subset != \"a\" then the Scanner will ensure that the files in the corresponding folder are never even read.15\nOkay, so now let’s go back to the dplyr query we made earlier, but use compute() to return a Table rather use collect() to return a data frame.\n\ntbl &lt;- ds |&gt;\n  filter(value &gt; 0) |&gt;\n  mutate(new_value = round(100 * value)) |&gt;\n  select(id, subset, new_value) |&gt;\n  compute()\n\nThis Table object has been created by concatenating three Record Batches, one for each of the three data files. As a consequence of this, the Chunked Array that defines a column of the Table has the same partitioning structure present in the data files:16\n\ntbl$subset\n\nChunkedArray\n&lt;string&gt;\n[\n  [],\n  [\n    \"a\",\n    \"a\"\n  ],\n  [\n    \"c\",\n    \"c\",\n    \"c\"\n  ]\n]"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-was-the-point",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#what-was-the-point",
    "title": "Unpacking Arrow Datasets",
    "section": "What was the point?",
    "text": "What was the point?\nDoes any of this matter? Well. That depends, I suppose. If you’re looking to analyse a Dataset using R, you don’t really need to know much of this. Frankly you probably don’t need to know any of it. But also there’s something uncomfortable about using tools when you don’t quite know what they’re doing. It makes me happier when I know just a little bit more than I actually need to know. More importantly, it matters in the sense that it works. Using Datasets leads to shockingly fast performance on data that would not normally be amenable to analysis with R. Which… yeah, that does matter quite a bit!"
  },
  {
    "objectID": "posts/2022-11-30_unpacking-arrow-datasets/index.html#footnotes",
    "href": "posts/2022-11-30_unpacking-arrow-datasets/index.html#footnotes",
    "title": "Unpacking Arrow Datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnbelievably, there are people out there who will start talking about predicate pushdown and not even give a girl a heads up? Rude. You don’t see me starting conversations at the pub about metric axiom violations in human similarity judgment do you? Well, okay, you might. But that’s not the point!↩︎\nOkay, now I’m a bit sorry.↩︎\nOr indeed, “any”.↩︎\nFor the purposes of this post we are going to pretend that Arrays behave like R vectors, which… they sort of do as long as you don’t try to push at the analogy too hard.↩︎\nThe Schema is the way Arrow formalises the metadata for rectangular data structures. I’m not going to dive into the details here: it’s enough for our purposes to recognise that it’s basically a list of variable names and their data types.↩︎\nI mean, this is where you start asking all sort of questions about what objects are mutable in R anyway, since we’re almost never doing modify-in-place operations. But whatever. This is not the post for that, and if you try to make me talk about that here I will cry.↩︎\nAgain, let’s just pretend that a Chunked Array behaves just like an R vector, except for the fact that it has these weird stitches from where we’ve sewn the individual Arrays together. It’s all a bit vivisectionist in nature, sure, but this is the mechanism that allows Chunked Arrays to behave more like R vectors than simple Arrays do. Dr Frankenstein may not have been entirely wrong on all counts, I guess.↩︎\nIt’s probably worth doing a tiny bit of foreshadowing here: there’s really no sense in which the files are “ordered”, right? So, logically it follows there’s no sense in which the Dataset as a whole has a total ordering of rows either, right? That’s worth keeping in mind because results don’t always come back in the same order unless you explicitly sort them. I’ll talk about this a little more later.↩︎\nThe name comes from Apache Hive: hive.apache.org.↩︎\nOne of these days I am going to write a proper blog post on parquet files for R users, I promise. I just don’t seem to have found the time yet. Not sure where all the time goes…↩︎\nAs usual there is esoteric knowledge buried in the C++ documentation, in this case describing backpressure control. It’s probably ancient forbidden lore and Dumbledore is going to turn me into a hobbit or something but whatever.↩︎\nStrictly speaking I am assuming a FileSystemDataset and not a more esoteric kind of Dataset like an InMemoryDataset or a UnionDataset, and I am assuming that there is a one to one mapping between files and Fragments, but honestly those assumptions are usually true in everyday data analysis and if you know these distinctions already you certainly shouldn’t be using this post to learn about Datasets now should you? So the only reason you’d be bringing this up would be to correct me on the internet to show off how smart you are and that really would be just a dick move. Just saying.↩︎\nThe scanning process is multi-threaded by default, but if necessary threading can be disabled by setting use_threads = FALSE when calling Scanner$create().↩︎\nOkay fine, yes you can set use_threads = FALSE like I said above to disable this, hush.↩︎\nFor files formats like Parquet that include metadata for row groups and organise data column-wise, there are additional optimisations ensuring that you don’t have to read the whole file.↩︎\nNotice that there is an empty chunk in there corresponding to the file that didn’t return any hits – that’s a deliberate act of seed hacking on my part! I rigged it so that one of the data subsets just happened to have all negative numbers. I wanted to highlight the fact that a scanned file that doesn’t return any rows will still return a Record Batch with zero rows, and this will still be reflected in any Table object that gets constructed. This doesn’t really matter for any practical purpose, but I think it helps get a feel for the mechanisms involved.↩︎"
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html",
    "href": "posts/2024-01-09_observable-js-art/index.html",
    "title": "Making generative art with observable.js",
    "section": "",
    "text": "About this time last year I wrote a little blog post about my first attempts to make generative art in javascript with p5.js. Because I was using this quarto-based blog as the vehicle for that exploration, and because quarto supports observable.js as a method for javascript-based computational notebooks – which is slightly different to embedding a js script in a regular webpage – I also talked a little bit about observable. To be honest though I didn’t talk about it very much: at the time I was focused mostly on p5.js, and didn’t really think too much about how observable works.\nLately, though, I’ve been making a few new generative art systems in javascript. The advent and pastiche series I posted to my art site were both written in javascript. In both cases I used node.js to run the js code locally, calling the scripts from the terminal rather than running them in a browser context.\nMaking art with javascript has been quite a lot of fun, and since it’s been on my mind lately, I thought it might be time to it prompted me take another look at observable in this post. I’m not planning a deep dive or anything, but I will talk a little bit about little art system I made that makes dynamic pieces like this one:\nart(100)"
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#what-is-observable-and-whats-it-doing-in-quarto",
    "href": "posts/2024-01-09_observable-js-art/index.html#what-is-observable-and-whats-it-doing-in-quarto",
    "title": "Making generative art with observable.js",
    "section": "What is observable, and what’s it doing in quarto?",
    "text": "What is observable, and what’s it doing in quarto?\nAt its heart, observable.js provides a method for creating computational notebooks that execute javascript code in the context of a document containing regular text, in much the same way that jupyter notebooks execute code cells that contain R, Python, or Julia code, and allow the author to interleave regular text with the code.\nI’ll talk more about the code execution and the structure of observable notebooks first, but it helps to start by making a clear distinction between the code execution engine that powers observable notebooks, and the hosted service that most people use when creating one. To see what I mean, notice that most of the time when someone wants to create an observable notebook, they use the hosted service provided by the company which – somewhat confusingly – is also called observable.1 For example, here is a notebook by Allison Horst: it uses the observable javascript library, and it’s hosted on observable service provided by observable-the-company. When people talk about “observable notebooks”, this is the kind of thing they usually mean.\nHowever, this isn’t the only possible approach. You could, for instance, include the observable.js core libraries in a regular webpage (i.e., using the javascript libraries but not the hosted service). Alternatively – and this is the approach I’m taking here – you can create quarto documents that rely on observable to execute javascript code, which is fairly painless to do because quarto provides native support for observable.js. To create an observable-javascript code chunk (“code cell”, in the terminology used by observable), you’d create an {ojs} chunk like this:\n\n```{ojs}\n//| echo: fenced\n1 + 1\n```\n\n\n\n\n\n\nHere you can see the javascript code (1 + 1), the output (2), and the quarto syntax used to create the chunk itself (the {ojs} bit), but from now on I’ll drop the quarto syntax.2 The important thing to realise here is that although superficially this page looks rather different to a notebook on the observable hosted service, the underlying “execution engine” is more or less the same."
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#code-cells-execute-in-logical-order",
    "href": "posts/2024-01-09_observable-js-art/index.html#code-cells-execute-in-logical-order",
    "title": "Making generative art with observable.js",
    "section": "Code cells execute in logical order",
    "text": "Code cells execute in logical order\nFor the most part, the code execution within an observable notebook (or a quarto document like this one that uses observable) follows all the usual syntax for vanilla javascript. You can use a cell to define variables:\n\nmsg = \"hello world\"\n\n\n\n\n\n\nand you can refer to those variables later:\n\nmsg\n\n\n\n\n\n\nHowever, a key characteristic to observable.js is its reactive runtime engine: much like a spreadsheet, cells are executed in “topological order” based on the logical dependencies among them. For example, the cell below refers to a variable called surprise, and it works even though (reading this document from top to bottom) I haven’t actually defined it yet:\n\nsurprise\n\n\n\n\n\n\nThis works because, at a later point in this document, there is a code cell that does specify a value for surprise, and observable.js detects that the later cell is the logical precursor to the one above, and executes that cell before this one. It is also the reason why I can call the art() function in the code cell below. Because the art() function is defined later in the document, I’m permitted to call it now, knowing that observable will determine the order in which all the cells need to be executed:\n\nart(101)\n\n\n\n\n\n\nTo unpack this still further, the image below shows all the dependencies of the call to art(101) that occurs in the cell above. As you can see, all of the cells upon which the cell above depends appear lower in the document, but that’s okay, because observable.js uses the logical dependencies (which take the form of a directed acyclic graph) among these cells to determine the order in which they execute:"
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#loading-modules-in-observable",
    "href": "posts/2024-01-09_observable-js-art/index.html#loading-modules-in-observable",
    "title": "Making generative art with observable.js",
    "section": "Loading modules in observable",
    "text": "Loading modules in observable\nOne of the things about javascript that sometimes gives me headaches is getting used to the numerous different ways you can import javascript code depending on the context in which you’re using the language.\nWhen I first started using javascript in web pages, I didn’t use any external javascript code at all. I’d write an html file that embedded my hand-rolled javascript coded directly into the document, with the code wrapped within a &lt;script&gt; tag:\n&lt;script type=\"text/javascript\"&gt;\n  console.log(\"hello world\") // comment\n&lt;/script&gt;\nLater, I learned that it was often better to separate the javascript code from the html document, so my web page would contain an .html file and a .js file. Within the html document, I would again have a &lt;script&gt; tag, but instead of including the javascript source code it would reference the .js file:\n&lt;script src=\"./myscript.js\"&gt;&lt;/script&gt;\nThis is the traditional way of using javascript in a webpage. All javascript code that I wanted to use in my webpage would be stored in local copies of the relevant .js files, and I’d include them with &lt;script&gt; tags. However, once I learned about content delivery networks (CDN) like jsDelivr, I started to realise that I didn’t have to have a local copy of all the files: instead, I could import the files I needed through a CDN. A common example of this in the javascript that I used to write during that stage of my life would be to load jquery with code like this:\n&lt;script src=\"https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js\"&gt;&lt;/script&gt;\nUntil quite recently, that was essentially everything I knew about loading javascript modules. I’d only ever used javascript in the context of an html document, and in that context it is the .html file that does the work of loading the javascript.\nEnter, stage left, the node.js runtime environment for javascript being chased by the node package manager npm. Traditionally javascript is a scripting language used in the context of a webpage, but it doesn’t actually have to be used that way thanks to node.js. Node allows javascript to function as a pure scripting language: you write code in javascript that you can then execute from the terminal without any browser at all. That’s what I’ve been doing with my recent generative art code that uses javascript. For instance, advent_06.js is one of those generative art scripts, and – because I have node installed on my machine – when executing the script I’d use this command at the terminal to create the images:\nnode advent_06.js\nOne question that you’d naturally have, when running javascript in this fashion, is how to import modules: you can’t use a &lt;script&gt; tag for this because there’s no html file and the javascript code is not executing within a browser. To that end node provides a require() function that you can use for this. As I have previously installed the seedrandom.js library (which allows you to create random number generators that give you control over the RNG seed) using npm, I can include a line of code like this in my js script:\n// this code is from the advent_06 script, it's not an ojs cell\nconst seedrandom = require('seedrandom') \nThis creates a seedrandom object that exposes all the functionality of the library, which is then used elsewhere in the script. This works just fine, but it’s important to recognise that require() is not part of vanilla javascript, it’s specific to nodeJS. You can’t use it in vanilla javascript.\n…and yet, you can use it with observable.js. This is because observable supplies its own require() function that behaves similarly to the node.js require() function. For this document, I decided to be smart and import a specific version of seedrandom, like so:\n\n// this is the ojs cell that imports seedrandom in this document\nseedrandom = require('seedrandom@3.0.5')\n\n\n\n\n\n\nThe key thing to remember here (and I’ll confess this threw me for a loop for a little while) is that although this looks like I’m writing “server-side” code like you’d do with node.js, this calls the observable.js version of the require() function. Per the documentation:\n\nBy default, require uses modules published on npm, a service that hosts over 1 million different modules created by thousands of individual developers. Because notebooks run in a web environment, we use another service, jsDelivr, that takes npm’s modules and makes them accessible to browsers.\n\nIn other words, when calling the require() function in an observable notebook (or quarto document that uses observable code cells), we’re still relying on a CDN to do the work of importing the javascript modules. The observable require() function abstracts away from the tedious details and means you don’t have to write any html to import from the CDN."
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#cells-implicitly-iterate-over-generators",
    "href": "posts/2024-01-09_observable-js-art/index.html#cells-implicitly-iterate-over-generators",
    "title": "Making generative art with observable.js",
    "section": "Cells implicitly iterate over generators",
    "text": "Cells implicitly iterate over generators\nSo now we turn to the generative art system itself. The core mechanic underpinning the system is that each piece is defined by a large number of “particles” that are drawn onto an html5 canvas. The particles move around over time, changing shape and size as they do, sometimes obscuring one another when they are drawn onto the canvas. This leads to dynamic pieces like this:\n\nart(102)\n\n\n\n\n\n\nLater on in the document I’ll define a makeParticles() function that returns an array containing many of these particles, each of which is an object that has fields like x, y, size, etc that defines the state of the particle, as well as a .draw() method used to draw said particle to the canvas, and a .move() method to update the location, shape, size, etc for the particle. The details of that don’t matter for the moment. Let’s just assume that this mechanism works, and examine the top-level art() function that creates the pieces:\n\nfunction* art(seed) {\n  // setup\n  const par = {height: 600, width: 900, nrow: 12, ncol: 18}\n  const rng = seedrandom(seed)\n  const ctx = DOM.context2d(par.width, par.height)  \n  const pal = pickOne(palettes, rng)\n  \n  // image\n  ctx.fillStyle = pickOne(pal, rng)\n  ctx.fillRect(0, 0, par.width, par.height)\n  let particles = makeParticles(par, pal, rng)\n  while (true) {\n    particles.map(p =&gt; {p.draw(ctx); p.move(rng)})\n    yield ctx.canvas\n  }\n}\n\n\n\n\n\n\nThe key part of this function is that it’s a generator function: it is defined with the function* keyword, and instead of having a return value it has a yield value. Generators (also called iterators) are stateful functions. The first time they are called they execute up to the point that the yield statement is encountered, and then return that value. Any subsequent time the generator is called, it doesn’t start from the beginning: it picks up from where it left off last time, and keeps executing until yield is encountered again. So, in this case, art() is a generator function that yields an html5 canvas object when it is called the first time; later invocations of art() will update the state of the canvas, and then yield the updated canvas as its return value. This provides a natural mechanism for animation: each yield value is, in effect, a frame of the animation.\nSo far, so good. While I’m not an expert at using generators in vanilla javascript, I grasp the basic idea. The part that is a little peculiar here – and is specific to the reactive runtime used in observable.js – is that nowhere in this document do I appear to be calling the art() generator repeatedly. In vanilla javascript, if I’d defined art() in the way I did in the previous cell and then called art(103), I would get a static canvas that corresponds to the first frame of the animation because I’ve only called the generator once. And yet…\n\nart(103)\n\n\n\n\n\n\nWhat’s happening here is that the reactive runtime used in observable.js implicitly iterates over a generator function. The code cell above appears to be invoking the art() generator once (with a seed of 103), but that’s not actually true: what’s really happening is that observable.js calls the generator repeatedly behind the scenes (i.e., implicitly iterates). I haven’t done a deep dive on how this behaviour works yet, but according to the documentation this iteration typically occurs 60 times a second. So that’s what’s happening here."
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#the-workhorse-code",
    "href": "posts/2024-01-09_observable-js-art/index.html#the-workhorse-code",
    "title": "Making generative art with observable.js",
    "section": "The workhorse code",
    "text": "The workhorse code\nHaving now discussed everything that is specific to observable.js, all that remains to do is add some code cells that contain the “workhorse” functions for our generative art system. Here’s the makeParticles() function that takes a collection of parameters par, a palette pal, and a random number generator rng as its arguments, and returns an array of particles. Initially the particles are all laid out on a grid, with x coordinates lined up in columns and y coordinates lined up in rows. As the system evolves, these particles are moved around using the move() function, and can be drawn to a canvas using the draw() function:\n\nfunction makeParticles(par, pal, rng) {\n  const rowheight = par.height / par.nrow\n  const colwidth = par.width / par.ncol\n  const s = Math.min(rowheight, colwidth) * .5\n  let particles = []\n  let k = 0\n  for (let c = 0; c &lt; (par.ncol - 1); c++) {\n    for (let r = 0; r &lt; (par.nrow - 1); r++) {\n      particles[k] = {\n        id: k,\n        col: c,\n        row: r,\n        size: 1 + (s - 1) * rng.double(),\n        sizemin: 1,\n        sizemax: s,\n        width: 3,\n        x: colwidth * (c + 1),\n        y: rowheight * (r + 1),\n        xmax: par.width,\n        ymax: par.height,\n        xstep: .3,\n        ystep: .3,\n        start: Math.PI * 2 * rng.double(),\n        length: Math.PI * rng.double(),\n        turn: .1,\n        shade: pickOne(pal, rng),\n        shrink: .005,\n        draw: function(ctx) {drawParticle(this, ctx)},\n        move: function(rng) {moveParticle(this, rng)}\n      }\n      k++\n    }\n  }\n  return particles\n}\n\n\n\n\n\n\nAs you can see, the draw() function for any given particle is just a call to the drawParticle() function, and similarly the move() function is just a call to moveParticle(). Here’s those two functions, each defined in their own code cell:\n\nfunction drawParticle(p, ctx) {\n  ctx.fillStyle = p.shade\n  ctx.strokeStyle = p.shade\n  ctx.lineWidth = p.width\n  ctx.beginPath()\n  ctx.arc(p.x, p.y, p.size, p.start, p.start + p.length)\n  ctx.fill()\n  ctx.stroke()\n  ctx.closePath()\n}\n\n\n\n\n\n\n\nfunction moveParticle(p, rng) {\n  p.y = p.y + p.ystep * (rng.double() - .5) * p.size\n  p.x = p.x + p.xstep * (rng.double() - .5) * p.size\n  if (p.y &gt; p.ymax) p.y = p.y - p.ymax\n  if (p.x &gt; p.xmax) p.x = p.x - p.xmax\n  if (p.y &lt; 0) p.y = p.ymax - p.y\n  if (p.x &lt; 0) p.x = p.xmax - p.x\n  if (p.size &gt; p.sizemin) {\n    p.size = p.size - p.shrink * (p.sizemax - p.sizemin)\n  } else {\n    p.size = p.sizemax\n  }\n  p.start = p.start + p.turn * (Math.PI * 2 * (rng.double() - .5))\n}\n\n\n\n\n\n\nAt several points in the code that defines this system, I’ve relied on a pickOne() function that uses a specific random number generator rng to sample a single element from an array. So of course that too needs to be defined as an observable code cell:\n\nfunction pickOne(items, rng) {\n  return items[Math.floor(rng.double()*items.length)]\n}\n\n\n\n\n\n\nFinally, in order for the art() function to choose a random palette every time a new piece is created, I need to have an array palettes that defines a bunch of different palettes (I created these palettes by playing around on cooolors.co and saving some that I liked):\n\npalettes = [\n  [\"#de9151\", \"#f34213\", \"#2e2e3a\", \"#bc5d2e\", \"#bbb8b2\"],\n  [\"#a63446\", \"#fbfef9\", \"#0c6291\", \"#000004\", \"#7e1946\"],\n  [\"#ffffff\", \"#ffcad4\", \"#b0d0d3\", \"#c08497\", \"#f7af9d\"],\n  [\"#aa8f66\", \"#ed9b40\", \"#ffeedb\", \"#61c9a8\", \"#ba3b46\"],\n  [\"#241023\", \"#6b0504\", \"#a3320b\", \"#d5e68d\", \"#47a025\"],\n  [\"#64113f\", \"#de4d86\", \"#f29ca3\", \"#f7cacd\", \"#84e6f8\"],\n  [\"#660000\", \"#990033\", \"#5f021f\", \"#8c001a\", \"#ff9000\"],\n  [\"#c9cba3\", \"#ffe1a8\", \"#e26d5c\", \"#723d46\", \"#472d30\"],\n  [\"#0e7c7b\", \"#17bebb\", \"#d4f4dd\", \"#d62246\", \"#4b1d3f\"],\n  [\"#0a0908\", \"#49111c\", \"#f2f4f3\", \"#a9927d\", \"#5e503f\"],\n  [\"#020202\", \"#0d324d\", \"#7f5a83\", \"#a188a6\", \"#9da2ab\"],\n  [\"#c2c1c2\", \"#42213d\", \"#683257\", \"#bd4089\", \"#f51aa4\"],\n  [\"#820263\", \"#d90368\", \"#eadeda\", \"#2e294e\", \"#ffd400\"],\n  [\"#f4e409\", \"#eeba0b\", \"#c36f09\", \"#a63c06\", \"#710000\"],\n  [\"#d9d0de\", \"#bc8da0\", \"#a04668\", \"#ab4967\", \"#0c1713\"],\n  [\"#012622\", \"#003b36\", \"#ece5f0\", \"#e98a15\", \"#59114d\"],\n  [\"#3c1518\", \"#69140e\", \"#a44200\", \"#d58936\", \"#fffb46\"],\n  [\"#6e0d25\", \"#ffffb3\", \"#dcab6b\", \"#774e24\", \"#6a381f\"],\n  [\"#bcabae\", \"#0f0f0f\", \"#2d2e2e\", \"#716969\", \"#fbfbfb\"],\n  [\"#2b4162\", \"#385f71\", \"#f5f0f6\", \"#d7b377\", \"#8f754f\"]\n]\n\n\n\n\n\n\nOh, and yeah… it’s not part of the generative art system, but because earlier on I referenced a surprise variable in order to illustrate the “spreadsheet-style” code execution order in observable, I’d better have a cell that actually defines that variable:\n\nsurprise = \"a variable defined at the bottom of the document\"\n\n\n\n\n\n\nAnd we are done!\n\nart(104)"
  },
  {
    "objectID": "posts/2024-01-09_observable-js-art/index.html#footnotes",
    "href": "posts/2024-01-09_observable-js-art/index.html#footnotes",
    "title": "Making generative art with observable.js",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI realise it’s a small gripe, but I do think it’s a problem when companies create this confusion by conflating the organisation with the product and/or service. For many years it was difficult to distinguish between rstudio-the-company and rstudio-the-IDE. Observable makes this confusion even messier, since we have observable-the-company, observable-the-javascript-library, and observable-the-hosting-service. They’re three different things that all have the same name and it’s a pain to write about.↩︎\nThis is a little different than what would have happened if I’d used {js} rather than {ojs}: when you use {js} in quarto, the javascript code is embedded as a script within the html page, rather than executed with observable.js↩︎"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html",
    "title": "How to mint digital art on HEN",
    "section": "",
    "text": "Cryptoart can be a touchy subject for generative artists, and it’s something a lot of us have messy feelings about. In my case it is no secret that I feel conflicted, and I completely understand why a lot of us are uncomfortable with it. I genuinely believe there are many perfectly good reasons why a generative artist would choose not to participate. On the other hand, I also recognise that there are some very sensible reasons why a generative artist would want (or need) to sell NFTs: artists have to pay rent, for example. So this post isn’t about passing judgment one way or the other. It’s intended to be a guide to help other artists get started in this area, particularly artists in the R community, if they should decide to try it out. That’s all.\nThis post is also not supposed to be an introduction to blockchains or cryptocurrencies. It doesn’t dive into the details on what these things are or even what an NFT is. I make art: I don’t care about any of these subjects. What I’m assuming is that you’re coming to this world from a similar position to me: you have a vague understanding of what blockchain is, what cryptocurrencies are about, and have a similarly vague notion that an NFT is kind of like a “digitally signed copy” of your art that you can sell to other people. That’s all you need."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#prologue-barriers-to-entry",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#prologue-barriers-to-entry",
    "title": "How to mint digital art on HEN",
    "section": "Prologue: Barriers to entry",
    "text": "Prologue: Barriers to entry\nOne thing I have noticed about the world of cryptoart is that there are many barriers to entry. Some barriers are obvious: if you want to sell art on Foundation, for example, you need to be invited. To be invited, you need to know someone who can and will invite you. As anyone who has ever been excluded from a fancy venue by virtue of their race, gender, sexual orientation, transgender status etc can attest, an invitation requirement is a non-trivial and frequently discriminatory barrier. “By invitation” systems create entry barriers by design: for good or ill, they are inherently exclusionary. But there are other ways in which cryptoart creates barriers to entry.\n\nEnvironmental costs matter\nAnother kind of barrier comes from the nature of cryptoart. Blockchains were not designed to be energy efficient, and they can be extraordinarily wasteful (much more than you’d think). Environmental considerations also create barriers to entry, albeit indirect barriers. For example, the biggest cryptocurrencies like Bitcoin and Ethereum operate on a “proof of work” principle (often abbreviated to “PoW”) and as the name suggests, operations on those chains require a lot of computational work. A lot. They are staggeringly wasteful, and as a consequence the total energy consumption of these chains is so high that an NFT minted on one of these chains has a very high carbon footprint. Proof of work chains are an environmental disaster, and so (in my mind) they are socially irresponsible. Don’t use them if you can avoid it.\nThis poses a problem for artists, unfortunately. The biggest cryptoart markets are based on the Ethereum chain, and Ethereum is a proof of work chain. True, there are plans to change this and make Ethereum more ethical, but it hasn’t happened yet and I personally am unwilling to participate until that switch actually occurs. This is deeply unfortunate from artistic point of view, because it rules out OpenSea. It sucks because OpenSea is the largest marketplace and it’s very easy to get started there. For instance, I have an unused account that I set up in a few minutes before I realised the problem. But for me the end-user convenience wasn’t worth the environmental costs, so I abandoned this idea at the outset. On the plus side, OpenSea have announced that they are planning to support the Tezos blockchain (see below), and when that day comes I will probably make use of my OpenSea account: the thing I take moral issue with is not OpenSea, it is with Ethereum (or more precisely, with proof-of-work chains). Personally, I don’t want to touch the stuff.\nSo what are the alternatives?\n\n\nThere are alternatives\nThe main alternative to the “proof of work” blockchains are the “proof of stake” (PoS) blockchains. These don’t require anywhere near as much computation, and as a consequence are much more energy efficient. For that reason, NFTs on those chains are often called “clean NFTs”. There are a multiple proof of stake chains (Tezos, Solana, etc), but the one I’m most familiar with is Tezos. To give you a sense of just how extreme the difference is, this is a screenshot that popped up on one of the sites while I was doing my initial exploration:\n\n\n\n\n\n\n\n\n\nEven if this claim is somewhat exaggerated for marketing purposes, the sheer scale of it is remarkable. A multiplicative factor of 1.5 million is… enormous. I could literally mint NFTs on Tezos for every single image that I have ever created for the rest of my life, and it would still be several orders of magnitude more energy efficient than minting one piece on Ethereum. To my way of thinking, that makes a massive difference to the moral calculus associated with minting NFTs. In fact, the difference between Tezos and Ethereum is so extreme that there is actually one art marketplace there – Bazaar – that is not just carbon neutral but is actually carbon negative. That’s only possible because Tezos is so much more efficient than Ethereum, and it becomes practical for the developers to impose a carbon tax on minting: the transaction costs are used to purchase sufficient carbon offsets to ensure the system as a whole remains carbon negative. Right now I wouldn’t recommend setting up on Bazaar because it’s so early in development that it’s hard to use, but I’m absolutely keeping an eye on it for the future!\nSetting up on the Tezos blockchain is particularly appealing because it has an established digital art marketplace called “hic et nunc”. The name is Latin in origin and translates to “here and now”. You’ll usually see it abbreviated to “HEN”, which is what I’ll call it in this post, but some people use “H=N”, I guess because it looks visually similar to the HEN logo. The HEN marketplace is completely open: you don’t need an invitation. There’s no super-secret club to be invited into (as far as I know!), and to my mind that’s a huge positive. Better yet, a few folks from the R art community are already there. I’m entirely certain that there are others I don’t know about yet, but so far on HEN I’ve already found Thomas Lin Pedersen, Will Chase, Antonio S. Chinchón, and George Savva. As of a few days ago, I’m there too.\nOpenness! Community! Yay!\nIf there’s one thing I have learned from the lovely R folks on twitter, everything is better when you are part of a supportive team of people who actually care about each other and work to build each other up. From my perspective, this makes HEN a very attractive option.\nThere is, unfortunately, a catch. There is always a catch.\n\n\nIt can be confusing\nOne big limitation to HEN is that it isn’t easy to get started there unless you are already somewhat enmeshed in the crypto world generally, or the cryptoart scene specifically. The ecosystem is distributed over several sites that have weird names without enough vowels, the user interfaces on the sites tend to be unconventional (often pointlessly so in my opinion), and the “how to” guides aren’t very easy to read. The overall aesthetic and typology screams out WE ARE THE COOL KIDS in capital letters. It doesn’t even have the good grace to be subtle about it. Taken together, all these little things add up, and it annoys me. I have been a professional educator for 15 years now, and I can absolutely guarantee that the overall effect of this is to create a de facto entry barrier. All these things act as signals to exclude people who aren’t already part of the clique. It feels disproportionately uncomfortable if you’re an outsider. It tells you that you’re not welcome if you’re not one of the cool kids. Are you one of the cool kids? No? Then sorry. No HEN for you babe.\nWell, fuck.\nYet again, there are barriers to entry to HEN, and that makes me uncomfortable. However, unlike the other cryptoart options I looked at, there’s something I can do to improve the situation: I can write a blog post explaining the process. This blog post.\n\n\nLet’s demystify it\nLet’s assume you’re not one of the cool kids. Let’s assume you’re just a regular human being who likes to make generative art in R, and are a little curious. You have a vague idea of what cryptocurrencies are (yeah, yeah, digital currency blah blah blah). You have a vague idea of what an NFT is (digitally signed copy of the art, whatever dude). Maaaaaybe you’ve sort of heard of HEN … but that’s only because you’ve seen some R people posting about it on twitter. And that’s it. That’s all you know. But maybe you want to try it out, just to see if it’s for you? Just to try. But you really, really, reaaaaaalllllllly don’t want to wade into all the details and you’re secretly worried that it’s all too complicated and you won’t be able to do it. Your impostor sydrome is going wild. Is that you? Do you feel the same way I felt?\nIf so, this post is written for you."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-an-overview",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-an-overview",
    "title": "How to mint digital art on HEN",
    "section": "1: Get an overview",
    "text": "1: Get an overview\nWhen I started setting up on, I wandered around the Tezos cryptoart landscape in confusion, wandering aimlessly over the terrain. It was all deeply unsettling. Eventually I pieced together some overall view of things, but I wouldn’t recommend doing things the same way I did. I think the best thing to do first is to “zoom out” and look at the landscape as a whole. The best site I’ve found for doing that is tezos.art. If you click on the link it will take you to a page with the following three sections:\n\nMarketplaces: Sites where you can mint, buy, and sell art\nWallets: Tools that handle your identity and store your funds\nCommunity: Places where you can go for help\n\nIt’s worth taking a quick look at this page because it gives you a feel for what all the moving parts are, but doesn’t dive into details. You’ve taken a quick peek, yes? Cool. Let’s get started…"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#create-a-wallet",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#create-a-wallet",
    "title": "How to mint digital art on HEN",
    "section": "2: Create a wallet",
    "text": "2: Create a wallet\nIt’s a little counterintuitive, but the natural place to start is not the art marketplaces: the first thing you need is a wallet. The reason for this is that your wallet serves two distinct purposes. As the name suggests, the wallet provides a method for storing funds: the currency itself is referred to as “tezos”, which you’ll see abbreviated to “tez” or denoted “ꜩ”. However, it also serves as your unique identifier on the Tezos blockchain. On blockchains as in life it is rather hard to do anything interesting without a public identity, so you need to create one first.\nOkaaaay… at this point you’d probably be wondering “where do I sign up for one of these wallets?” Excellent question. As you will have noticed by peeking at the tezos.art website, you have a few different options. Being offered choices is nice, of course, but it can also be anxiety-provoking when you don’t even know what the differences between the options are. So, for whatever it’s worth, I’ll mention that I chose Temple Wallet. I made that choice for two reasons and only two reasons. First, it was one of the options listed on the HEN wiki. Second, I was complaining privately to Will Chase about how confused I was and he told me uses Temple and I copied what he did. That being said, I suspect the choice is arbitrary.\nFor the sake of argument, I’ll assume you decided to use Temple too. So now you’re clicking through the link above in order to open an account with Temple Wallet and… wait, it’s just a browser extension? Yup. This seems to be very common in blockchain land, and initially it struck me as bizarre. The longer I hang around there, however, the more I realise it does make a kind of sense. Once you start doing things on Tezos, you’ll find that you have to validate everything you do. Any time you ask a website to undertake some action on your behalf, the first thing that will happen is that you’ll be asked to authorise the action using your public identity. What that means is that you have to use your wallet all the time, even for things that don’t cost money. A browser extension makes this a little easier. When the website asks you to authenticate, the wallet browser extension will create a little popup window that asks you to confirm the transaction. There’s a bit of friction to the process sometimes, and it feels a little alien, but it does start to feel normal after a while.\nMoving on… the next little strangeness is that when you set up the wallet you don’t create a username, only the password, and you’ll be given a “recovery phrase”, which is a sequence of 12 random words. Don’t lose either of these things. Here, as always, I strongly recommend that you use a password manager to store your password, because there aren’t that many options for recovery if you start losing passwords. Personally, I’ve been using 1password for a few years and I really like it. So yes. Use a password manager, store your wallet password there and store your recovery phrase there too.\nAt the end of this process you are assigned a public identity, which is a long string of complete gibberish. For example, this is me:\ntz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nNaturally, the first thing I did when seeing this is groan. The second thing I did is notice the Srmojf substring and it made me think of Smurfs. So I secretly think of this gibberish identifier as the Smurf, and that’s how I’ll refer to it for the rest of this post. Of course, in the long run you probably don’t want to be a random string of digits, you want to have a name! This is possible to do, and I’ll walk you through that later. But right now that’s not a complication you need to care about.\nWe’ll get to that a little bit later but the key thing for now is that your equivalent of the Smurf is both a public identifier and a bank account number. If someone wants to send you some tez, all they need to know is that string."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#tell-hen-who-you-are",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#tell-hen-who-you-are",
    "title": "How to mint digital art on HEN",
    "section": "3: Tell HEN who you are",
    "text": "3: Tell HEN who you are\n\nSynchronise with your wallet\nWhen you go to the HEN website you’ll see a little bit of text on the top right hand side that has a link that says “sync”. Click on that:\n\n\n\n\n\n\n\n\n\nThis will bring up an overlay that looks like this:\n\n\n\n\n\n\n\n\n\nIf you chose a Temple wallet choose the “Temple - Tezos Wallet (ex. Thanos)” option. It might ask for your password at this point but it probably won’t if you’re already logged in. What you’re more likely to see is a screen like this:\n\n\n\n\n\n\n\n\n\nThis is a message from your wallet asking you to confirm that yes, you do want to synchronise with HEN (it also shows you that I currently have a balance of 11 tez, which I guess is something like US$60). Click on connect, and HEN will now be synchronised with your identity. You can see that because the menu at the top now looks something like this:\n\n\n\n\n\n\n\n\n\nYou’re now synced: in effect, you are now logged in to HEN. You still don’t have a username, but you have authenticated yourself and you can now change some settings.\n\n\nThe HEN menu is weird\nOkay, let’s move to the next step. To the right of your Smurf, you’ll see the “hamburger” menu. It behaves pretty much the same as any menu you’d encounter on the internet, but some of the options have very non-intuitive names. Here’s what the menu looks like, with my annotations added:\n\n\n\n\n\n\n\n\n\nAs with everything about HEN, it’s very minimalist. Some of the options are easy to understand, but others are not. The options I’ve been using most are these:\n\nsearch takes you to the HEN search page\nedit profile allows you add some information about yourself (see next section)\nmanage assets will take you to your profile page (it took me a long time to realise this)\nOBJKT (mint) is the option you select when you want to create art. I’ll talk bout that later\n\n\n\nName, avatar and bio\nThe time has come to give yourself a name. If you do things in the right order and with the right mental model of what’s going on, this is pretty easy to do, but it’s easy to get a little confused because there are actually multiple things going on here, and you always have to keep in mind that your equivalent of my Smurf string is your actual identity.\nSo… your first step is to tell HEN to link your Smurf string to a name, bio and avatar. Click on “edit profile”. This brings up another slightly unconventional looking screen that has several options you can set. Here’s what mine currently looks like:\n\n\n\n\n\n\n\n\n\nThere are three things you can do immediately without any major hassle:\n\nFirst, if you click on “choose file” you can upload an image to give yourself a profile image.\nSecond, you can give yourself a username. The advice I read on the relevant HEN wiki page suggested that you should avoid spaces and special characters, and should stick to lower case letters because usernames are case sensitive.\nThird, you can write a brief description of yourself. It doesn’t have to be very thorough. Most people say something about who they are and what they do, but you don’t have to. For example, I’ve had a habit of identifying myself as “an object of type closure” on all my social media websites. It’s intended as a silly reference to the classic R error message:\n::: {.cell}\nidentity[]\n::: {.cell-output .cell-output-error} Error in identity[]: object of type 'closure' is not subsettable ::: :::\nAs it happens, this allowed me to make an even sillier double-layered joke in my HEN bio. When you create art on HEN the tokens that you generate are referred to as OBJKTs, so now I refer to myself as “an OBJKT of type closure”. I’m so funny.\n\nAaaaanyway… once you’ve done those three things, click on “save profile”, and you’re done for now. Ignore everything below the “save profile” button. All that stuff is useful, and it will let you do things like link to your twitter profile and your github profile, but it’s surprisingly finicky to set up and it costs money, so we’ll leave that until later.\n\n\nCheck out your profile\nBefore moving on, take a quick look at your profile. As I mentioned earlier, you can do this through the menu system, by selecting the “manage assets” option. Personally I wish they’d chosen a better name: I’m not an investor and I don’t think of my art as “assets”. The page that displays my art is my homepage on HEN, and it bothers me a little that the site frames it in such mercenary terms. It’s irritating. But whatever, it’s not a dealbreaker.\nIt’s worth quickly commenting on the URL for your profile. When you click on the “manage assets” link, it will take you to a URL that identifies you using the Smurf. For me, that URL is:\nhttps://www.hicetnunc.xyz/tz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/\nAs long as you have your very own Smurf in your wallet, you’ll have this URL. However, if you followed the instructions in the last section, HEN is kind enough to arrange it so that the ugly Smurf based URL will automatically redirect to one based on your username. For me, that URL is:\nhttps://www.hicetnunc.xyz/djnavarro/\nAt this point, you exist on HEN! Yaaaay!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#intermission-follow-people",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#intermission-follow-people",
    "title": "How to mint digital art on HEN",
    "section": "Intermission: Follow people",
    "text": "Intermission: Follow people\nThere’s more stuff you can do to get your account set up, but you might want to take a little breather and look for some art. Maybe you want to search for someone you know in the R community who might be on HEN, and you’d like to find them. As I mentioned earlier, the HEN site does have a search page, but there are some limitations. It’s okay if you want to search by keywords to find art or artists, but what it won’t let you do is follow them. Personally, I quite like being able to follow artists whose work I love, and it would be pretty cool to have a feed where I can see what they’ve posted, arranged in chronological order. That’s where the the “HEN explorer” website is handy:\nhttps://www.henext.xyz/\nLike HEN itself, the HEN explorer site has browsing and search capability. It’s a little clunky in places (on my browser, there seems to be a bug where the search box only works when you’re on the home page), but it does the job.\nTo use HEN explorer, you’ll need to synchronise with your wallet (i.e., log in). To do that you can click on the “profile” icon in the nav bar (the one that looks like a little person), or just visit\nhttps://henext.xyz/profile\nThat will bring up a screen that looks like this\n\n\n\n\n\n\n\n\n\nClick on the “connect wallet” button, and it will take you through the same steps that were involved when you connected your wallet to the HEN site.\nOnce you’ve done that, you’re logged in to HEN explorer, and you’re able to find artists you like and follow them! If you would like to follow me, you can search for “djnavarro” on the HEN explorer search box, or you can visit my HEN explorer profile page directly:\nhttps://www.henext.xyz/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7\nAdd a few artists you like, and you’ll get a sense of what the feed looks like. The location of the feed is\nhttps://www.henext.xyz/following\nHappy browsing!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-a-little-money",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#get-a-little-money",
    "title": "How to mint digital art on HEN",
    "section": "4: Get a little money",
    "text": "4: Get a little money\nOne slightly frustrating thing about this process is that it’s hard accomplish very much in this arena without spending money, and we’re rapidly reaching the point where you’ll need a little bit. Thankfully, if you’re an artist wanting to create your own art, and aren’t looking to collect anyone else’s, you don’t need very much to get started. If you’re in the R community there’s a good chance you can ask one of the other R folks on HEN to help out. That’s what I did, and I’m grateful to the people who sent me a few tez, and the others who spontaneously offered. R people are lovely.\nIf the “ask a friend” approach is an option for you, I’d recommend it for artists. The reason I say this is that you have a bigger set up cost (in terms of your time and effort) than someone who is joining in order to purchase art, so from the perspective of the artist all you need – right now – is a little start up fund. To use myself as the example, I made a lot of weird mistakes setting up and wasted quite a lot of transactions, but even with all that I think I only spent about 1 tez in total (at the exchange rate at the time that was about US$5).\nAssuming that you can solve the problem that way, you can take care of the other financials later (and there’s a guide on how to do that coming later in the post). There’s a part of me that hopes that if the R art community does end up with a larger presence on HEN, we’ll look after our own. We’re R folks, and we pay it forward because we care for each other.\nThat being said, I’m also not naive, and I know perfectly well that it doesn’t always work that way, so I’ll briefly mention other options. For example, the HEN website has some suggestions for other places you can ask for help. Alternatively if you have a Visa card, one possibility is to buy through https://tzkt.io/buy-tezos (the tzkt.io site will come up later in the post!), though you’ll need identification documents for this (or any other option) because it’s a financial institution. Finally, you can sign up at a currency exchange, which you’ll probably want to do later anyway because that’s going to be how you convert the funds from your HEN sales to regular currency. I’ll talk about that later on.\nRegardless of how you solve this part of the problem, I’m hoping that at this point you have a few tez to start out!"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#release-your-art",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#release-your-art",
    "title": "How to mint digital art on HEN",
    "section": "5: Release your art!",
    "text": "5: Release your art!\n\nMinting the art\nSurprisingly, the process of releasing your art on HEN is quite easy, at least when compared to how complicated everything else is. If you open the menu and click on the “OBJKT (mint)” option, it will take you to the minting page, which looks like this:\n\n\n\n\n\n\n\n\n\nAt this stage in the process you upload the file, give it a name and a description, and make some decisions about (a) how many tokens you want to create, and (b) your royalties, the percentage of future sales that are returned to you. Here’s me filling one out:\n\n\n\n\n\n\n\n\n\nClick on the preview button, and it will show you a preview of what the page will look like when it goes live. If you’re happy with it you can proceed and click the “mint OBJKT” button. You’ll be asked by your wallet to confirm the minting operation (this costs a small amount of tez), and then after a short time the OBJKT (i.e., the token) exists. In this case, here’s the page displaying the OBJKT that I’ve just created:\nhttps://www.hicetnunc.xyz/objkt/359761\n\n\nPutting OBJKTs up for sale\nThe tokens now exist, but as yet they have not been placed on the market. People can’t buy them from you. To place the token for sale, go to the page showing the token (i.e., the link above). It will look something like this:\n\n\n\n\n\n\n\n\n\nIf you want to put the art on sale, click on the “swap” link that I’ve highlighted here (and if you change your mind and want to destroy it, click on the “burn” link next to it). The interface will look like this:\n\n\n\n\n\n\n\n\n\nIt will then let you decide you many of your tokens you want to put up for sale, and set the price for each one. For this particular piece I’d decided to create a lot of tokens (there are 50 of them), and I’m going to put them all on sale at the very low price of 2 tez. I honestly know nothing about pricing, but I’m playing around with it at the moment: some pieces I mint only a single token and set the price high, others I mint a large number of tokens and set the price low. In any case, when you’re happy press the “swap” button, confirm with your wallet, and the pieces will now be on sale!\n\n\nCreating auctions\nThe mechanism I’ve shown above is the simplest way to put art on sale: you list a price and wait for someone to purchase it. However, if you want to try more exotic options like auctions, you can check out objkt.com.\n\n\nSome art…\nHere are the Native Flora pieces I posted while writing this post. They’re all available for purchase!\n\n\n\n\n\n\n\n\n\n\nOBJKT 359814\n\n\n\n\n\n\n\n\n\nOBJKT 359795\n\n\n\n\n\n\n\n\n\nOBJKT 359761\n\n\n\n\n\n\n\n\n\nOBJKT 359745"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#share-on-social-media",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#share-on-social-media",
    "title": "How to mint digital art on HEN",
    "section": "6: Share on social media",
    "text": "6: Share on social media\nAt some point you’ll probably want to advertise the fact that the artwork is available for purchase. You don’t have to, of course, and I’m honestly not sure how much of my online life I want to spend advertising art for sale, but it’s handy to have the option, and that probably means sharing on social media.\nMost of us in the R community who make art are primarily sharing on twitter. Yes, I have seen some people post on reddit, others on instagram, and no doubt many other places besides, but my social media world is dominated by twitter, and I’d like to be able to post to twitter. To my mild irritation, the HEN website doesn’t seem to do twitter cards properly, so if you share the link on its own, people won’t see a nice preview image.\nThere are a couple of ways to get around this. The first is to post the link on twitter and attach your art as an image: that way folks on twitter will get the link and and the image. But they won’t get an actual twitter card displaying the title of the piece.\nThe second solution is to use the hic.art website. At the moment, if you visit the website it will tell you that signups are closed, but that actually doesn’t matter. You don’t need to sign up to use the service. All you have to do is provide the OBJKT identifier. For instance, here’s one of my pieces on HEN:\nhttps://www.hicetnunc.xyz/objkt/354474\nThe identifier here is 354474. If I share the link above on twitter, it won’t display a very nice twitter preview. However, if I tweet this link\nhttps://hic.art/354474\nIt will display a very lovely looking twitter preview, and when the user clicks on the link or the preview it will automatically redirect to the official HEN page. It’s a nice service!\nHere’s an example from Antonio Sánchez Chinchón:\n\n\nMondrianomie 28Basic cellular multiorganism grown according to neoplasticism assumptions (2033 x 2400 PNG)3 ed - 28 tez at @hicetnunc2000https://t.co/TyNvt1zMBu#HEN #hicetnunc #hicetnunc2000 #nft #NFTs #nftcommunity #nftcollectors #cleannft #nftart #tezos #tezosart\n\n— Antonio Sánchez Chinchón (@aschinchon) September 25, 2021"
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#manage-your-identity",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#manage-your-identity",
    "title": "How to mint digital art on HEN",
    "section": "7: Manage your identity",
    "text": "7: Manage your identity\nThere are at least three additional tools that may be useful to you in managing your identity in the peculiar world of cryptoart on the Tezos blockchain: (1) you can set up a Tezos Profile, (2) you can establish an alias on the Tezos Blockchain Explorer, and/or (3) you can purchase a Tezos Domain. None of these are strictly necessary, but all of them offer some value to you as an artist on HEN so I’ll discuss each one.\n\nEstablishing a Tezos Profile\nEarlier in this post I mentioned that it’s possible to connect your twitter profile, github account, website, etc with your HEN profile? You can do this with the assistance of Tezos Profiles. So lets go back HEN, open the menu, click on the option that says “edit profile” and then take a closer look at the window that pops up. It’s almost impossible to notice, but the text that reads “Tezos Profiles” is in fact a link:\n\n\n\n\n\n\n\n\n\nClicking on that link will take you to https://tzprofiles.com/, where you will see a very prominent “connect wallet” button. Click on that button, confirm with your wallet that you want to allow tzprofiles to connect (the little popup window will appear, like it always does), and then you’ll see a screen that looks like this:\n\n\n\n\n\n\n\n\n\nThere are several different things you can do here, and any of them that you verify on tzprofiles will eventually end up on HEN. For example, if you want to verify your twitter account, you’ll go through a series of elaborate steps (which, yes, will have to be confirmed with your wallet) and in the end you’ll be forced to send a tweet like this one:\n\n\nI am attesting that this twitter handle @djnavarro is linked to the Tezos account tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7 for @tzprofilessig:edsigtaH3nvbQjpiAfMCnT4zcQESZefXoVLPf2NEYaZeUfhwHjzRYp4oeBiiyDFLdrUAUvjBhvepyDFoxuyE2ynVYxd7TvV9fj6\n\n— Danielle Navarro (@djnavarro) September 21, 2021\n\n\nTo verify your GitHub account it’s pretty similar, except that it forces you to create a gist, using your GitHub account, that includes a signature block similar to the one in the tweet. For a website, it’s the same idea except you have to insert it as a DNS record (which I found extremely irritating to do). You can verify as many or as few of these as you like, but there is some value to doing them. Because Tezos Profiles forces you to go through the clunky verification process, other people can check your HEN profile and verify for themselves that it really is you posting your artwork onto the site, and not someone else who has stolen your piece (apparently, that happens way too often)\nOnce you’re done verifying your accounts, you may need to use your wallet to confirm again so that the updated Tezos Profile information can be accessed by the HEN website. After that’s been done, you’ll see icons appear on your HEN page, linking to your twitter account, github account, etc:\n\n\n\n\n\n\n\n\n\nAt this point your HEN profile is meaningfully linked to your other public identities, and any artwork you mint on HEN can be traced back to you, the original artist.\n\n\nCreating an alias on Tezos Blockchain Explorer\nAll right. If you’re like me you’ve probably been exploring as you go and you’ve been encountering other sites that seem connected to this ecosystem. In particular, you may have clicked on links associated with transactions and it has taken you to the Tezos Blockchain Explorer website. As the name suggests, the role of this website is to publicly display transactions that take place on the Tezos blockchain. For example, here’s the page showing all the transactions that have involved me in some fashion:\nhttps://tzkt.io/tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7/operations/\nWhen I first started (oh so many days ago…) it looked something like this:\n\n\n\n\n\n\n\n\n\nA lot of it is gibberish, but you can kind of see what’s going on here. Yet again you can see my Smurf, there’s a bunch of transactions that show me minting NFTs, etc. It makes a kind of sense.\nWhat might be surprising, particularly if you’ve just gone to the effort of setting up a Tezos Profile, is that the account information doesn’t show my avatar. It doesn’t include my name, or a bio, and it doesn’t include my social media links. Instead, all I have is a cartoon image of a very suspicious looking cartoon cat. Unlike HEN, the tzkt.io site doesn’t pull information from your Tezos Profile.\nThe mystery deepens a little when you start noticing that the exact same cartoon cat appears on various other sites. For example, this was how my profile looked on objkt.com at the time:\n\n\n\n\n\n\n\n\n\nThe weird cryptocat was following me around across all these websites. Hm. The suspicious kitty is cute and everything, but honestly I’d prefer my usual name and profile image to follow me around instead.\nAs it turns out, the source for all these skeptical cats is the blockchain explorer site, tzkt.io, and you can submit an application to the people who run that site to create an alias for you. The process is described in this post on the “Baking Bad” blog (don’t let the name and silly images fool you, the blog is associated with the people who run the site). The post will take you to a Google Form that you can fill out, in order to have your alias created. When you do this, it won’t update immediately: there is a manual verification process that takes about three days, so you’ll need to be patient.\nOnce that happens you’ll discover that your links have appeared on your tzkt.io page, and more importantly perhaps, you have an avatar and description on other sites that make use of this alias. This is what my profile page on objkt.com looks like now:\n\n\n\n\n\n\n\n\n\nMine is a deliberately vague because I’m a peculiar person, but you can see a slightly more informative version if you look at Thomas Lin Pedersen’s profile:\n\n\n\n\n\n\n\n\n\n\n\nPurchasing a Tezos Domain\nWhen you look at the two profiles above, there’s something slightly peculiar. Notice how Thomas’ profile now links to thomasp85.tez and mine links to djnavarro.tez? That’s something slightly different again. Those addresses aren’t created by the Tezos Profile, nor are they created when you set your alias on the Tezos Blockchain Explorer. Those are Tezos Domains. The idea is very sensible: human beings don’t really enjoy memorising long strings of random digits. It would be much more convenient if I could message someone and say “hey send tez to me at djnavarro.tez, because that’s me!”. It’s certainly nicer than trying to say “send tez to me at tz1hXKn2BcU64HxSrmojfuf7cDoweJ9xwxZ7, because that’s me!”\nIf you’d like to do this, visit tezos.domains and follow the instructions there: it costs 1 tez per year to set one up."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#convert-tez-to-dollars",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#convert-tez-to-dollars",
    "title": "How to mint digital art on HEN",
    "section": "8: Convert tez to dollars",
    "text": "8: Convert tez to dollars\nAt some point, hopefully very soon, you’ll sell some artwork and you’ll want to get paid. To do that, you’ll probably need to sign up with one of the currency exchanges. Although you likely have no desire to be a currency trader, it’s a necessity if you want to get paid in real money. Yes, cryptocurrencies sound cool, but coolness does not pay the rent. My landlord expects to be paid in Australian dollars, and – by extension – so do I. That means exchanging your tez for regular money. The HEN wiki lists a couple of options along with the standard warning that you should definitely do your own research, because this is a thing that will depend a bit on where you live. I looked into one of their suggested options (Kraken) and it seemed fairly standard, but in the end used an Australian provider, CoinSpot. The sign up process was fairly standard, requiring identification documents for verification. Once that was completed, I was able to send money to my bank account. It ended up being a three-step process:\n\nSend tezos from the Temple wallet associated with my public identity (i.e., the one I’ve been using on HEN etc), to a tezos wallet that is provided for me through my CoinSpot account\nOn CoinSpot, sell the tezos in exchange for Australian dollars\nWithdraw money from my CoinSpot account and deposit it in my Australian bank account\n\nOnce I figured it all out it was surprisingly smooth. I imagine the process varies a lot from country to country and from exchange to exchange, but hopefully the description of my process is at least somewhat helpful."
  },
  {
    "objectID": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#epilogue-is-all-it-worth-it",
    "href": "posts/2021-09-26_setting-up-on-hic-et-nunc/index.html#epilogue-is-all-it-worth-it",
    "title": "How to mint digital art on HEN",
    "section": "Epilogue: Is all it worth it?",
    "text": "Epilogue: Is all it worth it?\nI haven’t been doing this for very long, but I’m a little surprised to find that I’m enjoying the process of minting art on HEN. I’ve sold three pieces to people who know me, and it is a nice feeling. I’m not making mountains of money, and I don’t expect that I will any time soon, but it is still quite satisfying. The fact that I’m doing it on HEN makes a big difference to how I feel about it too: the environmental costs worry me a lot and don’t think I could make myself use a site that relied on Ethereum. And to be honest, it really is nice to get paid for my art. Praise is nice, sure, but you can’t live off that.\nI suppose the other thing I’m noticing already is that I feel a little less constrained on HEN. When I post art to twitter or instagram, it’s always with the knowledge that the big social media websites are also places where my professional networks exist, and I’m obliged to behave, to be nice, to be the “good girl”. I might swear and be grumpy on twitter sometimes, but for the most part I try not to let other parts of my personality and my life show on those sites. That’s okay, and it’s probably how it should be. Twitter is a place where it’s okay to mix some parts of your personal life with some parts of your work life, but there’s a tacit understanding that you probably ought to keep some things carefully sequestered from the bird site. There are a lot of things about a person’s life that their employer and professional colleagues may not want to know.\nWhere that runs into some difficulty, for me at least, is that a lot of my generative art is deeply entwined with my personal life, with my moods, and my experiences. When done well, art is always intimate, and the intimacy of creating and sharing the art often entails personal disclosures that might not be welcome on twitter. Consider these pieces, for example:\n\n\n\n\n\n\n\n\n\n\nOBJKT 341833\n\n\n\n\n\n\n\n\n\nOBJKT 341852\n\n\n\n\n\n\n\n\n\nOBJKT 341868\n\n\n\n\n\n\n\n\n\nOBJKT 341880\n\n\n\n\n\n\n\nI am very fond of these pieces, but they aren’t the easiest ones to share on twitter. The title of the series is Bruises are how sadists kiss, and the pieces are tagged with “sadomasochism” on my HEN profile. The title isn’t deliberately intended to be provocative or anything of the sort. That’s not really my preferred style. It’s much more prosaic: those things are part of my world and part of my life, and sometimes they show up in my art. The emotional experience expressed through the art (via the code) was one in which a very polite sadist had turned up in my life after a long absence. I was reminiscing, trying to work out what he meant to me, and I wrote the code while I was thinking about it. This was the system that emerged.\nOn twitter I would not dream of referring to those parts of my world so overtly (nor would I typically do so on this blog, focused as it is on technical topics). On HEN though, it feels a little more natural: art is often raw, it is often personal, and those subjects do come up if you spend a little time exploring the cryptoart space. It feels like a place where that version of me is permitted to have an online existence. As it turns out, that’s a thing that has some value to me."
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html",
    "href": "posts/2023-07-15_torsten/index.html",
    "title": "Getting started with Torsten",
    "section": "",
    "text": "In recent months I’ve been gradually teaching myself pharmacometrics, and writing blog posts as I go. I started out writing about relatively simple methods for non-compartmental analysis, moved on to talking about compartmental analysis with Stan, and then to population pharmacokinetic models in Stan. Now it feels like time for me to move on to looking at Torsten.\nWhat’s Torsten, you ask?\nTorsten is essentially a forked copy of Stan that has a collection of functions added that can be useful in pharmacometric analyses. As described by Elmokadem et al (2023):\nThe name “Torsten” refers to Torsten Teorell, described as the father of pharmacokinetics. The preview image of this post is taken from the figures in a 1937 paper by Teorell:\nI do like knowing where the names of things come from, and the history to different disciplines. But to be fair that’s not the purpose of this post, so let’s set the history to one side and take a look at the software. The website is quite clear that Torsten is currently (as of version 0.89rc) a prototype:\nMy goals in this post are modest:\nAs usual, the notes are mostly intended for the benefit of future-me, who absolutely will have forgotten all this in a week from now. But it’s possible that other people may find them helpful too I suppose. Anyway, here goes…"
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#installation",
    "href": "posts/2023-07-15_torsten/index.html#installation",
    "title": "Getting started with Torsten",
    "section": "Installation",
    "text": "Installation\nInstalling Torsten starts by cloning the GitHub repository. Just recently I’ve gotten into the habit of using the GitHub command line tool for tasks like this, so the command I used was this:\ncd ~/GitHub\ngh repo clone metrumresearchgroup/Torsten\nBut really, anything that clones a repository will work.\nOnce you have a copy of the repo, you can build Torsten in any number of ways. Given that I’m usually working from R, the most convenient way for me to do this is with the cmdstanr package. If you don’t have the cmdstanr package, you need to install it first. It’s not on CRAN but you can install it by adding the Stan repository to the repos path when calling remotes::install_cran():\n\nremotes::install_cran(\n    pkgs = \"cmdstanr\", \n    repos = c(\n        \"https://mc-stan.org/r-packages/\", \n        getOption(\"repos\")\n    )\n)\n\nNext, you need to make sure that cmdstanr uses the Torsten version of Stan, and not any other version of Stan that you might have installed on your system. This is important because Torsten supplies various functions that we’ll need, and especially important in my case because I also have a “vanilla” copy of Stan installed elsewhere on my laptop. Here’s how I do that:\n\ntorsten_dir &lt;- \"~/GitHub/Torsten\"\ncmdstanr::set_cmdstan_path(fs::path(torsten_dir, \"cmdstan\"))\n\nCmdStan path set to: /home/danielle/GitHub/Torsten/cmdstan\n\n\nIn a moment, I’m going to try to compile a Stan/Torsten model (and indeed compile Torsten itself), so it’s important to make sure the C++ toolchain is set up properly. If you already have C++ compilers set up on your machine (which I do) then you probably don’t need to do anything special to make sure that everything compiles properly, but just to be safe we’ll check:\n\ncmdstanr::check_cmdstan_toolchain()\n\nThe C++ toolchain required for CmdStan is setup properly!\n\n\nExcellent. Now comes the acid test: let’s see if we can use Torsten to build and sample from a Stan model that specifically requires Torsten functions. I’m following the instructions on the installation page here, more or less. But I’m going to walk through the process a little more slowly than those instructions do.\nThe first step here is to compile the Stan model. The very first time you do this, it can take a moderately long time because two things are happening:\n\nFirst, the compiler need to compile the modified copy of Stan that Torsten ships with. That takes a while, but fortunately it only has to be done once.\nSecond, the compiler needs to compile the “pk2cpt” model itself. This is fairly fast. This compilation step only happens when the model binary is out of date.\n\nFortunately for me I’ve already done this step once before and I have both binaries compiled already so it all happens instantaneously. In any case, here’s the code:\n\nmodel_dir &lt;- fs::path(torsten_dir, \"example-models\", \"pk2cpt\")\nmodel_src &lt;- fs::path(model_dir, \"pk2cpt.stan\")\nmodel &lt;- cmdstanr::cmdstan_model(model_src)\n\nThe key thing to note here is that the “pk2cpt.stan” source code relies on Torsten-specific functions that don’t exist in vanilla Stan. It won’t work if you’re not using the Torsten version of Stan. The mere fact that it compiles is itself telling us that it’s all configured correctly.\nSo now we fit the model:\n\nmodel_fit &lt;- model$sample(\n    data = fs::path(model_dir, \"pk2cpt.data.R\"),\n    init = fs::path(model_dir, \"pk2cpt.init.R\"),\n    seed = 123,\n    chains = 4,\n    parallel_chains = 2,\n    refresh = 500,\n    show_messages = FALSE\n)\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 7.5 seconds.\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 finished in 7.5 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 7.1 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 7.7 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 7.5 seconds.\nTotal execution time: 15.4 seconds.\n\n\nWhen doing this interactively, you should set show_messages = TRUE so that you can see the “informational messages”. As I’m coming to learn, it’s grossly typical of ODE models that you get a few warning messages during the early stages of warmup. But Stan messages tend to be quite good, and in this case they’re quite helpful in reassuring us that there’s not a problem in this instance (they occur early while the MCMC sampler is in a very weird part of the space and then disappear). I’ve suppressed them here because they make the quarto blog output messier than it needs to be. All good.\nTo draw a pretty picture showing the posterior distribution of the clearance rate parameter for this model (CL), we can do this:\n\n# remotes::install_cran(\"bayesplot\")\nbayesplot::mcmc_dens_overlay(model_fit$draws(\"CL\"))\n\n\n\n\n\n\n\n\nThat looks right. Torsten is configured correctly, the model compiles, the sampler works, and the posterior distributions plotted here mirror the ones that are secretly tucked away in an output file here:\n[torsten-directory]/example-models/pk2cpt/deliv/figure/density.pdf\nMy first goal is accomplished. We are good to go!"
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#the-data-file",
    "href": "posts/2023-07-15_torsten/index.html#the-data-file",
    "title": "Getting started with Torsten",
    "section": "The data file",
    "text": "The data file\nOkay, so now that I’ve succeeded in doing something with Torsten, it would be nice to have a better sense of what precisely I’ve done. Obviously1 I’ve run a Stan model of some kind on some data set, but the Torsten documentation doesn’t really go into a lot of detail here.\nI’ll start by taking a look at the data. I’ve cached a copy of the data file along with this post, located at ./example/pk2cpt_data.R relative to this quarto document. The file defines the variables needed by Stan as R vectors, but to make my life a little easier I’ll organise them into a tibble that resembles a NONMEM-style event schedule:\n\nsource(\"./example/pk2cpt_data.R\")\npk2cpt_data &lt;- tibble::tibble(\n    cmt = cmt,\n    evid = evid,\n    addl = addl,\n    ss = ss,\n    amt = amt, \n    time = time,\n    rate = rate,\n    ii = ii,\n    dv = NA\n)\npk2cpt_data$dv[iObs] &lt;- cObs\npk2cpt_data\n\n# A tibble: 54 × 9\n     cmt  evid  addl    ss   amt  time  rate    ii    dv\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1    14     0 80000 0         0    12   NA \n 2     2     0     0     0     0 0.083     0     0  359.\n 3     2     0     0     0     0 0.167     0     0  663.\n 4     2     0     0     0     0 0.25      0     0 1106.\n 5     2     0     0     0     0 0.5       0     0 1185.\n 6     2     0     0     0     0 0.75      0     0 1802.\n 7     2     0     0     0     0 1         0     0 2296.\n 8     2     0     0     0     0 1.5       0     0 2008.\n 9     2     0     0     0     0 2         0     0 2001.\n10     2     0     0     0     0 3         0     0 1115.\n# ℹ 44 more rows\n\n\nI am firmly of the opinion that these aren’t good variable names, but they are completely standard in the field so I’m just going to have to memorise them. To that end, and despite the fact that I have written a version of this about a dozen times already, here’s what each of those variables refers to:\n\ncmt: compartment number to which the row refers\nevid: event id (0=observation, 1=dose, 2=other)\naddl: number of additional identical doses given\nss: is it steady-state dosing? (0=false, 1=true)\namt: dose amount administered at this time\ntime: time of observation/administration\nrate: rate of drug infusion (=0 for bolus administration)\nii: interdose interval: time between additional doses\ndv: the dependent variable (observed concentration)\n\nThe key point here is that (unlike in my previous post where I used a slightly different data structure in my bespoke Stan model), Torsten functions expect input variables that look very similar to those used in NONMEM. Fair enough.\nNow that I have a sense of the data structure, let’s plot it to see what the observed pharmacokinetic function looks like. In the plot below, the dotted vertical lines mark the moments at which additional doses were administered. The circular markers connected by solid lines represent the observed drug concentrations:\n\npk2cpt_data |&gt;\n    dplyr::filter(evid == 0) |&gt;\n    ggplot2::ggplot(ggplot2::aes(time, dv)) +\n    ggplot2::geom_vline(\n        xintercept = (0:14) * 12,\n        color = \"grey50\",\n        linetype = \"dotted\"\n    ) +\n    ggplot2::geom_path() + \n    ggplot2::geom_point() +\n    ggplot2::scale_y_continuous(breaks = (0:3) * 1000) +\n    ggplot2::labs(x = \"Time\", y = \"Concentration\") +\n    ggplot2::theme(panel.grid = ggplot2::element_blank())\n\n\n\n\n\n\n\n\nThe solid lines connecting the dots are a bit misleading. For the first, second, and last doses, measurements are taken regularly enough that you can see the rise and fall of drug concentration associated with each dose. For all other doses, however, there’s only a single measurement taken immediately before the dose is administered, with the result that it looks like a fairly flat function through the middle of the data. This makes total sense from an experimental design point of view, of course, it’s just important to remember that there’s a good reason why the observed data has this slightly odd shape.\nObjective #2 accomplished. Time to move to the next one."
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#the-model-file",
    "href": "posts/2023-07-15_torsten/index.html#the-model-file",
    "title": "Getting started with Torsten",
    "section": "The model file",
    "text": "The model file\nLet’s take a look at the source code for the Torsten/Stan model that I fit in the last section. For the sake of my sanity I’m not going to use the actual .stan file that Torsten distributes. In the original version there’s no explanation of what the parameters mean or what the data variables are.\nI’m about 99% certain that the reason for this is that among pharmacometricians it is “understood” that everyone already knows the notational specifications used in NONMEM, and consequently nobody bothers to say what those terms mean. To be honest I find it a little frustrating. If you want new users to consider Torsten as a viable modelling tool for pharmacometrics, I think it’s a bad idea to make it a prerequisite that new users already know NONMEM. But as I am Queen only of this blog and not of statistics generally, and my guess is that the primary target audience for Torsten are pharmacometricians who have already used NONMEM for many years, I’ll restrict myself to mild grumbling and simply fix the comments so that the “hidden curriculum” aspect to all this is no longer quite so hidden. Nevertheless, given that the vast majority of my readership belong to the 99.99% of statisticians and data scientists who aren’t professional pharamacometricians,2 I’ve added a lot more annotation to my version of the file:\n\n\n\n./example/pk2cpt.stan\n\n// Two compartment model using Torsten analytical solver \n\ndata{\n  int&lt;lower = 1&gt; nt;                // number of events\n  int&lt;lower = 1&gt; nObs;              // number of observations\n  array[nObs] int&lt;lower = 1&gt; iObs;  // indices of observation events\n  \n  // NONMEM data\n  array[nt] int&lt;lower = 1&gt; cmt; // compartment number\n  array[nt] int evid;           // event id (0=observation, 1=dose, 2=other)\n  array[nt] int addl;           // number of additional identical doses given\n  array[nt] int ss;             // is it steady-state dosing? (0=false, 1=true)\n  array[nt] real amt;           // dose amount administered at this time\n  array[nt] real time;          // time of observation/administration \n  array[nt] real rate;          // rate of drug infusion (0 for bolus administration)\n  array[nt] real ii;            // interdose interval: time between additional doses \n  \n  vector&lt;lower = 0&gt;[nObs] cObs;  // observed concentration (the dv)\n}\n\ntransformed data{\n  vector[nObs] logCObs = log(cObs);\n  int nTheta = 5;  // number of ODE parameters describing the pharmacokinetic function\n  int nCmt = 3;    // number of compartments in model (1=gut, 2=central, 3=peripheral)\n}\n\nparameters{\n  real&lt;lower = 0&gt; CL;    // clearance rate from central compartment\n  real&lt;lower = 0&gt; Q;     // intercompartmental clearance rate\n  real&lt;lower = 0&gt; V1;    // volume of distribution, central compartment\n  real&lt;lower = 0&gt; V2;    // volume of distribution, peripheral compartment\n  real&lt;lower = 0&gt; ka;    // absorption rate constant from gut to central \n  real&lt;lower = 0&gt; sigma; // standard deviation of measurement error on log-scale\n}\n\ntransformed parameters{\n  array[nTheta] real theta;        // parameters of the pharmacokinetic function\n  matrix&lt;lower = 0&gt;[nCmt, nt] x;   // drug amounts in each compartment over time\n\n  // predicted drug concentrations in the central compartment\n  row_vector&lt;lower = 0&gt;[nt] cHat;  // row vector, one element per event\n  vector&lt;lower = 0&gt;[nObs] cHatObs; // column vector, one element per *observation*\n\n  // bundle pharmacokinetic parameters into a vector\n  theta[1] = CL;\n  theta[2] = Q;\n  theta[3] = V1;\n  theta[4] = V2;\n  theta[5] = ka;\n\n  // compute the pharmacokinetic function (drug amounts in all compartments)\n  x = pmx_solve_twocpt(time, amt, rate, ii, evid, cmt, addl, ss, theta);\n\n  cHat = x[2, :] ./ V1;  // compute drug concentrations in central compartment\n  cHatObs = cHat'[iObs]; // transform to column vector & keep relevant cells only\n}\n\nmodel{\n  // informative prior\n  CL ~ lognormal(log(10), 0.25);\n  Q ~ lognormal(log(15), 0.5);\n  V1 ~ lognormal(log(35), 0.25);\n  V2 ~ lognormal(log(105), 0.5);\n  ka ~ lognormal(log(2.5), 1);\n  sigma ~ cauchy(0, 1);\n\n  // measurement errors are log-normally distributed\n  logCObs ~ normal(log(cHatObs), sigma);\n}\n\ngenerated quantities{\n  array[nObs] real cObsPred; // simulated observations\n  for(i in 1:nObs) {\n    cObsPred[i] = exp(normal_rng(log(cHatObs[i]), sigma));\n  }\n}\n\n\nEven with the additional commenting, it’s still a little impenetrable unless you’re a pharmacometric insider, because the pharmacokinetic model is not represented anywhere in this Stan code. It’s simply understood that this is a two-compartment model by virtue of the fact that the pmx_solve_twocpt() function is called, and all the details of what that means have been rendered invisible in the process.\nThat’s not wrong from the Torsten perspective – and probably necessary – but at the same time it makes the code difficult to follow for anyone who isn’t a pharmacometrician. So let’s make it a bit more explicit, yes? What precisely is the ODE system solved by the pmx_solve_twocpt() function? Fortunately, the actual ODEs are described by Margossian et al (2022) and are in fact the same ones I used in a previous post.\nI’ll reproduce the ODEs here in exactly the same form as they are presented by Margossian et al:3\n\\[\n\\begin{array}{rcl}\n\\displaystyle\\frac{du_{\\mbox{gut}}}{dt} & = & -k_a u_{\\mbox{gut}} \\\\ \\\\\n\\displaystyle\\frac{du_{\\mbox{cent}}}{dt} & = & k_a u_{\\mbox{gut}} - \\left( \\displaystyle\\frac{\\mbox{CL}}{V_{\\mbox{cent}}} + \\displaystyle\\frac{Q}{V_\\mbox{cent}} \\right) u_{\\mbox{cent}} + \\displaystyle\\frac{Q}{V_{\\mbox{peri}}} u_{\\mbox{peri}} \\\\ \\\\\n\\displaystyle\\frac{du_{\\mbox{peri}}}{dt} & = & \\displaystyle\\frac{Q}{V_\\mbox{cent}} u_{\\mbox{cent}} - \\displaystyle\\frac{Q}{V_{\\mbox{peri}}} u_{\\mbox{peri}}\n\\end{array}\n\\]\nThere’s still a little friction here because mathematical notation is never precisely identical to variable naming in code (nor should it be). But it does help to have a little lookup table like this one:\n\n\n\n\n\n\n\n\nStan variable\nMathematical notatation\nDescription\n\n\n\n\nx[1, :]\n\\(u_{\\mbox{gut}}\\)\nDrug amount in the gut\n\n\nx[2, :]\n\\(u_{\\mbox{cent}}\\)\nDrug amount in central compartment\n\n\nx[3, :]\n\\(u_{\\mbox{peri}}\\)\nDrug amount in peripheral compartment\n\n\nka\n\\(k_a\\)\nAbsorption rate constant from gut\n\n\nCL\n\\(\\mbox{CL}\\)\nElimination clearance from central\n\n\nQ\n\\(Q\\)\nIntercompartmental clearance\n\n\nV1\n\\(V_\\mbox{cent}\\)\nVolume of central compartment\n\n\nV2\n\\(V_\\mbox{peri}\\)\nVolume of peripheral compartment\n\n\n\nThe differential equations are all expressed in terms of drug amounts rather than drug concentrations, and the pmx_solve_twocpt() function solves for drug amounts in each compartment at each point in time. However, pharmacometric functions specify how drug concentrations change over time, so the Stan code makes the appropriate transformation. With that in mind I find it helpful to extend the table slightly:\n\n\n\n\n\n\n\n\nStan transformation\nMathematical notatation\nDescription\n\n\n\n\nx[2, :] ./ V1\n\\(u_{\\mbox{cent}} / V_\\mbox{cent}\\)\nDrug concentration in central compartment\n\n\nx[3, :] ./ V2\n\\(u_{\\mbox{peri}} / V_\\mbox{peri}\\)\nDrug concentration in peripheral compartment\n\n\n\nHaving written that out, the Stan code seems pretty interpretable. There’s still something missing here insofar as it’s not entirely clear how the pmx_solve_twocpt() function computes the drug amounts in each compartment at all time points, but at least it’s now clear what it is computing."
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#where-can-i-find-the-analytical-solutions",
    "href": "posts/2023-07-15_torsten/index.html#where-can-i-find-the-analytical-solutions",
    "title": "Getting started with Torsten",
    "section": "Where can I find the analytical solutions?",
    "text": "Where can I find the analytical solutions?\nIn my previous post on pop-PK modelling, I managed to get far enough along that I could implement my own two-compartment models in Stan, without using Torsten. However, in order to do that I had to rely on numerical ODE solvers to compute solutions to the pharmacokinetic functions. It turns out that was unnecessary. On the Torsten home page, it indicates that:\n\nOne and two compartment models are based on analytical solutions of governing ODEs.\n\nSo presumably there are some analytical solutions somewhere! A little awkwardly, the documentation doesn’t explicitly say what the analytical solutions for the two-compartment model are or where they are taken from, but a little bit of digging gives us some answers. First, looking through the Torsten source reveals the relevant parts of the code:\n\nThis file appears to be where the torsten::pmx_solve_twocpt() function is defined.\nThis file appears to be where the analytical solutions are specified.\n\nSecond, a little hunting around on the internet unearths this handy little paper by D’Argenio and Bae (2019) that derives the analytical solutions of interest. Skimming the paper quickly suggests it’s not too complicated an exercise to implement analytical solutions (especially if you’re not trying to optimise for performance), and even the derivations don’t look too painful. I may return to that topic in a later post, but for now I feel reassured that I know where to look when I want to dive deeper."
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#useful-resources",
    "href": "posts/2023-07-15_torsten/index.html#useful-resources",
    "title": "Getting started with Torsten",
    "section": "Useful resources",
    "text": "Useful resources\nI suspect that at a future date I’ll want to pick up from where this post leaves off. With that in mind, these are the resources I relied on when putting it together:\n\nBayesian PBPK modeling using R/Stan/Torsten and Julia/SciML/Turing.Jl. Journal article by Ahmed Elmokadem, Yi Zhang, Timothy Knab, Eric Jordie, and Bill Gillespie, January 2023.\nFlexible and efficient Bayesian pharmacometrics modeling using Stan and Torsten, Part I. Journal article by Charles Margossian, Yi Zhang, and Bill Gillespie, April 2022.\nBayesian modeling workflow for pharmacometric applications using bbr.bayes with Stan/Torsten. Slides by Bill Gillespie, June 2023.4\nBayesian Data Analysis Using Stan/Torsten for Pharmacometric Applications. Slides by Bill Gillespie, May 2018.\nAnalytical solution of linear multi-compartment models with non-zero initial condition and its implementation with R. Journal article by David D’Argenio and Kyun-Seop Bae, June 2019.\nKinetics of distribution of substances administered to the body. I. The extravascular modes of administration. Journal article by Torsten Teorell, 1937. Discussed in Torsten Teorell, the Father of Pharmacokinetics by Lennart Paalzow, 1995.\nStan and R for Pharmacometrics. Book by Casey Davis, Yasong Lu, Arya Pourzanjani, and Pavan Vaddady.5"
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#done",
    "href": "posts/2023-07-15_torsten/index.html#done",
    "title": "Getting started with Torsten",
    "section": "Done!",
    "text": "Done!\n…and with that, I’ve accomplished my very limited goals for this post, and for once in my life I shall bloody well refrain from expanding on it further and turning a short blog post into a monograph."
  },
  {
    "objectID": "posts/2023-07-15_torsten/index.html#footnotes",
    "href": "posts/2023-07-15_torsten/index.html#footnotes",
    "title": "Getting started with Torsten",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI mean… “obvious” in the sense that I’m a person who has used Stan before and the output from the previous section is very familiar to me as Stan output.↩︎\nNumber obviously fictitious but probably in the right ballpark.↩︎\nThe same ODE system appears in the Torsten documentation for the two-compartment model, using \\(y\\) in place of \\(u\\) and \\(y^\\prime\\) in place of \\(du/dt\\). I probably should have used that version in this post, but I found the Margossian et al version first and I’m too lazy to rewrite the LaTeX expressions.↩︎\nThank you to Mike Smith for pointing me to this resource.↩︎\nThank you to Tim Waterhouse for pointing me to this resource.↩︎"
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html",
    "href": "posts/2023-12-30_knitr-hooks/index.html",
    "title": "Writing knitr hooks",
    "section": "",
    "text": "A very common situation I encounter when writing a blog post or writing a book chapter using R markdown or quarto arises when the command I want to use generates a lot of output, and I don’t want all of the output to be displayed in the output. Every time I run into this problem, I have this vague recollection that “oh yeah, I need to write a knit hook for this”, but I can never quite remember how to do that and have to search online for the answer. In my last post I wrote a jokey footnote grumbling about this and saying I was thinking of writing a short blog post on it just so that I’d know where to look next time.\nSo, uh, yeah… that’s exactly what I did.\nThe answer to that specific question, incidentally, is described explicitly in the R Markdown Cookbook, and – to set expectations appropriately – there’s nothing in this post that isn’t already covered in the documentation and books. I don’t actually need to write a blog post about this. But I’m going to anyway, because every time I actually do need to write a knit hook, I find myself realising that I don’t understand them as well as I ought to. So here goes."
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html#chunk-options",
    "href": "posts/2023-12-30_knitr-hooks/index.html#chunk-options",
    "title": "Writing knitr hooks",
    "section": "Chunk options",
    "text": "Chunk options\nThis is a post about knit hooks, but it helps to start with a refresher on knitr chunk options. I’m assuming, for the purposes of this post, that anyone reading is already pretty familiar with R markdown and quarto, and knows that when I write a document like this and want to execute some R code, I include an appropriately annotated code chunk in the source like so:\n\n```{r}\n1 + 1\n```\n\n[1] 2\n\n\nWhen the document is rendered to HTML, it’s the job of the knitr package to parse this chunk, execute the code, and append the output to the document as necessary. You can customise the manner in which knitr does this via chunk options, but the code chunk above doesn’t specify any options, so default values are used.\nSo what are the defaults, and where are they stored?\nThe knitr::opts_chunk object is used to control the options for code chunks. The object is a list of several functions. The two we use most often are $get() and $set().\n\noptions &lt;- knitr::opts_chunk$get()\n\nThis options variable is a list containing all the default values that are applied when knitting the code chunks in the markdown document. If the user doesn’t specify a value for a specific chunk option, these are the default values that are applied. There’s a lot of these options:\n\nnames(options)\n\n [1] \"eval\"          \"echo\"          \"results\"       \"tidy\"         \n [5] \"tidy.opts\"     \"collapse\"      \"prompt\"        \"comment\"      \n [9] \"highlight\"     \"size\"          \"background\"    \"strip.white\"  \n[13] \"cache\"         \"cache.path\"    \"cache.vars\"    \"cache.lazy\"   \n[17] \"dependson\"     \"autodep\"       \"cache.rebuild\" \"fig.keep\"     \n[21] \"fig.show\"      \"fig.align\"     \"fig.path\"      \"dev\"          \n[25] \"dev.args\"      \"dpi\"           \"fig.ext\"       \"fig.width\"    \n[29] \"fig.height\"    \"fig.env\"       \"fig.cap\"       \"fig.scap\"     \n[33] \"fig.lp\"        \"fig.subcap\"    \"fig.pos\"       \"out.width\"    \n[37] \"out.height\"    \"out.extra\"     \"fig.retina\"    \"external\"     \n[41] \"sanitize\"      \"interval\"      \"aniopts\"       \"warning\"      \n[45] \"error\"         \"message\"       \"render\"        \"ref.label\"    \n[49] \"child\"         \"engine\"        \"split\"         \"include\"      \n[53] \"purl\"          \"fenced.echo\"   \"ft.shadow\"    \n\n\nThe fig.path option, for example, is used to specify where generated output images should be written. It’s a nice one to illustrate the customisability of knitr because you get a different output depending on context. The blog post is a quarto document, and has different knitr defaults to what you’d see if the same code were run at the console:\n\noptions$fig.path\n\n[1] \"index_files/figure-html/\"\n\n\nIf I’d run the same output at the console, I would get a different answer. When called from the console the default option for fig.path is \"figures\". For example, when I constructed my “knitr + eleventy” blog this is how I was calling knitr, and accordingly the images were written to a “figures” folder. The defaults, when knitr is used in the context of this on this quarto blog, are different."
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html#knit-hooks",
    "href": "posts/2023-12-30_knitr-hooks/index.html#knit-hooks",
    "title": "Writing knitr hooks",
    "section": "Knit hooks",
    "text": "Knit hooks\nSo now we turn to knitr hooks. Hooks are user-customisable functions that you can use to control how the knitr options are interpreted, and modify the output that knitr creates. In the same fashion that the knitr::opts_chunk object is used to control the chunk options, there’s a knitr::knit_hooks object used to control hooks. Again, this object is a list of functions, and the two we use most often are $get() and $set().1 We can retrieve the hooks by calling the $get() function:\n\nhooks &lt;- knitr::knit_hooks$get()\n\nThere are 12 default knit hooks in this list:\n\nnames(hooks)\n\n [1] \"source\"          \"output\"          \"warning\"         \"message\"        \n [5] \"error\"           \"plot\"            \"inline\"          \"chunk\"          \n [9] \"text\"            \"evaluate.inline\" \"evaluate\"        \"document\"       \n\n\nThe documentation for output hooks gives a nice summary for most of these. Seven of the hooks are quite specific, and are applied to only one type of output:\n\nsource: Handles how knitr processes the source code inside a chunk\noutput: Handles how knitr processes ordinary R output (i.e., not warnings, messages, or errors)\nwarning: Handles how knitr processes warning output (e.g., from warning())\nmessage: Handles how knitr processes message output (e.g., from message())\nerror: Handles how knitr processes error output (e.g., from stop())\nplot: Handles how knitr processes graphics output\ninline: Handles how knitr processes output from inline R code\n\nThere are two output hooks that are broader in scope:\n\nchunk: Applied to all output from a code chunk\ndocument: Applied to all output within the document\n\nThe other three (evaluate, evaluate.inline, and text) aren’t discussed as much, and while I did get a little curious and started going down a rabbit hole looking at them, for once in my life I’ll be smart and not get sucked all the way in."
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html#custom-output-hooks",
    "href": "posts/2023-12-30_knitr-hooks/index.html#custom-output-hooks",
    "title": "Writing knitr hooks",
    "section": "Custom output hooks",
    "text": "Custom output hooks\nThe general advice when writing custom output hooks is that you shouldn’t try to write the whole thing yourself. By design, knitr will create default hooks that are appropriate to the specific context, and your safest bet is to first retrieve the default hook by calling the $get() function, like this:\n\ndefault_hook_output &lt;- knitr::knit_hooks$get(\"output\")\n\nThen you can write your own hook that does some pre-processing to the inputs, before passing the modified inputs to the default hook. So, having already saved the default hook as default_hook_output I’d write my custom output hook like this:\n\ncustom_hook_output &lt;- function(x, options) {\n  n &lt;- options$out.lines\n  if(!is.null(n)) {\n    x &lt;- xfun::split_lines(x)\n    if (length(x) &gt; n) x &lt;- c(head(x, n), \"....\\n\")\n    x &lt;- paste(x, collapse = \"\\n\")\n  }\n  default_hook_output(x, options)\n}\n\nThere’s a few things going on here that are worth highlighting. First, notice that output hooks take two arguments x and options. The x argument is the raw text string that needs to be rendered: in this case, the string would correspond to the output that would normally be printed to the R console. The options argument is the list of knitr chunk options. The value of options that gets passed to the hook includes any values that were specified by the user in the chunk options, and also any default values that were not specified by the user. In this instance, out.lines is intended to indicate the maximum number of lines of R output to write to the rendered output document. It’s not one of the default chunk options (i.e., it wasn’t one of the options we saw in the previous section), and so if the user doesn’t specify a value for out.lines in the chunk options, options$out.lines will return a value of NULL in our custom hook, and so our custom_hook_output() will skip all the pre-processing in that case. However, if the user does specify a value for out.lines, it does a little text manipulation to alter the value of x before it is passed onto the default output hook.\nHaving written our custom hook, we apply it by using the $set() function:\n\nknitr::knit_hooks$set(output = custom_hook_output)\n\nNow that we have a knit hook that knows how to interpret out.lines as a chunk option, I can incorporate it into a knitr code chunk just like any other one:\n\n```{r, out.lines = 4}\nrunif(200)\n```\n\n  [1] 0.26550866 0.37212390 0.57285336 0.90820779 0.20168193 0.89838968\n  [7] 0.94467527 0.66079779 0.62911404 0.06178627 0.20597457 0.17655675\n [13] 0.68702285 0.38410372 0.76984142 0.49769924 0.71761851 0.99190609\n [19] 0.38003518 0.77744522 0.93470523 0.21214252 0.65167377 0.12555510\n....\n\n\nThe output here would normally be considerably longer than 4 lines, but we’ve applied a custom hook that enforces the truncation, so we get nicer output. Notice also that, in the same way that standard chunk options like fig.width and fig.height become fig-width and fig-height when you’re setting them via custom code comments, our new out.lines option becomes out-lines when used in that context:\n\n```{r}\n#| out-lines: 4\nrunif(200)\n```\n\n  [1] 0.26750821 0.21864528 0.51679684 0.26895059 0.18116833 0.51857614\n  [7] 0.56278294 0.12915685 0.25636760 0.71793528 0.96140994 0.10014085\n [13] 0.76322269 0.94796635 0.81863469 0.30829233 0.64957946 0.95335545\n [19] 0.95373265 0.33997920 0.26247411 0.16545393 0.32216806 0.51012521\n...."
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html#custom-chunk-hooks",
    "href": "posts/2023-12-30_knitr-hooks/index.html#custom-chunk-hooks",
    "title": "Writing knitr hooks",
    "section": "Custom chunk hooks",
    "text": "Custom chunk hooks\nIn the previous section, we effectively created a new chunk option called out.lines simply by virtue of modifying one of the standard output hooks that is able to interpret it and modify the output accordingly. That approach doesn’t always work, particularly if the new option that you want to create requires that code be executed before and after knitr processes the chunk. In those situations we may need to write a “chunk hook” that is triggered whenever the new chunk option has a non-null value. Chunk hooks have a different structure than output hooks. The R Markdown Cookbook has some nice examples of this, including one for timing how long it takes the chunk to execute. I’ll adapt that one here.\nTo understand how to write a chunk hook, the key thing to realise is that it gets called twice: once before knitr executes the code in the chunk, and once again afterwards. The function can take up to four arguments, all of which are optional:\n\nbefore is a logical value indicating whether the function is being called before or after the code chunk is executed\noptions is the list of chunk options\nenvir is the environment in which the code chunk is executed\nname is the name of the code chunk option that triggered the hook function\n\nAs a general rule, the chunk hook is called for its side effects not the return value. However, if it returns a character output, knitr will add that output to the document output as-is.\nDesigning a chunk hook that records the amount of time taken to execute takes a little thought. When the hook is triggered the first time (with before = TRUE) we want to record the system time somewhere (e.g., in a variable called start_time). Then, when the hook is triggered the second time (with before = FALSE) we want to record the system time again (e.g., as stop_time), and compute the difference in time. We can do this using a function factory to create stateful functions. Here’s what that looks like:\n\ncreate_timer_hook &lt;- function() {\n  start_time &lt;- NULL\n  function(before, options) {\n    if (before) {\n      start_time &lt;&lt;- Sys.time()\n    } else {\n      stop_time &lt;- Sys.time()\n      elapsed &lt;- difftime(stop_time, start_time, units = \"secs\")\n      paste(\n        \"&lt;div style='font-size: 70%; text-align: right'&gt;\",\n        \"Elapsed time:\", \n        round(elapsed, 2), \n        \"secs\",\n        \"&lt;/div&gt;\"\n      )\n    }\n  }\n}\n\nWhen create_timer_hook() is called it returns a function that will become our custom hook. Or – to be more precise, because in this instance the distinction matters – it returns a closure. When called with before == TRUE, it records the system time and uses the super assignment operator &lt;&lt;- to store that value as start_time. Normally, an assignment that takes place during the function execution isn’t persistent and can’t be reused on later calls to that function. But we’ve structured things differently here: in this case, the start_time variable is defined in the enclosing environment (the one in which the function was defined) rather than the execution environment (in which the function body code executes). That changes things: the execution environment is inherently ephemeral and lasts as long as a single function call is in progress. The enclosing environment, however, is persistent, and will survive for (at least) as long as the function itself exists. As a consequence, the value assigned to start_time is persistent also, and still exists when the hook is triggered a second time with before == FALSE. That makes it possible to compute the difference between start_time and stop_time with difftime().\nHaving computed the elapsed time, all that remains is to format it a little bit and then return a nice character string with some HTML that will be printed in the final document. To put this into action, we set the custom hook like this:\n\nknitr::knit_hooks$set(timer = create_timer_hook())\n\nBy doing this timer become the code chunk option that triggers the hook, and we can now use it in the document:\n\n```{r}\n#| timer: true\n#| out-lines: 4\nrunif(10000)\n```\n\n    [1] 0.6588776091 0.1850699645 0.9543781369 0.8978484920 0.9436970544\n    [6] 0.7236907512 0.3703570659 0.7810175403 0.0111495086 0.9403087122\n   [11] 0.9937492262 0.3574057452 0.7476350635 0.7929090238 0.7058590064\n   [16] 0.4758250387 0.4946545260 0.3080524488 0.6950122463 0.8227933056\n....\n\n\nElapsed time: 0.03 secs\n\n\nAnd with that, we are done!\nYes, there are other kinds of hooks that you can write for knitr,2 but the only two kinds of hooks I’ve ever actually had the need for myself are output hooks and chunk hooks, so in the interests of brevity I’ll leave it at that."
  },
  {
    "objectID": "posts/2023-12-30_knitr-hooks/index.html#footnotes",
    "href": "posts/2023-12-30_knitr-hooks/index.html#footnotes",
    "title": "Writing knitr hooks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn addition to knit_hooks and opt_chunks, knitr has several other objects that can be used to control the behaviour of the package. These are knit_patterns, knit_patterns, opts_current, and opts_knit. They all have the same basic structure, including $get() and $set() functions. These objects are documented here.↩︎\nThere are also option hooks that you can use to modify the value of some options based on the values of other options, and those are managed by opts_hooks in the same way that knit_hooks manages output hooks and chunk hooks.↩︎"
  },
  {
    "objectID": "posts/2024-11-24_bezier-curves/index.html",
    "href": "posts/2024-11-24_bezier-curves/index.html",
    "title": "Bézier curve",
    "section": "",
    "text": "It has annoyed me for many years that I have used Bézier curves without really understanding them. Consider, for example, the flametree package I wrote when first learning how to make generative art.\ntree &lt;- flametree::flametree_grow(\n  seed = 100L,\n  time = 7L,\n  split = 3L,\n  trees = 1L\n)\n\nflametree::flametree_plot(\n  data = tree, \n  background = \"white\", \n  palette = \"black\",\n  style = \"nativeflora\" \n)\nSetting aside the details of the system, you can see that the branching shapes here are constructed from many curved lines connected to one another. Each segment is a distinct Bézier curve. If I’d not used Bézier curves and instead constructed the art using straight lines, these pieces would lose any sense of flowing growth.\nOr consider these images, all of which are outputs from something I hastily cobbled together this morning in a desperate effort to wake my brain from its weekend stupor:\nThe influence of Bézier curves is more subtle in these pieces, because each image is comprised of many distinct “ribbon” shapes that are described by rules I talked about in the S7 post, and their individual character isn’t at all related to Bézier curves. But notice how, in all three pieces, there’s tendency for all the ribbons to “bend” in the same (or similar) direction? This happens because under the hood I’ve replaced the straight lines that were used in the original system with Bézier curves.\nAnd let’s not even get started on all the point-and-click GUI interfaces out there that let you create Bézier curves without writing a single line of code. Bézier curves are everywhere.\nNevertheless, until today I’d never thought to look into the mathematics. For some reason – possibly because my idiot brain sometimes confuses Bézier functions with Bessel functions – I had this weird idea in my head that Bézier curves were complicated.\nThey are not."
  },
  {
    "objectID": "posts/2024-11-24_bezier-curves/index.html#definition",
    "href": "posts/2024-11-24_bezier-curves/index.html#definition",
    "title": "Bézier curve",
    "section": "Definition",
    "text": "Definition\nReduced to the bare essentials a Bézier curve is nothing special: it’s just a polynomial function. More precisely, it’s a special case of the Bernstein polynomials\n\\[\nB_n(t) = \\sum_{v = 0}^n \\beta_v \\ b_{v,n}(t)\n\\]\nwhere \\(\\beta_v\\) describes the weight assigned to, \\(b_{v,n}(t)\\), which denotes a Bernstein basis polynomial:\n\\[\nb_{v,n}(t) = \\frac{n!}{(n - v)!v!} \\ t^v (1-t)^{n-v}\n\\] Looking at this expression, every data-minded person will immediately recognise that \\(n!/((n - v)!v!)\\) is the binomial coefficient, and indeed that for \\(t \\in [0, 1]\\), a Bernstein basis polynomial is no different mathematically to the expression for the binomial probability of observing \\(v\\) events out of \\(n\\) trials when the probability of a single event is \\(t\\).\nNone of this is very interesting.\nBut suppose we have a collection of \\(n + 1\\) control points \\(P_0, \\ldots, P_n\\) that lie in a coordinate space. In a Bézier curve, these control points are analogous the \\(\\beta_v\\) coefficients in a Bernstein polynomial, and (by definition) we restrict the domain of the function to the unit interval \\(t \\in [0, 1]\\)\n\\[\nB_n(t) = \\sum_{v = 0}^n P_v \\frac{n!}{(n - v)!v!} \\ t^v (1-t)^{n-v}\n\\] and we have the general form of the equation for a Bézier curve defined by these control points. A Bézier curve is a Bernstein polynomial, and as such you can think of a Bézier curve as being a particular kind of weighted average of the control points, where the weight assigned to the \\(v\\)-th of the \\(n\\) control points is identical to the binomial probability of \\(v\\) successes from \\(n\\) trials, if the probability of a single success is \\(t\\), where \\(t\\) refers to the proportion distance along the curve that you have travelled. Admittedly, this “explanation” doesn’t help very much in itself, but it did motivate me to construct a little gif that shows how the “weights” assigned to each of the control points change as you sweep through the values of \\(t\\):\n\nIn this plot:\n\nThe line is the Bézier curve\nThe coloured dots are the control points\nThe black dot is the value of the Bézier curve \\(B_n(t)\\) at the current value of \\(t\\)\nThe colour of the Bézier curve at each point represents the value of \\(t\\)\nThe colour of the control points is proportional to its numerical index (i.e., the colour of the first control point, where \\(v = 0\\), is mapped to the same colour as \\(t = 0\\); and similarly, the colour of the final control point where \\(v = n\\) is the same as the colour of the Bézier curve when \\(t = 1\\))\nThe label denotes the actual index of the control points (i.e., \\(v + 1\\))\nThe area of the control point dots is proportional to the weight \\(b_{n,v}(t)\\) assigned to the that control point: i.e., it represents the “pull” that this control point has when determining the location of the black dot\n\nThis is still not very interesting.\nEven so, making this connection also helped me realise (at long last) that the order of the control points matters a lot to the shape of the curve. The curve will always pass through the first and last of the control points, but it won’t necessarily go through any of the intermediate points. Not only that, if you change the order in which the intermediate control points are specified, you change the curve, even if the locations of those control points don’t change. Here’s what happens to the previous example if I switch the order of two intermediate control points (i.e., notice the colours of those two points have swapped):\n\n\n\n\nSigh."
  },
  {
    "objectID": "posts/2024-11-24_bezier-curves/index.html#implementation",
    "href": "posts/2024-11-24_bezier-curves/index.html#implementation",
    "title": "Bézier curve",
    "section": "Implementation",
    "text": "Implementation\nFrom a practical perspective there is no need to roll our own implementation of Bézier functions in R, because this has already been done many times before. Nevertheless – since the whole point of this post is to unpack how Bézier functions work – I’ll do so here for illustrative purposes. To that end, I’ll start by implementing my own bernstein() function:\n\nbernstein &lt;- function(beta, t = seq(0, 1, .01)) {\n  n &lt;- length(beta) - 1\n  w &lt;- choose(n, 0:n)\n  b &lt;- rep(0, length(t))\n  for(v in 0:n) {\n    b = b + beta[v + 1] * w[v + 1] * t^v * (1 - t)^(n-v)\n  }\n  b\n}\n\nArmed with a function that computes Bernstein polynomials, it’s very straightforward to construct Bézier curves and add them to plots. First, let’s load some packages:\n\nlibrary(ggplot2)\nlibrary(tibble)\n\nNext, I’ll define a set of control points as a tibble that stores the x and y coordinates of the control points as separate columns:\n\ncontrol &lt;- tibble(\n  x = c(1, 5, 8),\n  y = c(1, 1, 6)\n)\n\nI can now define a bezier curve by computing two bernstein() polynomials, one associated with the x-coordinates and the other associated with the y-coordinates:\n\nbezier &lt;- tibble(\n  t = seq(0, 1, .01),\n  x = bernstein(control$x, t),\n  y = bernstein(control$y, t)\n)\n\nBetter yet, I can plot them:\n\nggplot() + \n  aes(x, y) +\n  geom_path(data = bezier) + \n  geom_point(data = control, color = \"red\") + \n  coord_equal(xlim = c(0, 10), ylim = c(0, 10)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nIn this example there are only three control points, and if I’m honest I’ll admit that when I use Bézier curves in art I’ve only ever used curves with three control points, but there’s no principled reason for that restriction. Indeed, adding more control points can change the shape of the curve in interesting ways:\n\ncontrol &lt;- tibble(\n  x = c(1, 5, 6, 7, 8),\n  y = c(1, 1, 9, 8, 6)\n)\nbezier &lt;- tibble(\n  t = seq(0, 1, .01),\n  x = bernstein(control$x, t),\n  y = bernstein(control$y, t)\n)\nggplot() + \n  aes(x, y) +\n  geom_path(data = bezier) + \n  geom_point(data = control, color = \"red\") + \n  coord_equal(xlim = c(0, 10), ylim = c(0, 10)) + \n  theme_bw()\n\n\n\n\n\n\n\n\nNo, sorry, I lied. Still not interesting."
  },
  {
    "objectID": "posts/2024-11-24_bezier-curves/index.html#s7-bezier-classes-why-not",
    "href": "posts/2024-11-24_bezier-curves/index.html#s7-bezier-classes-why-not",
    "title": "Bézier curve",
    "section": "S7 bezier classes, why not",
    "text": "S7 bezier classes, why not\nIn order to play around with this a little further, and given that I mentioned the S7 post earlier, I’ll practice my S7 skills a little by defining a new “bezier” class:\n\nlibrary(S7)\n\nbezier &lt;- new_class(\n  name = \"bezier\",\n  parent = S7_object,\n  properties = list(\n    x = class_numeric,\n    y = class_numeric,\n    n = new_property(class = class_numeric, default = 100L),\n    curve = new_property(\n      class = class_data.frame,\n      getter = function(self) {\n        t &lt;- seq(0, 1, length.out = self@n)\n        data.frame(\n          x = bernstein(self@x, t),\n          y = bernstein(self@y, t)\n        )\n      }\n    )\n  ),\n  validator = function(self) {\n    if (length(self@x) != length(self@y)) return(\"x and y must have same length\")\n    if (length(self@x) &lt; 2) return(\"at least two control points are required\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (self@n &lt;= 0) return(\"n must be a non-negative number\")\n  })\n\nI call bezier() by passing the x and y coordinates of the control points, like so:\n\nb &lt;- bezier(\n  x = c(1, 5, 6, 7, 8),\n  y = c(1, 1, 9, 8, 6)\n)\n\nThe result is a data structure that stores points defining the Bézier curve as an internal data frame b@curve, and also keeps the control points as b@x and b@y. This is probably not a great class design, but whatever. In any case, here’s the object:\n\nb\n\n&lt;bezier&gt;\n @ x    : num [1:5] 1 5 6 7 8\n @ y    : num [1:5] 1 1 9 8 6\n @ n    : int 100\n @ curve:'data.frame':  100 obs. of  2 variables:\n .. $ x: num  1 1.16 1.32 1.47 1.62 ...\n .. $ y: num  1 1 1.02 1.04 1.07 ...\n\n\nThis data structure is not very interesting.\nWe can make this a little less tedious by noting that every S7 class is also an S3 class, which allows me to define an S3 plot method:\n\nplot.bezier &lt;- function(x, show_control = TRUE, ...) {\n  p &lt;- ggplot() + \n    aes(x, y) +\n    geom_path(data = x@curve) + \n    coord_equal() + \n    theme_bw()\n  if (show_control) {\n    p &lt;- p + geom_point(\n      data = data.frame(x = x@x, y = x@y), \n      color = \"red\"\n    ) \n  }\n  p\n}\n\nplot(b)\n\n\n\n\n\n\n\n\nI guess that’s nice? Sort of?\nAnyway, if we really wanted to we could push this a little further. For examples, we could define a “bezier_noise” class where objects are comprised of many Bézier curves, in which random noise is injected into some of the control points. I’ll spare you the terrible, horrible, no good, very bad code that I used to define this class. Instead I’ll just plot a whole bunch of Bézier curves atop one another:\n\n\n\n\n\n(The terrible, horrible, no good, very bad code)\nbezier_noise &lt;- new_class(\n  name = \"bezier_noise\",\n  parent = S7_object,\n  properties = list(\n    x = class_numeric,\n    y = class_numeric,\n    k = new_property(class = class_numeric, default = 100L),\n    n = new_property(class = class_numeric, default = 100L),\n    noise = new_property(class = class_numeric, default = 1),\n    seed = new_property(class = class_numeric, default = 1L),\n    bezier = new_property(\n      class = class_list, \n      getter = function(self) {\n        l &lt;- list()\n        np &lt;- length(self@x)\n        s &lt;- seq(0, 2, length.out = np)\n        s &lt;- s * (2 - s) \n        withr::with_seed(\n          self@seed, \n          {\n            for(i in 1:self@k) {\n              l[[i]] &lt;- bezier(\n                x = self@x + rnorm(np, sd = self@noise) * s^2,\n                y = self@y + rnorm(np, sd = self@noise) * s^2,\n                n = self@n\n              )\n            }\n          }\n        )\n        l\n      }\n    )\n  ),\n  validator = function(self) {\n    if (length(self@x) != length(self@y)) return(\"x and y must have same length\")\n    if (length(self@x) &lt; 2) return(\"at least two control points are required\")\n    if (length(self@n) != 1) return(\"n must be length 1\")\n    if (self@n &lt;= 0) return(\"n must be a non-negative number\")\n    if (length(self@k) != 1) return(\"k must be length 1\")\n    if (self@k &lt;= 1) return(\"k must be a positive number\")\n    if (length(self@noise) != 1) return(\"noise must be length 1\")\n    if (self@noise &lt;= 0) return(\"noise must be a non-negative number\")\n    if (length(self@seed) != 1) return(\"seed must be length 1\")\n    if (self@seed &lt;= 0) return(\"seed must be a non-negative number\")\n  })\n\nplot.bezier_noise &lt;- function(x, palette = \"bilbao\", dots = TRUE, ...) {\n  p &lt;- ggplot() + aes(x, y) + coord_equal() + theme_void()\n  pal &lt;- scico::scico(x@k, palette = palette)\n  pal &lt;- sample(pal)\n  if(dots) for(l in 1:x@k) p &lt;- p + \n      geom_point(\n        data = data.frame(\n          x = x@bezier[[l]]@x,\n          y = x@bezier[[l]]@y\n        ),\n        color = \"grey80\",\n        size = 1,\n        shape = 19\n      )\n  for(l in 1:x@k) p &lt;- p + \n      geom_path(\n        data = x@bezier[[l]]@curve,\n        color = pal[l],\n        lineend = \"round\"\n      )\n  p\n}\n\nplot(\n  bezier_noise(\n    x = c(0, 10, 15, 20, 25, 30, 35),\n    y = c(0,  0,  5, 15,  5,  5,  5),\n    k = 200L, \n    noise = 1.5 \n  ), \n  palette = \"lajolla\",\n  dots = TRUE\n)\n\n\n\n\n\n\n\n\n\nOkay? But why, Danielle? Why?"
  },
  {
    "objectID": "posts/2024-11-24_bezier-curves/index.html#what-sins-indeed",
    "href": "posts/2024-11-24_bezier-curves/index.html#what-sins-indeed",
    "title": "Bézier curve",
    "section": "What sins indeed",
    "text": "What sins indeed\nNone of this may seem like a great life accomplishment. But this is only because you do not realise how close I am to writing an R package that generates worm on a string memes in ggplot2.\n\n\n(worm in a plot)\nset.seed(1L)\nplot(\n  bezier_noise(\n    x = sort(rnorm(10L)),\n    y = rnorm(10L),\n    k = 200L, \n    noise = .1 \n  ),\n  palette = \"oslo\",\n  dots = FALSE\n) + \n  annotate(\n    \"point\", \n    x = c(.8, .85), \n    y = c(.7, .8), \n    size = 6, \n    shape = 19, \n    color = \"white\"\n  ) +\n  annotate(\n    \"point\", \n    x = c(.8, .85), \n    y = c(.7, .8), \n    size = 6, \n    shape = 1, \n    color = \"black\"\n  ) +\n  annotate(\n    \"point\", \n    x = c(.81, .86), \n    y = c(.7, .8), \n    size = 2, \n    shape = 19, \n    color = \"black\"\n  ) +\n  annotate(\n    \"label\",\n    x = .5,\n    y = .3,\n    size = 8,\n    label = \"🪱 i will subsett  closurs\"\n  ) +\n  annotate(\n    \"text\",\n    x = .2,\n    y = .9,\n    angle = 70,\n    size = 10,\n    color = \"pink\",\n    label = \"on purpse\"\n  )"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "",
    "text": "It’s been a couple of months since I published anything on this blog. In my defence, I’ve been busy: I spent the month of June developing a workshop and website on larger than memory workflows in R with Apache Arrow for the useR! conference, and I spent July doing the same thing for my art from code workshop at rstudio::conf. But I am back to blogging now and I’m going to ease myself into it with a post that mixes some ideas from both of those workshops: how to use Arrow to assist in visualising large data sets. Specifically, I’m going to construct a map showing the geographic distribution of pickup locations for a billion or so taxi rides in New York.1\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(tictoc)\nlibrary(tidyr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#the-nyc-taxi-data",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#the-nyc-taxi-data",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "The NYC taxi data",
    "text": "The NYC taxi data\nAt this point in my life I have used the “NYC Taxi Data” for so many illustrative examples I feel like I don’t need to explain it: doesn’t “everyone” know about this data by now? Yeah, no dice sweetie. That’s a terrible intuition. Most people don’t know the data, and those that do can just skip to the next section! :-)\nHere’s a quick summary of the data set. In its full form, the data set takes the form of one very large table with about 1.7 billion rows and 24 columns. Each row corresponds to a single taxi ride sometime between 2009 and 2022. There’s a complete data dictionary for the NYC taxi data on the useR workshop site, but the columns that will be relevant for us are as follows:\n\npickup_longitude (double): Longitude data for the pickup location\npickup_latitude (double): Latitude data for the pickup location\ndropoff_longitude (double): Longitude data for the dropoff location\ndropoff_latitude (double): Latitude data for the dropoff location\n\nOn my laptop I have a copy of both the full data set, located at \"~/Datasets/nyc-taxi\" on my machine, and a much smaller “tiny” data set that contains 1 out of every 1000 records from the original, located at \"~/Datasets/nyc-taxi-tiny/\". This tiny version has a mere 1.7 million rows of data, and as such is small enough that it will fit in memory. Instructions for downloading both data sets are available at the same location as the data dictionary."
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#loading-the-data",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#loading-the-data",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Loading the data",
    "text": "Loading the data\nSince I have local copies of the data, I’ll use the open_dataset() function from the {arrow} package to connect to both versions of the NYC taxi data:2\n\nnyc_taxi &lt;- open_dataset(\"~/Datasets/nyc-taxi/\")\nnyc_taxi_tiny &lt;- open_dataset(\"~/Datasets/nyc-taxi-tiny/\")\n\nStarting with Arrow 9.0.0 it’s been possible to use the {dplyr} glimpse() function to take a look at the data sets, so let’s do that:\n\nglimpse(nyc_taxi)\n\nFileSystemDataset with 158 Parquet files\n1,672,590,319 rows x 24 columns\n$ vendor_name             &lt;string&gt; \"VTS\", \"VTS\", \"VTS\", \"DDS\", \"DDS\", \"DDS\", \"DD…\n$ pickup_datetime  &lt;timestamp[ms]&gt; 2009-01-04 13:52:00, 2009-01-04 14:31:00, 200…\n$ dropoff_datetime &lt;timestamp[ms]&gt; 2009-01-04 14:02:00, 2009-01-04 14:38:00, 200…\n$ passenger_count          &lt;int64&gt; 1, 3, 5, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, …\n$ trip_distance           &lt;double&gt; 2.63, 4.55, 10.35, 5.00, 0.40, 1.20, 0.40, 1.…\n$ pickup_longitude        &lt;double&gt; -73.99196, -73.98210, -74.00259, -73.97427, -…\n$ pickup_latitude         &lt;double&gt; 40.72157, 40.73629, 40.73975, 40.79095, 40.71…\n$ rate_code               &lt;string&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ store_and_fwd           &lt;string&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_longitude       &lt;double&gt; -73.99380, -73.95585, -73.86998, -73.99656, -…\n$ dropoff_latitude        &lt;double&gt; 40.69592, 40.76803, 40.77023, 40.73185, 40.72…\n$ payment_type            &lt;string&gt; \"Cash\", \"Credit card\", \"Credit card\", \"Credit…\n$ fare_amount             &lt;double&gt; 8.9, 12.1, 23.7, 14.9, 3.7, 6.1, 5.7, 6.1, 8.…\n$ extra                   &lt;double&gt; 0.5, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, 0.5, 0.0, …\n$ mta_tax                 &lt;double&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ tip_amount              &lt;double&gt; 0.00, 2.00, 4.74, 3.05, 0.00, 0.00, 1.00, 0.0…\n$ tolls_amount            &lt;double&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ total_amount            &lt;double&gt; 9.40, 14.60, 28.44, 18.45, 3.70, 6.60, 6.70, …\n$ improvement_surcharge   &lt;double&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ congestion_surcharge    &lt;double&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ pickup_location_id       &lt;int64&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ dropoff_location_id      &lt;int64&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ year                     &lt;int32&gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 200…\n$ month                    &lt;int32&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\nIf you’ve used glimpse() before this output will look very familiar. Each line in the output show the name of one column in the data, followed by the first few entries in that column.3 However, when you look at the size of the data set, you might begin to suspect that some magic is going on. Behind the scenes there are 1.7 billion rows of data in one huge table, and this is just too big to load into memory. Fortunately, the {arrow} package allows us to work with it anyway!"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#plotting-a-million-rows",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#plotting-a-million-rows",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Plotting a million rows",
    "text": "Plotting a million rows\nOkay, let’s start with a data visualisation problem that wouldn’t be too difficult to manage on a small data set. I want to draw an image that plots the pickup location for every taxi ride in the data set. Here’s how I might go about that. First, I’ll do a minimal amount of data wrangling in {arrow}. Specifically, I’ll use the {dplyr} select() and filter() functions to limit the amount of data I have to collect() into R:\n\ntic()\nnyc_pickups &lt;- nyc_taxi_tiny |&gt;\n  select(pickup_longitude, pickup_latitude) |&gt;\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude)\n  ) |&gt;\n  collect()\ntoc()\n\n0.16 sec elapsed\n\n\nAt this point I have a regular R data frame, nyc_pickups, that contains only the data I need: the pickup locations for all those taxi rides (in the tiny taxi data set) that actually contain longitude and latitude data. Let’s use glimpse() again:\n\nglimpse(nyc_pickups)\n\nRows: 1,249,107\nColumns: 2\n$ pickup_longitude &lt;dbl&gt; -73.95557, -73.97467, -73.78190, -73.97872, -73.97400…\n$ pickup_latitude  &lt;dbl&gt; 40.76416, 40.76222, 40.64478, 40.75371, 40.77901, 0.0…\n\n\nCompared to the full NYC taxi data, this is a relatively small data set. Drawing a scatter plot from 1.2 million observations isn’t a trivial task, to be sure, but it is achievable. In fact the {ggplot2} package handles this task surprisingly well:\n\nx0 &lt;- -74.05 # minimum longitude to plot\ny0 &lt;- 40.6   # minimum latitude to plot\nspan &lt;- 0.3  # size of the lat/long window to plot\n\ntic()\npic &lt;- ggplot(nyc_pickups) +\n  geom_point(\n    aes(pickup_longitude, pickup_latitude), \n    size = .2, \n    stroke = 0, \n    colour = \"#800020\"\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal(\n    xlim = x0 + c(0, span), \n    ylim = y0 + c(0, span)\n  )\npic\n\n\n\n\n\n\n\ntoc()\n\n3.365 sec elapsed\n\n\nIt’s not lightning fast or anything, but it’s still pretty quick!\nAs neat as this visualisation is there are limitations.4 In some parts of the plot – notably midtown in Manhattan – the data are so dense that you can’t make out any fine detail. In other parts – Brooklyn and Queens, for instance – there are so few data points that you can’t see much at all:\n\n\n\n\n\n\n\n\n\nHow do we improve this image?"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#scaling-to-a-billion-rows",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#scaling-to-a-billion-rows",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Scaling to a billion rows",
    "text": "Scaling to a billion rows\nTo make a better version of this plot, we’re going to have to do two things at once:\n\nUse a lot more data. If we use the full NYC taxi data set, the visualisation will be a lot more detailed in areas where it is currently too sparse.\nShow gradation at each location. In the dense areas there are too many points plotted atop one another. Instead of overplotting, we’ll use shading to represent the number of pickups at each location.\n\nHow do we do this? Let’s say I want to create a 4000 x 4000 pixel image, and I want the “intensity” at each pixel to represent the number of pickups that fall in the geographic region spanned by that pixel. There are a total of 16 million pixels, so our task is to assign each of observation one of those those 16 million bins, and then count the number of observations in each bin. We’ll have to rely on Arrow to do all the heavy lifting here. This binning cannot be done natively in R: the data set is just too big. Even after filtering out missing and out-of-bounds data points, there are still 1.2 billion rows, and R can’t do that without assistance.\nHere’s what the solution looks like:\n\ntic()\npixels &lt;- 4000\npickup &lt;- nyc_taxi |&gt;\n  filter(\n    !is.na(pickup_longitude),\n    !is.na(pickup_latitude),\n    pickup_longitude &gt; x0,\n    pickup_longitude &lt; x0 + span,\n    pickup_latitude &gt; y0,\n    pickup_latitude &lt; y0 + span\n  ) |&gt;\n  mutate(\n    unit_scaled_x = (pickup_longitude - x0) / span,\n    unit_scaled_y = (pickup_latitude - y0) / span,\n    x = as.integer(round(pixels * unit_scaled_x)), \n    y = as.integer(round(pixels * unit_scaled_y))\n  ) |&gt;\n  count(x, y, name = \"pickup\") |&gt;\n  collect()\ntoc()\n\n31.101 sec elapsed\n\n\nMy laptop solves this binning problem in about 30 seconds. As before, I’ll use glimpse() to take a peek at the results:\n\nglimpse(pickup)\n\nRows: 4,677,864\nColumns: 3\n$ x      &lt;int&gt; 1058, 1024, 1162, 3525, 865, 794, 856, 705, 647, 762, 802, 1207…\n$ y      &lt;int&gt; 2189, 2040, 2265, 552, 1983, 1646, 2018, 1590, 1723, 2010, 1645…\n$ pickup &lt;int&gt; 6514, 5030, 3818, 67, 2408, 2415, 932, 3607, 2664, 1024, 2207, …\n\n\nThis is a data frame where x and y specify the pixel, and and a pickup counts the number of pickups associated with that pixel. Note that the pixels aren’t arranged in a meaningful order, and only those pixels with at least one pickup (a little under 30% of all pixels) are included in data.\nWe can visualise this in a number of ways. One possibility is to create a scatter plot, using the pickup value to specify the shading of each plot marker:\n\ntic()\nggplot(pickup) +\n  geom_point(\n    aes(x, y, colour = log10(pickup)), \n    size = .01, \n    stroke = 0, \n    show.legend = FALSE\n  ) +\n  scale_colour_gradient(low = \"white\", high = \"#800020\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  theme_void() +\n  coord_equal()\n\n\n\n\n\n\n\ntoc()\n\n12.159 sec elapsed\n\n\nAs you can see, {ggplot2} has no problems drawing a scatter plot from a few million observations, and it’s an improvement on our first attempt. However, we can do better. Instead of trying to draw a scatter plot of all the points listed in the pickup data frame, let’s use it to populate a bitmap. We’ll create a 4000x4000 matrix, and fill in the cells with the pickup counts at the corresponding pixel.\nThe computation is a two part process. First, we use expand_grid() to initialise a “grid like” tibble containing all combination of x and y values, and use left_join() to populate a column containing the pickup counts:\n\ntic()\ngrid &lt;- expand_grid(x = 1:pixels, y = 1:pixels) |&gt;\n  left_join(pickup, by = c(\"x\", \"y\")) |&gt;\n  mutate(pickup = replace_na(pickup,  0))\ntoc()\n\n8.228 sec elapsed\n\n\nNote that the elements of grid are complete (all 16 million pixels are there), and meaningfully ordered. We can check this by calling glimpse() again:\n\nglimpse(grid)\n\nRows: 16,000,000\nColumns: 3\n$ x      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ y      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n$ pickup &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n\n\nBecause the elements of grid$pickup are arranged in this fashion, it is easy to construct the required 4000x4000 matrix:\n\ntic()\npickup_grid &lt;- matrix(\n  data = grid$pickup,\n  nrow = pixels,\n  ncol = pixels\n)\ntoc()\n\n0.02 sec elapsed\n\n\nThis is our bitmap. It’s a matrix whose values correspond to the pixel intensities to be plotted. Just so you can see what it looks like, here’s a tiny 10x10 pixel section from that matrix:\n\npickup_grid[2000:2009, 2000:2009]\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]   11    2    2    4    6    3    7   15   54    96\n [2,]    5    3    3    1    5   27   47   55   74   100\n [3,]    5    6    7   38   39   48   60   99   95    75\n [4,]   16   37   51   45   35   61   64   67   51    18\n [5,]   67   50   97  141   55   24   26   26   40    29\n [6,]   65  133   56   18   11   10  659    6    4     9\n [7,]   35   78   13    3   82  105   68    2    2     4\n [8,]    7    7    4    3    7   25    4    2    2     3\n [9,]    8   10    3    3   17    5   98    2    4     3\n[10,]    8    6    8    2   19    6    1    2    3    23\n\n\nNow that the data are in an image-like format, all we have to do is write the image file. We don’t even need {ggplot2}: we can use image() to draw the bitmap directly. Here’s a little helper function I wrote to do this:\n\nrender_image &lt;- function(mat, cols = c(\"white\", \"#800020\")) {\n  op &lt;- par(mar = c(0, 0, 0, 0))\n  shades &lt;- colorRampPalette(cols)\n  image(\n    z = log10(t(mat + 1)),\n    axes = FALSE,\n    asp = 1,\n    col = shades(256),\n    useRaster = TRUE\n  )\n  par(op)\n}\n\nHere’s what happens when I call it:\n\ntic()\nrender_image(pickup_grid)\n\n\n\n\n\n\n\ntoc()\n\n2.149 sec elapsed\n\n\nThis method is slightly faster the previous version, but the real advantage isn’t speed – it’s clarity. There’s less blurring in the denser parts of the plot (midtown Manhattan), and there’s also more clarity in the sparser areas (e.g., the Brooklyn streets are sharper).\nWe can push it slightly further by tweaking the colour palette. Plotting the logarithm of the number of pickups ensures that all the streets are visible (not just the extremely common ones), but it does have the downside that it’s hard to tell the difference between moderately popular pickup locations and extremely popular ones. A well-chosen diverging palette helps rectify this a little:\n\nrender_image(pickup_grid, cols = c(\"#002222\", \"white\", \"#800020\"))\n\n\n\n\n\n\n\n\nAt long last we have a visualisation that shows all the billion rows of data, crisply delineates all the streets on which taxi pickups are at least moderately frequent, and does a reasonable job of highlighting those locations where taxi pickups are extremely common. Yay! 🎉"
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#lessons-learned",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#lessons-learned",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Lessons learned?",
    "text": "Lessons learned?\nTo wrap this post up, I think it’s useful to reflect on the process I went through in constructing this image. In one sense, the process I’ve gone through here isn’t actually much different to what we do when creating any other data visualisation in R. For example, if you’re working in {tidyverse}, a typical work flow is to use {dplyr} to wrangle the data into an appropriate format and then use {ggplot2} to plot the data. What I’ve done here isn’t that different: okay yes, my {dplyr} code only works because it’s backed by the {arrow} engine, and in the end I decided to use base graphics rather than {ggplot2} to draw the final image, but I don’t think those differences constitute a major departure from my usual approach.\nThat being said, I think there are two key principles I’ve taken away from this. When trying to visualise very large data sets in R, the things I’m going to try to keep in mind are:\n\nPush as much of the computational work onto {arrow} as possible. The {arrow} package is designed specifically to handle these kinds of data manipulation problems, and things go much more smoothly when I don’t make {ggplot2} do the computational heavy lifting.\nThink carefully about the data representation. The reason why the final plot drawn with image() is nicer than the earlier ones drawn with {ggplot2} has nothing at all to do with the “base R vs tidyverse” issue. Instead, it’s because the data structure I created (i.e., pickup_grid) is the exact bitmap that needed to be rendered, and that’s exactly what image() is good for."
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#acknowledgments",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#acknowledgments",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you to Kae Suarez, Keith Britt, and François Michonneau for reviewing this post."
  },
  {
    "objectID": "posts/2022-08-23_visualising-a-billion-rows/index.html#footnotes",
    "href": "posts/2022-08-23_visualising-a-billion-rows/index.html#footnotes",
    "title": "How to visualise a billion rows of data in R with Apache Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post is adapted from the NYC taxi scatter GitHub repository that I put together to chat about on The Data Thread live interviews series, Pulling the Thread↩︎\nIt’s worth noting that you can connect to remote data sets as well as local ones, but that’s a bit beyond the scope of this post.↩︎\nThere are a few small hints that the underlying data structure is different though. For instance, the data types associated with each column refer to Arrow data types (e.g., timestamp, int32, int64, etc) rather than R data types. I’m not going to talk about those here, but if you’re looking for information about this topic, there’s a short summary of Arrow data types on the workshop website, and a longer blog post on Arrow data types on this blog.↩︎\nI’m not talking about the fact that there’s no legend or explanatory text: although those are real failures of data visualisation, they’re easily fixable. {ggplot2} has lots of tools for annotating plots appropriately.↩︎"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "",
    "text": "In the last post on this blog I showed how Apache Arrow makes it possible to hand over data sets from R to Python (and vice versa) without making wasteful copies of the data.\nThe solution I outlined there was to use the reticulate package to conduct the handover, and rely on Arrow tools both sides to manage the data. In one sense it’s a perfectly good solution to the problem… but it’s a solution tailor made for R users who need access to Python. When viewed from the perspective of a Python user who needs access to R, it’s a little awkward to have an R package (reticulate) governing the handover.1 Perhaps we can find a more Pythonic way to approach this?\nA solution to our problem is provided by the rpy2 library that provides an interface to R from Python, and the rpy2-arrow extension that allows it to support Arrow objects. Let’s take a look, shall we?"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#setting-up-the-python-environment",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#setting-up-the-python-environment",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Setting up the Python environment",
    "text": "Setting up the Python environment\nFor the purposes of this post I’ll create a fresh conda environment that I’ll call “continuation”, partly because this post is a continuation of the previous one and partly because the data set I’ll use later is taken from a database of serialised fiction called To Be Continued….\nI was able install most packages I need through conda-forge, but for rpy2 and rpy2-arrow I was only able to do so from pypi so I had to use pip for that. So the code for setting up my Python environment, executed at the terminal, was as follows:\nconda create -n continuation\nconda install -n continuation pip pyarrow pandas jupyter\nconda activate continuation\npip install rpy2 rpy2-arrow\nAs long as I render this post with the “continuation” environment active everything works smoothly.3"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#introducing-rpy2",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#introducing-rpy2",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Introducing rpy2",
    "text": "Introducing rpy2\nThe purpose of the rpy2 library is to allow users to call R from Python, typically with the goal of allowing access to statistical packages distributed through CRAN. I’m currently using version 3.5.4, and while this blog post won’t even come close to documenting the full power of the library, the rpy2 documentation is quite extensive. To give you a bit of a flavour of it, let’s import the library:\n\nimport rpy2\nrpy2.__version__\n\n'3.5.4'\n\n\nThis does not in itself give us access to R. That doesn’t happen until we explicitly import either the robjects module (a high level interface to R) or import the rinterface model (a low level interface) and call rinterface.initr(). This post won’t cover rinterface at all; we can accomplish everything we need to using only the high level interface provided by robjects. So let’s import the module and, in doing so, start R running as a child process:\n\nimport rpy2.robjects as robjects\n\n \n\n\nR version 4.2.1 (2022-06-23) 🌈\n\n\nYou’ll notice that this prints a little startup message. If you’re following along at home you’ll probably see something different on your own machine: most likely you’ll see the standard R startup message here. It’s shorter in this output because I modified my .Rprofile to make R less chatty on start up.4\nAnyway, our next step is to load some packages. In native R code we’d use the library() function for this, but rpy2 provides a more Pythonic approach. Importing the packages submodule gives us access to importr(), which is allows us to load packages. The code below illustrates how you can expose the base R package and the utils R package (both of which come bundled with any minimal R installation) to Python:\n\nimport rpy2.robjects.packages as pkgs\n\nbase = pkgs.importr(\"base\")\nutils = pkgs.importr(\"utils\")\n\nOnce we have access to utils we can call the R function install.packages() to install additional packages from CRAN. However, at this point we need to talk a little about how names are translated by rpy2. As every Python user would immediately notice, install.packages() is not a valid function name in Python: the dot is a special character and not permitted within the name of a function. In contrast, although not generally recommended in R except in special circumstances,5 function names containing dots are syntactically valid in R and there are functions that use them. So how do we resolve this?\nIn most cases, the solution is straightforward: rpy2 will automatically convert dots in R to underscores in Python, and so in this instance the function name becomes install_packages(). For example, if I want to install the fortunes package using rpy2, I would use the following command:6\nutils.install_packages(\"fortunes\")\nThere are some subtleties around function name translation, however. I won’t talk about them in this post, other to mention that the documentation discusses this in the section on calling functions.\nIn any case, now that I have successfully installed the fortunes package I can import it, allowing me to call the fortune() function:\n\nftns = pkgs.importr(\"fortunes\")\nftn7 = ftns.fortune(7)\nprint(ftn7)\n\n\nWhat we have is nice, but we need something very different.\n   -- Robert Gentleman\n      Statistical Computing 2003, Reisensburg (June 2003)\n\n\n\n\nI’m rather fond of this quote, and it seems very appropriate to the spirit of what polyglot data science is all about. Whatever language or tools we’re working in, we’ve usually chosen them for good reason. But there is no tool that works all the time, nor any language that is ideal for every situation. Sometimes we need something very different, and when we do it is very helpful if our tools able to talk fluently to each other.\nWe’re now at the point that we can tackle the problem of transferring data from Python to R, but in order to do that we’ll need some data…\n\n\n\n\n\nThis was the header illustration to a story entitled “The Trail of the Serpent” by M. E. Braddon. It was published in the Molong Express and Western District Advertiser on 4 August 1906. The moment I saw it I knew I had to include it here. I can hardly omit a serpent reference in a Python post, now can I? That would be grossly irresponsible of me as a tech blogger. Trove article 139469044"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#about-the-data",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#about-the-data",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "About the data",
    "text": "About the data\nI’ve given you so many teasers about the data set for this post that it almost feels a shame to spoil it by revealing the data, but all good things must come to an end I suppose. The data I’m using are taken from the To Be Continued… database of fiction published in Australian newspapers during the 19th and early 20th century. Originally collected using the incredibly cool Trove resource run by the National Library of Australia, the To Be Continued… data are released under a CC-BY-4.0 licence and maintained by Katherine Bode and Carol Hetherington. I’m not using the full data set here, only the metadata. In the complete database you can find full text of published pieces, and in the Trove links you can find the digitised resources from which they were sourced, but I don’t need that level of detail here. All I need is an interesting data table that I can pass around between languages. For that, the metadata alone will suffice!\nTo give you a sense of what the data set (that is, the restricted version I’m using here) looks like, let’s fire up pandas and take a peek at the structure of the table. It’s stored as a CSV file, so I’ll call read_csv() to import the data:\n\nimport pandas\n\nfiction = pandas.read_csv(\"fiction.csv\", low_memory = False)\nfiction.head()\n\n\n\n\n\n\n\n\nTrove ID\nCommon Title\nPublication Title\nStart Date\nEnd Date\nAdditional Info\nLength\nCurated Dataset\nIdentified Sources\nPublication Source\n...\nOther Names\nPublication Author\nGender\nNationality\nNationality Details\nAuthor Details\nInscribed Gender\nInscribed Nationality\nSignature\nName Category\n\n\n\n\n0\n1\nThe Mystery of Edwin Drood\nThe Mystery of Edwin Drood\n1871-03-04\n1871-06-03\nNaN\n0.0\nY\nLCVF\nNaN\n...\nNaN\nDickens, Charles\nMale\nBritish\nNaN\nLCVF\nMale\nBritish\nNaN\nAttributed\n\n\n1\n2\nThe Mystery of Edwin Drood\nThe Mystery of Edwin Drood\n1871-03-07\n1871-05-16\nNaN\n0.0\nY\nLCVF\nNaN\n...\nNaN\nDickens, Charles\nMale\nBritish\nNaN\nLCVF\nMale\nBritish\nNaN\nAttributed\n\n\n2\n3\nSporting Recollections in Various Countries\nSporting Recollections in Various Countries\n1847-06-16\n1847-07-07\nNaN\n0.0\nY\nWPEDIA\nSunday Times\n...\nNaN\nViardot, M. Louis\nMale\nFrench\nNaN\nWPEDIA\nMale\nBritish\nNaN\nAttributed\n\n\n3\n4\nBrownie's Triumph\nThe Jewels\n1880-05-08\n1880-08-14\nNaN\n0.0\nY\nTJW\nNaN\n...\nSarah Elizabeth Forbush Downs; Downs, Mrs Geor...\nUnattributed\nFemale\nAmerican\nNaN\nWPEDIA\nUninscribed\nBritish\nNaN\nUnattributed\n\n\n4\n5\nThe Forsaken Bride\nAbandoned\n1880-08-21\n1880-12-18\nFiction. From English, American and Other Peri...\n0.0\nY\nTJW\nNaN\n...\nSarah Elizabeth Forbush Downs; Downs, Mrs Geor...\nUnattributed\nFemale\nAmerican\nNaN\nWPEDIA\nUninscribed\nBritish\nNaN\nUnattributed\n\n\n\n\n5 rows × 28 columns\n\n\n\nOkay, that’s helpful. We can see what all the columns are and what kind of data they contain. I’m still pretty new to data science workflows in Python, but it’s not too difficult to do a little bit of data wrangling with Pandas. For instance, we can take a look at the distribution of nationalities among published authors. The table shown below counts the number of distinct publications (Trove IDs) and authors for each nationality represented in the data:\n\nfiction[[\"Nationality\", \"Trove ID\", \"Publication Author\"]]. \\\n  groupby(\"Nationality\"). \\\n  nunique()\n\n\n\n\n\n\n\n\nTrove ID\nPublication Author\n\n\nNationality\n\n\n\n\n\n\nAmerican\n3399\n618\n\n\nAustralian\n4295\n757\n\n\nAustralian/British\n95\n12\n\n\nAustrian\n3\n2\n\n\nBritish\n10182\n1351\n\n\nBritish/American\n2\n2\n\n\nCanadian\n185\n29\n\n\nDutch\n1\n1\n\n\nEnglish\n2\n2\n\n\nFrench\n187\n64\n\n\nGerman\n39\n15\n\n\nHungarian\n2\n1\n\n\nIrish\n63\n33\n\n\nItalian\n12\n1\n\n\nJapanese\n1\n1\n\n\nMultiple\n3\n2\n\n\nNew Zealand\n67\n23\n\n\nPolish\n1\n1\n\n\nRussian\n18\n13\n\n\nScottish\n2\n2\n\n\nSouth African\n14\n5\n\n\nSwedish\n1\n1\n\n\nSwiss\n2\n1\n\n\nUnited States\n2\n2\n\n\nUnknown\n13133\n2692\n\n\nUnknown, not Australian\n882\n88\n\n\n\n\n\n\n\nIt would not come as any surprise, at least not to anyone with a sense of Australian history, that there were far more British authors than Australian authors published in Australian newspapers during that period. I was mildly surprised to see so many American authors represented though, and I have nothing but love for the lone Italian who published 12 pieces.\nNow that we have a sense of the data, let’s add Arrow to the mix!\n\n\n\n\n\nAn illustration from “The Lass That Loved a Miner” by J. Monk Foster. Published in Australian Town and Country Journal, 14 April 1894. The story features such fabulous quotes as “Presently the two dark figures slid slowly, noiselessly, along the floor towards the scattered gold dust and he canisters filled with similar precious stuff. Inch by inch, foot by foot the two thieves crept like snakes nearer and nearer to the to the treasure they coveted”. Admit it, you’re hooked already, right? Trove article 71212612"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#pandas-to-arrow-tables",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#pandas-to-arrow-tables",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Pandas to Arrow Tables",
    "text": "Pandas to Arrow Tables\nTo give ourselves access to Apache Arrow from Python we’ll use the PyArrow library. Our immediate goal is to convert the fiction data from a Pandas DataFrame to an Arrow Table. To that end, pyarrow supplies a Table object with a from_pandas() method that we can call:\n\nimport pyarrow\n\nfiction2 = pyarrow.Table.from_pandas(fiction)\nfiction2\n\npyarrow.Table\nTrove ID: int64\nCommon Title: string\nPublication Title: string\nStart Date: string\nEnd Date: string\nAdditional Info: string\nLength: double\nCurated Dataset: string\nIdentified Sources: string\nPublication Source: string\nNewspaper ID: int64\nNewspaper: string\nNewspaper Common Title: string\nNewspaper Location: string\nNewspaper Type: string\nColony/State: string\nAuthor ID: int64\nAuthor: string\nOther Names: string\nPublication Author: string\nGender: string\nNationality: string\nNationality Details: string\nAuthor Details: string\nInscribed Gender: string\nInscribed Nationality: string\nSignature: string\nName Category : string\n----\nTrove ID: [[1,2,3,4,5,...,35491,35492,35493,35494,35495]]\nCommon Title: [[\"The Mystery of Edwin Drood\",\"The Mystery of Edwin Drood\",\"Sporting Recollections in Various Countries\",\"Brownie's Triumph\",\"The Forsaken Bride\",...,\"The Heart of Maureen\",\"His Lawful Wife\",\"Love's Reward\",\"Only a Flirt\",\"The Doctor's Protegee\"]]\nPublication Title: [[\"The Mystery of Edwin Drood\",\"The Mystery of Edwin Drood\",\"Sporting Recollections in Various Countries\",\"The Jewels\",\"Abandoned\",...,\"The Heart of Maureen\",\"His Lawful Wife\",\"Love's Reward\",\"Only a Flirt\",\"The Doctor's Protegee\"]]\nStart Date: [[\"1871-03-04\",\"1871-03-07\",\"1847-06-16\",\"1880-05-08\",\"1880-08-21\",...,\"1914-01-06\",\"1912-10-26\",\"1911-02-04\",\"1916-05-06\",\"1911-11-25\"]]\nEnd Date: [[\"1871-06-03\",\"1871-05-16\",\"1847-07-07\",\"1880-08-14\",\"1880-12-18\",...,\"1914-01-06\",\"1912-10-26\",\"1911-02-04\",\"1916-05-06\",\"1911-11-25\"]]\nAdditional Info: [[null,null,null,null,\"Fiction. From English, American and Other Periodicals\",...,\"Published by special arrangement. All rights reserved.\",\"Published by special arrangement. All rights reserved.\",\"Published by special arrangement. All rights reserved.\",\"All  Rights Reserved\",\"Published by special arrangement. All rights reserved.\"]]\nLength: [[0,0,0,0,0,...,0,0,0,0,0]]\nCurated Dataset: [[\"Y\",\"Y\",\"Y\",\"Y\",\"Y\",...,\"N\",\"N\",\"N\",\"N\",\"N\"]]\nIdentified Sources: [[\"LCVF\",\"LCVF\",\"WPEDIA\",\"TJW\",\"TJW\",...,null,null,null,null,null]]\nPublication Source: [[null,null,\"Sunday Times\",null,null,...,null,null,null,null,null]]\n...\n\n\n\nThe fiction2 object contains the same data as fiction but it is structured as an Arrow Table, and the data is stored in memory allocated by Arrow. Python itself only stores some metadata and the C++ pointer that refers to the Arrow Table. This isn’t exciting, but it will be important (and powerful!) later in a moment we transfer the data to R.\nSpeaking of which, we have arrived at the point where we get to do the fun part… seamlessly handing the reins back and forth between Python and R without needing to copy the Arrow Table itself."
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#passing-tables-from-python-to-r",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#passing-tables-from-python-to-r",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Passing Tables from Python to R",
    "text": "Passing Tables from Python to R\nTo pass Arrow objects between Python and R, rpy2 needs a little help because it doesn’t know how to handle Arrow data structures. That’s where the rpy2-arrow module comes in. As the documentation states:\n\nThe package allows the sharing of Apache Arrow data structures (Array, ChunkedArray, Field, RecordBatch, RecordBatchReader, Table, Schema) between Python and R within the same process. The underlying C/C++ pointer is shared, meaning potentially large gain in performance compared to regular arrays or data frames shared between Python and R through the conversion rules included in rpy2.\n\nI won’t attempt to give a full tutorial on rpy2-arrow in this post. Instead, I’ll just show you how to use it to solve the problem at hand. Our first step is to import the conversion tools from rpy_arrow:\n\nimport rpy2_arrow.pyarrow_rarrow as pyra\n\nHaving done that, the pyarrow_table_to_r_table() function allows us to pass an Arrow Table from Python to R:\n\nfiction3 = pyra.pyarrow_table_to_r_table(fiction2)\nfiction3\n\n&lt;rpy2.rinterface_lib.sexp.SexpEnvironment object at 0x7f71bfb8a6c0&gt; [RTYPES.ENVSXP]\n\n\nThe printed output isn’t the prettiest thing in the world, but nevertheless it does represent the object of interest. On the Python side we have fiction2, a data structure that points to an Arrow Table and enables various compute operations supplied through pyarrow. On the R side we have now created fiction3, a data structure that points to the same Arrow Table and enables compute operations supplied by the R arrow package. In the same way that fiction2 only stores a small amount of metadata in Python, fiction3 stores a small amount of metadata in R. Only this metadata has been copied from Python to R: the data itself remains untouched in Arrow.\n\n\n\n\n\nHeader illustration to “Where flowers are Rare” by Val Jameson. Published in The Sydney Mail, 8 December 1909. I honestly have no logical reason for including this one. But I was listening to Kylie Minogue at the time I was browsing the database and the title made me think of Where the Wild Roses Grow, and anyway both the song and the story have death in them. So then I simply had to include the image because… it’s Kylie. Obviously. Sheesh. Trove article 165736425"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#accessing-the-table-from-the-r-side",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#accessing-the-table-from-the-r-side",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Accessing the Table from the R side",
    "text": "Accessing the Table from the R side\nWe’re almost done, but the tour isn’t really complete until we’ve stepped out of Python entirely, manipulated the object on the R side, and then passed something back to Python. So let’s do that next.\nIn order to pull off that trick within this quarto document – which is running jupyter under the hood – we’ll need to employ a little notebook magic, again relying on rpy2 to supply all the sparkly bits. To help us out in this situation, the rpy2 library supplies an interface for interactive work that we can invoke in a notebook context like this:\n\n%load_ext rpy2.ipython\n\nNow that we’ve included this line, all I have to do is preface each cell with %%R and the subsequent “Python” code will be passed to R and interpreted there.7 To start with I’ll load the dplyr and arrow packages, using the suppressMessages() function to prevent them being chatty:\n\n%%R\n\nsuppressMessages({\n  library(dplyr)\n  library(arrow)\n})\n\nHaving loaded the relevant packages, I’ll use the dplyr/arrow toolkit to do a little data wrangling on the fiction3 Table. I’m not doing anything fancy, just a little cross-tabulation counting the joint distribution of genders and nationalities represented in the data using the count() function, and using arrange() to sort the results:\n\n%%R -i fiction3\n\ngender &lt;- fiction3 |&gt; \n  count(Gender, Nationality) |&gt;\n  arrange(desc(n)) |&gt;\n  compute()\n  \ngender\n\nTable\n\n\n\n\n\n63 rows x 3 columns\n$Gender &lt;string&gt;\n$Nationality &lt;string&gt;\n$n &lt;int64&gt;\n\nSee $metadata for additional Schema metadata\n\n\n\n\n\nThe output isn’t very informative, but don’t worry, by the end of the post there will be a gender reveal I promise.8 Besides, the actual values of gender aren’t important right now. In truth, the part that we’re most interested in here is the first line of code. By using %%R -i fiction3 to specify the cell magic, we’re able to access the fiction3 object from R within this cell and perform the required computations.\nOh, and also we now have a new gender object in our R session that we probably want to pull back into Python!"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#the-journey-home-a-tale-of-four-genders",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#the-journey-home-a-tale-of-four-genders",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "The journey home: A tale of four genders",
    "text": "The journey home: A tale of four genders\nOkay. So we now have an object in the embedded R session that we might wish to access from the Python session and convert to a Python object. First we’ll pass the Arrow Table from R to Python and then convert to a Pandas DataFrame. Here’s how that process works. If you recall from earlier in the post, we imported robjects to start the embedded R session. When we did so, we also exposed robjects.r, which provides access to all objects within that R session. To create a Python object gender2 that refers to the R data structure we created in the last section, here’s what we do:\n\ngender2 = robjects.r('gender')\ngender2\n\n&lt;rpy2.robjects.environments.Environment object at 0x7f71b6784bc0&gt; [RTYPES.ENVSXP]\nR classes: ('Table', 'ArrowTabular', 'ArrowObject', 'R6')\nn items: 36\n\n\nImportantly, notice that this is the same object. The gender2 variable still refers to the Arrow Table in R: it’s not a pyarrow table. If we want to convert it to a data structure that pyarrow understands, we can again use the rpy-arrow conversion tools. In this case, we can use the rarrow_to_py_table() function:\n\ngender3 = pyra.rarrow_to_py_table(gender2)\ngender3\n\npyarrow.Table\nGender: string\nNationality: string\nn: int64\n----\nGender: [[\"Unknown\",\"Male\",\"Female\",\"Male\",\"Female\",...,\"Both\",\"Female\",\"Female\",\"Female\",null]]\nNationality: [[\"Unknown\",\"British\",\"British\",\"Australian\",\"Australian\",...,\"Australian/British\",\"British/American\",\"South African\",\"Polish\",\"Australian\"]]\nn: [[12832,6420,3346,2537,1687,...,1,1,1,1,1]]\n\n\nJust like that, we’ve handed over the Arrow Table from R back to Python. Again, it helps to remember that gender2 is an R object and gender3 is a Python object, but both of them point to the same underlying Arrow Table.\nIn any case, now that we have gender3 on the Python side, we can use the to_pandas() method from pyarrow.Table to convert it to a pandas data frame:\n\ngender4 = pyarrow.Table.to_pandas(gender3)\ngender4\n\n\n\n\n\n\n\n\nGender\nNationality\nn\n\n\n\n\n0\nUnknown\nUnknown\n12832\n\n\n1\nMale\nBritish\n6420\n\n\n2\nFemale\nBritish\n3346\n\n\n3\nMale\nAustralian\n2537\n\n\n4\nFemale\nAustralian\n1687\n\n\n...\n...\n...\n...\n\n\n58\nBoth\nAustralian/British\n1\n\n\n59\nFemale\nBritish/American\n1\n\n\n60\nFemale\nSouth African\n1\n\n\n61\nFemale\nPolish\n1\n\n\n62\nNone\nAustralian\n1\n\n\n\n\n63 rows × 3 columns\n\n\n\nAnd with that our transition home is complete!"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#summary",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#summary",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Summary",
    "text": "Summary\nThis post has wandered over a few topics, which is perhaps to be expected given the nature of polyglot data science. To make it all work smoothly I needed to think a little about how my Python and R environments are set up: the little asides I buried in footnotes mention the frictions I encountered in getting rpy2 to work smoothly for me, for instance. As someone who primarily uses R it took me a little while to work out how to get quarto to switch cleanly from a knitr engine to a jupyter engine. The R and Python libraries implementing Apache Arrow make it look seamless when we handover data from one language to another – and in some ways they actually do make it seamless in spite of the many little frictions that exist with Arrow, no less than any other powerful and rapidly-growing tool – but a lot of work has gone into making that transition smooth. Whether you’re an R focused developer using reticulate or a Python focused developer who prefers rpy2, the toolkit is there. I’m obviously biased in this because so much of my work revolves around Arrow these days, but at some level I’m still actually shocked that it (and other polyglot tools) works as well as it does. Plus, I’m having a surprising amount of fun teaching myself “Pythonic” ways of thinking and coding, so that’s kind of cool too.\nHopefully this post will help a few other folks get started in this area!\n\n\n\n\n\nHeader illustration to “The Black Motor Car” by J. B. Harris Burland. Published in – just to bring us full circle – The Arrow, 25 November 1905. I cannot properly do justice to this work of art so I will merely quote: “Again he took her in his arms, and this time she did not try to free herself from his embrace. But she looked up at him with pleading eyes. He bent down his face and kissed her tenderly on the forehead. His whole nature cried out for the touch of her lips, but he was man enough to subdue the passion that burnt within him.” Trove article 103450814"
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#acknowledgements",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#acknowledgements",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nIn writing this post I am heavily indebted to Isabella Velásquez, whose fabulous post on calling R from Python with rpy2 helped me immensely. The documentation on integrating PyArrow with R was extremely helpful too! Thank you to Kae Suarez for reviewing this post."
  },
  {
    "objectID": "posts/2022-09-16_arrow-and-rpy2/index.html#footnotes",
    "href": "posts/2022-09-16_arrow-and-rpy2/index.html#footnotes",
    "title": "Data transfer between Python and R with rpy2 and Apache Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRelatedly, if you’re a Python user blogging in quarto, you are very unlikely to be using the knitr engine to execute code like I did in the last blog post. Instead you’re almost certainly using the jupyter engine. With that in mind, and with the goal of making this post a little more Pythonic, I’m using Jupyter this time.↩︎\nA note on image copyright. As far as I can tell all images in this post are public domain. They’re all sourced from Trove and are all over a century old, meaning that they are all covered by the “plus 50 years” rule in Australian copyright law (the current “plus 70” rule does not apply retroactively). The original illustrator is difficult to determine, and given the age of the images so too is any potential copyright holder, but it seems extremely unlikely that any are still covered by any copyright. As always, I will remove any image if I discover that I am incorrect in this.↩︎\nHa ha. Just kidding. Aaaaaaaaactualllllllly, it will probably work smoothly for most people. But there are exceptions, and because I am a foolish tinkerer and have a nonstandard R configuration I am one of them. I have recently made the decision to use the rig manager to configure multiple concurrent R installations on my laptop. This introduces a some complexity, because rig necessarily installs R to non standard locations. Now, rig does the right thing and correctly sets the PATH environment variable so that rpy2 (and bash) can find R, but it does lead to some peculiar behaviour where rpy2 doesn’t find some of the C libraries need. In the rpy2 readme there’s a discussion of this issue. In such cases you need to tweak the LD_LIBRARY_PATH environment variable before starting Python: export LD_LIBRARY_PATH=\"$(python -m rpy2.situation LD_LIBRARY_PATH)\":${LD_LIBRARY_PATH}↩︎\nAs an aside, it’s worth noting that rpy2 has run R with my default configuration (notwithstanding the fact that my defaults are configured using rig). It hasn’t loaded any specific R environment. It did occur to me that a complete discussion of this topic would also describe how a Python user could use rpy2 to configure the R environment using the renv package for instance, but to be honest that started to feel a little beyond the scope of the post. About the only thing I will mention here is that in this particular use case (namely, passing Arrow objects between R and Python) I would not recommend trying to configure the Python environment and the R environment within the same conda environment. Because that thought occurred to me too. I tried it and oh my… the number of unsolvable conflicts was truly impressive.↩︎\nThe dot is typically used to denote an S3 method in R, but because R embraces chaos this is not universally adhered to and in any case S3 is… look, I love S3 but as Hadley Wickham once observed it’s an object oriented programming system that absolutely allows you to shoot yourself in the foot if you want to. Anyway. This is not the post for ramblings about the chaotic splendour of R.↩︎\nDepending on how blank your R configuration is, you may need to specify which CRAN mirror you want to download the package from before attempting the installation. To do that, include a command like utils.chooseCRANmirror(ind=1) to select the first mirror on the list of known servers.↩︎\nOkay, that brings me to something I didn’t really cover in my last post. Some R users might be wondering about what was going on in the last post where I was flipping back and forth between R and Python without apparently doing anything like this. The answer is that when using knitr as the engine rather than jupyter, python code is automatically interpreted with the help of reticulate. However, that feature is exposed by default in the knitr engine so I didn’t need to invoke it explicitly the way I’m doing here in jupyter.↩︎\nI’m sorry. The joke was too obvious, yet too hard to resist.↩︎"
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html",
    "title": "Non-compartmental analysis",
    "section": "",
    "text": "Back in my academic days, one thing that always made me sad about being “technically” a psychologist rather than a statistician or computer scientist is that it was always incumbent on my to use my technical skills in areas that would be of interest to behavioural scientists. As John Tukey once remarked, “The best thing about being a statistician is that you get to play in everyone’s backyard”. Fortunately for me, my academic backyard was quite large. On the applied side I had the opportunity to work in cognitive science, social psychology, forensic psychology, linguistics, and more, while still finding opportunities to write the occasional paper on machine learning or information theory. I even managed to find a way to play with quantum walks at one point.\nNevertheless, my envy persisted. There are so many other interesting things out there, and I never tire of learning new things. As such, I require very little pretext to start reading papers on… um… [checks notes]… statistical techniques in pharmacokinetics. Pharmacokinetics (PK) is a subfield of pharamacology that studies the concentrations of drugs and other substances over time when they are administered. Pharmacokinetic models are typically contrasted with (and sometimes paired with) pharmacodynamic (PD) models used to study the effect that drugs (and other substances) have on the organism. When these two things are paired – as you would often need to do if, say, you’d like to understand the behaviour of a new drug – it’s referred to as PK/PD modelling.1"
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#a-little-background",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#a-little-background",
    "title": "Non-compartmental analysis",
    "section": "A little background",
    "text": "A little background\nTo the surprise of exactly no-one who has experience with applied statistics, it turns out that there is quite a bit of nuance to pharmacometric analyses, and I’m only just starting to wrap my head around it all.2 To help make sense of it, let’s imagine a simple data set that looks like this when plotted:\n\n\n\n\n\n\n\n\n\nIt’s a very simple data set because we’re only looking at data from a single dose of some drug administered only to a single organism, and we’re measuring the concentration3 of that drug at several time points. This is quite sensibly referred to as a concentration-time curve.\nGiven data that take the form of one or more concentration-time curves, what kind of modelling strategy should you pursue? 4 5 As usual there are many choices an analyst needs to make, but one key distinction is between noncompartmental and compartmental analyses.\nIn a compartmental analysis, the statistician makes some (generally simplified) assumptions about the biological processes6 at play when a drug is administered, and estimates parameters using this model. For example, you might use a two-compartment model with a central compartment that corresponds to the circulatory system through which the drug is distributed, and a peripheral compartment corresponding to the body tissues to which the drug is delivered. The good thing about compartmental models is that they can provide a somewhat realistic description of what happens when a drug is administered. The bad thing is that it can be difficult to work out how to formulate the model correctly, with all the attendant worries about model misspecification. I’m not going to talk about these models in this post.\nAn alternative strategy – one that has complementary strengths and weaknesses to compartmental models – is to try to estimate quantities of interest (more or less) directly from the concentration-time curve. This approach is referred to as non-compartmental analysis (NCA), and it has the advantage of being simpler to implement, and as such inherits much of the robustness that comes from model simplicity.7\nWith that in mind it is useful to list some of the quantities of interest that are typically used in pharmacokinetic modelling. They vary a bit. Some of them are defined in a way that relates straightforwardly to the data. They’re things that you can directly measure, or they’re part of the study design, etc:\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nNotes\nSymbol\nUnit\nFormula\nExample\n\n\n\n\nDose\nAmount of drug administered\n\\(D\\)\nmmol\n(Design)\n500 mmol\n\n\nCmax\nPeak plasma concentration of drug after administration\n\\(C_{max}\\)\nmmol/L\n(Measured)\n60.9 mmol/L\n\n\ntmax\nTime taken to reach Cmax\n\\(t_{max}\\)\nh\n(Measured)\n3.9h\n\n\nVolume of distribution\nVolume over which the drug is distributed\n\\(V_d\\)\nL\n\\(D/C_0\\)\n6.0 L\n\n\n\n\n\nNot everything is that simple however. For example, the half-life parameters for absorption and elimination are not directly observable, and estimates of those quantities must be computed from other quantities:\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nNotes\nSymbol\nUnit\nFormula\nExample\n\n\n\n\nAbsorption half-life\nThe time taken for 50% of the dose to be absorbed into circulation\n\\(t_{\n\\frac{1}{2}\na}\\)\nh\n\\(\\ln(2) / k_a\\)\n1.0 h\n\n\nElimination half-life\nThe time taken for the drug concentration to fall to 50% of its initial value\n\\(t_{\n\\frac{1}{2}\ne}\\)\nh\n\\(\\ln(2) / k_e\\)\n12 h\n\n\n\n\n\nOf course, when an uninitiated reader looks at these formulas, the first question they’d ask is “what the heck do the \\(k_a\\) and \\(k_e\\) quantities refer to? In a slightly circular fashion, we can extend the table somewhat and refer to these as the corresponding rate constant parameters for absorption and elimination:\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nNotes\nSymbol\nUnit\nFormula\nExample\n\n\n\n\nAbsorption rate constant\nParameterises the rate at which the drug enters into circulation\n\\(k_a\\)\n\\(h^{-1}\\)\n\\(\\frac{\\ln 2}\n{t_{\\frac{1}{2}a}}\\)\n0.693 h-1\n\n\nElimination rate constant\nParameterises the rate at which the drug is eliminated from the body\n\\(k_e\\)\n\\(h^{-1}\\)\n\\(\\frac{\\ln 2}\n{t_{\\frac{1}{2}e}}\n= \\frac{CL}{V_d}\\)\n0.0578 h-1\n\n\n\n\n\nFor the most part, these extra lines in our table don’t do much other than rephrase the last two… but there is something new here. One way of thinking about the underlying biology here is to suppose that the body has the capacity to “clean” or “clear” some volume of blood plasma per unit time. This is referred to as the clearance rate (denoted \\(CL\\) in the table).\nOnce we have the concept of a clearance rate \\(CL\\) – and implicitly, a concept of how it relates to the elimination rate constant \\(k_e\\) and volume of distribution \\(V_d\\) – we can start linking it to observable (or at least, estimable) quantities…\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantity\nNotes\nSymbol\nUnit\nFormula\nExample\n\n\n\n\nArea under the curve\nIntegral of the concentration- time curve\n\\(AUC\\)\n\\(M s\\)\n\\(\\int_I C dt\\)\n1,320 h mmol/L\n\n\nClearance rate\nVolume of plasma cleared per unit time\n\\(CL\\)\n\\(m^3/ s\\)\n\\(V_d \\ k_e =\n\\frac{D}{AUC}\\)\n0.38 L/h\n\n\n\n\n\n…where I’m being slightly imprecise here in the formula for the area under the curve, because I haven’t specified the interval \\(I\\) over which we should integrate. In the simplest case where as single dose is administered, the interval is generally taken to be \\([0, \\infty]\\), and that is good enough for this post.8\nAt long last, we get to the point… you can think of the AUC as a kind of “total drug exposure” measure, but it’s not immediately obvious whether that’s a biologically meaningful quantity. However, it is naturally related to the clearance rate… which does have an inherent meaning from a biological perspective. And from this observation we arrive at the logic behind noncompartmental analysis. Instead of constructing a model of the process by which the body eliminates a drug, we instead aim to estimate AUC (and other quantities) from the observed data.9 10\nYay! Now we understand a minimal amount of the basic science. We know why the lovely pharma folks might want an estimate of AUC, and we can see why a nonparametric11 12 estimate of the AUC might useful to have."
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#a-simple-approach",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#a-simple-approach",
    "title": "Non-compartmental analysis",
    "section": "A simple approach",
    "text": "A simple approach\nHow might we estimate this? The simplest approach would be to linearly interpolate between observations, and then compute the area under the resulting curve. Represented visually, area we would compute is the sum of the trapezoidal regions shown below:\n\n\n\n\n\n\n\n\n\nFor a single trapezoid defined by time values \\(t_1\\) and \\(t_2\\), at which concentration levels \\(c_1\\) and \\(c_2\\) are observed, the corresponding area is \\((t_2 - t_1) (c_2 + c_1) / 2\\), so applying the trapezoidal rule to the finite set of observations provides an approximation to the integral we care about, yielding a simple estimator for the area under the curve:\n\\[\nAUC = \\sum_{i=1}^{n-1} (t_{i+1} - t_i) \\frac{c_{i+1} + c_i}{2}\n\\]\nwhere \\(n\\) denotes the total number of time points at which a measurement has been taken. To tidy the notation slightly I’ll let \\(\\Delta t_i = t_{i + 1} - t_i\\), and thus\n\\[\nAUC = \\sum_{i=1}^{n-1} \\Delta t_i \\frac{c_{i+1} + c_i}{2}\n\\]\nThat’s easy enough to implement as an R function:\n\nauc &lt;- function(time, conc) {\n  n &lt;- length(time)\n  sum((time[-1] - time[-n]) * (conc[-1] + conc[-n]) / 2)\n}\n\nThis auc() function is incredibly limited, but it will suffice for now.13 Let’s say we have a data frame df that contains the data I used to draw the figure above:\n\ndf\n\n# A tibble: 7 × 2\n   time concentration\n  &lt;dbl&gt;         &lt;dbl&gt;\n1     0             0\n2     1             8\n3     2            12\n4     4            14\n5     8             9\n6    16             4\n7    32             2\n\n\nThen:\n\nwith(df, auc(time, concentration))\n\n[1] 186\n\n\nUsing such calculated values of AUC I could then go on to calculate values for clearance, bioavailability, and more. Yay!"
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#a-concern-about-tails",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#a-concern-about-tails",
    "title": "Non-compartmental analysis",
    "section": "A concern about tails",
    "text": "A concern about tails\nIn practice, a pharmacometrician wouldn’t calculate AUC the way I’ve just done it, for all sorts of reasons. To see why, I’ll let \\(f(t)\\) denote the true concentration-time curve, such that the observed concentration \\(c_i\\) at time \\(t_i\\) is given by\n\\[\nc_i = f(t_i) + \\epsilon_i\n\\]\nwhere \\(\\epsilon_i\\) denotes the measurement errors that – purely for simplicity – I’ll pretend are i.i.d. with mean zero.14 By definition then, the true value of the AUC we seek to estimate is given\n\\[\n\\int_0^\\infty f(t) \\ dt\n\\]\nwhich we can decompose into the sum of two definite integrals, one taken over the range of times for which we have data \\([t_1, t_n]\\) where \\(t_1 = 0\\), and the other corresponding to the tail area \\([t_n, \\infty]\\)\n\\[\n\\int_{t_1}^{t_n} f(t) \\ dt + \\int_{t_n}^{\\infty} f(t) \\ dt\n\\]\nIt doesn’t take a lot of work to notice that the AUC that we calculate is a numerical approximation to the first term only.15 16 17 Unless we have designed our study such that the concentration \\(c_n\\) is effectively zero at the final time point \\(t_n\\), we’re going to end up with an estimate of the AUC that systematically underestimates the true value. Given that AUC is used as a measure of “total drug exposure” and is rather important for estimating other important quantities, we’d prefer to avoid that, but in order to do so we might have to – gasp! – start using our knowledge of the domain to make some assumptions."
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#making-sensible-assumptions",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#making-sensible-assumptions",
    "title": "Non-compartmental analysis",
    "section": "Making sensible assumptions",
    "text": "Making sensible assumptions\nAt this point it’s useful to consider the reasons why the curve I’ve used as my toy data set starts at zero, rises to a maximum concentration after a moderate amount of time, and then smoothly decreases afterwards. This pattern might be observed from a drug administered orally. At the time of administration the blood plasma concentration of the drug will be zero because it hasn’t yet been absorbed into systemic circulation – because bodies have that pesky gastrointestinal system that has to do its job before ingested substances pass into the bloodstream. In other words, at the beginning the absorption process is the main factor at play, and blood concentration rises. Later on, however, once the substance has been (mostly) absorbed the situation changes, and it’s the elimination process that dominates: drug concentration falls as the kidneys etc do their job and clear it from the system. For a drug administered intravenously, however, the situation is going to be a little different because we bypass the absorption process: the plasma concentration starts at its maximum value and falls steadily as the elimination process takes place.\nGiven all this let’s return to NCA estimate of AUC, and think about what happens in the tails. Assuming we have designed our study sensibly, the last time point \\(t_n\\) will be chosen so that absorption (if relevant) is substantially complete, and the blood concentration is falling: when estimating the tail area, it’s not unreasonable to assume that elimination is the only process in play, and that the body is able to clear some fixed volume of blood (i.e., the clearance, CL) per unit time. Or, to put it another way, for the purposes of estimating the tail area, we’ll use a one-compartment bolus model that assumes drug concentrations over time follow an exponential decay:18\n\\[\nc(t) = c_0 \\ e^{-k_e t}\n\\]\nwhere \\(c_0\\) refers to the initial concentration and \\(k_e\\) is the elimination rate constant discussed earlier in the post. To be slightly more precise, we take the final observed concentration \\(c_n\\) to be our starting concentration, and then assume that the remaining time course of elimination produces an exponential decay, giving us this as our estimate of the tail area:\n\\[\n\\int_{t_n}^{\\infty} c_n \\ e^{-k_e (t - t_n)} \\ dt  = \\frac{c_n}{k_e}\n\\]\nThat’s nice and neat, except for one small problem: we don’t actually know the value of \\(k_e\\). Fortunately, since we’re assuming that the concentration-time curve is exponential in the tail, we expect that a plot of log concentration against time will be linear for the later time points. Better yet, per the one-compartment bolus model, the slope of this plot will give us an estimate of \\(k_e\\):\n\\[\n\\ln c(t) = -k_e t + \\ln c_0\n\\]\nIn a truly shocking turn of events, when we plot the entirely made-up data I’ve used for this post on a logarithmic scale, we see precisely the expected pattern:\n\n\n\n\n\n\n\n\n\nExpressed as R code, we can estimate the elimination rate constant by regressing log concentration against time for the final few data points (about 3 or 4 is considered okay) and using the slope19 as our estimate:\n\n-lm(log(concentration) ~ time, df[4:7, ])$coef[2]\n\n      time \n0.06791393 \n\n\nSo we could refine our auc() function as follows:\n\nauc &lt;- function(time, conc, tail_n = 4) {\n  n &lt;- length(time)\n  tail_points &lt;- (n - tail_n + 1):n\n  auc_body &lt;- sum((time[-1] - time[-n]) * (conc[-1] + conc[-n]) / 2)\n  k_e &lt;- -lm(log(conc[tail_points]) ~ time[tail_points])$coef[2]\n  auc_tail &lt;- unname(conc[n] / k_e)\n  auc_body + auc_tail\n}\n\nApplying it to the toy data set, we can see that the estimated AUC has increased quite considerably:\n\nwith(df, auc(time, concentration))\n\n[1] 215.449\n\n\nThat’s not entirely surprising, given that I deliberately created a toy data set where the plasma concentration was still relatively high at the final time point."
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#further-refinement",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#further-refinement",
    "title": "Non-compartmental analysis",
    "section": "Further refinement",
    "text": "Further refinement\nMy goal in this post was very modest: all I really wanted to do was wrap my head around some core concepts in pharmacokinetics and implement a simple analytic method. It’s worth noting, however, that in real life there are further refinements to NCA that are usually considered. For example, in order to construct an estimate of the tail area I relied on some sensible assumptions (exponential decay) about the shape of the underlying pharmacokinetic function. That’s very sensible, but there’s nothing stopping us making use of the same ideas to refine the estimate of the “AUC body” as well as the “AUC tail”. Using the trapezoid rule (linear interpolation) to construct our estimate makes sense if we have no idea whether the function is concave or convex at a particular point in the curve. But, as the plot below illustrates, this method systematically underestimates concave areas, and overestimates convex areas.\nSuppose, for example, that instead of a one-compartment bolus model, the pharmacokinetics of our drug are better described by a Bateman curve that arises if one assumes a constant rate of absorption and a constant rate of elimination:\n\\[\nc(t) = \\frac{F \\times D \\times k_a}{V_D (k_a - k_e)} \\left( e^{-k_e t} - e^{-k_a t} \\right)\n\\] In this expression \\(F\\) denotes the bioavailability (fraction of the drug that gets absorbed, e.g., 0.8), \\(D\\) is the dose (e.g., 200mmol), and \\(V_D\\) is the volume over which the drug is distributed (e.g. 6L). This kind of system gives us pharmacokinetic profiles that look like this:20\n\n\n\n\n\n\n\n\n\nNow suppose we measured this function at a few data points and used the trapezoid rule to estimate the integral:\n\n\n\n\n\n\n\n\n\nNotice that there are systematic (and non-trivial) overestimates on the right side of the curve where the concentrations are decaying exponentially but the estimator is interpolating linearly. The corresponding underestimates on the left hand side – where the curve is concave – are smaller because we’ve designed the study to measure more frequently early on (which is sensible). Because these overestimates tend to occur in the part of the curve where the elimination process dominates, there are variations of non-compartmental AUC calculations that use nonlinear interpolation for those parts of the concentration-time curve.21 However, I’m not going to dive into that in this post, since I wanted to keep the discussion as simple as possible.\nI am sure you are all deeply thrilled to hear that."
  },
  {
    "objectID": "posts/2023-04-26_non-compartmental-analysis/index.html#footnotes",
    "href": "posts/2023-04-26_non-compartmental-analysis/index.html#footnotes",
    "title": "Non-compartmental analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe specific discipline that refers to the formal modelling aspect to pharmacology is called pharmacometrics.↩︎\nWhenever learning a new thing I try to force myself to be the precise opposite of this person.↩︎\nTypically the blood plasma concentration, which might be something like mass concentration but of course doesn’t have to be. The plots I’m using in this post don’t bother to specify units because I’m not doing that deep a dive yet, but obviously that would… um… matter quite a lot in real life. One should not confuse mass concentration with molar concentration or volume concentration etc etc. So much undergraduate chemistry coming back to me as I write this… it’s kind of fun.↩︎\nRealistically you’d expect to obtain more than one such curve in a real study, and as such you have all the usual issues around aggregating data from possibly-heterogeneous sources. This line of thought leads you naturally to considering population pharmacokinetics, and there will be exactly zero statisticians surprised to learn that this leads you to thinking about mixed models (see, for example, this lovely talk by Joseph Standing. I’m not going to get to that in this post but I did start reading about some of the models used for this purpose and, not gonna lie, they look like fun.↩︎\nI mean, this morning I discovered the PKPDposterior R package that provides Stan wrappers for Bayesian PKPD models and am awfully excited to play with it at a later point.↩︎\nI’m still learning my way around the territory here but even I can see that when constructing a specific model you’ll likely want to think about processes of absorption, distribution, metabolism, and elimination (ADME).↩︎\nThis simplicity also makes it an attractive place for me to start learning about a new field. Learning a new discipline always requires learning the terminology and notation, understanding the assumptions and practices, and so on. I find it much easier to wrap my head around those things in the context of a comparatively-simple statistical technique rather than attempting to do the same thing in the context of a more complex one.↩︎\nIn more complex cases where a steady state is reached (e.g., when doses are administered at regular intervals, eventually the concentration-time curve becomes periodic and the system is in dynamic equilibrium), the interval is computed from some time \\(t\\) after the steady state is reached, to some time \\(t+\\tau\\).↩︎\nOkay, fine, I confess I am oversimplifying here. Noncompartmental analysis is somewhat more general than this. For the purposes of this post I’m restricting myself to thinking about the area under the concentration-time curve, but as noted in review papers (e.g., Gabrielsson & Weiner 2012) there are other curves to which you can apply much the same logic. For example, if you plot the first moment (concentration x time) against time and compute the area under that curve (typically denoted AUMC), you can use this to estimate other quantities that pharmacometricians care about. That being said, I’m not going to go into that here.↩︎\nSlightly confusingly, it appears to be a convention in the literature that, unless otherwise stated, “AUC” refers specifically to the area under the concentration-time curve. Per the previous footnote, the area under the first moment curve is referred to as the “AUMF”. I’d complain about this imprecision, but I too have worked as a research scientist. I am acutely aware that “AUC” also has an equally-confusing default meaning in psychology – in that context, “AUC” is almost always taken to refer to the area under the receiver operating characteristic curve, because signal detection theory often lies in the background as an unstated assumption. Sigh. Science is a cruel mistress.↩︎\nI hate that word. Seriously. If you’re not a statistician, but happen to know a devout frequentist and a devout Bayesian, gather the two of them together and ask them what “nonparametric” means. Then, measure the size of the resulting mushroom cloud. It’s an empty term that doesn’t mean anything. But whatever. The reason I use it here has fuck all to do with Bayesian/frequentist squabbles, and everything to do with the fact that we are deliberately trying our best not to construct a model that makes strong theoretical assumptions. My statistical logic for thinking of this as “nonparametric” follows naturally from the scientific logic for calling it “noncompartmental”.↩︎\nOn top of that, there’s the fact that as soon as one moves past the simplest possible approach, we actually do start making some assumptions about the structure of the concentration-time curve, partially expressing it in terms of a low dimensional model, so there is a sense in which “nonparametric” is a bit misleading. Whatever. I’m not entirely convinced that terminology was ever very helpful.↩︎\nI’ll talk about its limitations from a statistical perspective in a moment, but it’s worth noting that assumes the observations are ordered chronologically, and I’ve not bothered with input checking or anything that I’d want to do in real life to prevent accidental misuse. But that’s not the point of the post and I’m getting off track.↩︎\nOr, more precisely, I’m simply going to ignore them for now. I promise I am not this sloppy when working professionally but honestly right now I’m focusing on learning the basics.↩︎\nI did briefly consider writing something more detailed about this point but it’s sort of silly: the trapezoid rule for numeric integration is the average of the left and right Riemann sums, so if we’re assuming that \\(f()\\) is a Riemann-integrable function, it’s pretty clear that the thing we’re doing here is employing a well-established method for numerically approximating the area under the curve, but only measured up to the last observed time point.↩︎\nWe could go slightly further in this hypothetical analysis that I didn’t bother with and show that under a simple design where we sample the values of \\(t\\) in such a way that as \\(n \\rightarrow \\infty\\) the longest time interval also goes to infinity \\(t_n \\rightarrow \\infty\\), this method will produce an asymptotically consistent estimator for the true AUC. But, honestly, who cares? Asymptotic consistency is the coldest of cold comforts, and guarantees very little in practice. There’s a good reason why pharmacometricians prefer methods that attempt to estimate the tail area in the pre-asymptotic (i.e., real world) scenario.↩︎\nThe reason, of course, being that they are not idiots and they care about making sensible inferences.↩︎\nI suppose this is one of those models that is so widely used that it’s almost weird to cite, but for what it’s worth I’ve found Holz & Fahr (2001) to be a nice introduction to compartment modelling.↩︎\nWell, -1 times the slope, but whatever.↩︎\nOkay yes my toy data set does look rather like a Bateman curve. In truth I made up the values intuitively, but I’m amused to see that they closely adhere to a model that I hadn’t yet read about when I created the data.↩︎\nOne question that I haven’t yet found the answer for is the one that pops up when you look at the curve above: there’s a systematic underestimate that occurs when you don’t have many data points near the peak concentration. In that region the curve is concave. Intuitively it seemed to me that if you’re going to apply corrections in regions where you are pretty sure the curve is convex, it might be wise to do similarly in regions where you’re pretty sure it’s concave. I imagine there’s an answer to this in the literature somewhere.↩︎"
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html",
    "href": "posts/2023-01-14_p5js/index.html",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "",
    "text": "Be sweet to me, baby  I wanna believe in you  I wanna believe  Be sweet  Be sweet to me, baby  I wanna believe in you  I wanna believe in something    – Japanese Breakfast\nOkay, so… I write this blog using quarto, and quarto has native support for observable.js … and observable.js supports third-party javascript libraries such as p5.js executing in code cells… so, like… I can use p5.js to create generative art, inside the browser, inside a blog post? Right?\nApparently the answer to that is yes.\nThere is but one tiny problem. I don’t know anything about observable.js or p5.js. I supposed I’d best remedy that."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#enabling-p5js",
    "href": "posts/2023-01-14_p5js/index.html#enabling-p5js",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Enabling p5js",
    "text": "Enabling p5js\nThe first step in the process is enabling p5.js, which is not one of the core libraries in observable, and is not immediately available. To use a third-party library that exists as an NPM modules we can import it using require().\n\nP5 = require(\"p5\")\n\n\n\n\n\n\nJust like that, thanks to the joy of the jsDelivr CDN, p5.js is now available to me in this post.\nWell, sort of. As you can see from the output,1 the P5 object is a function that takes three inputs. To do anything useful with it, I’ll use a trick I learned from this fabulous notebook by Tom MacWright to run p5.js in “instance mode”. Normally, p5.js works by defining a lot of global objects. That works fine if you’re only doing one “sketch” on a single page, but it’s not so clean if you want to write modular code where a single page (like this one) could contain multiple p5.js sketches.\nTo run p5.js in instance mode, and in a way that plays nicely with observable.js and quarto, I’ll define createSketch as a generator function:\n\nfunction* createSketch(sketch) {\n  const element = DOM.element('div');\n  yield element;\n  const instance = new P5(sketch, element, true);\n  try {\n    while (true) {\n      yield element;\n    }\n  } finally {\n    instance.remove();\n  }\n}\n\n\n\n\n\n\nUsing this approach, each instantiation of P5 is attached to a div element that created when createSketch is called. If you want to know more about how this approach works, it’s probably best to go to the original source that I adapted it from, because Tom has commented it and explained it nicely: observablehq.com/@tmcw/p5"
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-1",
    "href": "posts/2023-01-14_p5js/index.html#donut-1",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 1",
    "text": "Donut 1\nIn keeping with the tradition I’ve set up in the last few blog posts, all the examples are donut themed.2 When calling createSketch I’ll pass an anonymous function that takes a single argument s, the document element to which all the p5 functions are attached. I’ll use the arrow notation, so my code is going to look something like this:\n\ncreateSketch(s =&gt; {\n  // add some p5.js code \n})\n\n\n\n\n\n\nThe idea in p5.js is all the work is done by two functions. The setup function includes code that is called only once, and if you want to draw static images you can do everything at the setup stage. In contrast the draw function is called repeatedly, so you can use that to add dynamic elements.\nHere’s an example of a static sketch that draws a single donut shape using two circles:\n\ncreateSketch(s =&gt; {\n    s.setup = function() {\n      s.createCanvas(500, 500);\n      s.background(\"black\");\n      s.fill(\"red\").circle(250, 250, 100);\n      s.fill(\"black\").circle(250, 250, 30);\n    };\n  }\n)\n\n\n\n\n\n\nIn this example:\n\ncreateCanvas creates the drawing area in which the sketch will be rendered. Arguments are the width and height in pixels\nbackground sets the background colour. The colour specification is flexible: it can be a recognised colour name, a hex string, or a numeric RGB specification\nfill sets the fill colour\ncircle draws a circle: the first two arguments specify the origin of the circle, and the third argument specifies the diameter\n\nI’ve used method chaining here to remind me that the first fill and the first circle go together: writing s.fill(\"red\").circle(250, 250, 100) on a single line helps me group code together conceptually. It’s mostly for my own convenience though."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-2",
    "href": "posts/2023-01-14_p5js/index.html#donut-2",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 2",
    "text": "Donut 2\nOkay Danielle, that’s nice but it’s not that nice. Can we do something a little more interesting? Maybe with some dynamics? Well okay, Other Danielle, since you asked so sweetly, here’s an example with a moving circle that changes colour and traces out a donut shape:\n\ncreateSketch(s =&gt; {\n  \n    s.setup = function() {\n      s.createCanvas(500, 500);\n      s.background(0);\n      s.noStroke();\n    };\n    \n    s.draw = function() {\n      s.translate(\n        100 * s.cos(s.millis() * .001 * s.PI),\n        100 * s.sin(s.millis() * .001 * s.PI),\n      );\n      if (s.random(0, 1) &lt; .1) {\n        s.fill(s.random(0, 255));\n      }\n      s.circle(250, 250, 100);\n    };\n    \n  }\n)\n\n\n\n\n\n\nThis example makes use of some geometry functions included in p5.j (sin, cos, translate), a random number generator (random), and timer that returns the number of milliseconds since the sketch started (millis). These are all documented in the p5.js reference."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-3",
    "href": "posts/2023-01-14_p5js/index.html#donut-3",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 3",
    "text": "Donut 3\nFor the third example we’ll introduce some fonts, adapting an example from observablehq.com/@tmcw/p5. First, I’ll add some CSS to import the Courgette font:\n\n@import url(https://fonts.googleapis.com/css?family=Courgette);\n\n\nNow we can use that font in a p5.js scrolling window:\n\ncreateSketch(s =&gt; {\n  \n    s.setup = function() {\n      s.createCanvas(746, 300);\n      s.textFont('Courgette');\n      s.textStyle(s.BOLD);\n      s.textAlign(s.CENTER, s.CENTER)\n    };\n    \n    s.draw = function() {\n      s.translate(\n        s.millis() * (-0.1) % (s.width + 1000), \n        s.height / 2\n      );\n      s.background('#222222');\n      s.fill('#DC3F74').textSize(100);\n      s.text('Donuts: A Hole World', s.width + 500, 0);\n    };\n    \n  }\n)\n\n\n\n\n\n\nCould life be any more thrilling than this?"
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-4",
    "href": "posts/2023-01-14_p5js/index.html#donut-4",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 4",
    "text": "Donut 4\nWell, maybe it can. We could make it a little more interesting by using webGL to move our donut plots into the… THIRD DIMENSION! (Gasp!)\n\ncreateSketch(s =&gt; {\n\n  s.setup = function() {\n    s.createCanvas(746, 746, s.WEBGL);\n    s.noStroke();\n  }\n\n  s.draw = function() {\n\n    s.background(0);\n\n    let locX = s.mouseX - s.height / 2;\n    let locY = s.mouseY - s.width / 2;  \n    \n    s.ambientLight(60, 60, 60);\n    s.pointLight(190, 80, 190, locX, locY, 100);\n    s.pointLight(80, 80, 190, 0, 0, 100);\n  \n    s.specularMaterial(255);\n    s.rotateX(s.frameCount * 0.01);\n    s.rotateY(s.frameCount * 0.01);\n    s.torus(150, 80, 64, 64);\n  }\n\n})\n\n\n\n\n\n\nIf you move the mouse over the donut3 you’ll see that the light source moves with it."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#donut-5",
    "href": "posts/2023-01-14_p5js/index.html#donut-5",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Donut 5",
    "text": "Donut 5\nFor the final example, I’ll do a tiny bit of object-oriented programming. Inspired by a generative art course by Bernat Ferragut (ga-course.surge.sh) that I was skimming yesterday, I’ll define a Dot class that creates a particle that moves around on the canvas and has the ability to bounce off circular boundaries:\n\nclass Dot {\n  constructor(sketch, x, y, colour, size) {\n    this.s = sketch;\n    this.x = x | 0;\n    this.y = y | 0;\n    this.colour = colour;\n    this.size = size;\n    this.velX = this.s.random(-2, 2);\n    this.velY = this.s.random(-2, 2);\n  }\n\n  on() {\n    this.s.noStroke();\n    this.s.fill(this.colour);\n    this.s.circle(this.x, this.y, this.size);\n  }\n\n  move() {\n    this.x += this.velX;\n    this.y += this.velY;\n  }\n  \n  bounce(radius, inside) {\n    let x = this.x - this.s.width/2;\n    let y = this.y - this.s.height/2;\n    if (\n      inside && x*x + y*y &gt; radius * radius ||\n      !inside && x*x + y*y &lt; radius * radius\n    ) {\n    \n      // https://math.stackexchange.com/a/611836\n      let nx = x / this.s.sqrt(x*x + y*y);\n      let ny = y / this.s.sqrt(x*x + y*y);\n      let vx = this.velX;\n      let vy = this.velY;\n      this.velX = (ny*ny - nx*nx)*vx - 2*nx*ny*vy;\n      this.velY = (nx*nx - ny*ny)*vy - 2*nx*ny*vx;\n    \n    }\n  }\n  \n}\n\n\n\n\n\n\nNaturally, I will use this to draw a donut:\n\ncreateSketch(s =&gt; {\n\n  let n = 100;\n  let dot;\n  let dotList = [];\n  let palette = [\n    s.color(\"#6B1B00\"),\n    s.color(\"#AE8B70\"),\n    s.color(\"#F9FEFB\"),\n    s.color(\"#56382D\") \n  ];\n\n  s.setup = function() {\n    s.createCanvas(746, 746);\n    for(let i = 0; i &lt; n; i++) {\n      let angle = s.random(0, s.TWO_PI);\n      let radius = s.width * s.random(.12, .33);\n      dotList.push(dot = new Dot(\n        s,\n        s.width/2 + s.cos(angle) * radius,\n        s.height/2 + s.sin(angle) * radius,\n        s.random(palette),\n        s.random(1, 5)\n      ));\n    }\n  };\n    \n  s.draw = function() {\n    dotList.map(dot =&gt; {\n      dot.on();\n      dot.move();\n      dot.bounce(s.width * .35, true);\n      dot.bounce(s.width * .1, false);\n    });\n  };\n})\n\n\n\n\n\n\nMmmm…. donuts."
  },
  {
    "objectID": "posts/2023-01-14_p5js/index.html#footnotes",
    "href": "posts/2023-01-14_p5js/index.html#footnotes",
    "title": "Fun and games with p5.js and observable.js in quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn assignment like this would not normally produce any visible output for an observable.js code cell within in a quarto document, but I’ve set output: all for expository purposes.↩︎\nA tradition that, like most things, will last only until I get bored with it.↩︎\nI can’t make up my mind if the colour scheme implies this is a bisexual donut or a trans donut. Oh wait, it’s probably both.↩︎"
  },
  {
    "objectID": "posts/2023-06-15_dark-times/index.html",
    "href": "posts/2023-06-15_dark-times/index.html",
    "title": "On living in dark times",
    "section": "",
    "text": "As a rule, I don’t do politics on this blog. In fact, I have never previously discussed political or social issues on this blog. I’ve scrupulously avoided discussing anything that might carry the hint of politics, because I don’t wish to be drawn into the fray. I’m a data scientist, a statistician, and a generative artist. I do not have the temperament required to involve myself in matters political, and it is fundamentally not what I wish to be doing with this blog. However, sometimes exceptions need to be made, and this post is one such exception.\nOne of the feeds I follow fairly regularly is Erin in the Morning, a substack written by Erin Reed. Erin is an activist and independent journalist who writes about transgender issues, and one of the few people who diligently attempts to document the full scope of the anti-trans laws that are currently being passed across large swathes of the United States, and the consequences that those laws are having on transgender people who live there. It’s grim reading.\nA couple of days ago she published a very depressing post entitled US internal refugee crisis: 130-260k trans people have already fled, documenting the scale of the crisis currently affecting trans people in the US, and presenting personalised accounts from people who have been forced to leave their lives behind and flee to safer territory. A staggeringly large number of trans people have been internally displaced. They are so frightened by what is happening right now that they have upended their lives and moved in the hope of finding safety.\nIf you haven’t read the article yet, read it first… I’ll wait.\nOkay, you’ve read it now, right? Good.\nAt this point, if you’re a statistician (or any other data-focused person) you have one of two reactions. If you’re a decent human being, your reaction will be something like this:\nUnfortunately, many statisticians will have this reaction:\nAdmittedly the second group would probably choose to phrase their reaction differently, because they don’t want to admit that statistical pedantry is not an appropriate response to a catastrophic situation. Nevertheless, I’ve met statisticians before. We all have. We know perfectly well that pedantry is precisely what many of them will resort to when presented with an article like this one.\nAnd so, in order to cut that off at the pass and do my best to forestall anyone who might be tempted to dismiss the substance of Erin’s point by nitpicking the statistics I’m going to redo her calculations in a somewhat more statistically careful way, and you can decide for yourself whether you want to be an asshole about it."
  },
  {
    "objectID": "posts/2023-06-15_dark-times/index.html#what-proportion-of-the-transgender-population-has-been-displaced",
    "href": "posts/2023-06-15_dark-times/index.html#what-proportion-of-the-transgender-population-has-been-displaced",
    "title": "On living in dark times",
    "section": "What proportion of the transgender population has been displaced?",
    "text": "What proportion of the transgender population has been displaced?\nThe data source is this article by Data For Progress.\nFrom the pdf report embedded at the bottom of the page, the point estimate suggests 8% of transgender adults (defined here as people aged 18+) in the United States have been forced to migrate interstate. However, the weighted N associated with that point estimate is only 166, because sampling transgender people is hard. We don’t have any more detailed breakdown to work with, but as a “back of the envelope” style calculation, I’ll treat this as if it were a simple random sample in which 13 of 166 transgender adults indicated that they have already been forced to move interstate because of the current crisis. To the extent that this is a reasonable first-pass approximation, a simple beta-binomial model will suffice to provide an uncertainty estimate:\n\\[\n\\begin{array}{rcl}\n\\theta & \\sim & \\mbox{Beta}(1, 1) \\\\\nn & \\sim & \\mbox{Binomial}(\\theta, N = 166)\n\\end{array}\n\\]\nGiven \\(n = 13\\) displaced people from a simple random sample of \\(N = 166\\) transgender people, the posterior proportion of displaced trans people is given by a Beta(14, 154) distribution.1 As such our 95% equal-tail credible interval is straightforwardly calculated as follows:\n\nqbeta(c(.025, .975), 14, 154)\n\n[1] 0.0465905 0.1294375\n\n\nIn other words, the data from this survey suggest that somewhere between 5% and 13% of all transgender adults in the United States have been internally displaced as a consequence of the deluge of anti-trans legislation in the last few years.2 How many people is that, really? To answer that question we need to know something about how many trans people there are in the United States."
  },
  {
    "objectID": "posts/2023-06-15_dark-times/index.html#what-proportion-of-the-us-population-is-transgender",
    "href": "posts/2023-06-15_dark-times/index.html#what-proportion-of-the-us-population-is-transgender",
    "title": "On living in dark times",
    "section": "What proportion of the US population is transgender?",
    "text": "What proportion of the US population is transgender?\nFor this we can use a relatively recent survey by the Williams Institute. The webpage provides point estimates in a digestible form, but Tables 4 and A4 of the associated pdf report includes a 95% credible interval that suggests the adult transgender population in the United States (where again age 18+ is used as the cutoff) is somewhere between 816,644 and 1,964,330 people. Or, to express it as a percentage, somewhere between 0.32% and 0.77% of the US adult population of 255,201,250 persons identifies as transgender (the point estimate is 0.52%).\nIn my ideal world I’d have access to the actual posterior distribution from the Williams Institute modelling, but alas I do not. However, since this is intended as a back-of-the-envelope style calculation, I’ll again try to make some sensible assumptions. In most situations I’d be willing to assume that the posterior is approximately normal, but that doesn’t work here because the percentages are too close to zero. Instead what I’ll do is use a beta distribution and choose parameters that ensure the relevant quantiles approximately mirror the numbers from the Williams Institute study:3\n\nqbeta(c(.025, .5, .975), 20.65, 3922.84)\n\n[1] 0.003230431 0.005153068 0.007716206\n\n\n\n\nParameter estimation code\noptim(\n  par = c(20.3, 3800), # the values I hand tuned originally\n  fn = \\(par) {\n    prd &lt;- qbeta(c(.025, .5, .975), par[1], par[2])\n    obs &lt;- c(.0032, .0052, .0077)\n    sum((obs - prd)^2)\n  }\n)\n\n\nIt’s awfully crude, but it works: the 95% equal-tail intervals that you’d get if this were the real posterior match the numbers reported by the Williams Institute, the distribution is bounded appropriately, and the point estimate (in this case the median) is pretty decent too. Good enough for the back of an envelope calculation I’d say."
  },
  {
    "objectID": "posts/2023-06-15_dark-times/index.html#estimating-the-number-of-displaced-persons",
    "href": "posts/2023-06-15_dark-times/index.html#estimating-the-number-of-displaced-persons",
    "title": "On living in dark times",
    "section": "Estimating the number of displaced persons",
    "text": "Estimating the number of displaced persons\nOkay, now I have some (slightly crude) posterior densities to express what we know about (a) the proportion of adults in the United States are transgender, and (b) the proportion of transgender adults in the United States that have been displaced courtesy of the anti-trans legislation sweeping the nation. Again using the numbers from the Williams Institute study as the basis for the calculation, I’ll assume that the adult population of the US is approximately 255,201,250 persons. Now, I personally don’t know how to convolve two beta distributions analytically, but it’s not even slightly hard to do numerically:\n\nsim &lt;- tibble::tibble(\n n_adults = 255201250,\n prop_trans = rbeta(1000000, shape1 = 20.65, shape2 =  3922.84),\n prop_displaced = rbeta(1000000, shape1 = 14, shape2 = 154),\n n_displaced = n_adults * prop_trans * prop_displaced\n)\n\nHaving done so, we can plot a distribution reflecting what we know about the number of transgender adults who have been displaced:\n\nlibrary(ggplot2)\nggplot(sim, aes(n_displaced)) + \n  geom_histogram(bins = 100) + \n  scale_x_continuous(labels = scales::label_comma())\n\n\n\n\n\n\n\n\nSo… how many transgender people within the United States do we estimate have already been forced from their homes as a consequence of the dire political climate there? Here’s the headline number:\n\nmean(sim$n_displaced)\n\n[1] 111379.2\n\n\nAbout 111000 people. That’s… a lot, and that number doesn’t even include the families of transgender adults, or transgender children, or the families of transgender children. I mean, there aren’t many of us. We are a small population, and this is a humanitarian disaster for transgender people in the United States. It’s something that has been building for several years now, and every trans person knows it.\nThe precise scale of the disaster isn’t entirely clear from the data. The point estimate of 111k people could be out by a factor of 2 in either direction, which you can see by calculating the 95% credible interval:\n\nquantile(sim$n_displaced, c(.025, .975))\n\n     2.5%     97.5% \n 51998.81 199427.60 \n\n\nBetween 52000 and 199000 transgender adults have been displaced. No matter how you look at it, a lot of people have been forced to flee already.\nAt some point the rest of the American population will start to actually do something about this, right? I mean, something other than make it worse or waste your time and effort by whining about the threat to society posed by trans women in sports and asking “what is a woman?”."
  },
  {
    "objectID": "posts/2023-06-15_dark-times/index.html#footnotes",
    "href": "posts/2023-06-15_dark-times/index.html#footnotes",
    "title": "On living in dark times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHuge thank you to Martin Modrák for noticing the mistake in which I’d originally specified a Beta(14, 167) distribution here like an idiot.↩︎\nYou could do the same thing in a frequentist way, of course, but that would be no less crude than this Bayesian method, and anyway I already did that and found essentially the same answer. This isn’t a situation where Bayes-vs-orthodox matters very much. In the real world, the nuance is entirely around the SRS assumption and the accuracy of the responses. In that respect I’m of course oversimplifying, but let’s be honest… how much do you really think this would change things? Be honest.↩︎\nThe original version of this post used hand tuned parameter values because I had a brain fade and forgot that it’s absurdly easy to find least squares estimates with optim().↩︎"
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html",
    "title": "Setting CRAN repository options",
    "section": "",
    "text": "The shiny new work laptop arrives. Pristine and beautiful in its factory-fresh state. I am in love.\nI remain in love right up the moment I remember that with new work laptop comes the peculiar torture of setting up the machine. My gorgeous little Dell XPS 13 shipped with Windows 11, and while I do quite like Windows these days, I’ve become very accustomed to working in linux, so my first task was to install Ubuntu 20.04. These days that’s a pretty easy task, and the Ubuntu installer was even thoughtful enough to give me an option to enable full disk encryption. It all went smoothly. Yay me!\nEquipped with my fabulous new operating system, my next steps were to install R and RStudio, and for the first time in my life I was smart enough to remember to install the latest version of git along with the build-essential packages that I’m pretty much guaranteed to need the moment I need to build anything from source. Yay me again!\nThen comes the horror. Installing R packages. On linux. A small part of me dies.\nI’m sure every linux-based R user shares my pain and needs no explanation, but some of the gentler souls who use Windows or Mac OS may not be aware of how tiresome package installation is on linux. The problem that linux users face is that CRAN does not maintain binaries for linux, so every time a linux user wants to install a package, it has to be built locally from the source code. This is both time consuming and frustrating, and very often you have to go hunting around to discover what other system dependencies need to be installed. So many tears have been shed over this.\nSo.\nMany.\nTears."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#rstudio-package-manager",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#rstudio-package-manager",
    "title": "Setting CRAN repository options",
    "section": "RStudio package manager",
    "text": "RStudio package manager\nRecently, however, I have become aware that a better world is possible thanks to the magic of RStudio package manager. I’d sort of known that this existed as an option, but it wasn’t until today that I realised that — in addition to the fancy commercial options — RStudio maintains a public package manager as a free service: the FAQ page is here. Anyone can configure R to install packages from the RStudio public package manager, if they want to.\n\n\n\n\n\n\n\n\n\nBut first a tiny bit of context… back in the distant part there was this strange, nightmarish time where I was teaching students R, but RStudio was not yet a thing. Many of the little niceties that RStudio users now take for granted didn’t yet exist. In those dark years I had to spend a lot of time explaining to students that CRAN — the comprehensive R archive network — isn’t actually a single website that contains lots of R packages. It’s more like a whole network of mirrors distributed all over the world, and you’d have to manually choose which one you wanted to install packages from. It was mildly annoying. It’s considerably simpler now, because you can use the cloud.r-project.org service that automatically directs you to an appropriate server. In fact, if you’re using RStudio you’ve probably been using this service all along.\nRStudio package manager provides a modern alternative: it works like a CRAN mirror, but it has a lot of additional functionality. It has broader coverage, for instance: it includes R packages on Bioconductor as well as packages on CRAN. For my purposes, however, the attractive property is that it hosts binaries suitable for Ubuntu and other flavours of linux.\n“But how do I try it out, Danielle?” I hear you ask.\nI’m so glad you asked, dear reader, because it’s so much easier than it sounds."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#method-1-rstudio-settings",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#method-1-rstudio-settings",
    "title": "Setting CRAN repository options",
    "section": "Method 1: RStudio settings",
    "text": "Method 1: RStudio settings\nIf you’re using RStudio, the easiest way to switch to RStudio PPM is to change your settings inside RStudio. Go to the RStudio Tools menu and select Global Options. When the popup window appears, click on Packages. You’ll see a screen that looks like this:\n\n\n\n\n\n\n\n\n\n\nIf it says “RStudio Global CDN” next to “Primary CRAN repo”, then you’re using cloud.r-project.org as your CRAN repository. To switch to RStudio PPM, click on the “change” button. It will bring up a list of CRAN mirrors, and if you want you can choose one of those. However the RStudio PPM isn’t technically a CRAN mirror, so it’s not listed there. If you want to switch to using the RStudio PPM, you have to enter the URL manually.\nSo what URL do you want? Well, it depends on whether you want to install packages from binaries or from source, and on what operating system you’re using. I’m on Ubuntu 20.04, “Focal Fossa”, and the URL that serves binaries for my operating system is:\nhttps://packagemanager.rstudio.com/all/__linux__/focal/latest\nHere’s me in the process of entering the URL:\n\n\n\n\n\n\n\n\n\n\nOkay, but what if you’re not on Ubuntu 20.04? If you’re on a different version of Ubuntu or some other operating system, you can find the link you need from the package manager setup page. The relevant part of the page should look something like this:\n\n\n\n\n\n\n\n\n\n\n\nTo get the URL you’re looking for, click on the “change” link to choose your operating system, or toggle between the binary and source options."
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#method-2-edit-your-r-profile",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#method-2-edit-your-r-profile",
    "title": "Setting CRAN repository options",
    "section": "Method 2: Edit your R profile",
    "text": "Method 2: Edit your R profile\nThere are a couple of limitations to this method. The most obvious one is that it’s no help if you don’t use RStudio, and even for RStudio users it can be awkward if you don’t always use RStudio. If that’s your situation, you may want to manage your CRAN repository links by editing your R profile. To do this, open the .Rprofile file — using usethis::edit_r_profile(), for example — and add the following line:\n\noptions(repos = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\")\n\nYou’ll need to restart your R session for this change to take effect.\nIf you want to be fancy, you can list multiple URLs. If the package you want to install is not found at the first link, R will try the second link, and so on. That can be useful. For instance, this is what I use in my R profile:\n\noptions(repos = c(\n  binary = \"https://packagemanager.rstudio.com/all/__linux__/focal/latest\",\n  source = \"https://packagemanager.rstudio.com/all/latest\",\n  CRAN = \"https://cloud.r-project.org\",\n  djnavarro = \"https://djnavarro.r-universe.dev\"\n))\n\nUsing this configuration, R will look for a suitable binary version of the package on RStudio PPM. If that fails it will try to install from RStudio PPM by building the package from the source code. If that fails, it checks CRAN in the usual way. Finally, if that fails, it looks to see if the package I’m requesting is one of the packages I listed at djnavarro.r-universe.dev, my very own tiny corner of the R-universe. Obviously, you’re very unlikely to want to use my R-universe repository since it only consists of a handful of my own packages: but it’s quite handy for me since they aren’t all on CRAN!"
  },
  {
    "objectID": "posts/2022-01-10_setting-cran-repositories/index.html#epilogue",
    "href": "posts/2022-01-10_setting-cran-repositories/index.html#epilogue",
    "title": "Setting CRAN repository options",
    "section": "Epilogue",
    "text": "Epilogue\nIf you’re a Windows or Mac user, you might not be aware of how much of a game changer this is for linux users. For example, in my previous blog post I wrote about my experiences getting started using Apache Arrow. I’m a big fan of Arrow — which should come as no surprise as I’ve recently started work at Voltron Data — but if you’re installing the arrow R package on linux, it’s extremely time consuming to build all the C++ libraries from source. It was a little cumbersome, but after switching to RStudio PPM, I can install arrow on my Ubuntu machine using the exact same command I’d use on Windows…\n\ninstall.packages(\"arrow\")\n\n…and everything works. As easy on linux as it is on other operating systems! Yay! 🎉"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html",
    "href": "posts/2024-03-03_julia-plots/index.html",
    "title": "Plotting data in Julia",
    "section": "",
    "text": "And so we come to the third post in my extremely ill-considered foray into learning Julia. In the first part of the series I taught myself some of the foundations for writing Julia code, in the second part I discussed data wrangling in Julia, and so in the third and – dear god in heaven please – last of them, I’ll take a look at data visualisation using the Plots package, and using PalmerPenguins as my data set.\nIt will be brief this time, right? Right????"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#penguins-data",
    "href": "posts/2024-03-03_julia-plots/index.html#penguins-data",
    "title": "Plotting data in Julia",
    "section": "Penguins data",
    "text": "Penguins data\nIt is hardly a deep insight to say this, but if you’re going to play around with data visualisation tools it does help somewhat to have some data that you can plot. In the last post I pretty much exhausted my ability to look at the Star Wars data set that comes bundled in with the dplyr R package, so I’m picked something different this time. For no particular reason I decided to go with the Palmer Penguins data set that also exists in R, and has a Julia package in PalmerPenguins. However, due to a painful little episode part way through writing all these posts I mistakenly decided that the PalmerPenguins package was causing me problems in the context of this quarto blog post (it wasn’t… the actual problem was that I had my Julia environment configured incorrectly), so instead I ended up writing a copy of the data to a CSV file and used that instead. Sigh.\nOkay, let’s start by loading the packages I’ll need for this post:\n\nusing CSV\nusing DataFrames\nusing Plots\nusing StatsPlots\n\nAmazing. Thrilling. Et cetera. Now that we’re all overwhelmed by the feeling of rising anticipation, let’s load the data set from the CSV file:\n\npenguins = DataFrame(CSV.File(\"penguins.csv\"; missingstring = \"NA\")) \n\n344×8 DataFrame319 rows omitted\n\n\n\nRow\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\nString15\nString15\nFloat64?\nFloat64?\nInt64?\nInt64?\nString7?\nInt64\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\nmissing\nmissing\nmissing\nmissing\nmissing\n2007\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n7\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nfemale\n2007\n\n\n8\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmale\n2007\n\n\n9\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nmissing\n2007\n\n\n10\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nmissing\n2007\n\n\n11\nAdelie\nTorgersen\n37.8\n17.1\n186\n3300\nmissing\n2007\n\n\n12\nAdelie\nTorgersen\n37.8\n17.3\n180\n3700\nmissing\n2007\n\n\n13\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nfemale\n2007\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n333\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nfemale\n2009\n\n\n334\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmale\n2009\n\n\n335\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmale\n2009\n\n\n336\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nfemale\n2009\n\n\n337\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmale\n2009\n\n\n338\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nfemale\n2009\n\n\n339\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nfemale\n2009\n\n\n340\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmale\n2009\n\n\n341\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nfemale\n2009\n\n\n342\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmale\n2009\n\n\n343\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmale\n2009\n\n\n344\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nfemale\n2009\n\n\n\n\n\n\nIndeed, that is a lot of penguins. Precisely what I was looking for. I have some data, now I can start drawing some plots.\n\n\n\n\n\nThe Handmaids Tale by Margaret Atwood, because obviously what every queer person and woman wants to be thinking about in 2024 is what’s probably going to happen to us in the near future"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#scatter-plots",
    "href": "posts/2024-03-03_julia-plots/index.html#scatter-plots",
    "title": "Plotting data in Julia",
    "section": "Scatter plots",
    "text": "Scatter plots\nAs you might imagine, there’s more than one way you can go about constructing a data visualisation in Julia. For example, there’s the AlgebraOfGraphics system that appears share some design philosophy with the ggplot2 package in R. There’s also Compose, which aims to be a modernised version of the grid package in R, and the Gadfly data visualisation system built on top of it. Any of these might have been better choices for me to explore in the first instance, but for whatever reason1 I chose instead to look at the Plots package and its extension package StatsPlots.\nThe impression I get from playing around with Plots/StatsPlots is that its design has more in common with the base graphics system in R than with ggplot2. There’s a generic function plot() that you can use to construct plots, and lots of more specific functions that are essentially wrappers to plot(). For example, I’m going to start by drawing a scatter plot. I could use the wrapper function scatter() for this purposes, but you don’t actually need to do this because it’s effectively the same thing as calling plot() with different defaults. For learning purposes I find it helpful to understand what the different arguments to plot() actually do, so I’ll avoid using the convenience functions here.\nOkay, so here’s a simple scatter plot that plots the bill length of each penguin against the bill depth:\n\nplot(\n  penguins.bill_length_mm, \n  penguins.bill_depth_mm, \n  seriestype=:scatter,\n  size=(500,500)\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code chunk, the first argument specifies the variable to be plotted on the x-axis and the second specifies the variable to be plotted on the y-axis. No surprises there. The key thing to note is that in the later arguments I’ve specified two plot attributes. The seriestype attribute is what gives me a scatter plot (each observation is plotted with a single discrete plot marker), whereas the size attribute is used to control the size of the image produced in the output (in pixels).\nCool. Well, I can definitely draw something. That feels like a win."
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#using-the-df-macro",
    "href": "posts/2024-03-03_julia-plots/index.html#using-the-df-macro",
    "title": "Plotting data in Julia",
    "section": "Using the @df macro",
    "text": "Using the @df macro\nOne thing that immediately irritates me about the code I wrote in the last section is that I’ve was passing the raw vectors penguins.bill_length_mm and penguins.bill_depth_mm to the plotting function. When you’re working with a data frame that always feels clunky to me. What you really want to do is just use the column names :bill_length_mm and :bill_depth_mm. On the surface though that seems a little tricky to do, because the plots() function doesn’t necessarily need to be given data that are contained within a data frame.\nThis is where the @df macro comes in handy. Somehow, I have made it three posts into Julia and I have not yet actually had to use a macro for anything, but now is the time. I’ll talk about macros in a just a moment, but for now let’s simply note that I can prefix my call to plot() with a magic bit of syntactic sugar, and now all of a sudden I can simply pass column names and everything works:\n\n@df penguins plot(\n  :bill_length_mm,\n  :bill_depth_mm,\n  seriestype=:scatter,\n  group=:species,\n  size=(500,500)\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI’ve made a nice little change in this version of the plot too. By mapping the group attribute onto the :species column in the penguins data set, I’ve ended up with a plot that shows each of the three penguin species in a different colour and gives me a nice little explanatory legend.\nI can take this line of thinking a little further and modify other relevant attributes:\n\n@df penguins plot(\n  :bill_length_mm,\n  :bill_depth_mm,\n  seriestype=:scatter,\n  group=:species,\n  title=\"Palmer Penguins\",\n  xlabel=\"Bill Length (mm)\",\n  ylabel=\"Bill Depth (mm)\",\n  size=(500,500)\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow I have a plot that has a mildly informative title, and axis labels that are nicer to read. Definitely making some progress now.\n\n\n\n\n\nThe Day of the Triffids by John Wyndham. Well, it’s more optimistic than Handmaids Tale I guess"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#macros",
    "href": "posts/2024-03-03_julia-plots/index.html#macros",
    "title": "Plotting data in Julia",
    "section": "Macros",
    "text": "Macros\nSiiiiiiigh. Okay, I used a macro. I suppose I’d better take a quick look at how those things work, right? Macros are part of the Metaprogramming toolkit in Julia, and can be used to manipulate Julia expressions passed by the user, making it possible for code that wouldn’t otherwise be valid Julia to be executed. In essence we’re talking about something qualitatively similar to “non-standard evaluation” in R.\nBased on that description, you can kind of see what the @df macro is doing in the example plots from the last section. It’s taking a call to plots() that specifies only the column names that are associated with a particular vector of observations, and replaces them with the actual data values stored in the relevant data frame when the code is evaluated.\nYou can see this in action here:\n\n@df penguins println(:bill_length_mm[1:3])\n\nUnion{Missing, Float64}[39.1, 39.5, 40.3]\n\n\nIn this code, what @df is doing is transforming :bill_length_mm[1:3] (which really shouldn’t work at all) into penguins.bill_length_mm[1:3]. So as a consequence, the code that actually gets executed here is something like println(penguins.bill_length_mm[1:3]), and you get sensible output.\nWell, sort of. I haven’t quite looked into this in a lot of detail yet, and I’m not quite at the point where I’m really prepared to start writing macros of my own, but it does look like @df is slightly more sophisticated, which you can see by using the @macroexpand macro to see what actually gets executed in my little example above:\n\n@macroexpand @df penguins println(:bill_length_mm[1:3])\n\n:(((var\"##297\"-&gt;begin\n          ((var\"##bill_length_mm#298\",), var\"##299\") = (StatsPlots).extract_columns_and_names(var\"##297\", :bill_length_mm)\n          (StatsPlots).add_label([\"(bill_length_mm)[13]\"], println, var\"##bill_length_mm#298\"[1:3])\n      end))(penguins))\n\n\nOkay yeah, there’s a little bit more going on than meets the eye (presumably in part because @df is designed to work in the context of the Plots package), but the basic idea makes sense at least. I’m happy to move on and defer a proper dive into macros for a future occasion.\n\n\n\n\n\nThe Earthsea Quartet by Ursula Le Guin. Much better!"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#violin-plots",
    "href": "posts/2024-03-03_julia-plots/index.html#violin-plots",
    "title": "Plotting data in Julia",
    "section": "Violin plots",
    "text": "Violin plots\nAt this point I am getting exhausted. I have written far more on Julia than I intended to, so I’m going to do one more plot and call it a day. In my previous examples I used the seriestype argument to plot() ensure that what I got out at the end was a scatter plot. I could alternatively set seriestype=:violin to get a violin plot.\nHere’s an example, just to prove to myself that I understand plot() well enough to create various different kinds of data visualisations. In the code below I’ll first define a bill_lengths data frame that contains only the columns I need and – importantly – removes the missing values (because the violin series can’t handle missing data). Then I’ll use plots() to create a violin plot:\n\nbill_lengths = penguins |&gt; \n  d -&gt; subset(d, :bill_length_mm =&gt; b -&gt; .!ismissing.(b)) |&gt;\n  d -&gt; select(d, [:species, :bill_length_mm])\n\n@df bill_lengths plot(\n  string.(:species),\n  :bill_length_mm,\n  seriestype=:violin,\n  legend=false,\n  xlabel=\"Species\",\n  ylabel=\"Bill Length (mm)\",\n  size=(500,500)\n)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYes, that’s what I wanted. Good enough.\n\n\n\n\n\nA science fiction omnibus edited by Brian Aldiss. Yes I was getting lazy at this point"
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#wrap-up",
    "href": "posts/2024-03-03_julia-plots/index.html#wrap-up",
    "title": "Plotting data in Julia",
    "section": "Wrap up",
    "text": "Wrap up\nThis has been a long series of posts, written all in one go. I was originally planning to write only the one blog post: I mean, all I wanted to do here was teach myself a tiny bit of Julia and scribble down a few notes. But learning a new language always involves introducing yourself to a lot of new concepts, and so the post got very long and needed to be broken down into parts.\nBesides, I’ve really enjoyed learning the basics of Julia. It feels surprisingly comfortable to me, capturing a lot of what I really love about R and also reminding me of the bits about Matlab that I didn’t hate. But it’s also designed for performance in a way that both R and Matlab sometimes struggle with (e.g., in R the solution to “how to I make it fast?” is so often “rewrite the slow bit in C++”), so I guess I can see why a lot of people I admire have a lot of positive things to say about Julia.\nSo yeah. I’m exhausted. I’ve written too much. But I’m happy nevertheless."
  },
  {
    "objectID": "posts/2024-03-03_julia-plots/index.html#footnotes",
    "href": "posts/2024-03-03_julia-plots/index.html#footnotes",
    "title": "Plotting data in Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHonestly, at this point I don’t even know why I’m making the choices I’m making in this increasingly-unhinged series of posts↩︎"
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html",
    "href": "posts/2024-01-09_emax-models/index.html",
    "title": "Notes on the Emax model",
    "section": "",
    "text": "I’m back at work after my all-too-brief summer vacation, and thankfully it’s not too busy since everyone else has been on break too. It’s given me a bit of breathing room to do some reading for professional development purposes, and – since I find it helps me a lot to write up my notes – it’s time for another pharmacometrics post. This time around I’m going to talk about the “Emax model” that is often used in exposure-response analyses. The Emax model is based on the Hill equation, and is used to model a continuous-valued “effect” or “response” that is observed when a drug is administered."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#the-model",
    "href": "posts/2024-01-09_emax-models/index.html#the-model",
    "title": "Notes on the Emax model",
    "section": "The model",
    "text": "The model\nTo introduce the basic model, let’s have some notation:\n\n\\(E\\) denotes the observed biological effect, or response. In the context of this kind of model, the response is a continuous variable bounded below at 0 and has some theoretical upper bound corresponding to the maximum possible effect, denoted \\(E_{max}\\).\n\\(C\\) denotes the plasma concentration (typically the molar concentration, in this context) of the molecule that produces the effect, either the drug itself or a metabolite.\n\nThe goal in this exercise is to model the effect \\(E\\) as a function of the concentration \\(C\\). It is therefore a pharmacodynamic model (modelling the effect of a drug at a given concentration) rather than a pharmacokinetic model (modelling the concentration of a drug when administered in a particular dose). Ignoring the statistical issues around measurement error etc (errors are typically assumed to be log-normally distributed), here’s what the Emax model asserts about the relationship between \\(E\\) and \\(C\\):\n\\[\nE = E_{max} \\frac{C^n}{{EC}_{50}^n + C^n}\n\\]\nThere are three parameters in this model:\n\nAs mentioned earlier, \\(E_{max}\\) is the maximum value of the effect\n\\({EC}_{50}\\) is the concentration that produces a 50% of the maximum effect. That is, it refers to the concentration \\(C\\) at which \\(E = E_{max} / 2\\))\n\\(n\\) is the Hill coefficient, which controls the steepness of the curve. In the “simple” model \\(n\\) is fixed at 1, but in practice that often doesn’t work.\n\nThis is a model in which the effect \\(E\\) increases monotonically but with constantly decreasing slope, and asymptotically approaches \\(E_{max}\\) as \\(C \\rightarrow \\infty\\). More precisely, it’s a model in which \\(E\\) is described by a three-parameter logistic function of \\(\\ln C\\),\n\\[\nE =  \\frac{E_{max}}{1 + \\exp(-n(\\ln C - \\ln EC_{50}))}\n\\] where \\(EC_{50}\\) is a location parameter that shifts the curve (expressed as a function of \\(\\ln C\\)) left or right, \\(n\\) is a gain parameter that controls the steepness of the curve, and \\(E_{max}\\) is the maximum height of the logistic curve.\nAfter a little rearrangement, we can also express this relationship as follows:\n\\[\n\\ln \\left(\\frac{E}{E_{max} - E} \\right) = n(\\ln C - \\ln EC_{50})\n\\]\nHere’s what it looks like:\n# define the emax model\nemax &lt;- function(conc, emax, ec50, n = 1) {\n  (emax * conc^n) / (ec50^n + conc^n)\n}\n\n# load packages\nlibrary(ggplot2)\nlibrary(tibble)\n\n# a fake data set\ndat &lt;- tibble(\n  concentration = seq(1, 10000),\n  response = emax(concentration, emax = 100, ec50 = 100) \n)\n\n# plot the data on linear and logarithmic x-axis scales\np &lt;- ggplot(dat, aes(concentration, response)) + geom_path() + theme_bw()\np + coord_cartesian(xlim = c(0, 2500))\np + scale_x_log10() + xlab(\"concentration (log scale)\")\n\n\n\n\n\n\n\n\n\n\nThis is not the only version of the Emax model. For example, if there is some baseline level of effect (i.e., the curve starts at \\(E_0\\) not 0), then we would have a relationship of the following form:\n\\[\nE = E_0 + E_{max} \\frac{C^n}{{EC}_{50}^n + C^n}\n\\]\nAlong the same lines, notice that this is a model that implicitly assumes that the drug is an agonist: increasing the concentration \\(C\\) increases the effect \\(E\\). There is also a version that can be use for drugs that decrease the effect:\n\\[\nE = E_0 - I_{max} \\frac{C^n}{{IC}_{50}^n + C^n}\n\\]\nwhere again \\(E_0\\) is the baseline effect, and the inhibition effect is described in terms of a maximum inhibition \\(I_{max}\\) and the concentration that produces 50% of the maximum inhibition \\({IC}_{50}\\).\nFor the purposes of this post I’m just going to consider the basic model."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#why-this-model",
    "href": "posts/2024-01-09_emax-models/index.html#why-this-model",
    "title": "Notes on the Emax model",
    "section": "Why this model?",
    "text": "Why this model?\nLooking at this from the perspective of a scientist with a strong statistics and modelling background but who is still relatively new to pharmacometrics, I completely understand the value of the three-parameter logistic model: it’s used in many different disciplines, and I’ve encountered it before in psychometric contexts (and others). What wasn’t immediately clear to me is why it is considered sensible to model the effect \\(E\\) of a drug as a logistic function of the log-concentration \\(\\ln C\\). If I’ve learned nothing else from my long career in science that has somehow bounced around from discipline to discipline, it’s that if you don’t understand the theoretical foundations that underpin why researchers in the field use this specific statistical model and not that other possible statistical model, you’ll eventually make a silly mistake. I’ve made a lot of silly mistakes in my life and would like to minimise the chances of making even more.\nWith that in mind, I turned to Pharmacokinetic and Pharmacodynamic Data Analysis (5th ed) by Johan Gabrielsson and Daniel Weiner, which is the book I’ve been reading at work to try to get myself up to speed on these kinds of things. It turns out that the basic ideas here can be derived from the law of mass action, which I vaguely recall from my long-disused undergraduate chemistry classes, and Chapter 3 of the book shows how this is done. What follows are my own notes based on that section of the book, and as is traditional for such acknowledgements I note that I’ve lifted the core ideas from the authors, but if there are mistakes then yeah those are probably mine alone.\nSo here goes."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#applying-the-law-of-mass-action",
    "href": "posts/2024-01-09_emax-models/index.html#applying-the-law-of-mass-action",
    "title": "Notes on the Emax model",
    "section": "Applying the law of mass action",
    "text": "Applying the law of mass action\nFirst, let’s start by switching notation slightly. Let \\(R\\) refer to a receptor, a macromolecule that is involved in signalling within biological systems (e.g., cell surface receptors). Receptors can bind to chemical messengers (e.g., ligands outside a cell), that triggers some physiological response (e.g., a messenger is released inside a cell). In the context of the Emax model, we’re implicitly assuming a mechanism of action in which the drug (or metabolite) is an agonist molecule \\(A\\) that can bind to the receptor (forming the compound molecule \\(AR\\)), and trigger some response that produces the drug effect. Since this is a chemical reaction that can go both ways, we can denote it as follows:\n\\[\nA + R \\ \\leftrightharpoons \\ AR\n\\]\nThe concentrations of the receptor \\(R\\), the agonist molecule \\(A\\), and the compound \\(AR\\) are denoted \\([R]\\), \\([A]\\) and \\([AR]\\) respectively.\nOkay, so now for some science rather than mere notation. The law of mass action tells us that the rate of the forward reaction \\(A + R \\rightarrow AR\\) is proportional to the product of the concentrations of the two reactants, \\([A] \\times [R]\\), with \\(k_1\\) denoting the constant of proportionality. Similarly, the rate of the backward reaction \\(AR \\rightarrow A + R\\) is proportional to the concentration of the compound \\([AR]\\), with proportionality constant \\(k_{-1}\\). This gives us the differential equation to describe how the concentration of the compound \\(AR\\) changes over time:\n\\[\n\\frac{d}{dt} [AR] \\propto k_1 [A][R] - k_{-1} [AR]\n\\]\nWhen this reaction is at equilibrium \\(d/dt [AR] = 0\\) by definition, so it follows that \\(k_1 [A][R] = k_{-1} [AR]\\) and therefore\n\\[\n\\frac{[A][R]}{[AR]} = \\frac{k_{-1}}{k_1} \\overset{def}{=} K_d\n\\] where \\(K_d\\) is referred to as the equilibrium dissociation constant for this reaction and defined as the ratio of the rate constants for the forward and backward reactions.\nNext, we define \\([R_t]\\) to be the total concentration of receptors regardless of whether they are bound or unbound to the agonist:\n\\[\n[R_t] \\overset{def}{=} [R] + [AR]\n\\]\nWith this definition we can do the following:\n\\[\n\\begin{array}{rrcl}\n& \\displaystyle\\frac{[A][R]}{[AR]} &=& K_d \\\\\n\\implies & \\displaystyle\\frac{[A]([R_t] - [AR])}{[AR]} &=& K_d \\\\\n\\implies & [R_t] - [AR] &=& \\displaystyle\\frac{[AR]}{[A]} K_d \\\\\n\\implies & [R_t] &=& [AR] \\left(1 + \\displaystyle\\frac{K_d}{[A]} \\right)\\\\\n\\implies & \\displaystyle\\frac{[R_t]}{[AR]} &=& \\displaystyle\\frac{[A] + K_d}{[A]}\\\\\n\\implies & \\displaystyle\\frac{[AR]}{[R_t]} &=& \\displaystyle\\frac{[A]}{[A] + K_d}\n\\end{array}\n\\]\nIn this expression \\([AR]/[R_t]\\) is referred to as the fractional occupancy of the total receptor pool. That is, it describes the proportion of the receptors that are currently bound to the agonist molecule."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#motivating-a-simple-model",
    "href": "posts/2024-01-09_emax-models/index.html#motivating-a-simple-model",
    "title": "Notes on the Emax model",
    "section": "Motivating a simple model",
    "text": "Motivating a simple model\nAt this point we have the something that we can turn into theoretical model for the effect of a drug, but we need to impose additional assumptions in order to flesh it out. One proposal is that the effect \\(E\\) of the drug is proportional to the fractional occupancy \\([AR]/[R_t]\\) of the relevant receptor pool. That is,\n\\[\nE = \\alpha \\frac{[AR]}{[R_t]}\n\\] for some proportionality constant \\(\\alpha\\). Now, noting that the maximum possible value of \\([AR]\\) is \\([R_t]\\), it follows that the maximum possible value for \\(E\\) is in fact \\(\\alpha\\), so it makes more sense to denote it as \\(E_{max}\\):\n\\[\nE = E_{max} \\frac{[AR]}{[R_t]}\n\\]\nand by substitution back into the earlier equation we obtain something that is beginning to look a lot like the Emax model with a Hill coefficient \\(n = 1\\):\n\\[\nE = E_{max} \\frac{[A]}{[A] + K_d}\n\\]"
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#incorporating-signal-amplification",
    "href": "posts/2024-01-09_emax-models/index.html#incorporating-signal-amplification",
    "title": "Notes on the Emax model",
    "section": "Incorporating signal amplification",
    "text": "Incorporating signal amplification\nIt comes as little surprise to discover that the theoretical model just derived is a bit of an oversimplification. Taken at face value, you’d be tempted to assume that the value of the \\({EC}_{50}\\) parameter in an Emax model is equal to the value of the equilibrium dissociation constant \\(K_d\\) for the reaction between a ligand \\([A]\\) and receptor \\([R]\\). This is not generally the case, because biology is more complicated than this simple model suggests. In many situations we observe some kind of signal amplification, in which you get a much stronger response than you’d expect on the basis of this simple response. One way to frame it is like this. We consider the fractional occupancy of the receptor \\([AR]/[R_t]\\) to be a stimulus,\n\\[\n\\mbox{stimulus} = \\frac{[A]}{[A] + K_d}\n\\] and this stimulus triggers a nonlinear response (equivalent to our effect \\(E\\) in the previous notation) that rises quickly to some asymptote level:\n\\[\n\\mbox{response} = \\frac{\\mbox{stimulus}}{\\mbox{stimulus} + \\beta}\n\\]\nIn this expression \\(\\beta\\) denotes the value of the stimulus that produces 50% of the maximum response.\nAs it turns out, this doesn’t affect the overall structure of the model predictions, since\n\\[\n\\begin{array}{rcl}\nE &=& \\displaystyle\\frac{\\left(\\displaystyle\\frac{[A]}{[A] + K_d}\\right)}{\\left(\\displaystyle\\frac{[A]}{[A] + K_d} \\right) + \\beta} \\\\\n&=& \\displaystyle\\frac{[A]}{[A] + ([A] + K_d) \\beta} \\\\\n&=& \\displaystyle\\frac{[A]}{(\\beta + 1) [A] + \\beta K_d} \\\\\n&=& (\\beta + 1)^{-1} \\displaystyle\\frac{[A]}{[A] + \\beta (\\beta + 1)^{-1} K_d} \\\\\n\\end{array}\n\\]\nIn other words, we again have something that looks like Emax model (with \\(n=1\\)),\n\\[\nE = E_{max} \\frac{[A]}{{EC}_{50} + [A]}\n\\]\nbut this time \\(E_{max} = 1/(\\beta + 1)\\) and \\(EC_{50} = (\\beta/(\\beta + 1)) \\times K_d\\). Since the \\(\\beta\\) quantity is interpreted as a fraction (i.e., it’s on the same scale as the fractional occupancy \\([AR]/[R_t]\\)), the net effect is that under a model like this one what we should expect to see is \\(EC_{50} &lt; K_d\\). In the event that there is a stimulus-response cascade at play (e.g., the initial response is the stimulus for another response, etc, ultimately leading to an effect), the overall shape of the function would remain the same."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#interpreting-the-hill-coefficient",
    "href": "posts/2024-01-09_emax-models/index.html#interpreting-the-hill-coefficient",
    "title": "Notes on the Emax model",
    "section": "Interpreting the Hill coefficient",
    "text": "Interpreting the Hill coefficient\nOkay, so at this point we have a sensible motivation for using an Emax model in an exposure-response analysis (at least in some situations). What we don’t have is an explanation of the Hill coefficient \\(n\\). The Gabrielsson & Weiner book doesn’t say much about this, other than to note that:\n\nThe exponent \\(n\\) does not have a direct biological interpretation and should be viewed as an extension of the original \\(E_{max}\\) model to account for the curvature [i.e., steepness of the logistic function]. It provides a further degree of flexibility in the sensitivity of the response-concentration relationship.\n\nThat doesn’t mean it’s an arbitrary parameter though. It was introduced by Hill originally to describe cooperative binding in the oxygen-haemoglobin relationship. Essentially, the derivation earlier is based on the assumption that every time a drug molecule binds to a receptor it’s entirely independent of the all other binding events. That doesn’t have to be the case. Quoting from the linked Wikipedia page:\n\nCooperative binding occurs in a molecular binding system where two or more ligand molecules can bind to a receptor molecule. Binding can be considered “cooperative” if the actual binding of the first molecule of the ligand to the receptor changes the binding affinity of the second ligand molecule. The binding of ligand molecules to the different sites on the receptor molecule do not constitute mutually independent events. Cooperativity can be positive or negative, meaning that it becomes more or less likely that successive ligand molecules will bind to the receptor molecule.\n\nThat makes sense to me. It’s not a situation covered by the derivation I considered at the start, because if a receptor can bind to two or more ligands you’d need to consider bound-receptor compounds like \\(AAR\\) and, \\(AAAR\\) (to use an oversimplified notation) as well as \\(AR\\), and then your model would need to include reactions like \\(AR + A \\leftrightharpoons AAR\\) and their associated \\(K_d\\) values. The derivation above is – like any such excercise – a simplification of the real situation. So while the Hill coefficient \\(n\\) doesn’t immediately fall out of a simplified model, it’s not too hard to see how cooperative binding could give rise to logistic curves (on the log-concentration scale) with steeper slopes than those observed with \\(n = 1\\)."
  },
  {
    "objectID": "posts/2024-01-09_emax-models/index.html#summary",
    "href": "posts/2024-01-09_emax-models/index.html#summary",
    "title": "Notes on the Emax model",
    "section": "Summary",
    "text": "Summary\nOkay, so now the Emax model makes more sense to me. That’s all I was aiming for, and it’s 6pm now so I’m quitting work for the day!"
  },
  {
    "objectID": "posts/2021-10-19_rtistry-posts/index.html",
    "href": "posts/2021-10-19_rtistry-posts/index.html",
    "title": "Generative art resources in R",
    "section": "",
    "text": "People often ask me if I have any words of advice for young people. No wait, that’s not right. Nobody wants to hear my words of advice for young people, largely because I have none. What they often do ask me is if I have any words of advice for aspiring generative artists who want to use R to make pretty pictures. To be honest, I don’t have a lot of advice there either, but I’ll try my best.\nLet’s start with the big picture: there are no books or general purpose introductions out there. There are no books, no CRAN task views, no courses you can take. In fact, until quite recently generative art in R was an extremely niche topic. To my knowledge, the #rtistry hashtag on twitter is where you’ll find the most art and the most posts about the topic, but that hashtag is pretty new.1 There were resources that existed prior to that, of course: how could there not be? After all, Thomas Lin Pedersen has been building a toolkit for generative art in R for quite some time now. In his keynote talk at celebRation2020, he refers to an “art driven development” process that has led him to create several packages that are valuable to the would-be generative artist. For example:\nThese tools are great, but if you’re just getting started it can be helpful to play around in a more constrained environment. If you want something extremely simple, you could play around with the flametree package I wrote. It’s not very flexible (it just draws branching things!) but it does have the advantage that you can get started with something as simple as this:\nlibrary(flametree)\n\n# pick some colours\nshades &lt;- c(\"#1b2e3c\", \"#0c0c1e\", \"#74112f\", \"#f3e3e2\")\n\n# data structure defining the trees\ndat &lt;- flametree_grow(time = 10, trees = 10)\n\n# draw the plot\ndat %&gt;% \n  flametree_plot(\n    background = \"antiquewhite\",\n    palette = shades, \n    style = \"nativeflora\"\n  )\nPlaying around with a package like flametree – or jasmines if you want something a little more flexible – is a nice way to start drawing things, but at some point you might want to understand the process involved in creating a system like this. I’ve occasionally used art as a way to help teach people how to program in R, so you might find these programming of aRt slides helpful, and the precursor to flametree is discussed in my slides on functional programming.\nResources like mine can help get you started, but there are many other great artists out there who often post tutorials and walkthroughs. For instance, Antonio Sánchez Chinchón has a lot of really useful tutorials on his blog fronkonstin.com. Ijeamaka Anyene has written a lovely and gentle introduction to her system for rectangle subdivision. Will Chase writes about his process on his blog sometimes: here’s an example on a grid system. Jiwan Heo has a wonderful post on how to get started with flow fields in R among many other lovely posts! You can look outside of the R community too: Tyler Hobbs writes a lot of essays about generative art that describe algorithms in fairly general terms. For instance, one of my systems is built from his essay on simulating watercolour paints. And of course there’s also the walkthrough I wrote for one of my systems here and the piece I wrote that talks a little bit about the psychological process of making art in R.\nMy hope is that these resources will point you in the right direction to get started, but more than anything else I would emphasise that it takes time and effort. Art is a skill like any other. I’ve been practicing for about three years now, and while I am happy with the pieces I make, I still have a lot to learn. And that’s okay – one of the big things I always want to stress is that play is a huge part of the process. Making polished systems comes later!\nIn any case, I’ll leave this post as it is for now but if folks would like to suggest additional resources, I can always update it if need be!"
  },
  {
    "objectID": "posts/2021-10-19_rtistry-posts/index.html#postscript",
    "href": "posts/2021-10-19_rtistry-posts/index.html#postscript",
    "title": "Generative art resources in R",
    "section": "Postscript",
    "text": "Postscript\nOkay, I’m going to start adding things. This is just a completely unstructured list for now, but I know how my brain works: if I don’t bookmark the cool posts and resources I see pop up on my timeline I’ll never find them again…\n\nR specific\n\nThinking outside the grid by Meghan Harris\nGradients repository by Sharla Gelfand\nGenerative art package by Nicola Rennie\nVarious art posts by Claus Wilke\nggbenjamini package by Urs Wilke\nGenerative art examples by Pierre Casadebaig\nThe art in randomness by Dorit Geifman\nGenerative art galleries by Jacquie Tran\nArt portfolio site by Ijeamaka Anyene\nMystery curves by George Savva\n\n\n\nMore general\n\nthatcreativecode.page is a great general resource\nthe description of asemi.ca shows a design process in detail\nTyler Hobbs generative art essays"
  },
  {
    "objectID": "posts/2021-10-19_rtistry-posts/index.html#footnotes",
    "href": "posts/2021-10-19_rtistry-posts/index.html#footnotes",
    "title": "Generative art resources in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA little bit of searching on twitter suggests that I proposed the hashtag on January 20th 2021 in a now-deleted tweet.↩︎"
  },
  {
    "objectID": "posts/2023-12-16_another-year-ends/index.html",
    "href": "posts/2023-12-16_another-year-ends/index.html",
    "title": "Another year ends",
    "section": "",
    "text": "Warning\n\n\n\nContent note: This post refers briefly to sexual assault\nAt the end of last year I wrote a wrap-up post listing the things I’d done during 2022, (A Very Strange Year), and perhaps I’ll do another one for 2023. Much like last time, I’ll break with my usual policy on this blog and be a little more open about my personal life. If you read last year’s post, you’d know that 2022 was a grim year for me. Among other things I got raped,1 and – in the world’s least-surprising plot twist – that messed me up rather badly. Losing my job at the end of 2022 didn’t help matters either.\nWhat I didn’t anticipate when I wrote that post is that I was going to be unemployed for the majority of 2023. That was unpleasant. What I also didn’t see coming, is that rape trauma and unemployment interact with each other rather badly. On the one hand you start to feel worthless to society because you can’t find work, and on the other hand you also feel worthless as a human being because that’s what sexual assault does to you. So then you find yourself losing motivation to be upbeat and outgoing in the job search (and the job search gets worse), and you spend days on end lying in bed trying to believe you’re not a worthless person (and the trauma gets worse). Not exactly a deep insight, I’ll confess. Nevertheless, despite the triteness of the observation, the consequences are still real. Before the unemployment started to cut deep, I’d been making genuine progress in recovery from the sexual assault stuff, but after being out of work for a few months I went backwards.\nComing back from all that has been rough, and trying to work on recovery has been the dominant feature of 2023. I found work in the middle of the year, and am enjoying my new role. It’s nice to feel useful again. I’ve been doing better with the sexual assault fallout over the last few months too. I rarely get flashbacks these days, and I don’t fall to pieces (not often, anyway) whenever there’s another sexual assault story in the news. That’s real progress, and I’m proud of myself for that. But it’s still hard, and my self-confidence is in tatters. I’m working at it, but I suspect it will be a long and slow journey.\nAnyway. Enough of that. Life is hard sometimes, and there’s not much to do except acknowledge from time to time that, yes, bad things happen. Let’s move on to other topics, shall we?"
  },
  {
    "objectID": "posts/2023-12-16_another-year-ends/index.html#data-science-tech-and-statistics",
    "href": "posts/2023-12-16_another-year-ends/index.html#data-science-tech-and-statistics",
    "title": "Another year ends",
    "section": "Data science, tech, and statistics",
    "text": "Data science, tech, and statistics\nNot surprisingly given the focus on data science on this blog, I wrote a number posts about R packages and related topics in tech during 2023. Several of them were posts where I chose an R package that I wanted to play around with and wrote about it:\n\nThe santoku package\nThe pins package\nThe asciicast package\nThe tabulizer package\n\nOthers were built around a specific topic, though still very heavily focused on workflows in R:\n\nRunning R on the web with webR\nAssertion checking in R\n\nNot all of my tech posts were about R though. Some are general purpose posts in which I taught myself new tools: learning about Docker and Kubernetes was fun, though in truth I’ve not had much opportunity to use either of these skills.2 On the other hand, learning about Makefiles was super useful and I’ve found myself using them a lot.3\n\nPlaying with docker and the github container registry\nDeploying R with kubernetes\nMakefiles. Or, the balrog and the submersible\n\nIn a refreshing change for this blog – which has historically focused on code – I started writing a little more about statistical inference this year. Not as much as I’d have liked, and not as in-depth as I’d have liked either, but perhaps it will be the beginning of a trend for me. Two old statistics posts returned from the dead in 2023, both relics from my former life as an academic,4 and a third post estimating the number of internally displaced trans people within the United States:\n\nA gentle introduction to the Metropolis-Hastings algorithm\nA personal essay on Bayes factors\nOn living in dark times\n\nFinally, I wrote a couple of other posts on other topics broadly related to data science and software. My new personal machine is (for reasons!) using Windows as the primary OS, with an Ubuntu installation (no, not Arch) running concurrently thanks to Windows Subsystem for Linux. I wrote about some of the practicalities of using a setup like this, and specifically about how to use the RStudio IDE and VS Code effectively if you’re using linux for your data science work but you want your IDE to run smoothly as a windows application:\n\nRStudio and VS Code in WSL\n\nOh, and I wrote a book review of Greg Wilson’s “Software design by example” in which I followed his guidance and wrote a regular expression tokeniser:\n\nSoftware design by example\n\nThat one was fun, and I learned a lot."
  },
  {
    "objectID": "posts/2023-12-16_another-year-ends/index.html#pharmacometrics",
    "href": "posts/2023-12-16_another-year-ends/index.html#pharmacometrics",
    "title": "Another year ends",
    "section": "Pharmacometrics",
    "text": "Pharmacometrics\nAround the middle of the year I managed to find work, which came as something of a relief. My new role is in pharmacometrics, and while the job is pretty intense – honestly, I’m working longer hours now than I did even as an academic – it’s been rewarding. There’s a lot of new and fascinating things to learn, and I do love being able to learn new things. That’s shown up in this blog quite a bit this year, with several posts discussing pharmacokinetic (PK) models generally and population-pharmacokinetic (pop-PK) models specifically:\n\nNon-compartmental analysis\nSimple PK models in Stan\nMinimal pop-PK models in Stan\nGetting started with pop-PK in Torsten\nSimulating from PK models with mrgsolve\nSimulating from PK models with rxode2\n\nThere’s several other topics in this area I want to write, especially now that my knowledge of the field is considerably stronger than it was when I wrote these pieces.5 It’s likely I’ll write more posts about pharmacometrics in 2024, but probably at a slower rate. That’s mostly because the nature of my employment is different these days: all my previous jobs before this one were in some sense public-facing roles, and writing blog posts about the substantive topic were part and parcel of the job itself. That’s not the case in my current role, so any writing I do is “off the books” so to speak, and only happens when I have spare time. Which is entirely okay, of course, but it’s hardly a surprise that it’s easier to find time to do a thing when it is central to your job than it is to do the same thing when it’s not actually part of the role!6"
  },
  {
    "objectID": "posts/2023-12-16_another-year-ends/index.html#art",
    "href": "posts/2023-12-16_another-year-ends/index.html#art",
    "title": "Another year ends",
    "section": "Art",
    "text": "Art\nOn the art front, I changed course a bit in 2023. Early in the year I reluctantly concluded that there isn’t any point in trying to generate income from my art. I haven’t been taking commissions, I turned down a few opportunities to write books on generative art, and I haven’t been doing workshops the way I used to. There are a couple of reasons for that, but mostly they boil down to “return on investment”. It takes a lot of time, energy, and work to do the things that allow you to make money from art, and the bitter truth to it is that unless you’re one of the lucky few who “makes it big”, you really don’t make enough money from the art to justify the effort.7 People love art. They don’t love paying for it. Nowadays I don’t even bother to try: I make art when I feel like making art, I write about art when I feel like writing about art, and that’s it.\nThat doesn’t mean I abandoned making art of course, or even writing about it. I wrote three blog posts on generative art this year, for example:\n\nGenerative art with p5.js\nMaking shattered landscapes in R\nGenerative art with grid\n\nI published seven art series on my art website, mostly created with R but also with javascript in some cases:\n\nSubdivision\nBroken lands\nBound\nCurled\nSplotches\nPastiche\nAdvent\n\nI even wrote a custom arttools R package that I use to help me manage my art workflows,8 and gave an invited talk at the University of Auckland on generative art (unpredictable paintings). I’m grateful to the lovely folks at Auckland for the invitation, and it was really wonderful to give the talk to such a kind audience.\nSo yeah. I’m still making art and sharing it on mastodon, but mostly as a hobbyist these days. I’ve lost any real desire to be a professional artist, and that’s okay. Sometimes a hobby can be just a hobby. Not everything in life needs to be a side hustle."
  },
  {
    "objectID": "posts/2023-12-16_another-year-ends/index.html#footnotes",
    "href": "posts/2023-12-16_another-year-ends/index.html#footnotes",
    "title": "Another year ends",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI honestly don’t like talking about any of this, but I also don’t want to hide the fact that it happened, so I’m not going to shy away from it. But also please don’t be like the stranger who messaged me after my 2022 post to tell me I should carry a gun. That’s not a helpful thing for anyone to tell a rape survivor, and it’s an especially terrible thing to say to her when you’re a man she doesn’t know.↩︎\nI mean, let’s face it: while there’s a strong case to be made that Docker is an important tool for a lot of scientists, Kubernetes is … not. I learned it because it’s occasionally relevant for deploying applications at scale, but that’s not at all relevant in my current role.↩︎\nYes yes, I know, there are many alternatives, and if I’m an R person I should probably get off my lazy arse and learn targets but life is short and I am tired okay?↩︎\nStrangely, despite being out of academia for over two years now, papers from that life still continue to trickle out, and another six journal articles emerged in 2023 that have my name on them. Academia really makes it hard for you to draw a line in the sand, which… actually is not great when the reason you left is not a pleasant one. Sigh.↩︎\nGonna be honest: I now look at most of these pieces and smile. They’re good posts, really, but I can now spot a lot of things in each of them that reveal that the author didn’t have a lot of practical experience with PK modelling at the time. But that’s good – that’s what happens when you learn things!↩︎\nThis lack of time, incidentally, is also the reason why (despite Hadley’s very kind support and encouragement) I haven’t made much progress on the project to work on the 3rd edition of the ggplot2 book, and haven’t done much of the work on updating my “learning statistics with R” book this year either. I’m sad about both of those things, and hoping that somehow I’ll be able to find some time in the future.↩︎\nAnd, yes, with the rise of tools like DALL-E and midjourney, the situation has gotten worse over the past year.↩︎\nNote that it’s not a tool for making generative art, it’s a tool for managing art repos and organising output in a format that makes it easy for me to publish art to my website. It’s also not intended for general use. I wrote the package for myself. For anyone else it’s a “use at your own risk” kind of deal. You can use it if it’s useful for you, but I have no intention whatsoever of maintaining it for any purpose except my own use.↩︎"
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html",
    "href": "posts/2021-04-05_welcome/index.html",
    "title": "Welcome to the jungle",
    "section": "",
    "text": "I’ve decided the time has come to restart my blog. I’ve tried blogging many times before with mixed success, and this time around I’d like to avoid the mistakes of the past. I’ve set up this blog with a few principles in mind:\nAt this stage I’m not entirely certain how I’ll use the blog. There are a lot of possibilities, and I have some thoughts on which ones I’d like to explore. A self-contained blog such as this seems nicely suited to teaching materials. An obvious example would be to write blog posts to accompany the data science slides and youtube videos I’ve been making. The lack of written material to go with those talks has bothered me for some time. Another possibility might be to write tutorials on generative art. I use my art website to post the art itself, but the site functions as a gallery rather than a classroom. I get a lot of people asking questions about how I make my art, and this blog might be a good place to provide answers. Those aren’t the only possibilities, of course, but they are appealing ones.\nNot sure how this will go, but fingers crossed!"
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html#last-updated",
    "href": "posts/2021-04-05_welcome/index.html#last-updated",
    "title": "Welcome to the jungle",
    "section": "Last updated",
    "text": "Last updated\n\n2023-05-27 18:17:34.354031"
  },
  {
    "objectID": "posts/2021-04-05_welcome/index.html#details",
    "href": "posts/2021-04-05_welcome/index.html#details",
    "title": "Welcome to the jungle",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html",
    "href": "posts/2023-04-12_bayes-factors/index.html",
    "title": "A personal essay on Bayes factors",
    "section": "",
    "text": "You like my gin and tonic kisses ’cause you know they taste so sweet And I know you got your missus, but there ain’t no one like me    –Elle King and Miranda Lambert\nSo. As my loyal1 readers may have worked out, I have decided that now is the time to resurrect blog posts from my dark past. Oh sure, I can walk in the bright and blessed world of data science and adorn myself in all that hard glossy armour that is the perquisite of trade, but I have a dark past. Once upon a time I delved too deep into the dark realms of statistics. I fought with dragons (frequentists), trolls (objective Bayesians), and balrogs (subjective Bayesians). Occasionally I emerged victorious, in the sense that I am not yet dead. In this respect I am proud of myself, but oh my… to survive this in perilous land, I too have had to scorch the earth and burn all the bridges. These are dangerous paths to tread, and there are few things a statistician loves more than a whimsical death.\nYeah, anyway. I have opinions about Bayes factors, and since I am no longer required to give a fuck about academic norms, I shall raise them from the dead. Or, to use normal language, I’m gonna repost a rant I wrote a few years back because I bloody well can…"
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#the-post",
    "href": "posts/2023-04-12_bayes-factors/index.html#the-post",
    "title": "A personal essay on Bayes factors",
    "section": "The post…",
    "text": "The post…\n\n’Cause you’re hot then you’re cold  You’re yes then you’re no  You’re in then you’re out  You’re up then you’re down  You’re wrong when it’s right  It’s black and it’s white    –Katy Perry\n\nI have mixed feelings about Bayes factors. As Katy Perry once observed, it’s extremely hard to make valid inferences when you aren’t even sure what the hell it is you’re trying to make inferences about. Oh sure, it’s easy enough to tell pretty stories about rational reasoning with respect to a prior, but if we’re going to have a serious statistical relationship you need to bring more than a Dutch book argument to the table."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#love-at-first-sight",
    "href": "posts/2023-04-12_bayes-factors/index.html#love-at-first-sight",
    "title": "A personal essay on Bayes factors",
    "section": "Love at first sight",
    "text": "Love at first sight\nI first discovered Bayes factors in 1999, as a new Ph.D. student working on problems in similarity judgment and categorisation. I read Kass and Raftery (1995) and was head over heels. As long as one accepts the principle of inverse probability, a Bayesian reasoner can evaluate a hypothesis \\(h\\) in light of data \\(d\\) in a simple fashion:\n\\[\nP(h|d) = \\frac{P(d|h) P(h)}{P(d)}\n\\]\nI suspect most people reading this post already knows what Bayes rule says, but not everyone who follows me cares that much so let’s break this down:2\n\n\\(P(h)\\) is my prior belief: how plausible was \\(h\\) before the data arrived?\n\\(P(h|d)\\) is my posterior belief: how plausible is \\(h\\) now that I’ve seen the data?\n\\(P(d|h)\\) is the likelihood: the probability that we would have observed data \\(d\\) if the hypothesis \\(h\\) describes the true data generating mechanism 3 4\n\nWhen comparing two competing hypotheses \\(h_0\\) and \\(h_1\\), I can compute the posterior odds favouring one over the other simply by dividing the two posterior probabilities,\n\\[\n\\frac{P(h_1 | d)}{P(h_0 | d)} = \\frac{P(d|h_1)}{P(d|h_0)} \\times \\frac{P(h_1)}{P(h_0)}\n\\]\nOr, in something closer to every day language:\n\\[\n\\mbox{posterior odds} = \\mbox{Bayes factor} \\times \\mbox{prior odds}\n\\]\nThus the Bayes factor (BF) is defined by the ratio of the two likelihoods, and it has a natural interpretation as a weight of evidence. It tells me how I need to adjust my beliefs in light of data. And it’s so simple…\n\\[\n\\mbox{BF} = \\frac{P(d|h_1)}{P(d|h_0)}\n\\]\nWhat’s not to love?\nBetter yet, it even extends naturally from simple hypotheses to full fledged models. Suppose I have a theoretically meaningful computational model \\(\\mathcal{M}\\) for some psychological phenomenon, with parameter(s) \\(\\theta\\). For any choice of parameter values \\(\\theta\\) my model provides me with a likelihood function for the data \\(P(d|\\theta)\\), and my researcher knowledge of the world provides a prior \\(P(\\theta|\\mathcal{M})\\) belief about the relative plausibility of different parameters. So the a priori prediction that my model makes about the probability of observing data \\(d\\) in my experiment is calculated with the marginal likelihood 5\n\\[\nP(d | \\mathcal{M}) = \\sum_\\theta P(d | \\theta) P(\\theta | \\mathcal{M})\n\\]\nThe intuition is dead simple (or so I thought at the time)…. if I don’t know which parameter \\(\\theta\\) is the right one, I should hedge my bets by constructing an appropriate weighted average. Easy-peasy. This gives me a Bayes factor that I can use to compare two computational models like so:\n\\[\n\\mbox{BF} = \\frac{P(d|\\mathcal{M}_1)}{P(d|\\mathcal{M}_0)}\n\\]\nHonestly, I don’t see why this “statistics business” is so hard, I thought. All you have to do to scale up from simple hypotheses to serious theory evaluation is turn an italicised \\(h\\) into a squiggly \\(\\mathcal{M}\\) and you’re done! I read the Myung and Pitt (1997) paper on model selection with Bayes factors and thought yep, this is it. Problem solved. Easy!\nOh, you sweet summer child."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#seeds-of-doubt",
    "href": "posts/2023-04-12_bayes-factors/index.html#seeds-of-doubt",
    "title": "A personal essay on Bayes factors",
    "section": "Seeds of doubt",
    "text": "Seeds of doubt\n\nI’m feelin’ electric tonight  Cruisin’ down the coast, goin’ about 99 Got my bad baby by my heavenly side I know if I go, I’ll die happy tonight    – Lana Del Rey\n\nDuring my PhD I used Bayes factors (or similar tools) a lot. One of my very first papers (this one sought to combine multidimensional scaling methods with overlapping clustering methods in a way that would allow someone to estimate stimulus representations that have both continuous and discrete parts (e.g., our intuitions about number are partly continuous insofar as they pertain to magnitude, but also discrete when they pertain to other arithmetic properties), using Laplace approximations to the Bayes factor to automatically determine the appropriate number of clusters and dimensions. The technique had some problems. Collections of weighted binary features (as used in featural reprentations; Tversky 1977, Shepard and Arabie 1979) induce a qualitatively different parameter space than co-ordinates in a Minkowski space6 (Shepard 1974), and so when you try to mix them together into a hybrid similarity representation you get… weirdness.\nAny time you compute the marginal likelihood \\(P(d | \\mathcal{M})\\) you are implicitly introducing a penalty for excess complexity, so the Bayes factor incorporates a form of automatic Ockham’s razor. But when I built the hybrid model I found that the (implied) penalty term for “adding one more continuous dimension for an MDS solution” doesn’t seem to be commensurate with the (implied) penalty term for “adding one more discrete feature” and while I could get some decent solutions in some cases (the numbers example worked pretty well…) I never did find a general version that would “just work”.\nI put it down to the fact that the priors \\(P(\\theta|\\mathcal{M})\\) were kind of ad hoc… after all, I didn’t know what would make sense as a plausible prior that would render continuous things and discrete things commensurate with one another in a way that made sense for the psychological problems I wanted to solve. I assumed the right answer would come to me one day.\nIt hasn’t yet, but I’m still hoping it will."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#seeing-other-statistics-at-least-thats-what-i-said-i-was-doing",
    "href": "posts/2023-04-12_bayes-factors/index.html#seeing-other-statistics-at-least-thats-what-i-said-i-was-doing",
    "title": "A personal essay on Bayes factors",
    "section": "Seeing other statistics (at least that’s what I said I was doing)",
    "text": "Seeing other statistics (at least that’s what I said I was doing)\n\nWe lay on the bed there  Kissing just for practice  Could we please be objective?  ’Cause the other boys are queuing up behind us     – Belle & Sebastian\n\nAt about this point in time, I became fascinated with some of Jay Myung and Mark Pitt’s other papers on alternative ways to do model selection. For instance, in 2003 they advocated the use of model selection by minimum description length (MDL). The MDL approach to statistical inference comes out of algorithmic information theory and can be viewed as a stripped down form of Kolmogorov complexity (KC). In KC we would say something like this…\n\nThe Kolmogorov complexity of a string S with respect to programming language L is the length (in bits) of the shortest program P that prints S and then halts.\n\n… so the idea would be to think of a model \\(\\mathcal{M}\\) as a program and use it as a tool to compress the data \\(d\\). Whichever model compresses the data the most is the winner. Strictly speaking, KC is useless in real life because it’s uncomputable7 but there are many ways of taking the idea and transforming it to something that you can use. The best known (I think?) is Jorma Rissanen’s stochastic complexity approach (borrowing from work by Shtarkov) but I’ve always had a soft spot for Wallace and Dowe’s explicitly Bayesian formulation of the problem.\nAs you can probably tell, during the early 2000s I read a lot of statistics papers that I didn’t understand all that well.\nWhat I did notice though is that many of these techniques end up constructing some version of the marginal likelihood \\(P(d|\\mathcal{M})\\). They all have different motivations and not all of them allow a clear probabilistic interpretation (Rissanen doesn’t endorse a Bayesian interpretation of MDL, for instance), but they have more in common with one another than I’d originally thought. I even started reading some information geometry and found roughly the same thing. A large number of these model selection criteria can be viewed as series expansions of \\(\\ln P(d|\\mathcal{M})\\), with “small” terms omitted (usually \\(O(1)\\)). Yay, I thought! This is fantastic. Particulars notwithstanding, there is a strong theoretical justification for basing my inferences on the marginal likelihood.\nIt didn’t take long for my enthusiasm to fade again. The first time I tried to use this for model selection in the wild (selecting between different retention functions in recall memory tasks) I broke it pretty badly. It turns out that \\(O(1)\\) terms can be very fucking large in practice, and you can get all sorts of absurd results (e.g., a nested model that is judged to be more complex than the full one) when you use these model selection criteria with “small” (say, a mere 1000 or so observations) samples.\nI expanded my dating pool further. I had an on again off again thing with Bayesian nonparametrics (here, here, here), I dated normalised maximum likelihood, and various other things besides. They all let me down somehow. It turns out that NML is mostly useless in real life, Bayesian nonparametric models don’t converge to anything sensible in some situations, and so on.\nI never dated a p-value though. I do have standards."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#what-problems-do-we-study",
    "href": "posts/2023-04-12_bayes-factors/index.html#what-problems-do-we-study",
    "title": "A personal essay on Bayes factors",
    "section": "What problems do we study?",
    "text": "What problems do we study?\n\nBut I got smarter, I got harder in the nick of time  Honey, I rose up from the dead, I do it all the time  I’ve got a list of names and yours is in red, underlined     – Taylor Swift\n\nJust lately I’ve been wondering how many of the practical problems I’ve encountered stem from the fact that almost no statistical problems worth caring about are \\(\\mathcal{M}\\)-closed (this time around it’s Dan Simpson’s fault I’m thinking about this, but it’s been a recurring theme in my thoughts for a long time). At the moment I’m reading this paper by Clarke, Clarke and Yu (2013), and I’ll steal their words. The first paragraph of the paper starts with this\n\nPrediction problems naturally fall into three classes, namely M-closed, M-complete, and M-open, based on the properties of the data generator (DG) (Bernardo and Smith 2000). Briefly, M-closed problems are those where it is reasonable to assume that the true model is one of the models under consideration, i.e., the true model is actually on the model list (at least in the sense that error due to model mis-specification is negligible compared to any other source of error). This class of problems is comparatively simple and well studied.\n\nOuch. That’s about 99% of the statistical methodology that I was taught (and see in the psychological literature) and they’ve discarded it as too simplistic to be bothered talking about. It’d hurt less if they weren’t entirely correct. Almost all of what we talk about in psychology frames the problem of inference as one of “choosing the true model”, and it’s implicit that one of the models is presumed to be correct.\nThis is never accurate in real life. We often hand wave this way by quoting George Box’s famous aphorism all models are wrong but some are useful, yet we are rarely explicit in psychology in saying what we mean by “useful”. At one point I tried formulating what I thought I meant: for many cognitive science experiments that are designed to be “operationalised” versions of a more complex real world situation, I think it makes little sense to bother making predictions about low-level features of the data, and a model is most useful if when makes the correct a priori predictions about theoretically-relevant ordinal patterns in the data. But that’s not a very generalisable criterion, it doesn’t apply in situations where you actually do have to care about all the features in the data, and so on. I’ve never seen anyone come up with anything that I found compelling either.\nThat’s the thing about stepping outside of the \\(\\mathcal{M}\\)-closed world… nothing really works the way it’s supposed to. In the most difficult case you have \\(\\mathcal{M}\\)-open problems:\n\nM-open problems are those in which the DG does not admit a true model. The DG is so complex (in some sense) that there is no true model that we can even imagine. For instance, one can regard the Collected Works of William Shakespeare as a sequence of letters. Unarguably this data set had a DG (William Shakespeare), but it makes no sense to model the mechanism by which the data was generated. One might try to use the first n letters to predict the n + 1 letter and do better than merely guessing, but one should not expect such a predictor, or any model associated with it, to generate more great literature. The same point applies to the nucleotide sequence in a chromosome, the purchases of a consumer over time, and many other settings. In these cases, we are only able to compare different predictors without reference to a true model.\n\nOh yes. \\(\\mathcal{M}\\)-open problems are nasty. You have to find some sensible way to discuss what it means to make good prediction that doesn’t rely on any notion of “true models”, because there is no sense in which the data generating mechanism can possibly be mapped to anything that you or I would ever call a “model”. I suspect that this is part of the reason why some of the MDL people (e.g. Jorma Rissanen) don’t want to formulate their model selection procedures with reference to any notion of a “true model”. The moment you allow yourself the “crutch” of assuming that a true model exists, you’re left unable to justify any claims in an \\(\\mathcal{M}\\)-open world. Clarke et al comment on that actually…\n\nFrom a log-loss point of view, the Shtarkov solution (Shtarkov 1987) has also been extensively studied in the M-open case, see Cesa-Bianchi and Lugosi (2006), but has not caught on partially because the conditions for it to exist are so narrow\n\n… where (assuming it’s the paper I’m thinking of) Shtarkov’s work is linked to Rissanen’s approach to MDL that some folks in psychology (such as myself, once upon a time!) had argued for. But it’s like Clarke et al say, this approach is basically useless in real life because there are so few scenarios where you can do anything with it.\nOn the other hand, there’s a sense in which the \\(\\mathcal{M}\\)-open scenario above is more pessimistic than it needs to be. Not every statistical problem is as hard as generating new Shakespeare novels….\n\nBy contrast, M-complete problems are those where the DG has a true model that can be imagined but is not identifiable in any closed form. Inability to write a model explicitly may arise because the model is too complicated or because its constituent pieces are not known. The key point for an M-complete problem is that it is plausible to assume that a true model - also called a “belief model” - exists because this enables its use in reasoning even if a prior cannot be meaningfully assigned in the usual way. For instance, if a true model exists a bias-variance decomposition can be developed, at least in principle, even when the true model is not explicitly known.\n\nI think this is where most of our practical problems in science lie. If we knew enough about a phenomenon to put us in \\(\\mathcal{M}\\)-closed world we wouldn’t bother to study it, and if we knew so little that we couldn’t even imagine a true model (putting us in \\(\\mathcal{M}\\)-open land) it would be foolish to try. So in practice we live in the land of \\(\\mathcal{M}\\)-complete inference problems. There are interesting results in this situation. I haven’t read much about this in a long time, but my recollection from earlier reading was that in this situation a Bayes factor selection procedure will asymptotically converge to the model \\(\\mathcal{M}\\) that is closest to the true distribution in Kullback-Leibler divergence.\nI used to find this reassuring. I’m less sure now."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#its-the-little-things",
    "href": "posts/2023-04-12_bayes-factors/index.html#its-the-little-things",
    "title": "A personal essay on Bayes factors",
    "section": "It’s the little things",
    "text": "It’s the little things\n\nOh, life is bigger  It’s bigger  Than you and you are not me  The lengths that I will go to  The distance in your eyes  Oh no, I’ve said too much  I set it up    – R.E.M.\n\nThe worry I have with leaning so heavily on “convergence in KL terms” comes from a few sources. For one thing I’m starting to go beyond the limits of my own skill. You actually have to have a very good grasp of the theory to know what the hell this actually means, and I’m not sure I do. I’m a little unsure about what practical conclusions I should draw about a model if all I can say is that it is closer to the truth in the sense of a very specific information distance measure defined over distributions.\nThe impression I have had when working with KL divergence is that it really does seem to depend on every property of the distributions, but as a researcher I often don’t care about every little thing in the data. Worse, to the extent that Bayes factors specifically depend on the prior to specify the marginal distribution in question, I have this intuition that even modest mistakes in you specify the prior (especially the tails) could do very strange things. Looking back over the various papers I’ve written about in this post, I feel like it’s been a recurring theme that the details really matter. Just in this little reminiscence…\n\nWhen thinking about similarity modelling, I found stimulus features and stimulus dimensions don’t seem to have commensurate complexity as judged by the most sensible Bayesian method I could think of\nWhen doing very simple memory modelling, the best approximations I knew of (Fisher information approximation to MDL) gave absurd predictions because of the weird structure of the models\nIn categorisation, when using “infinite dimensional” nonparametric Bayesian models … oh, don’t even get me started.\n\n… and the thing is these issues have caused my inferences to misbehave every single time I have tried to automate them.\nIn real world data analysis, nothing works the way it’s supposed to and I have grown deeply skeptical that any rule governed approach to automating statistical inference makes much sense."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#what-to-do",
    "href": "posts/2023-04-12_bayes-factors/index.html#what-to-do",
    "title": "A personal essay on Bayes factors",
    "section": "What to do?",
    "text": "What to do?\n\n’Cause I’m gonna be free and I’m gonna be fine  (Holding on for your call)  ’Cause I’m gonna be free and I’m gonna be fine  (Maybe not tonight)     – [Florence and the Machine)(https://youtu.be/zZr5Tid3Qw4)\n\nHonestly, I don’t know. I like Bayesian inference a great deal, and I still find Bayes factors useful in those circumstances where I (1) trust the prior, (2) understand the models and (3) have faith in the (either numerical or analytic) approximations used to estimate it. I don’t have a better alternative, and I’m certainly not going to countenance a return to using p-values8. More than anything else, the one thing I don’t want to see happen is to have the current revival off Bayesian methods in psychology ossify into something like what happened with p-values.\nWhat I think happened there is not necessarily that p-values are inherently useless and that’s why our statistics went bad. Rather, it’s that introductory methods classes taught students that there was A RIGHT WAY TO DO THINGS and those students became professors and taught other students and eventually we ended up with an absurd dogs breakfast of an inference system that (I suspect) even Fisher or Neyman would have found ridiculous. If I’ve learned nothing else from my research on cultural evolution and iterated learning it’s that a collection of perfectly-rational learners can in fact ratchet themselves into believing foolish things, and that it’s the agents with most extreme biases that tend to dominate how the system evolves.\nWhatever we do with Bayesian methods, whatever role Bayes factors play, whether we use default or informed priors, the one thing I feel strongly about is this… we should try to avoid anything that resembles a prescriptive approach to inference that instructs scientists THIS IS HOW WE DO IT and instills in them the same fear of the Bayesian gods that I was once taught to have for the frequentist deities.\nIt doesn’t help anyone, and it makes science worse."
  },
  {
    "objectID": "posts/2023-04-12_bayes-factors/index.html#footnotes",
    "href": "posts/2023-04-12_bayes-factors/index.html#footnotes",
    "title": "A personal essay on Bayes factors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nlol↩︎\nThroughout this post I will follow my usual tendency to ignore things like the difference between densities and probabilities, and I will absolutely not waste everyone’s time by introducing \\(\\sigma\\)-algebras, because this is a blog post not a bloody measure theory textbook. If such things unnerve you too greatly, I refer you whichever section of Mark Schervish’s very excellent textbook will allow you to feel love again.↩︎\nAgain… if you feel inclined to instruct me on the difference between \\(P(x|\\theta)\\) and \\(\\mathcal{L}(\\theta|x)\\)… don’t. Go take it up with Fisher’s ashes. He’s the one who tried to misuse the ordinary natural language meaning of the word “likelihood” by inappropriately attaching probabilistic connotations to a score function that frequentists are explicitly forbidden to interpret as a probability↩︎\nNo seriously. Go visit his ashes. Fisher retired to St Mark’s college in Adelaide, and his ashes are kept in St Peter’s Cathedral in North Adelaide, a short walk from the University. The staff there are very friendly and will gladly show you to them.↩︎\nIF YOU EMAIL ME TO TALK ABOUT UNCOUNTABLY INFINITE SETS OR TRY TO DISCUSS LEBESGUE MEASURABLE FUNCTIONS IN MY PRESENCE I WILL HUNT YOU DOWN, CUT YOU INTO INFINITESMALLY THIN HORIZONTAL SLICES AND FEED THE SLICES TO MY CHILDREN.↩︎\nSomething about metric MDS rather than nonmetric MDS… don’t @ me↩︎\nFOR REASONS↩︎\nThat’s not to say I think there is no role for orthodox inference, nor that controlling error rates is a thing we should just not think about anymore. I just don’t think that this is a sensible idea to build an entire theory of inference around↩︎"
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html",
    "href": "posts/2022-12-26_strange-year/index.html",
    "title": "A very strange year",
    "section": "",
    "text": "Warning\n\n\n\nContent note: This post refers briefly to sexual assault\nWe all saw this back in 2020, right? It seemed pretty funny to me at the time – 2020 was such a messed up year that it seemed hard to believe that 2022 could really be even stranger. Oh how very wrong I was.\nMy personal life has taken the biggest upheavals. After leaving academia in late 2021, I started my first ever tech job in January 2022… and was let go from my first ever tech job in December 2022. Not gonna lie, that one stings a bit, but mostly because it’s recent and I still haven’t really emotionally processed it all. I’m objective enough to recognise that the situation I’m in now really isn’t so bad. I lost a job that I was enjoying. I feel sad and hurt about losing it because I’m a normal human being who feels sad about things that are sad. It’ll pass. I’ll heal.\nBesides, let’s be honest. In a year when I find myself in the situation where I can say things like “okay sure I’ve been sexually assaulted four times in the last 12 months, but technically speaking, only two of the men raped me” in a conversation and have it be entirely true, losing a job just doesn’t carry the level of trauma that it probably should.\nOn reflection, that is a very horrible sentence to have written.\nBut it is true. Sexual assault has been a recurring theme in 2022 for me, and while I don’t have much desire to talk about what it feels like to have been repeatedly violated in my personal life, I also don’t feel like I should be ashamed that it happened. I’m not going to make a secret of something that was not my fault. It happened, and I cannot change the fact that it happened. I’ve picked up the pieces as best I can and gone about my life again. What else can I do?\nBut let’s turn to happier topics, shall we? I’ve said the thing I wanted to say about the dark topics and there’s no need to dwell."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#contributing-to-the-ggplot2-book",
    "href": "posts/2022-12-26_strange-year/index.html#contributing-to-the-ggplot2-book",
    "title": "A very strange year",
    "section": "Contributing to the ggplot2 book",
    "text": "Contributing to the ggplot2 book\n\n\n\n\n\n\n\nI keep a little log of things that I’ve been doing with my time, and it’s interesting to look back at what I was doing in January 2022. It feels like a lifetime ago: on January 9th I merged a big pull request into the work-in-progress 3rd edition of the ggplot2 book that reorganised the scales chapters. I haven’t had time to do anything else on that since January, but I really like how the writing worked out for that (plus it’s always fun to work with Hadley!) The book now has four separate chapters on scales. Three of the chapters focus on the practicalities of working with scales:\n\nPosition scales: ggplot2-book.org/scale-position.html\nColour scales: ggplot2-book.org/scale-colour.html\nScales for other aesthetics: ggplot2-book.org/scale-other.html\n\nThere’s a fourth one too, which talks more about the underlying theory:\n\nScales and guides ggplot2-book.org/scales-guides.html\n\nHaving done earlier work helping out with revising the Maps and Annotations chapters, it felt really nice to be able to work on that. It’s possible I’ll have more time to revisit in 2023, but at this point all my plans are up in the air so who knows."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#writing-about-apache-arrow",
    "href": "posts/2022-12-26_strange-year/index.html#writing-about-apache-arrow",
    "title": "A very strange year",
    "section": "Writing about Apache Arrow",
    "text": "Writing about Apache Arrow\n\n\n\nFor most of the year I was gainfully employed to work on open source projects – Apache Arrow in particular – and I’ve written a lot over the last year about it. On this blog alone here’s what I wrote this year:\n\nGetting started with Apache Arrow in R. A now slightly dated primer on how to get started. Okay this one was technically November 2021, but it’s the logical beginning of the sequence of posts and I’m including anyway\nBinding Apache Arrow to R. A post about how the dplyr bindings in the arrow R package work and how you can write them yourself\nData types in Arrow and R. A very long post that walks you through the low level data types used by Arrow and R, and some of the subtle details around translating from one to the other\nArrays and tables in Arrow. This post builds on the previous one and talks about some of the higher level data structures used in Apache Arrow (Arrays, Tables, Record Batches, etc), and the ways in which they are similar to and different from similar data structures in R (vectors, data frames, etc).\nHow to visualise a billion rows of data in R with Apache Arrow. This one was a practical post, walking you through the process of plotting a very large data set\nPassing data between R and Python with reticulate Part one of a two-part series about how you can use Arrow to pass data between R and Python without incurring serialisation costs. In part one I talked about it from an R-centric perspective, using the reticulate R package as the primary tool\nPassing data between Python and R with rpy2 Part two of the same series. This one takes a Pythonic perspective and uses the rpy2 Python library as the primary tool\nBuilding an Arrow Flight server. One of the underrated features of the Arrow toolkit is that is the Flight RPC protocol: you can use it to efficiently communicate Arrow data over a network. This post is a walkthrough of how to do that in R and Python (mostly Python, really)\nThe Arrow Dataset API. The last Arrow post I wrote this year talked about the Arrow Dataset API in more detail than I had done previously.\n\nA lot of these posts are… well, they’re long and they’re detailed. The intention was always to try to create a collection of useful resources with code walkthoughs that I could later fold back into documentation, books, workshops and so on. The nice thing is that this actually did happen. For example…\n\nI wrote an entire workshop on Larger than memory data workflows with Apache Arrow for the R community. The slides, walkthrough, tutorial, etc are all up on the website\nI contributed a chapter on Arrow to the 2nd edition of R for Data Science. Given the centrality of R4DS in the R community I kind of feel like that’s probably one of the more useful things I actually managed to get done!\n\nBut probably the biggest thing is that a lot of the content from my other writing worked its way into a big pull request I wrote updating the documentation for the arrow R package. It hasn’t quite gone properly live yet, and I don’t think it will migrate to the front page until the 11.0.0 release in January, but it’s currently available on the dev version of the documentation.\n\nI completely rewrote the Get started page so that it is now more novice friendly and helps orient new users\nI added a new article highlighting the read/write capabilities of Arrow\nI added a new article talking about the data wrangling using the dplyr interface that was partly new material, and partly reworked existing content\nI tidied up the article on multi-file Dataset objects\nI added a new article on data objects in Arrow that reworked a lot of content I’d originally written for my blog\nI added a new article on data types in Arrow that, again, reworked a lot of content I’d written for my blog\nI laid the groundwork for a tidier discussion of metadata in Arrow that I’d intended to expand on later\n\nOh, and I also wrote the Arrow Visual Identity page and all the code for generating the various logos!\nThere was a lot more I wanted to do with Arrow, to be honest. For example, the R package doesn’t handle Arrow Flight correctly at the moment (it works in special cases but it badly needs updating), and – before I found myself unemployed – I had talked about writing bindings so that the R package implemented the Arrow Flight protocol correctly. Maybe one day I’ll write them. I don’t think it’s actually very hard because the hard work is already done in the C++ library, but… oddly, I think I’ll take a bit of a break from Arrow work while I let my emotions about my employment situation settle."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#art",
    "href": "posts/2022-12-26_strange-year/index.html#art",
    "title": "A very strange year",
    "section": "Art",
    "text": "Art\n\n\n\nI made a lot of generative art in 2022, though maybe not quite as much as in 2021. I added nine new galleries on my art website art.djnavarro.net, but really the big thing for me in art this year was being invited to give a generative art workshop at rstudio::conf. So, thanks to the support of folks at Posit, there is now a fully fledged freely available tutorial – okay it’s closer to being an entire book, really – on how to make generative art with R. It’s online at art-from-code.netlify.app. It makes me really happy that I was lucky enough to be able to write that one. I was also interviewed for the DSxD book on The Future of Data Science for my artistic work! Oh, and one of my art pieces was used – with my permission freely given to the authors because they are lovely – as a the cover of a book on transgender sexual health, which I think is really cool."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#academia",
    "href": "posts/2022-12-26_strange-year/index.html#academia",
    "title": "A very strange year",
    "section": "Academia",
    "text": "Academia\nSomehow, despite the fact that I am no longer in academia, I managed to publish some academic papers… in my spare time, like normal people do. To be fair though I didn’t actually do much of the work this year: these were all project that I’d committed to while I was still in academia. They were mostly papers that had been accepted or in the final stages of revision at the start of 2021 and have been slowly emerging from the pipeline one by one. It’s hard to know what to say about my academic output given that I’m no longer invested in the peculiar norms of the academia. It’s not my world anymore. I care a lot about some of the values, and so I’m still maintaining a personal archive as well as contributing my papers to institutional and other public archives, but… look, if ever you feel a need to ask “wait, what was Danielle’s academic research all about?” I moved all that to papers.djnavarro.net at the end of 2022."
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#what-else",
    "href": "posts/2022-12-26_strange-year/index.html#what-else",
    "title": "A very strange year",
    "section": "What else?",
    "text": "What else?\n\n\n\nI did a lot of other things in 2022. There’s really no need to try to go through all of them. But here are some other personal favourites that made an appearance on this blog:\n\nI wrote an R package on multi-threaded task queues: blog.djnavarro.net/queue\nI wrote about Crayola crayon colours and some fun data wrangling problems: blog.djnavarro.net/crayola-crayon-colours\nI wrote an absurdly popular blog post about mastodon: blog.djnavarro.net/what-i-know-about-mastodon/\nI wrote a post that I thought was absurdly-popular (until the mastodon thing happened) about porting this blog from distill to quarto: blog.djnavarro.net/porting-to-quarto"
  },
  {
    "objectID": "posts/2022-12-26_strange-year/index.html#so-what-happens-next",
    "href": "posts/2022-12-26_strange-year/index.html#so-what-happens-next",
    "title": "A very strange year",
    "section": "So what happens next?",
    "text": "So what happens next?\nArriving at the end of this post, I find myself very unsure about what comes next. It’s been such a strange year. I’m proud of the work that I’ve done and the projects I’ve completed. It feels good to be able to look at the list above and think yeah that’s actually a pretty decent body of work, and I could have added more if I’d wanted to. I think I’ve used my time well? I’ve accomplished a lot and learned a lot. But at the same time these things sit against a stunningly horrible backdrop, and it hasn’t been easy coping with that. In general I try not to talk about those kinds of topics on this blog – this is a data science blog, after all, and it’s not really a place for talking about sexual assault and what can do to your sense of self worth – but I suppose I would like it if 2023 goes a little differently. Not sure I can manage another year like this one."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html",
    "title": "Everything I know about Mastodon",
    "section": "",
    "text": "Hello there fellow data science person. Have you heard rumours that a lot of folks from our community are moving to use mastodon for social networking? Are you curious, but maybe not quite sure about how to get started? Have you been thinking “twitter is a hellsite and I need to escape” a lot lately?\nIf yes, this post is for you!\nIt’s written from the time-tested pedagogical perspective of “the writer who is only one chapter ahead of her audience in the textbook”. I’ve been on mastodon for a few days, but this isn’t my first rodeo over there: I signed up for it very early on several years ago, and tried again a few years after that. This time I’m a lot more enthusiastic about it than the last two, so I’m writing a quick introductory post to help my fellow data science folks test out the waters. I sort of know what I’m doing there but not completely!\nIf you want a more detailed guide on navigating Mastodon and the fediverse, I recommend fedi.tips. There’s a lot of answers to common questions over there, from someone who actually does know what they are doing! Alternatively you can read this thread which covers a lot of the same things I’m saying here!\nOkay, let’s dive in…"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-is-mastodon-what-is-the-fediverse",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-is-mastodon-what-is-the-fediverse",
    "title": "Everything I know about Mastodon",
    "section": "What is Mastodon? What is the fediverse?",
    "text": "What is Mastodon? What is the fediverse?\nIf you’re entirely new to this, your mental model of mastodon is probably something like “mastodon is an open source twitter clone”. To a first approximation that’s right, but if you want to really feel at home there you’re going to want to refine that mental model in a few ways. Mastodon is very similar to twitter in design, but there are some important differences\nFirst off, mastodon is not a single application: it’s a distributed network of servers that all talk to each other using a shared protocol.1 If two servers talk to each other they are said to be “federated” with one another, and the network as a whole is referred to as the “fediverse”.\nThere are many different servers out there that are independently running mastodon: these are called mastodon instances. You can sign up for an account at one or more of these servers. The most popular instance is mastodon.social, but for reasons I’ll talk about in a moment this might not be the best choice for you! For example, my primary account is on fosstodon.org and my art-only account is on an instance for generative artists, genart.social.\nFortunately, it usually doesn’t matter too much which instance you pick: the servers all communicate with each other so you can follow people on different servers, talk with them, etc, and it’s entirely possible to migrate your account from one server to another (I’ll talk about that later in the post). It’s only when you get into the details that it starts to matter!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#terminology-toots-and-boosts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#terminology-toots-and-boosts",
    "title": "Everything I know about Mastodon",
    "section": "Terminology: Toots and boosts",
    "text": "Terminology: Toots and boosts\nPosts on twitter are called “tweets” and have a 280 character limit. Posts on mastodon are called “toots” and have a 500 character limit. If you’re thinking of making a joke about “haha it’s called tooting” well fine, but there’s a pretty good chance that everyone has already heard it. Very few of us are actually that original :-)\nSharing someone else’s post on twitter is called a “retweet”. The mastodon equivalent is called “boosting”. One deliberate design choice on mastodon is that there is no analog of “quote retweeting”: you can either boost someone else’s toot and you can post your own. You can’t share someone else’s post to your own followers with your commentary added. This is a deliberate design choice to prevent people from “dunking” on each other."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-handles-and-tags-look-like-email-addresses",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-handles-and-tags-look-like-email-addresses",
    "title": "Everything I know about Mastodon",
    "section": "Mastodon handles and tags look like email addresses",
    "text": "Mastodon handles and tags look like email addresses\nOn twitter, you simply have a username: I’m djnavarro there, and people would tag me into a conversation by typing @djnavarro.\nOn mastodon, you have to specify both your username and the server. It’s more like an email address. My primary handle on mastodon is djnavarro@fosstodon.org and people can tag me into a conversation by typing @djnavarro@fosstodon.org.\nIt looks a little odd when you’re used to twitter, but it gets easier.\n\n\n\nA handle. Image credit: Arawark chen. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#is-there-a-data-science-community-there",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#is-there-a-data-science-community-there",
    "title": "Everything I know about Mastodon",
    "section": "Is there a data science community there?",
    "text": "Is there a data science community there?\nYes! There’s definitely a data science community there. It’s much smaller than the one on twitter and things tend to move at a slightly slower pace, but there are some advantages.\nYou can find your data science friends by searching for hashtags. R folks will quickly find other R users posting with the #rstats hashtag, but you can also find #TidyTuesday and other familiar hashtags. I’ll talk about this more later, but hashtags are much more useful (and more important) on mastodon than they are on twitter. The interface for hashtags is basically the same as twitter: you can search for them in the search box (see interface section below), hashtags are clickable links, etc.\nOnce you’ve found some people, you can find more by taking a look at who they follow and who follows them. Again, the interface for that is essentially the same as twitter: click on someone’s profile, and you’ll be able to find a list of people they follow and the people who follow them. However, what you will often finds is that these lists are incomplete: generally, the follower counts are accurate, but servers only publish the list of account names for accounts on that server.2\nFinally, when you’re ready to get started you can make an introduction post: all you have to do is send out a post tagged with #introduction. With any luck that will be picked up and shared with others!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#which-server-should-i-sign-up-on",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#which-server-should-i-sign-up-on",
    "title": "Everything I know about Mastodon",
    "section": "Which server should I sign up on?",
    "text": "Which server should I sign up on?\nThere’s a nicely server list at fediscience.org that has some commentaries. Here’s a few possibilities you might consider:\n\nmastodon.social: The largest instance. It’s general interest, so you get a reasonably diverse audience. However it’s also the focal point so any time there’s a wave of migrations from twitter it will probably be the first one to show performance hits.\nfosstodon.org: An instance with a focus on open source software. There are a lot of tech people on this one, which means you can watch the local timeline scroll by (more on that coming!) and see lots of random techy posts.\nfediscience.org: A science focused instance, including natural and social sciences.\nvis.social: Lots of cool data visualisation folks here.\ntech.lgbt: An instance for folks who work in tech, science, academia, etc who are LGBTIQ or allies.\n\nFor any server, you should look carefully at the server rules that will be posted on the “About” page. Each server has different policies that will affect moderation. Don’t sign up for vis.social if you want to post about NFTs (I’ll talk about NFTs later actually), and don’t join fosstodon.org if you want to post in languages other than English. Don’t join any of these servers if you want to post anti-trans content.3\nTake a little time to look around but don’t worry about the choice too much. You can move your account across servers without too much difficulty if you need to, and I’ll show you how later in the post."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#can-you-tell-me-about-the-web-interface",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#can-you-tell-me-about-the-web-interface",
    "title": "Everything I know about Mastodon",
    "section": "Can you tell me about the web interface?",
    "text": "Can you tell me about the web interface?\nOkay so you’ve decided on a server, signed up for an account, and are ready to get started. Let’s take a look at the interface!\nFor the purposes of this post I’ll assume you’re looking to get started by using the web interface. There are, unsurprisingly, apps you can download onto your phone (e.g., I’m using the standard mastodon app on iOS), but I’m trying not to complicate things in this post so let’s assume you’re using your laptop and are browsing through the web interface!\nMy main account is djnavarro@fosstodon.org. In my browser I’m logged in already, so when I navigate to fosstodon.org I’m automatically shown the logged in view. There are two versions you can choose between, the “standard view” and the “advanced view”.\nThe “standard view” interface looks pretty similar to what you’d expect from twitter. On the left you can write posts, in the middle there’s a column where your feed is shown (I’ve edited these screenshots to remove the actual posts, just so we can focus on interface), and on the right sidebar there’s a menu with various options you can click on:\n\nIn a lot of cases this view will work well for you, but if you want to track hashtags – more on that later because hashtags are important! – you might find it useful to switch to the “advanced view”. To switch, click on the “Preferences” option on the right hand side, which brings up a preferences screen that looks like this:\n\nClick on the “Enable advanced web interface” option, like I’ve done here, and click save changes. When you then go back to mastodon, the interface will have changed to one that looks very similar to the Tweetdeck interface that a lot of folks on Twitter use:\n\nThere are more columns. As before, the left column shows an area where you can compose posts, and on the right column a menu with options is shown. Posts will appear in the “Home” column. Mentions, favourites (similar to Twitter “likes”), and boosts (similar to Twitter retweets), will be shown in the “Notifications” column."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-make-a-post",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-make-a-post",
    "title": "Everything I know about Mastodon",
    "section": "How do I make a post?",
    "text": "How do I make a post?\nWriting a post on mastodon is similar to how you would write a post on twitter. The compose window looks like this:\n\n\n\n\n\nYou type the text you want into the box, adding hashtags, and then click the “Toot!” button (it may look different on your instance – I’ve seen the same button labelled “Post” sometimes). As you type, you’ll see the character count in the bottom right corner change to let you know how many characters you have left: you’re allowed 500 characters for a post on mastodon.\nThe nuances are important though. Those other buttons on the bottom left… those are all useful features. From left to right:\n\nThe paperclip button: clicking this will let you attach an image. When you do, there will be an option to edit the image and (especially important!) to add alt text for accessibility purposes. Mastodon has a stronger norm about alt text than twitter: always add alt text. I have a section on alt text later in this post.\nThe barchart button: this will let you add a poll. Polls on mastodon work similarly to twitter, but are a bit more flexible. You can add more options and let users select multiple options.\nThe world button: this will let you set the visibility for the post. If you click on it you will see four options: “public” means everyone can view it, “unlisted” means everyone can view it but it doesn’t get indexed by discovery features (very handy in replies and threads where you don’t want everyone to be automatically shown your comment), “followers only” means only your followers can see it, and “mentioned people only” means only the people mentioned can see it. This last one is effectively how direct messages work on mastodon, which is important to note because posts aren’t end-to-end encrypted. Do not treat your mastodon direct messages as private (see later).\nThe “CW” button: This is used to attach content warnings to your post. Use this button! It’s important. I cannot stress this enough: the content warning button is right there, and it is considered extremely poor form in the fediverse to force your followers to look at content they might not want to see. There is a whole section on this later, but remember that mastodon is not twitter – people will mute you or choose not to share your post if you don’t use content warnings appropriately. In fact, if you consistently boost posts that don’t have content warnings when they should, people may unfollow you also.\nThe “EN” button: This is used to specify the language in which the post is written. Clicking on it will show you a dropdown list you can use to select the language.\n\nTry to use these features: it makes a difference!\n\n\nPosts. Image credit: Kristina Tripkovic. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-hashtags-so-important",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-hashtags-so-important",
    "title": "Everything I know about Mastodon",
    "section": "Why are hashtags so important?",
    "text": "Why are hashtags so important?\nHashtags play a much more important role on mastodon than they do on twitter. There’s no analog of the twitter algorithm scanning post content to determine what to show people. If you want your post to be discovered by people who aren’t following you (which, admittedly, isn’t always the case), make sure to choose the appropriate hashtags. As on twitter #rstats is used to specify that this is a post about R, there’s a #TidyTuesday tag used for Tidy Tuesday, etc. I post my generative art using #GenerativeArt, a general purpose generative art hashtag, and also #rtistry to specify that it’s art made with R. It’s generally considered fine – good, even! – to use several hashtags on mastodon. Tagging is your way of categorising posts for others to find…"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#tracking-hashtags",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#tracking-hashtags",
    "title": "Everything I know about Mastodon",
    "section": "Tracking hashtags",
    "text": "Tracking hashtags\nSpeaking of which, perhaps you want to monitor a hashtag. Maybe you even want to follow the hashtag, so that every post that has that hashtag will appear in your mastodon feed. Good news, you can!\nFrom what I can tell, this is something where your options might be a little different on each server. For instance, on mastodon.social you can follow a hashtag directly in the standard view: when you search for a hashtag there will be a little “follow” icon that appears that you can click on (see this comment on fedi.tips). When you do that, posts with that hashtag will appear in your feed. However, not every server implements this: fosstodon.org doesn’t do that right now.\nSo let’s go with a method that seems to work everywhere I’ve looked. This post by David Hood summarises it in a single image, but I’ll go through it more slowly here…\nFirst off, you’ll need to be in “advanced view” to do this. That’s the one with lots of columns that I showed earlier in the post. You can customise this view by adding columns that correspond to the hashtags you want to follow. For example, let’s say I want to follow the #rstats hashtag. The first thing I’d do is type #rstats into the search bar (in the top left corner). The results will be shown directly below the search bar, like this:\n\nThese are clickable links. When I click on the #rstats hashtag in the results, a new column appears… containing a chronological feed that consists of posts tagged with #rstats:\n\nAgain, in real life this won’t be empty: you’ll actually see the posts! You are now tracking #rstats on mastodon, albeit temporarily.\nSuppose you want to make sure the column sticks around every time you open mastodon. We can “pin” the column in place. To do that, I click on the little “settings” icon at the top right of the #rstats column. It’s the one on the far right here:\n\n\n\n\n\nWhen you do that, you will see a small menu that gives you the option to pin! Easy.\nWe can make our #rstats column more useful. For example, there are several hashtags I want to bundle together when following R content: #TidyTuesday, #TidyModels, and #ggplot2. I don’t want a separate column for each one, I want to group them into a single feed. Click on that little settings button again. Now you’ll see a richer menu:\n\nOne of the options there is “include additional tags”. When I click on that, I can type in the other hashtags to track:\n And now we are done! I have a feed that tracks R related content on mastodon."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-content-warnings-everywhere",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#why-are-content-warnings-everywhere",
    "title": "Everything I know about Mastodon",
    "section": "Why are content warnings everywhere?",
    "text": "Why are content warnings everywhere?\nOne huge – and hugely important – difference between twitter and mastodon is that mastodon has a system that allows users to mask their posts behind content warnings. Now… if you’re coming from twitter you might be thinking “oh that doesn’t apply to me I don’t post offensive content”.\nIf that’s what you’re thinking, allow me to disabuse you of that notion quickly. Content warnings are not about “hiding offensive content”, they are about being kind to your audience. This thread by Mike McHargue is a very good summary. The whole thread is good, but I’ll quote the first part here:\n\nIf you’re part of the #twittermigration, it may seem strange the people use CWs so much here. But, CWs are really helpful. So much of our world is overwhelming, and feed presentation can bombard our nervous systems with triggers. CWs give people time and space to engage with that they have the resources to engage with. It gives them agency. I follow news and politics AND it’s helpful for my PTSD to have the chance to take a deep breath before I see a post.\n\nIf you’re posting about politics, that should be hidden behind a content warning. If you’re posting about sexual assault, definitely use a content warning. If you’re posting about trans rights, again put it behind a content warning.\nYou should use the content warning even – or perhaps especially – when you think your post is this is an important social justice issue that other people need to see, because there is a really good chance that people whose lives are affected by it will be part of the audience… and yes, some of us have PTSD.\n\n\n\n\n\n\nContent warning: trans rights, sexual assault\n\n\n\n\n\nI’ll give examples relevant to my own experience.\nI get really, really angry when people post about trans rights without a content warning. Same with sexual assault. Why? Well, because I am transgender and I am frightened about the general direction the world is headed for people like me. I am not an activist and I don’t have the kind of resilience needed to constantly hear all the worst stories in the world about attacks against people like me. It’s one of the big reasons I left twitter: twitter algorithms prioritise engagement, and I cannot help but engage with this content because I am afraid. My experience on twitter is one of emotional abuse: twitter keeps showing me my worst fears and I click on them because the threats are real. I don’t appreciate it when my friends try to support me by forcing me to see even more of that content. For that reason, if you want to be supportive of people like me, use a content warning when posting about trans rights.\nAn even more extreme example relevant to my personal experience is sexual assault. I am a rape survivor. Every time there is a highly visible discussion about sexual assault (e.g., the Brett Kavanaugh hearings in the US, the Brittney Higgins discussions in Australia), I would get bombarded with content about rape. Over and over again. Sometimes it would trigger panic attacks and rape flashbacks.\nWhen you post those things without providing me a content warning to help me make an informed choice, what you’re really telling me is that you simply don’t care if you’re forcing me to relive the experience of being raped.\n\n\n\nSo if you’re thinking about posting about these topics, the question of “should I attach a content warning?” isn’t a matter of “is this important?” it’s a matter of “could I be causing distress to people?” When you answer that question, don’t think about the typical case, think about that 1% of people who might be most severely affected and the reasons why.\nPlease, please, please… take content warnings seriously. Even if you’re “just posting about politics” or “venting some feelings”. It’s a kindness and courtesy to your audience.\nMastodon isn’t twitter."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-add-a-content-warning",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-add-a-content-warning",
    "title": "Everything I know about Mastodon",
    "section": "How do I add a content warning?",
    "text": "How do I add a content warning?\nHopefully the previous section has convinced you that you should use content warnings and err on the side of caution when deciding when to use them. Your next question is probably: how do I add a content warning?\nLuckily, it is super easy. It’s so simple that it fits into a single toot, like this post by Em on infosec.exchange. Here’s how.\n\nIn the composer box, click on the little “CW” button. This will reveal an extra title field that says “Write your warning here”.\n\n\n\n\n\nWrite a brief but informative message in that title field. This could be something very serious like “Sexual assault discussion”, but it could also be something mild like “Spoiler alert: The Good Wife season 5” or something like “Photo with direct eye contact”. Even things like “US politics” or “Australian politics” can be helpful.\nWrite your post. (Okay you could write the post first and the content warning text after. Whatever)\nWhen you post it, other users will only be shown the title field at first. If they decide they want to read, they can click on the post, and then the full text will be revealed.\nProfit! Everybody is happy.\n\nContent warnings are good for everybody.\n\n\n\nA warning. Image credit: Fleur. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-can-i-make-threads",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-can-i-make-threads",
    "title": "Everything I know about Mastodon",
    "section": "How can I make threads?",
    "text": "How can I make threads?\nMaking threads on mastodon is pretty similar to twitter. Just post each new toot as a reply to the previous one. Problem solved!\nThere is one subtlety to be aware of though, which is described in this thread by Quokka on scicomm.xyz. Remember earlier I mentioned that you can set the visibility of each post? The polite way to do a thread is set the first post to “public”, and then all the later ones to unlisted. The reason for that is that all public posts (including replies) will show up in various timelines. Usually, that’s not what you want. What you want is something where the first post reads “I have this important and exciting thing to to talk about: A thread…”, and only that first post shows up on people’s timelines. Then if they’re interested they can click on the first post and the rest of the thread will be revealed. That’s why people on mastodon usually set the first post to public and the later ones to unlisted."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-favourites-vs-twitter-likes",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#mastodon-favourites-vs-twitter-likes",
    "title": "Everything I know about Mastodon",
    "section": "Mastodon favourites vs twitter likes",
    "text": "Mastodon favourites vs twitter likes\nMastodon favourites (the little star under each toot) are very similar to twitter likes (the loveheart under each tweet). They aren’t identical though. The big difference is that mastodon implementation is far, far better and not deceptive.4\nOn twitter, we’ve all become accustomed to the obnoxious fact that “likes” do two different things: they send a signal to the person that you liked what they said (i.e., what they’re supposed to do!), but whenever you do that it will trigger a “stochastic retweet”: some proportion of people who follow you will also see that tweet because you liked it. This is annoying because very often you actually enjoy a thing but don’t think it is appropriate to retweet.\nThis bothers me because it seems to me that twitter doesn’t respect your boundaries. The fact that I like something is not an act in which I give twitter permission to share that fact to other people. I think it’s abusive behaviour by twitter.\nHappily, mastodon doesn’t do anything like that. Favourites don’t trigger anything. They do exactly the thing they claim to do: they are a mechanism by which you can communicate to the other person “hey I liked this!” So you should use favourites a lot! Show people you appreciate them!\nQuite separate from that, if you think this is something your followers would appreciate seeing, then boost it too! The key thing is that on mastodon the two functions are separated cleanly… do both when both are appropriate, do one when one is appropriate. You are in control of your sharing behaviour here."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-are-the-local-and-federated-timelines",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#what-are-the-local-and-federated-timelines",
    "title": "Everything I know about Mastodon",
    "section": "What are the local and federated timelines?",
    "text": "What are the local and federated timelines?\nAt some point on mastodon you will find yourself discovering the local timeline and the federated timeline. There are links to these on the right hand side of the interface. The local timeline is every public-visibility post on your server, shown chronologically. This timeline has a very different feel on different servers. On fosstodon.org my local timeline has a lot of people posting about tech; on genart.social it shows a lot of generative art.\nThe federated timeline is slightly different: it shows all public posts from all users who are “known” to your instance. That is, it includes every user on your instance, but it also includes everyone that users on your instance follow – even if those users are on other servers. It’s not the same thing as “every user on mastodon” though. People on genart.social tend to follow other artists, so there is still a local “flavour” to the posts from outside the instance: they reflect people and topics that the users on your instance are interested in.\nThese timelines are useful for discovery purposes, and they’re also a reason to think carefully about the instance you’re on. It’s easier to find tech content on a tech-focused server!"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-move-my-account-to-a-new-server",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#how-do-i-move-my-account-to-a-new-server",
    "title": "Everything I know about Mastodon",
    "section": "How do I move my account to a new server?",
    "text": "How do I move my account to a new server?\nSo that brings me naturally to a question… what if you realise you’ve made a mistake and you want to change instances? This happens to many of us at one point or another. For instance, I initially signed up as djnavarro@mastodon.social. That wasn’t ideal for me: the server is a bit too big, it was being very slow at the time, and the content isn’t focused on things I wanted to see. So I decided I wanted to move, and become djnavarro@fosstodon.org.\nA very nice feature of mastodon is that you can “migrate” your account, so that when you switch accounts all your followers will come along for the ride! Here’s how you do it:\nStep 1: Prepare the way\n\nIt’s probably a good idea to post from your old account that you’re about to initiate a move. That way people will not be surprised when they find themselves following a new account (I didn’t do this… I should have. Oops!)\nSet up your account, with the avatar, bio, etc on the new account using the same (or similar) images and descriptions on the old account: anyone who clicks through on the new account will see that it’s you!\n\nStep 2: Export your follows from the old account\nWhen you migrate, it takes your followers across automatically. It doesn’t automatically make your new account follow everyone you were following on the old account. Luckily you don’t have to manually re-follow everyone. Instead, you export a csv file with the list of everyone you’re following at the old account, and later on you can import it as a follow list on the new one. Here’s how we export the csv at the old account:\n\nClick on the “preferences” option\nWithin preferences, select “import and export”\nOn the data export page, you’ll see a list of possible csv files you can download. Download the ones you want, especially the “follows” csv.\n\nStep 3: Set up alias on the new account\nMastodon requires both accounts to authorise the move in some fashion, to prevent anyone from trying to steal other people’s accounts. First, your new account needs to signal that yes, it does wish to be an “alias” for your old account. From the new account – djnavarro@fosstodon.org for me – we need to set that up:\n\nClick on the “preferences” options\nWithin preferences, select “account”\nOn the account settings page, scroll down to the bottom to the section called “moving from a different account” and click on the link “create an account alias”\nOn the account aliases page, specify the handle of your old account – e.g., djnavarro@mastodon.social in my case – and click “create alias”\n\nYou’re done: the alias is set up. You may have to wait a while for this to propagate to the old account. When I moved I had to wait overnight because mastodon.social was running very slowly due to the massive spike of new users from twitter. Hopefully it won’t be that long for most people now.\nStep 4: Initiate the move from the old account\nWith the new account now signalling that it is ready to be an alias for the old one, we can authorise the move from the old account. On the old account (i.e., djnavarro@mastodon.social for me) do the following:\n\nClick on the “preferences” options\nWithin preferences, select “account”\nOn the account settings page, scroll down to the bottom to the section called “moving to a different account” and click on the link “configure it here”\nOn the moving accounts page, type the handle of the new account – in my case djnavarro@fosstodon.org – and enter the password for your old account to confirm. Click “move followers”.\n\nThis will initiate the move. All your followers at the old account will automatically unfollow the old account and then follow the new one. It’ll take a little while and it might happen in bursts.\nStep 5: Import your follows at the new account\nThe last step (optionally) is to have your new account re-follow everyone from that you were following at the old account. We can do that using the csv that you downloaded in step 2. So, again from your new account:\n\nClick on the “preferences” options\nWithin preferences, select “import and export”\nOn the menu on the left, click the “import” submenu\nOn the import screen, select the import type (e.g., “following list”), click on “browse” to select the csv file you exported earlier, and then click “upload”.\n\nYour new account will now automatically follow all the accounts your old account followed.\nWith any luck, you are now successfully moved into your new account!\n\n\n\nMoving. Image credit: Michal Balog. Freely available via unsplash"
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-alt-text",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-alt-text",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on alt-text?",
    "text": "Etiquette on alt-text?\nOn twitter, you’ve probably found that there’s some pressure and expectation to supply alt-text for your images. The norm is much stronger on mastodon: people will expect that images have alt-text, and that the alt-text be informative. Here’s a walkthrough. First I might start writing a post, and after clicking on the paperclip icon to attach an image, I have a screen that looks like this:\n\n\n\n\n\nAs usual I’d write the content of my post in the composer box, but I would also click on the “edit” link in the top-right hand corner of my image. That brings up the image editing screen that looks like this:\n\nThere are two things I usually do with this. On the right hand side I can drag and drop the focus circle around to help improve the image preview that gets shown to users. More importantly, on the left hand side I can write my alt-text. For some images it’s easy to come up with a good description, for others it is hard. For something like this one I’d usually aim to write a short paragraph that captures this information:\n\nthis is generative art made with R\nthe title of the piece is “Gods of Salt, Stone, and Storm”\nthe palette is blue/green with a hint of white against a very dark background\nthe image is comprised of swirling patterns throughout\nthe overall impression is something akin to dark storm clouds overhead or maybe unsettled seas\n\nIt’s not a perfect description, but it does capture what I think is important about the artwork."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-nfts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-nfts",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on NFTs?",
    "text": "Etiquette on NFTs?\nA lot of artists on twitter, especially generative artists, like to post NFTs. It’s understandable: for generative artists, it’s the one reliable income stream they have for their art. However, you need to be very, very careful. NFTs are not well liked on the fediverse, and a lot of servers have outright bans on any form of NFT posting. For instance, you cannot post about NFTs at all on vis.social or mastodon.art. It is written into the server rules, so you should not sign up on those servers if that’s something you’re interested in. However, even on servers that do permit NFTs, there is often a strong suggestion that you should be polite and respect the preferences that folks outside the instance will have. For example, the generative art instance I’m on genart.social does not impose an outright ban on NFTs but it is discouraged, and in the rare instance that you do post NFT content, it must be behind a content warning.\nPersonally I’ve stopped even trying to make money from my art, so it doesn’t affect me: I’ve given up. I’m only bothering to mention it here because I don’t want to see generative art folks run afoul of the local norms."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-cross-posting-from-twitter",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-cross-posting-from-twitter",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on cross-posting from twitter?",
    "text": "Etiquette on cross-posting from twitter?\nCross-posting from twitter is another one where you have to be careful. There are tools that will let you automatically repost from one to the other, but it’s worth thinking about this from a social perspective rather than a technical one. What will people on mastodon start thinking when your mastodon feed is just a long series of posts where you’re responding to something on twitter, or retweeting something on twitter? What will they conclude when they try to reply to you and you don’t respond because you were on twitter, not mastodon? Probably what will happen is people will realise you’re not actually on mastodon at all and unfollow you. I’ve done this a few times already. I’m trying to leave twitter for a reason, and it irritates me when people who are ostensibly (but not really) on mastodon keep trying to direct me to content on there."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-bots-and-automated-accounts",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#etiquette-on-bots-and-automated-accounts",
    "title": "Everything I know about Mastodon",
    "section": "Etiquette on bots and automated accounts?",
    "text": "Etiquette on bots and automated accounts?\nBots are allowed on mastodon, but you should check the local server rules and you should make certain that the bot is marked as an automated account in the account preferences."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#direct-messages-and-privacy",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#direct-messages-and-privacy",
    "title": "Everything I know about Mastodon",
    "section": "Direct messages and privacy",
    "text": "Direct messages and privacy\nAs a final point, a note on direct messages. Direct messages on mastodon are just regular posts whose visibility is set to include only those people tagged in that post. That’s all. This is important to recognise because – at present – posts are not transmitted with end-to-end encryption: they are “private” only in the sense that a postcard is private or an unencrypted email is private. They won’t be broadcast to anyone else, but they aren’t secured while in transit.\nYou should never send any sensitive information via mastodon."
  },
  {
    "objectID": "posts/2022-11-03_what-i-know-about-mastodon/index.html#footnotes",
    "href": "posts/2022-11-03_what-i-know-about-mastodon/index.html#footnotes",
    "title": "Everything I know about Mastodon",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe most widely used protocol is called ActivityPub, but we don’t have to care about that here. Similarly, while mastodon is the most widely used application built on top of ActivityPub, there are others: pixelfed is an instagram-like application built on ActivityPub, and pixelfed servers are also considered part of the fediverse.↩︎\nI’m guessing that happens to optimise bandwidth usage?↩︎\nYes there are servers where transphobia is permitted, but they aren’t common and are typically blocked at the server level because server admins just don’t want to deal with bigotry directed at their users. Weird huh?↩︎\nYes, this is something I feel very strongly about! I hate how twitter likes work.↩︎"
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html",
    "title": "btw I use Arch now",
    "section": "",
    "text": "I installed Arch Linux on an aging Dell XPS 13 today.\nDid I plan to do this when I woke up this morning? No, it was an impulsive decision. Was it a sensible way to spend the last Friday of 2022? Also no. Was it the kind of thing that will pay off in the long run through a deeper technical understanding of… something? Again, the answer is no.\nAh, but will it impress people and make them think I am cool?\nOh honey.\nNo.1\nBut look…\nSo pretty. So pointless. So fun."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#installing-arch",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#installing-arch",
    "title": "btw I use Arch now",
    "section": "Installing Arch",
    "text": "Installing Arch\nI have absolutely no intention of writing a “how to install Arch” guide, for many excellent reasons. Top of that list is that I am a complete amateur when it comes to Arch and I have no idea whatsoever what I’m doing. The only reason any of this happened is that I happened to have a spare laptop2 and couldn’t think of anything better to do with it. My point is I have no business whatsoever in trying to guide anyone else through the installation process. What I am going to do, however, is jot down my notes to myself on the process. I may need them again…\n\nThe installation guide (https://wiki.archlinux.org/title/installation_guide) is good but dense, and I had to read a lot of the documents it links to in order to make progress. It took me three tries to get a working installation, because I missed some subtle thing (e.g., the first time I didn’t have the bootable USB booting in UEFI mode, because obviously I would have thought of that at the beginning… sigh).\nEverything became easier once I started feeling comfortable using pacman.\nThe network configuration step gave me some grief. Eventually I got it to work when I installed NetworkManager and enabled NetworkManager.service, which in turn only made sense to me after I’d read about start/enable and realised that “starting” a service sets it running in the background now, and “enabling” it means it will start automatically on startup.\nToward the end of the installation it tells you to install a bootloader. I thought I wouldn’t need to since I already had grub on my machine from the previous Ubuntu install but in the end it didn’t work without going through the GRUB install process\nSetting up users was an odd exercise in reminding me that I’d forgotten what life was like before sudo. During the initial installation I set up a root user, but no regular users, so my first step was to give myself a … um, me. This article on adding users on Arch Linux was helpful. Partly because it’s a nice walkthrough, but also because it clued me into the fact that Arch doesn’t come with sudo, so I had to install that. It also highlighted things I’d never thought about with sudo before, mostly around which users have sudo privileges. Anyway, the article walked me through the process so now I am danielle on my Arch box (insert the usual Australian joke about getting a root). I added myself to the wheel group so that I can escalate to admin privileges using sudo, and I’m done.\nAs much as the thought of returning to my childhood and running a computer without any desktop environment at all amused me… no. No we are going to have a desktop. I did think about other possible desktop environments besides old faithful, but in the end decided that I actually quite like the look and feel of default GNOME (even without the various tweaks that distros usually overlay on top of it) so I installed that. I did, however, make one concession to nostalgia. I decided not to have the machine automatically boot into GNOME. Instead I followed the instructions so I could start it with startx as soon as I log in, or choose to stay in the shell.\nInstalling a decent browser (firefox, obviously…) was easy, but not surprisingly the font situation for the web was a bit tricky. Arch doesn’t come with an extensive font library so the browser would often rely on fallback fonts for pages that don’t bundle the fonts, making a lot of pages look a bit unpleasant. Fixing that took a bit of digging. The best description on fonts I found was this gist. I have a suspicion that it’s the noto-fonts package that does a lot of the work in fixing the issues\nFixing the touchpad scroll direction and speed was awkward too. Initially the settings panel in GNOME didn’t acknowledge that I even had a touchpad, which was annoying. So I started trying to edit the xorg.conf settings and… actually I don’t think that fixed anything but weirdly after editing /etc/X11/xorg.conf the touchpad settings magically showed up in the GNOME settings panel and then I could edit them. Yeah. I have no idea whatsoever whether this had anything to do with me, or if the gods at large were messing with me or what. So um… yeah, future Danielle, best of luck!\n\nMoving on…"
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#setting-up-r-on-arch",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#setting-up-r-on-arch",
    "title": "btw I use Arch now",
    "section": "Setting up R on Arch",
    "text": "Setting up R on Arch\nAny time I get a new machine set up, my first discretionary task is to make sure that I can do data science work with it (that’s a depressingly revealing statement about me). Once upon a time that really meant getting R set up, but I’m a bit more polyglot nowadays (gasp!). Nevertheless, R is my first love so I always start there…\nSome of the set up tasks are easy, or at least as easy as anything gets on linux. For example, installing R is “easy”, and installing git is “easy”. Similarly, if you use VS code as your IDE, that too is “easy”. These are all easy tasks because they’re officially supported. You can install all three with one command:\nsudo pacman -S r git code\nThat installs the most recent R release, the current git release, and the current VS code release. It’s considerably easier than installing on a Ubuntu LTS release, especially for R, to be honest. On Ubuntu there’s a lot of messing about trying to get the latest versions. The Arch repositories are up to date, which does simplify matters considerably.\nThings become a little trickier when you have to venture outside the official repositories. For example, suppose I want to use RStudio as my IDE. There isn’t an Arch package for RStudio, and – to the surprise of nobody – Posit doesn’t release one either. However, I’m hardly the first person to want to use RStudio on Arch, so it is equally unsurprising that the Arch community has stepped in to help fill the gap.\nIn other words, I’ve reached the point where I have to start installing from the Arch user repository (AUR). This is a community resource, so you have to be a bit more careful in checking that the packages you install from here are good ones, but that’s no different to investigating an R package before installing from CRAN or GitHub. It’s also really important to read through the AUR guidelines before you start trying to use it, because it talks about the tools you’ll need to install first and has a nice walkthrough of the process. So I did that, installed everything from the base-devel group and got started…3\nThere are a few different user submitted packages for RStudio on the AUR. The one I decided to use was rstudio-desktop-bin, largely because it’s a binary4 and because other Arch users seem to like it. It’s a multi-step process. First I had to get a copy of the package files. The easiest way is with git:\ngit clone https://aur.archlinux.org/rstudio-desktop-bin.git\nThe rstudio-desktop-bin folder that I just downloaded contains a PKGBUILD file… opening it in a text editor reveals that it’s basically a recipe for building a package. It doesn’t actually contain any of the components you need, and in fact for RStudio what you’ll notice when you take a peek at the inside is that it’s essentially a wrapper telling Arch how to use the .deb binary that Posit releases for Ubuntu/Debian systems.\nTo build the package I navigate to this folder and call makepkg:\ncd rstudio-desktop-bin\nmakepkg -s\nThe -s flag attempts to synchronise: it will download and install any dependencies, as long as those are official dependencies (I think?). It doesn’t install dependencies when those are also packages on the AUR. Those you just have to do manually.\nAnyway once makepkg does its job, you end up with a whole lot of new files in that folder. The one that we care most about is the one with the .pkg.tar.zst file extension. That’s the one that pacman can install:\nsudo pacman -U rstudio-desktop-bin-2022.12.0.353-1-x86_64.pkg.tar.zst\nThis will install RStudio and… it almost works. When I attempt to open RStudio I get a message in the Rstudio window complaining about a missing library. Digging into the comments on the rstudio-desktop-bin revealed the problem. One of the system dependencies for RStudio is missing from the PKGBUILD file: you have to install openssl-1.1 to make it work. This did the trick for me:\nsudo pacman -s openssl-1.1\nAnd that’s it. Just like that I have RStudio running on Arch…\n… and just like that I felt an immediate need to also get quarto running on Arch so that I could write this blog post on my new Arch box. Quarto is also available on the AUR, and I installed it using the quarto-cli package, following essentially the same process described above. There were no missing dependencies, and everything seems to work fine.\nI’m suspicious. Things aren’t supposed to work first time. I fully expect my laptop to catch fire simply because I am doing something foolish."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#was-it-worth-it",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#was-it-worth-it",
    "title": "btw I use Arch now",
    "section": "Was it worth it?",
    "text": "Was it worth it?\nProbably not. But I don’t care. I had fun. Sometimes we do things because it’s useful to do them. Sometimes we do things because we like doing them. It’s important to know the difference. This was fun."
  },
  {
    "objectID": "posts/2022-12-31_btw-i-use-arch/index.html#footnotes",
    "href": "posts/2022-12-31_btw-i-use-arch/index.html#footnotes",
    "title": "btw I use Arch now",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNo.↩︎\nDon’t ask why, it’s a long and unbearably stupid story.↩︎\nAfter I posted this on mastodon, Urs Wilke kindly told me about yay, which simplifies the process of installing packages from AUR. I intend to look into this! It looks really nice↩︎\nI have indeed managed to build the RStudio IDE from source on Arch. Once. I shan’t be doing that again. All my horrible memories of building all the Apache Arrow R and C++ libraries from source for the very first time came flooding back.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes from a data witch",
    "section": "",
    "text": "Hidden link used to verify my account on fosstodon.org Hidden link used to verify my account on genart.social Hidden link used to verify my account on hachyderm.io\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThree short stories about targets\n\n\n\nR\n\n\nReproducibility\n\n\nParallel Computing\n\n\nBlogging\n\n\nLiterate Programming\n\n\n\nIn which our intrepid adventurer turns a hacky data visualisation exercise into an analysis pipeline; builds an R blog with litedown and targets; and tries to wrap her head…\n\n\n\nDanielle Navarro\n\n\nJan 8, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe schools of magic\n\n\n\nR\n\n\nData Visualisation\n\n\nTidy Tuesday\n\n\n\nCode and explanations for a Tidy Tuesday data visualisation exercise\n\n\n\nDanielle Navarro\n\n\nJan 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code VII: Pixel filters\n\n\n\nR\n\n\nArt\n\n\n\nAn introduction to the ggfx package. Some weirdly distorted images are created. Good times are had by all concerned\n\n\n\nDanielle Navarro\n\n\nDec 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code VI: Tiles and tessellations\n\n\n\nR\n\n\nArt\n\n\n\nThere are a lot of rectangles in this post. Also some foolishness with voronoi tessellations I guess\n\n\n\nDanielle Navarro\n\n\nDec 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code V: Iterated function systems\n\n\n\nR\n\n\nArt\n\n\n\nA deep dive into the fractal flame algorithm, an egregious coding error, and a lot more C++ than I would prefer to be writing\n\n\n\nDanielle Navarro\n\n\nDec 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code IV: Shading tricks\n\n\n\nR\n\n\nArt\n\n\n\nLet’s be honest, this entire post is a thinly-veiled excuse to play around and do terrible things with the rayshader package\n\n\n\nDanielle Navarro\n\n\nDec 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code III: Polygon tricks\n\n\n\nR\n\n\nArt\n\n\n\nIt’s almost offensive how much you can do with 500 nearly-transparent polygons\n\n\n\nDanielle Navarro\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code II. Spatial tricks with ambient\n\n\n\nR\n\n\nArt\n\n\n\nFun and games with spatial noise patterns, courtesy of the ambient package\n\n\n\nDanielle Navarro\n\n\nDec 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt from code I: Generative art with R\n\n\n\nR\n\n\nArt\n\n\n\nAn act of wholesale, unapologetic theft. In which the author discusses the core logic of generative art, and (ab)uses the ggplot2 package for her own ungodly purposes\n\n\n\nDanielle Navarro\n\n\nDec 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBaby got backreferences\n\n\n\nR\n\n\nRegular Expressions\n\n\n\nNow I have \\2 problems\n\n\n\nDanielle Navarro\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBézier curve\n\n\n\nR\n\n\nArt\n\n\nWorms\n\n\n\nWhat sins will she commit?\n\n\n\nDanielle Navarro\n\n\nNov 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian estimation for Emax regression\n\n\n\nR\n\n\nPharmacometrics\n\n\nBayes\n\n\nStatistics\n\n\nStan\n\n\n\nIn which the author sighs a lot\n\n\n\nDanielle Navarro\n\n\nNov 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor fs\n\n\n\nR\n\n\nCommand Line\n\n\n\nLet’s be honest… I wrote this post because I wanted an excuse to be sweary. But I guess it’s also a post about interacting with the file system in R\n\n\n\nDanielle Navarro\n\n\nOct 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse of flextable\n\n\n\nR\n\n\nData Wrangling\n\n\n\nIn which the author explores the flextable R package for table construction, and gushes far too much about the work of Iain M. Banks\n\n\n\nDanielle Navarro\n\n\nJul 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking tables in R with table1\n\n\n\nR\n\n\nData Wrangling\n\n\n\nA pretty reckless dive into a package that hits the sweet spot for me\n\n\n\nDanielle Navarro\n\n\nJun 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting data in Julia\n\n\n\nJulia\n\n\nData Visualisation\n\n\n\nPart three of this three-part series of Julia posts, in which I try to figure out how to draw some nice plots\n\n\n\nDanielle Navarro\n\n\nMar 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with data in Julia\n\n\n\nJulia\n\n\nData Wrangling\n\n\n\nPart two of a three-part series on Julia, in which the author teaches herself the basics of wrangling rectangular data in Julia\n\n\n\nDanielle Navarro\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA foundation in Julia\n\n\n\nJulia\n\n\n\nPart one of a three-part series in which I swear I was going to keep it brief but ended up writing a staggeringly long set of notes on learning a bit of Julia\n\n\n\nDanielle Navarro\n\n\nMar 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating new generative art tools in R with grid, ambient, and S7\n\n\n\nR\n\n\nObject-Oriented Programming\n\n\nGrid Graphics\n\n\nArt\n\n\n\nThere might be a darker undercurrent in this one\n\n\n\nDanielle Navarro\n\n\nFeb 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSplatter\n\n\n\nArt\n\n\nR\n\n\n\nHow to make a mess with ggplot2 and ambient\n\n\n\nDanielle Navarro\n\n\nJan 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on the Emax model\n\n\n\nPharmacometrics\n\n\n\nA post that is little more than notes-to-self, on a model I want to make sure I understand\n\n\n\nDanielle Navarro\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaking generative art with observable.js\n\n\n\nJavascript\n\n\nArt\n\n\nObservable\n\n\n\nI’ve used observable.js before, but this time around I actually tried to understand how it works\n\n\n\nDanielle Navarro\n\n\nJan 9, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWriting knitr hooks\n\n\n\nR\n\n\nLiterate Programming\n\n\n\nCustomising knitr output with hook functions is a handy skill, and I can never quite remember it when I need to, so here’s a blog post to remind myself\n\n\n\nDanielle Navarro\n\n\nDec 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFine-grained control of RNG seeds in R\n\n\n\nR\n\n\nRandomness\n\n\n\nIn which the author was trying to fuck a spider but ended up shaving a yak with a fabulous Dua Lipa soundtrack in the background\n\n\n\nDanielle Navarro\n\n\nDec 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe blogdown of theseus\n\n\n\nJavascript\n\n\nR\n\n\nBlogging\n\n\nLiterate Programming\n\n\n\nBecause you know what? I am here to fuck spiders\n\n\n\nDanielle Navarro\n\n\nDec 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClosed form solutions for a two-compartment pharmacokinetic model\n\n\n\nPharmacometrics\n\n\n\nThere are times when you simply have to derive a thing yourself in order to understand it, even if no-one else cares\n\n\n\nDanielle Navarro\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnother year ends\n\n\n\nEnd of Year\n\n\n\nSome say the year ends in fire, some say in ice. As to my own desire, any end at all will be nice.\n\n\n\nDanielle Navarro\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPharmacometric simulation with rxode2\n\n\n\nR\n\n\nPharmacometrics\n\n\n\nBasically the same post as the last one. It’s a brief introduction to pharmacometric simulation, but this time I’m using rxode2 instead of mrgsolve\n\n\n\nDanielle Navarro\n\n\nAug 28, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPharmacometric simulation with mrgsolve\n\n\n\nR\n\n\nPharmacometrics\n\n\n\nYet another post in the ever-expanding series where Danielle teaches herself pharmacometric modelling, which I’m certain is thrilling to everyone\n\n\n\nDanielle Navarro\n\n\nAug 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFour ways to write assertion checks in R\n\n\n\nR\n\n\n\nIt’s not 50 ways to leave your lover, but spend enough time talking about assertive programming in the bedroom and you’ll only have 49 more to discover\n\n\n\nDanielle Navarro\n\n\nAug 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with Torsten\n\n\n\nR\n\n\nStan\n\n\nTorsten\n\n\nPharmacometrics\n\n\n\nAnother small step along the path to Bayesian pharmacometrics\n\n\n\nDanielle Navarro\n\n\nJul 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeware the IDEs of Windows (Subsystem for Linux)\n\n\n\nLinux\n\n\nR\n\n\n\nSetting up nice developer environments on a Windows / Ubuntu-for-WSL system with the help of RStudio Server and the VS Code WSL extension\n\n\n\nDanielle Navarro\n\n\nJul 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMakefiles. Or, the balrog and the submersible\n\n\n\nReproducibility\n\n\n\nFly, you fools!\n\n\n\nDanielle Navarro\n\n\nJun 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExtracting tables from pdf files with tabulizer\n\n\n\nR\n\n\nData Wrangling\n\n\n\nSomething nicer than the last post\n\n\n\nDanielle Navarro\n\n\nJun 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn living in dark times\n\n\n\nR\n\n\nStatistics\n\n\n\nEstimating the number internally displaced transgender people in the United States.\n\n\n\nDanielle Navarro\n\n\nJun 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA little ANSI trickery\n\n\n\nR\n\n\n\nFun and games with asciicast and cli. There’s really not much to this one except a little bit of personal entertainment\n\n\n\nDanielle Navarro\n\n\nJun 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPins and needles\n\n\n\nR\n\n\nReproducibility\n\n\n\nLearning my lessons the hard way\n\n\n\nDanielle Navarro\n\n\nJun 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a population pharmacokinetic model with Stan\n\n\n\nStatistics\n\n\nPharmacometrics\n\n\nR\n\n\nStan\n\n\nNONMEM\n\n\nBayes\n\n\n\nIn which the author works her way through the first part of an online tutorial, rewrites some NONMEM code in Stan, and takes notes\n\n\n\nDanielle Navarro\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware design by example\n\n\n\nSoftware Design\n\n\nJavascript\n\n\nR\n\n\nRegular Expressions\n\n\n\nA book review, sort of. Not really. Look, Greg sent me a copy and I had fun reading it. okay?\n\n\n\nDanielle Navarro\n\n\nMay 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSantoku\n\n\n\nR\n\n\nData Wrangling\n\n\n\nOn the small joys of clean, single-purpose R packages.\n\n\n\nDanielle Navarro\n\n\nMay 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPharmacokinetic ODE models in Stan\n\n\n\nStatistics\n\n\nPharmacometrics\n\n\nStan\n\n\nBayes\n\n\nMCMC\n\n\nR\n\n\n\nA discussion of one- and two-compartment pharmacokinetic models with model implementations in Stan, covering both first-order dynamics and Michaelis-Menten kinetics.\n\n\n\nDanielle Navarro\n\n\nMay 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-compartmental analysis\n\n\n\nStatistics\n\n\nPharmacometrics\n\n\nR\n\n\n\nA heavily oversimplified discussion of a simple pharmacokinetic tool, written as an exercise in teaching myself something new. Learning new things is always fun\n\n\n\nDanielle Navarro\n\n\nApr 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA personal essay on Bayes factors\n\n\n\nStatistics\n\n\nBayes\n\n\n\nIn which the dead return to life and the author has opinions\n\n\n\nDanielle Navarro\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Metropolis-Hastings algorithm\n\n\n\nStatistics\n\n\nMCMC\n\n\n\nA note that I wrote for a computer science class I taught all the way back in 2010\n\n\n\nDanielle Navarro\n\n\nApr 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to run R code in the browser with webR\n\n\n\nR\n\n\nWebR\n\n\n\nIn which the author gets unreasonably excited about a new thing\n\n\n\nDanielle Navarro\n\n\nApr 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative art with grid\n\n\n\nArt\n\n\nR\n\n\nGrid Graphics\n\n\n\nI decided it was time to learn more about the grid package in R, and so naturally used it to make generative art\n\n\n\nDanielle Navarro\n\n\nMar 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShattered landscapes\n\n\n\nArt\n\n\nR\n\n\n\nUsing ambient and rayshader to create weird, broken landcape images in R\n\n\n\nDanielle Navarro\n\n\nMar 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFun and games with p5.js and observable.js in quarto\n\n\n\nArt\n\n\nP5\n\n\nObservable\n\n\nJavascript\n\n\nQuarto\n\n\n\nOkay it’s a short post in which I teach myself a bit of p5.js, but it does have five different donut examples which seems cool?\n\n\n\nDanielle Navarro\n\n\nJan 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying R with kubernetes\n\n\n\nR\n\n\nDocker\n\n\nKubernetes\n\n\nPlumber\n\n\n\nIn which it is painfully clear that the author is trying to figure it all out as she goes\n\n\n\nDanielle Navarro\n\n\nJan 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlaying with docker and the github container registry\n\n\n\nLinux\n\n\nR\n\n\nDocker\n\n\n\nIn which the author learns how to use docker and, in her usual fashion, goes a little bit overboard. But there are also generative art whales, so that’s nice.\n\n\n\nDanielle Navarro\n\n\nJan 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbtw I use Arch now\n\n\n\nLinux\n\n\nR\n\n\n\nThere is no reason for this.\n\n\n\nDanielle Navarro\n\n\nDec 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA very strange year\n\n\n\nEnd of Year\n\n\n\nA wrap up of my 2022. Even by the standards of the recent past this has been a very weird year for me. A lot has happened, both professionally and personally. I’m still…\n\n\n\nDanielle Navarro\n\n\nDec 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQueue\n\n\n\nParallel Computing\n\n\nR\n\n\nObject-Oriented Programming\n\n\n\nIt is a truth universally acknowledged, that a post about multithreading must be in want of an async trick\n\n\n\nDanielle Navarro\n\n\nDec 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCrayola crayon colours\n\n\n\nR\n\n\nData Wrangling\n\n\nData Visualisation\n\n\n\nAm I bored in the house? Yes. And am I in the house bored? Also yes. But do I have rvest and a stubborn desire not to allow the horrors of ‘data encoded in the CSS style’ to…\n\n\n\nDanielle Navarro\n\n\nDec 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnpacking Arrow Datasets\n\n\n\nApache Arrow\n\n\nR\n\n\n\nA comment on how Datasets work in Apache Arrow. I’m not really sure who the audience for this one. Am I just talking to myself? Probably.\n\n\n\nDanielle Navarro\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEverything I know about Mastodon\n\n\n\nMastodon\n\n\nSocial Media\n\n\n\nA hastily written guide for data science folks trying to navigate the fediverse.\n\n\n\nDanielle Navarro\n\n\nNov 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding an Arrow Flight server\n\n\n\nApache Arrow\n\n\nNetworking\n\n\nR\n\n\nPython\n\n\n\nA step by step walkthrough introducing the reader to Arrow Flight, with examples in R and Python\n\n\n\nDanielle Navarro\n\n\nOct 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData transfer between Python and R with rpy2 and Apache Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\nPython\n\n\n\nA Pythonic approach for sharing Arrow Tables between Python and R. This is the second in a two-part series on data transfer. In this post I discuss how the rpy2 Python…\n\n\n\nDanielle Navarro\n\n\nSep 16, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassing Arrow data between R and Python with reticulate\n\n\n\nApache Arrow\n\n\nR\n\n\nPython\n\n\n\nIn a multi-language ‘polyglot’ data science world, it becomes important that we are able to pass large data sets efficiently from one language to another without making…\n\n\n\nDanielle Navarro\n\n\nSep 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSudo ask me a password\n\n\n\nLinux\n\n\nCredentials\n\n\nR\n\n\n\nResolving a little quirk in managing packages with pak on linux\n\n\n\nDanielle Navarro\n\n\nSep 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to visualise a billion rows of data in R with Apache Arrow\n\n\n\nApache Arrow\n\n\nData Visualisation\n\n\nR\n\n\n\nIn which the author grapples with the awkward question of what data visualisation really means when you have a staggering amount of data to work with\n\n\n\nDanielle Navarro\n\n\nAug 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArrays and tables in Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\n\nIn which the author discusses arrays, chunked arrays, record batches, tables, and datasets in the R interface to Apache Arrow. Masochism is mentioned, and the music of…\n\n\n\nDanielle Navarro\n\n\nMay 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPorting a distill blog to quarto\n\n\n\nQuarto\n\n\nBlogging\n\n\nReproducibility\n\n\nLiterate Programming\n\n\n\nI have moved this blog from distill over to quarto, and taken notes. A year after starting the blog, this promises to be an interesting reproducibility test\n\n\n\nDanielle Navarro\n\n\nApr 20, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Amazon S3 with R\n\n\n\nAmazon S3\n\n\nR\n\n\n\nSomehow I was convinced that using Amazon S3 would be a supremely difficult thing to learn, kind of like learning git and GitHub for the first time. Thankfully, it’s…\n\n\n\nDanielle Navarro\n\n\nMar 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData types in Arrow and R\n\n\n\nApache Arrow\n\n\nR\n\n\n\nA post describing fundamental data types in R and Apache Arrow, and how data is exchanged between the two systems. It covers conversion of logicals, integers, floats…\n\n\n\nDanielle Navarro\n\n\nMar 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR scripts for twitter mutes and blocks\n\n\n\nTwitter\n\n\nR\n\n\nSocial Media\n\n\n\nSocial media safety tools like muting and blocking are often misused, but for people who are targeted for harassment they are powerful and important. This is a brief…\n\n\n\nDanielle Navarro\n\n\nFeb 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBinding Apache Arrow to R\n\n\n\nApache Arrow\n\n\nR\n\n\n\nI’ve been learning how to program with Apache Arrow inside R, and also I have been watching the SyFy show “The Magicians” obsessively. For no sensible reason I wrote a blog…\n\n\n\nDanielle Navarro\n\n\nJan 18, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSetting CRAN repository options\n\n\n\nR\n\n\n\nA quick post on how to use RStudio public package manager instead of a standard CRAN mirror, and an example of why that can be useful sometimes.\n\n\n\nDanielle Navarro\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started with Apache Arrow\n\n\n\nApache Arrow\n\n\nR\n\n\n\nI’ve been wanting to learn the basics of Apache Arrow a while: this is the story of how an R user learned to stop worrying and love a standardised in-memory columnar data…\n\n\n\nDanielle Navarro\n\n\nNov 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData serialisation in R\n\n\n\nSerialisation\n\n\nR\n\n\n\nA terrifying descent into madness, or, an explanation of how R serialises an in-memory data structure to summon a sequence of bytes that can be saved or transmitted.…\n\n\n\nDanielle Navarro\n\n\nNov 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnpredictable paintings\n\n\n\nArt\n\n\nR\n\n\n\nAn example showing how to build a generative art system in R. The post walks through some of the creative and design choices that are involved, and highlights how much of a…\n\n\n\nDanielle Navarro\n\n\nNov 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative art resources in R\n\n\n\nArt\n\n\nR\n\n\n\nAn extremely incomplete (and probably biased) list of resources to help an aspiring generative artist get started making pretty pictures in R\n\n\n\nDanielle Navarro\n\n\nOct 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow to mint digital art on HEN\n\n\n\nArt\n\n\n\nNot every artist wants to make cryptoart, and that’s okay. Others do, and that’s okay too. But if you want to try it out in a socially responsible way, it takes a bit of…\n\n\n\nDanielle Navarro\n\n\nSep 26, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising the hits of Queen Britney\n\n\n\nTidy Tuesday\n\n\nData Visualisation\n\n\nR\n\n\n\nA gentle walkthrough of a few data wrangling and visualisation tools using the Billboard 100 data for this weeks Tidy Tuesday. Pitched at beginners looking to refresh their…\n\n\n\nDanielle Navarro\n\n\nSep 14, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArt, jasmines, and the water colours\n\n\n\nArt\n\n\nR\n\n\n\nAn essay and tutorial covering a few useful art techniques in R\n\n\n\nDanielle Navarro\n\n\nSep 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nManaging GitHub credentials from R, difficulty level linux\n\n\n\nGit\n\n\nCredentials\n\n\nLinux\n\n\nR\n\n\n\nA sick sad story in which a humble R user was forced to learn something about how linux stores passwords and, more importantly, got R to use her GitHub credentials properly\n\n\n\nDanielle Navarro\n\n\nAug 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenerative art in R\n\n\n\nArt\n\n\nR\n\n\n\nComments on an exhibit I contributed to as part of useR!2021\n\n\n\nDanielle Navarro\n\n\nJul 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstrap cards in distill\n\n\n\nBlogging\n\n\nBootstrap\n\n\nDistill\n\n\nR\n\n\n\nHow to enable bootstrap 4 on a distill website, even though you probably don’t need to. I like it though because I get to add pretty bootstrap cards\n\n\n\nDanielle Navarro\n\n\nApr 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPretty little CLIs\n\n\n\nCommand Line\n\n\nR\n\n\n\nHow to make a gorgeous command line interface in R using the cli package. Somewhere along the way I accidentally learned about ANSI control codes, which strikes me as…\n\n\n\nDanielle Navarro\n\n\nApr 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to the jungle\n\n\n\nBlogging\n\n\nReproducibility\n\n\nDistill\n\n\n\nI have reluctantly decided to create a new blog. Some thoughts on what I hope to achieve, having tried my hand at blogging so very many times before\n\n\n\nDanielle Navarro\n\n\nApr 5, 2021\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "",
    "text": "After many years of using Ubuntu as my primary operating system for data science work, I’ve found myself in a situation where my primary laptop runs Windows. There are reasons for this. It does, however, introduce some problems. My workflows are often built around Unix-like tools, and life without bash is too heavy a burden for me to bear. Fortunately, there is a solution: Windows Subsystem for Linux (WSL). The process for getting set up with WSL is essentially as follows:\nRepeat step 3 for all the other command line tools you love (and replace step 2 with your favourite distro if you don’t love Ubuntu), and just like that you have a working setup for all your favourite data science workflows on Linux, living happily within a Windows environment."
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html#some-bits-are-tricksy",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html#some-bits-are-tricksy",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "Some bits are tricksy",
    "text": "Some bits are tricksy\nThere’s a sense in which WSL is a kind of dark magic, one that allows me to run two operating systems side by side on the same machine, and allowing extensive interoperability between them. By installing WSL and Ubuntu for WSL on my laptop, I’ve created a situation where I have two filesystems co-existing on my machine, and two qualitatively different kinds of shell to interact with them. On the Windows side I can use powershell, and on the Ubuntu side I can use bash. Regardless of which shell I use (and which operating system powers it), I have the ability to find files on both filesystems, as long as I know what paths to look for.\nThe key thing to understand is that Ubuntu mounts the Windows C: drive at /mnt/c/, and Windows locates the Ubuntu root directory at \\\\wsl.localhost\\Ubuntu-22.04\\. So, for instance, given that my Windows user is called strin,1 and my Ubuntu username is danielle, the paths to the user home directories are:\n\n\n\n\n\n\n\n\n\nFolder:\nUbuntu path (bash):\nWindows path (powershell):\n\n\n\n\nUbuntu user directory\n/home/danielle\n\\\\wsl.localhost\\Ubuntu-22.04\\home\\danielle\n\n\nWindows user directory\n/mnt/c/Users/strin\nC:\\Users\\strin\n\n\n\n\nThis illustrates some of the subtleties. Decisions need to be made about which files and applications should live in the Windows filesystem, and which should live in the Ubuntu filesystem. For example, I don’t use my Dropbox folder for data science work, so it lives in Windows at C:\\Users\\strin\\Dropbox. On the other hand, my GitHub repositories are used mostly for data science and developer work, so they all live in Ubuntu and are all kept in the /home/danielle/GitHub folder.\nIt becomes trickier still when thinking about applications that require a GUI. The Ubuntu for WSL installation doesn’t come with a desktop environment. You certainly can install one, and if you’re willing to do the finicky work setting up an X server for Windows, you can in fact set things up so that Ubuntu for WSL has its own floating desktop environment in Windows (just like the one you’d get if you were running Ubuntu as a virtual machine). However, I don’t fancy doing all that work just to get a web browser up and running… so I made the practical decision that my Firefox installation should be the regular Windows version, living in Windows like any other Windows application.\nThe trickiness reaches its zenith when you start thinking about where your favourite IDE should live. At its heart an IDE is a GUI, and as such it’s going to run so much more smoothly if you install it as a Windows application. But IDEs are also used to start terminals, to write code, and execute all sorts of tasks that require access to the data science tooling… all of which live on Ubuntu.\nHow do we resolve this? In my case, I have two IDEs that I cannot live without. For my everyday work I am almost perfectly split between RStudio and VS Code. Here’s the approach I took with each."
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html#rstudio-setup",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html#rstudio-setup",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "RStudio setup",
    "text": "RStudio setup\nWhat are my options for RStudio?\n\nOption 1: RStudio Desktop for Windows. If I go down this path the RStudio GUI will look very nice, but I won’t be able to use the Ubuntu installation of R. I’d have to install R for Windows, and then I’d have to configure RTools if I want to do developer work, and now I’d need C/C++ compilers on Windows, and… at that point I’d essentially have to manage an entire Windows data science stack in parallel with my Ubuntu data science stack. Not loving this as an option.\nOption 2: RStudio Desktop for Ubuntu. This version of RStudio will have no problems finding the Ubuntu installation, but without a lot of tinkering it won’t be able to create the RStudio GUI. Again, there’s a solution to this (and I did actually get it up and running), if you’re okay with running an X-server for Windows like X410. It does work, but it’s finicky. Fonts and cursors don’t look right if you’re running X410 in windowed mode, and if you want to run it in floating desktop mode then you have to set up the entire Ubuntu desktop environment just to run RStudio. Not loving this one either.\nOption 3: RStudio Server for Ubuntu. This is easily my preferred option. RStudio Server loves to run on Ubuntu, and so of course it has no problems finding the Ubuntu installation of R. Even better, it doesn’t require an Ubuntu desktop environment. When you start RStudio Server running from bash (sudo rstudio-server start), it starts a server running (most likely at http://localhost:8787/). Back on Windows, I can open up my browser and point it to that address, and can login using my Ubuntu username and password. Works beautifully. I have Ubuntu-native versions of R and RStudio Server, and a Windows-native RStudio GUI through the browser.\n\nHaving made my choice, I sort of know what I need to search for to solve the problem. There are good resources out there for this, and I’m not doing a walkthrough in this post. The purpose of the post is to document the approach I’ve taken so that I can remember how to do it next time, and point future-me at the right resources. That said, these are the resources I relied upon to set up option 3:\n\nUsing RStudio Server in Windows WSL2\nRStudio Server\nWhat is my username on my RStudio Workbench / RStudio Server installation?\n\nThe result:\n\n\n\n\nWorking on this blog post within RStudio. The GUI runs within Firefox on Windows, connected to RStudio Server running on Ubuntu for WSL, both of which exist on my local machine."
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html#vs-code-setup",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html#vs-code-setup",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "VS Code setup",
    "text": "VS Code setup\nOkay that’s RStudio sorted. What about VS Code?\nThe solution for VS Code is similar but not the same. The core principle is the same: the GUI should be native to Windows, and the underlying engine should be native to Ubuntu. The way you actually do this for VS Code, however, is slightly different. Essentially, what you want to do is install the Windows version of VS Code, and then – within VS Code – install the WSL extension. You’ll also need to install a bunch of other extensions too but that’s the normal experience for VS Code, for every language and every operating system.\nAs before, I’m not going to do a walkthrough, but these are the resources I used when getting VS Code set up properly on my Windows/Ubuntu-on-WSL box:\n\nGet started using Visual Studio Code with Windows Subsystem for Linux\nWSL 2 with Visual Studio Code\n\nThe result:\n\n\n\n\nWorking on this blog post within VS Code. The GUI for VS Code is a native Windows application, but in the lower left hand corner you can see that it connects to Ubuntu for WSL to handle code execution and so forth."
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html#ubuntu-desktop",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html#ubuntu-desktop",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "Ubuntu desktop",
    "text": "Ubuntu desktop\nAs a kind of postscript… it’s entirely possible to have a fully functional Ubuntu desktop in a setup like this. The main thing you need is an X server. I’ve no idea what the best X server for Windows options are: I went with X410 because it seems pretty professional and work has clearly gone into it, but you do have to pay actual money for it. There might be better options. Anyway, the instructions I followed are in this blog post. At the end of it you have the Ubuntu desktop environment installed, and a handy bash script that does the configuration you need. You can start the session like this:\n. ~/.bash_ubuntu_desktop\ngnome-session --session=ubuntu\nIf you’ve gone to the additional effort of setting up snap, and installing the Firefox snap on your Ubuntu system (yes, I do have two copies of Firefox on my machine), you can do something unbearably absurd like run R-within-RStudio-Server-within-Firefox-within-Ubuntu-desktop-within-Windows. Screenshot, simply to prove that I actually did this:\n\n\n\n\nWorking on this blog post within RStudio… within a browser… within Ubuntu on WSL… within X410… on Windows. Yes this is absurd.\n\n\n\nNo, it’s not a very good idea."
  },
  {
    "objectID": "posts/2023-07-02_the-ides-of-wsl/index.html#footnotes",
    "href": "posts/2023-07-02_the-ides-of-wsl/index.html#footnotes",
    "title": "Beware the IDEs of Windows (Subsystem for Linux)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA truncation of stringrbelle, which I occasionally use as a username.↩︎"
  },
  {
    "objectID": "posts/2023-12-24_blogdown-of-theseus/index.html",
    "href": "posts/2023-12-24_blogdown-of-theseus/index.html",
    "title": "The blogdown of theseus",
    "section": "",
    "text": "It is the first day of my summer vacation. The out-of-office autoreply is on. I have a full tank of gas, half a pack of cigarettes, the sun is shining, and I’m wearing a sequined dress. Blues Brothers it is most certainly not, but a certain attitude is in force. And so it is that I’ve decided to get the band back together. Where “the band” in this case happens to be “a tool chain that looks like a shit version of blogdown”.\nIs it a good use of my time? No. Will I do a good job of it? No. But will it it make a good blog post? Also no.\nOkay. So here’s the backstory. Literate programming in R has been around for a very long time. So much so that we’ve all become accustomed to thinking about tools like R markdown, blogdown, and quarto, as baked-in aspects to the language. That’s not actually a bad thing. They’re good tools. I have no intention of abandoning any of them. But they aren’t primitives. Each of them is an opinionated tool that takes a code execution engine like knitr as a starting point, and builds from it in different ways. R markdown and quarto both use knitr to execute the R code within an appropriately annotated markdown document and then feed the results to pandoc to create outputs in different formats. Blogdown takes the same idea, but passes the output to the hugo static site generator to create full featured blogs and static websites. Et cetera.\nWhat would happen if those “upstream” tools were taken away? What if you needed to create an R blog from scratch and the only part of this tool chain you had available to you was knitr. What choices would you make? Could you cobble together something vaguely similar to a blogdown site or a quarto blog, using entirely different constituent parts?\nWhy would you do this? You wouldn’t.\nBut for reasons that absolutely made sense to me at the time, this is precisely what I did. The website is live at knitr-11ty.djnavarro.net/, and – because I have absolutely no intention of writing the same blog post twice – if you want to “read more” as they used to say, you can check out the actual blog post here.\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{navarro2023,\n  author = {Navarro, Danielle},\n  title = {The Blogdown of Theseus},\n  date = {2023-12-23},\n  url = {https://blog.djnavarro.net/posts/2023-12-24_blogdown-of-theseus/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNavarro, Danielle. 2023. “The Blogdown of Theseus.”\nDecember 23, 2023. https://blog.djnavarro.net/posts/2023-12-24_blogdown-of-theseus/."
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html",
    "title": "Bootstrap cards in distill",
    "section": "",
    "text": "When creating R markdown websites, I often find myself wanting to organise content into a nice-looking grid of links. For example, in a recent project I wanted to be able to create something like this:\nStarting R markdown\n\nAn introduction to R markdown. The target audience is a novice R user with no previous experience with markdown.\n\n\n\n\n\n\n\n\nStarting ggplot2\n\nAn introduction to ggplot2. The target audience is a novice user with no previous experience with R or ggplot2.\n\n\n\n\n\n\n\n\nStarting programming\n\nThis is primarily a tutorial on making generative art in R, but in doing so introduces core programming constructs and data structures. It is assumed the user has some previous experience with ggplot2.\nIt bothered me that this wasn’t as straightforward as I was expecting, so for one of my side projects I’ve been putting together a small package called bs4cards to make this a little easier inside an R markdown document or website. There are some introductory articles posted on the bs4cards package website showing how the package works, and there’s no need to duplicate that content here. However, because this website uses the distill package (Allaire et al. 2021) and the package website is built using pkgdown (Wickham, Hesselberth, and Salmon 2021), it seems like a good idea to have at least one post on both sites that uses bs4cards."
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#enabling-bootstrap-4",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#enabling-bootstrap-4",
    "title": "Bootstrap cards in distill",
    "section": "Enabling bootstrap 4",
    "text": "Enabling bootstrap 4\nThe reason for doing this is that the first step in using the package is to make sure that your R markdown document uses version 4 of bootstrap: the bs4cards package takes its name from the cards system introduced in bootstrap version 4, and will not work properly if used in R markdown documents that rely on bootstrap version 3, or don’t use bootstrap at all. To ensure that you are using bootstrap 4, you need to edit the YAML header for your document to specify which version of bootstrap you want to use. The instructions are slightly different depending on what kind of document you’re creating:\n\nVanilla R markdown\nFor a plain R markdown document or website (i.e., one where the output format is html_document) here is the relevant section of YAML you might use:\noutput:\n  html_document:\n    theme:\n      version: 4\nThis overrides the R markdown defaults (Xie, Dervieux, and Riederer 2020) to ensure that the output is built using bootstrap 4.5.\n\n\nPkgdown\nTo enable bootstrap 4 in a pkgdown site, the process is similar but not identical. Edit the _pkgdown.yml file to include the following\ntemplate:\n  bootstrap: 4\nNote that this relies on a currently-in-development feature, so you may need to update to the development version of pkgdown to make this work.\n\n\nDistill\nDistill R markdown does not use bootstrap, which is a little inconvenient if you want to use bs4cards with distill. With a little effort it is possible to enable the entire bootstrap library in a distill site, but this can lead to undesirable side-effects because bootstrap has a lot of styling that doesn’t look visually appealing when mixed with the istill styling. The solution I’ve adopted for this is to use a custom bootstrap build that includes a minimal number of bootstrap components. If you want to try the same approach, you can download the strapless.css file to the same folder as the distill post you want to enable it for, and include the following YAML in the post header:\noutput:\n  distill::distill_article:\n    css: \"strapless.css\"\nIf you want to enable strapless for the entire site, this markup goes in the _site.yml file and the css file should go in the home folder for the project. Once that’s done you should be ready to go. That being said, you’d be wise to be careful when adopting this approach: the strapless build is a crude hack, and I haven’t tested it very thoroughly."
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#testing-with-pretty-pictures",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#testing-with-pretty-pictures",
    "title": "Bootstrap cards in distill",
    "section": "Testing with pretty pictures",
    "text": "Testing with pretty pictures\nJust to make certain, let’s check that it does what we want by generating cards using the galleries data that comes bundled with the bs4cards package:\n\nlibrary(bs4cards)\ngalleries %&gt;% \n  cards(title = long_name, image = image_url)\n\n\n\n\n\n\nAsh Cloud and Blood\n\n\n\n\n\nGhosts on Marble Paper\n\n\n\n\n\nIce Floes\n\n\n\n\n\nNative Flora\n\n\n\n\n\nSilhouettes\n\n\n\n\n\nTrack Marks\n\n\n\n\n\nViewports\n\n\n\n\n\n\n\nLooks about right to me?"
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#last-updated",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#last-updated",
    "title": "Bootstrap cards in distill",
    "section": "Last updated",
    "text": "Last updated\n\n2023-05-27 18:28:18.962217"
  },
  {
    "objectID": "posts/2021-04-19_bs4cards-in-distill/index.html#details",
    "href": "posts/2021-04-19_bs4cards-in-distill/index.html#details",
    "title": "Bootstrap cards in distill",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html",
    "href": "posts/2022-09-04_sudo-askpass/index.html",
    "title": "Sudo ask me a password",
    "section": "",
    "text": "One peculiar feature of earning one’s keep in society by writing a data science blog is that it provides the opportunity to be unabashedly weird. Personality is important. Other developers will read your strange content – no matter how weird and unprofessional it is – because professional corporate style is very nearly as dull as academic writing, and it is a relief to learn a new thing from an actual human being who write with a certain level of human character.\nEven if she is an irredeemably catty bitch.\nAll of which is by way of preamble, and a way to acknowlege that when the topic is package dependencies in R, the queer double entendres kind of write themselves.1 And so without further ado, today’s unhinged rambling…"
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#managing-package-dependencies-in-r",
    "href": "posts/2022-09-04_sudo-askpass/index.html#managing-package-dependencies-in-r",
    "title": "Sudo ask me a password",
    "section": "Managing package dependencies in R",
    "text": "Managing package dependencies in R\nOkay so you’ve done your due diligence. You’ve read his pkgdown site, checked out his repo, and you still like him. You really, really want to install his package. You’ve inspected the documentation and it is a very impressive package. I mean, I get it babe.\nI. have. been. there.\nThere’s a thing with packages though. When you’re installing them into wherever you like to put packages (the precise location doesn’t matter for this post2), what you see isn’t necessarily all you get.\nI’ll give a concrete example. For reasons unknown even to me I woke up this morning and decided today was the day I’d explore the rig installation manager for R that lets you manage multiple R installations on the one machine. It’s very nice, and possibly the topic for a future post. However, one side consequence to adopting rig is that I ended up with adorably fresh copies of R that had no installed packages and needed to be properly set up. In the process, I started thinking a little about the tools I use to install packages. When I first started using R my go to method was to use the install.packages() function supplied by the utils package: after all, it comes bundled with R, which makes it an easy place to start. As I matured as an R user I found myself switching to the remotes package because it provides a coherent set of functions for installing packages from CRAN, Bioconductor, GitHub, and more. I’m a huge fan of remotes, but for reasons I’ll explain in a moment I’m starting to prefer the pak package developed by Gábor Csárdi and Jim Hester. When using pak, the function you use to install packages is called pkg_install(). I’ll walk you through it. Here’s what happens when I try to install the quarto R package without properly configuring my setup. First I call the function:\n\npak::pkg_install(\"quarto\")\n\nWhen I hit enter, pak starts doing its job, resolving the R dependencies and then asking if I want to continue:\n✓ Loading metadata database ... done\n                                                                            \n→ Will install 2 packages.\n→ Will update 1 package.\n→ Will download 3 packages with unknown size.\n+ packrat         0.8.1  [bld][dl]\n+ quarto    1.1 → 1.2    [bld][dl]\n+ rsconnect       0.8.27 [bld][dl]\n? Do you want to continue (Y/n) \nI really like this approach. The interface is very clear about precisely what is happening, and pak doesn’t download any more packages than is absolutely necessary to give you what you asked for (unless you specify upgrade = TRUE in the install command).\nI agree to continue, so off pak goes, fetching the appropriate R packages:\nℹ Getting 3 pkgs with unknown sizes\n✓ Got quarto 1.2 (source) (67.58 kB)                                             \n✓ Got rsconnect 0.8.27 (source) (685.57 kB)                                      \n✓ Got packrat 0.8.1 (source) (681.50 kB)                                         \n✓ Downloaded 3 packages (1.43 MB)in 6.7s\nSo far, so good. The output is informative and succinct at the same time. It appeals to my aesthetic sensibilities. But then pak – which is very diligent about managing all dependencies including system dependencies – attempts to install the external libraries upon which quarto depends,3 and this happens:\nℹ Installing system requirements\nℹ Executing `sudo sh -c apt-get install -y make`\nError: System command 'sudo' failed, exit status: 1, stdout + stderr:\nE&gt; sudo: a terminal is required to read the password; either use the -S option to read from standard input or configure an askpass helper\nAh."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#what-went-wrong-here",
    "href": "posts/2022-09-04_sudo-askpass/index.html#what-went-wrong-here",
    "title": "Sudo ask me a password",
    "section": "What went wrong here?",
    "text": "What went wrong here?\nAn important thing to understand here is that neither pak nor sudo are to blame for the installation failure.4 On the pak side, it’s a good thing that it tries to uncover and install system dependencies: the package isn’t going to work if you don’t have those dependencies installed, and it can be a nightmare trying to track them all down when the package manager doesn’t help identify them for you.5 On the sudo side, it is extremely reasonable to expect the user to authenticate before enabling superuser privileges. Speaking for myself, I have a very strong expectation that I will be explicitly asked for my consent before packages are installed on my system.6\nThe breakdown happens because pak has invoked sudo outside of the terminal context. If you haven’t configured sudo to handle this situation, there’s no opportunity for the user to authenticate, and sudo throws an error.\nHow can we resolve this?"
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#configuring-sudo",
    "href": "posts/2022-09-04_sudo-askpass/index.html#configuring-sudo",
    "title": "Sudo ask me a password",
    "section": "Configuring sudo",
    "text": "Configuring sudo\nA little bit of digging revealed that sudo is a lot more configurable than I had preciously realised, and you can deal with this issue in a few different ways. One possibility would be to enable passwordless sudo, in which case the system dependencies would be installed without requiring a password at all. That would certainly minimise the amount of hassle at my end, but it’s also a hell of a security risk. Even if I personally felt willing to take that risk with my own property, this is a work laptop and I think a little risk-aversion is a good idea in this case.7\nFortunately, the error message itself contains some hints that there is an alternative fix that doesn’t require you to weaken your security settings (or not by very much, at any rate). Specifically, the error message refers to an “askpass helper”: a program, usually with a GUI, that sudo will invoke whenever it needs authentication from the user but is not running in a terminal. However, in order to get sudo to invoke one of these helpers, you have to explicitly configure it within the sudo configuration file, sudo.conf. This configuration file is located at /etc/sudo.conf is discussed pretty thoroughly in the sudo help documentation. Here’s the relevant part of the output when I type man sudo.conf:8\n\n askpass   The fully qualified path to a helper program used to read the\n           user's password when no terminal is available.  This may be\n           the case when sudo is executed from a graphical (as opposed to\n           text-based) application.  The program specified by askpass\n           should display the argument passed to it as the prompt and\n           write the user's password to the standard output.  The value\n           of askpass may be overridden by the SUDO_ASKPASS environment\n           variable.\n\nOkay, so I need to do two things. I need to edit sudo.conf to configure sudo to use the askpass helper, and I also need the askpass helper itself. So where do I find one of these askpass helper programs? The one I chose to go with is ssh-askpass, which I installed on my system using the following:\nsudo apt-get install ssh-askpass ssh-askpass-gnome\nNotice that I installed both ssh-askpass and ssh-askpass-gnome. The latter isn’t the askpass helper itself, and isn’t intended to be invoked separately from ssh-askpass. Rather it’s there because ssh-askpass on its own uses X11 to do the graphical user interface part and it’s not very pretty on my Ubuntu installation. By installing ssh-askpass-gnome as well, the dialog box that comes up when ssh-askpass is invoked is much nicer.\nAt the terminal, I can invoke ssh-askpass manually if I want to. It doesn’t do much by itself: all it does is create the dialog box and return the text input by the user.\nssh-askpass\nIn practice you don’t actually call this directly. Instead, you configure sudo to that whenever it needs authentication but doesn’t have access to a terminal. In order to accomplish this, here’s the lines I added to my sudo.conf file:9\n# specify ssh-askpass as my helper\nPath askpass /usr/bin/ssh-askpass\nSo I did this and then10 tried to install quarto using pkg_install(). This time around sudo no longer errored when pak tried to install system dependencies. Instead it brought up the askpass dialog box:\n\nWhen I typed in my password, pak and sudo were able to play nicely together and the installation worked just fine. Well, mostly.."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#can-we-relax-just-a-little-sweetie",
    "href": "posts/2022-09-04_sudo-askpass/index.html#can-we-relax-just-a-little-sweetie",
    "title": "Sudo ask me a password",
    "section": "Can we relax just a little sweetie?",
    "text": "Can we relax just a little sweetie?\nThe only problem is that quarto installation requires five system dependencies to be installed, and as the output below shows, pak starts a new shell process every single time…\nℹ Executing `sudo sh -c apt-get install -y make`\nℹ Executing `sudo sh -c apt-get install -y libcurl4-openssl-dev`\nℹ Executing `sudo sh -c apt-get install -y libicu-dev`\nℹ Executing `sudo sh -c apt-get install -y libssl-dev`\nℹ Executing `sudo sh -c apt-get install -y pandoc`\n…and as a consequence of this I had to enter my password five times.\nThat’s mildly irritating, and I was not expecting it. My original assumption would be that entering the password the first time would invoke the sudo password cache: that is, after entering my password once, the elevated permissions11 would persist for about 15 minutes. That’s what happens by default at the terminal, and I had presumed the same would apply when the call to sudo occurs within an R session. However, that’s not quite accurate. This little gem in man sudo explains the relevant security policy issue:\n     Security policies may support credential caching to allow the\n     user to run sudo again for a period of time without requiring\n     authentication.  By default, the sudoers policy caches creden‐\n     tials on a per-terminal basis for 15 minutes.  See the\n     timestamp_type and timestamp_timeout options in sudoers(5) for\n     more information.  By running sudo with the -v option, a user\n     can update the cached credentials without running a command.\n\nThe reason why the “15 minutes” rule doesn’t apply here is that the credentials are cached on a “per-terminal” basis. Each sudo sh command invoked by pak effectively runs a new instance of the shell and the password caching doesn’t transfer. Gr."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#editing-the-sudoers-file",
    "href": "posts/2022-09-04_sudo-askpass/index.html#editing-the-sudoers-file",
    "title": "Sudo ask me a password",
    "section": "Editing the sudoers file",
    "text": "Editing the sudoers file\nAs a general rule I can understand why sudo is conservative and does not permit the credential cache to transfer across processes. Who knows what other processes are running and what they might be doing? But in this instance I’m willing to slightly relax the security policy to ensure that the pak/sudo combination doesn’t drive me crazy by relentlessly asking for permission on every little thing.\nThe security policies in sudo are managed by plugins12 configured using the “sudoers” file(s) located at /etc/sudoers. I’ll talk about this file momentarily, but first here’s the relevant extract from man sudoers that tells us what setting we need to modify:\n     timestamp_type    sudoers uses per-user time stamp files for\n                       credential caching.  The timestamp_type op‐\n                       tion can be used to specify the type of time\n                       stamp record used.  It has the following\n                       possible values:\n\n                       global  A single time stamp record is used\n                               for all of a user's login sessions,\n                               regardless of the terminal or parent\n                               process ID.  An additional record is\n                               used to serialize password prompts\n                               when sudo is used multiple times in\n                               a pipeline, but this does not affect\n                               authentication.\n\n                       ppid    A single time stamp record is used\n                               for all processes with the same par‐\n                               ent process ID (usually the shell).\n                               Commands run from the same shell (or\n                               other common parent process) will\n                               not require a password for\n                               timestamp_timeout minutes (15 by\n                               default).  Commands run via sudo\n                               with a different parent process ID,\n                               for example from a shell script,\n                               will be authenticated separately.\n\n                       tty     One time stamp record is used for\n                               each terminal, which means that a\n                               user's login sessions are authenti‐\n                               cated separately.  If no terminal is\n                               present, the behavior is the same as\n                               ppid.  Commands run from the same\n                               terminal will not require a password\n                               for timestamp_timeout minutes (15 by\n                               default).\n\n                       kernel  The time stamp is stored in the ker‐\n                               nel as an attribute of the terminal\n                               device.  If no terminal is present,\n                               the behavior is the same as ppid.\n                               Negative timestamp_timeout values\n                               are not supported and positive val‐\n                               ues are limited to a maximum of 60\n                               minutes.  This is currently only\n                               supported on OpenBSD.\n\n                       The default value is tty.\n\n                       This setting is only supported by version\n                       1.8.21 or higher.\n\nThis documentation makes clear where the problem lies. When pak invokes sudo, a new process is spawned and unless the value of timestamp_type is set to global, the sudo credential cache doesn’t get shared across processes.\nIt’s possible to modify this setting, and I’ll show you how to do that below, but first I strongly recommend that you read this article on how to edit the sudoers file carefully. For realsies, my dears, read it. Editing policies for sudo needs to be done with a lot of care. You don’t want to mess it up and lose the ability to invoke sudo because it’s been incorrectly configured. So please, please read the linked page.\nYou read it, right?\nGood.\nAfter reading through the linked article, I made the decision that instead of editing the main sudoers file, I would instead add a small file to the /etc/sudoers.d/ directory. By default, files in this folder are automatically included when the sudoers plugin is loaded, so it’s a convenient place to add your customisations rather than editing the main file. I created one that exists solely to manage the timestamp settings for my primary user:\nsudo visudo -f /etc/sudoers.d/timestamp_type\nNotice that I’ve used visudo, and not some other editor. If you read the linked article you know why I did that, and why it is astonishingly important to do it this way in order to practice safe sudo13 policy editing. If you didn’t read the linked article… well, you would be extremely ill-advised to try the next step without actually reading it.\nOkay, that feels like enough warning. Let’s look at what I included in my new /etc/sudoers.d/timestamp_type file:\n# specify the timeout type (usual default=tty)\nDefaults:danielle timestamp_type=global\n\n# specify the timeout interval (usual default=15)\nDefaults:danielle timestamp_timeout=2\nI’ve done two things. First, in order to allow the sudo password cache to work everywhere regardless of which process invokes it, I set timestamp_type=global. Second, because this makes me a tiny bit nervous (it’s a very mild softening of security policies), I shortened the cache expiry time from 15 minutes to 2 minutes by setting timestamp_timeout=2. In practice, I very rarely do anything requiring superuser privileges that requires more than two minutes, and it seems best to let those privileges expire quickly."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#youre-a-star",
    "href": "posts/2022-09-04_sudo-askpass/index.html#youre-a-star",
    "title": "Sudo ask me a password",
    "section": "You’re a star",
    "text": "You’re a star\nAt this point I have a set up that lets me use pak without needing to weaken my security policies (well, not by much) and won’t cause me to lose my mind by typing in my password over and over until I beg for the sweet release14 of death. Was it all worth it?\nWell, let me just say this: out of curiosity I decided to try installing the stars package, which necessarily entails installing a lot of geospatial dependencies. Back when I first tried installing these tools on linux a couple of years ago it was a nightmare. I had to track down the dependencies myself and manually install them, which was pretty daunting at the time because I was very new to the whole business.15 Here’s what happened when I tried it with pak after configuring sudo to ask for my password only the once:\n\npak::pkg_install(\"stars\")\n\n→ Will install 1 package.\n→ Will download 1 package with unknown size.\n+ stars   0.5-6 [bld][dl]\nℹ Getting 1 pkg with unknown size\n✓ Got stars 0.5-6 (source) (3.42 MB)                                  \n✓ Downloaded 1 package (3.42 MB)in 4.2s                               \nℹ Installing system requirements\nℹ Executing `sudo sh -c apt-get install -y libgdal-dev`\nℹ Executing `sudo sh -c apt-get install -y gdal-bin`\nℹ Executing `sudo sh -c apt-get install -y libgeos-dev`\nℹ Executing `sudo sh -c apt-get install -y libssl-dev`\nℹ Executing `sudo sh -c apt-get install -y libproj-dev`\nℹ Executing `sudo sh -c apt-get install -y libudunits2-dev`\nℹ Building stars 0.5-6\n✓ Built stars 0.5-6 (1.4s)                                       \n✓ Installed stars 0.5-6  (98ms)                                    \n✓ 1 pkg + 16 deps: kept 12, added 1, dld 1 (3.42 MB) [20.7s]    \nOne line of code. One line of code, that worked the first time. One line of code that worked the first time and installed everything quickly. It’s a true Christmas miracle."
  },
  {
    "objectID": "posts/2022-09-04_sudo-askpass/index.html#footnotes",
    "href": "posts/2022-09-04_sudo-askpass/index.html#footnotes",
    "title": "Sudo ask me a password",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLike so many moments in life when I get this way, Dan Simpson is at least partially to blame.↩︎\nPer Lawrence v Texas.↩︎\nIt took all the self-restraint I possess not to refer to this as “the undisclosed viagra problem” in the title. Footnotes are the place for such things Danielle. Footnotes.↩︎\nThese things happen to everyone sometimes honey, it’s no big deal, etc.↩︎\nConveniently, pak also allows you to identify these dependencies separately from the R package installation. To do this, use the pkg_system_requirements() function.↩︎\nThere’s an unpleasant name for people who break that convention, and I can’t think of a joke for this one because it’s astonishingly horrible. If you don’t know from personal experience what it feels like, well, consider yourself fortunate. Sigh.↩︎\nOkay sure, I haven’t technically asked DevOps for their opinion about the possibility of me dumping the authentication requirements on superuser privileges on a work machine. Nevertheless I do have a suspicion I know what their answer would be. So, in the interests of not causing unnecessary distress, perhaps a different approach is in order…↩︎\nThe manual page is online at the linked URL.↩︎\nIn the interests of strict accuracy, I should add that had to create this file myself because it didn’t already exist on my system: prior to making this edit, sudo was using all the default settings. Relatedly, I had to use whereis ssh-askpass in order to find the path to ssh-askpass because the advice I’d seen online listed a different path and, shockingly, it doesn’t work if you don’t provide the correct path.↩︎\nAfter restarting R, just in case.↩︎\nNegotiations are vital and CNC isn’t for newbies.↩︎\nWith flared bases one hopes.↩︎\nSurely you cannot have expected me to pass up the opportunity to make a safe sudo joke?↩︎\nNah, too obvious. Feel free to come up with your own joke here, you don’t need my help.↩︎\nWe all were at one time.↩︎"
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html",
    "href": "posts/2023-04-09_webr/index.html",
    "title": "How to run R code in the browser with webR",
    "section": "",
    "text": "A little over a month ago, the tidyverse blog announced that the 0.1 version of webR had been released, opening up the possibility of running R code client side within a browser. To quote from the announcement,\nThis is, to put it mildly, extremely cool, and unsurprisingly there was quite a bit of enthusiasm for it on Mastodon.1 And so, now that I find myself with an unfortunate amount of free time on my hands, I decided to play around with it – just for fun – to see if I could get a better sense of how it works.\nI haven’t dived very deep into webR yet, but I managed to get far enough to build a simple cellular automaton simulation using webR, and host it with Netlify at webr-automata.djnavarro.net.2 If you visit the site, you’ll first see some text asking you to wait for webR to load, and then a text-based grid with cells coloured in different shades of grey. Over time, the cells change colours and the boundaries between different colour blocks tend to become more coherent, as shown in the sequence of images (time runs from left to right) below:\nIf you let the simulation run long enough, eventually one colour comes to dominate the entire grid.3 It’s not the most exciting or artistic code I’ve ever written, but I have to confess I find it oddly soothing to watch."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#some-background",
    "href": "posts/2023-04-09_webr/index.html#some-background",
    "title": "How to run R code in the browser with webR",
    "section": "Some background",
    "text": "Some background\nBefore diving into the code, it’s probably a good idea to say a little bit about how webR works. If you’re at all like Danielle-from-a-month-ago, you might be approaching this with a very simple mental model. Normally when we run R code, we’re using a copy of the R interpreter that has been installed on a local machine. The R code chunks on this quarto blog are all executed locally, for example: I have a copy of R installed on my laptop, the code is run using this copy of R, and the only things that get pushed up to my website are the HTML documents that quarto creates. None of the R code is executed within the browser. Even when we shift to something fancier like a Shiny app or Posit Cloud, the browser does not execute R code: somewhere there is a server that has R installed, and the computational work is done using that R installation, outside of the browser. The reason for this is awfully simple: browsers don’t ship with a copy of R, so they can’t execute R code! 😢\nWith that as your mental model, the concept behind webR – in which the browser really does execute R code – feels like dark magic. How can this be?\nThe answer to this question is, to reduce it to two words, web assembly. I’d been vaguely aware of web assembly for a little while, but until webR came into being I hadn’t paid much attention to it. The underlying idea, as described on the website, is as follows:\n\nWebAssembly (abbreviated Wasm) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable compilation target for programming languages, enabling deployment on the web for client and server applications.\n\nSounds fancy, right? Again, if you’re like me, there’s a part of your brain that just shuts down when reading this description. Okay sure, I’ve written code in compiled languages like C, C++, and Rust, so I have a pretty decent sense of what “binary instruction format” refers to… I mean, if you’re compiling your source code to an executable binary file that contains the instructions that a computer will execute then – obviously – the machine needs to understand those binary instructions. Duh. But wtf is a “stack-based virtual machine” and what does it mean to have a “portable compilation target”, and why does that mean I can now write R code for the web?????\nHm.\nOkay, I’ll be honest: I don’t really understand the low-level details. But also, I don’t care about them either. What I do care about are the following facts:\n\nWeb assembly is a binary format you can compile source code to.\nThere are existing compilers like emscripten that create web assembly binaries from C and C++\nModern browsers ship with web assembly in the same way they ship with javascript: the browser can execute a wasm binary\nThe R interpreter is written in C\n\nWhen you put these things together, you can see how webR might (and indeed does) work. It’s entirely possible to take the source code for the R interpreter and use emscripten to compile R itself to web assembly. If there were a wasm version of R, your web page could import it in much the same way that it imports a javascript script, and then that “wasm-R” binary could execute an R script.\nWhat a fabulous idea. Someone should do that. Oh wait… George Stagg already did. That’s precisely what webR does. 🤦‍♀️\nOf course, if you spend a bit of time thinking about the practicalities of making this work, you start to realise there are some tricky things to manage. How exactly would wasm-R read an R script? How would it interact with the DOM?4 When you’re writing code for the web using javascript, this isn’t an issue: you just write code like this…\ndocument.getElementById(\"the_id_for_a_div\").innerHTML = \"your_code_here\"\n…or whatever. Javascript has a native way of interacting with HTML. Web assembly doesn’t have that, and so the web assembly version of R inherits this limitation. As a consequence, we have to use javascript as an intermediary. Therein lies a lot of the magic. If you dig into the documentation for webR, what you notice is that the bulk of it is devoted to describing APIs that let webR and javascript interact with each other. It has to be that way if we want a web assembly version of R to have the ability to modify the state of the web page!\nOr, to put it another way, if you’re an R developer who wants to write webR code, you’re also going to have to write some javascript that controls the interactions between your (webR interpreted) R code and the HTML document. And, because code execution on the web has a variety of security risks that browsers try hard to mitigate, you might guess that you’re going to have a lot of finicky details to manage in order to get your code to run without the browser screaming at you.\nSo in practice, a lot of what you need to learn if you want to jump from “localR” to webR is going to relate to those issues. It makes sense when you look at it in these terms, and feels kind of obvious in hindsight, but it wasn’t until I started trying to write webR code that I realised that this is how it works."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#making-a-plan",
    "href": "posts/2023-04-09_webr/index.html#making-a-plan",
    "title": "How to run R code in the browser with webR",
    "section": "Making a plan",
    "text": "Making a plan\nOkay, that’s enough background. Let’s start building something that tries to make these ideas a bit more concrete. To do that we’ll need to write a simple R application of some kind. If you know anything about me at all, you’d guess (correctly) that the applications I like to make for fun tend to be artistic in nature. I can’t help it… I like creating pretty visuals with R. Unfortunately, this is one of the trickier things to do with webR in its current state of development. It is possible, of course: if you take a look at the webR REPL you’ll see that the webpage has an HTML canvas to which R graphics can be drawn. That’s super cool, but I’m certainly not going to start by trying to implement something like that myself. Instead, I’ll do something simpler: I’ll write an R function that takes text input and produces text output. Then, I’ll do something sneaky and display that text in a way that looks like a picture.\nBasically, it’s ASCII art. I’m going to make ASCII art, but because its the 21st century I’ll do it with UTF-8 characters rather than literally using ASCII."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#the-r-code",
    "href": "posts/2023-04-09_webr/index.html#the-r-code",
    "title": "How to run R code in the browser with webR",
    "section": "The R code",
    "text": "The R code\nI’ll start by showing the R code. The site contains a script called automaton.R and it defines a function called automaton() that takes a single character string as input, and returns a modified string as output. The string is used as the data to define a two dimensional matrix, with one character per cell, and on each iteration of the simulation, one character is updated by copying the value of one of its neighbours in the grid. Here’s the code:\n\n\n\nautomaton.R\n\nautomaton &lt;- function(str = NULL,\n                      linebreak = \"&lt;br&gt;\",\n                      n_rows = 30,\n                      n_cols = 50,\n                      iterations = 100,\n                      symbols = c(\"░\", \"▒\", \"▓\", \"█\")) {\n\n  # create random string if none is given\n  if (is.null(str)) {\n    str &lt;- sample(symbols, n_rows * n_cols, TRUE)\n    str &lt;- paste(str, collapse = \"\")\n  }\n\n  # make matrix from (possibly break-delimited) string\n  str &lt;- gsub(\n    pattern = linebreak,\n    replacement = \"\",\n    x = str,\n    fixed = TRUE\n  )\n  dat &lt;- matrix(\n    data = strsplit(str, \"\")[[1]],\n    nrow = n_rows,\n    ncol = n_cols,\n    byrow = TRUE\n  )\n\n  # run simple automaton\n  for (i in 1:iterations) {\n    \n    # choose an interior cell in the grid\n    r &lt;- sample(2:(n_rows - 1), 1)\n    c &lt;- sample(2:(n_cols - 1), 1)\n    \n    # choose one of its neighbours (sort of)\n    h &lt;- sample(-1:1, 1)\n    v &lt;- sample(-1:1, 1)\n    \n    # copy the original cell value to the neighbour cell\n    dat[r + v, c + h] &lt;- dat[r, c]\n  }\n\n  # convert matrix to string and return\n  str &lt;- \"\"\n  for (i in 1:n_rows) {\n    row &lt;- paste(dat[i, ], collapse = \"\")\n    str &lt;- paste(str, row, sep = linebreak)\n  }\n  str &lt;- paste(str, linebreak, sep = \"\")\n  str\n}\n\n\nNow, for the purposes of this post I’m assuming you’re already pretty familiar with writing R code, and I’ve deliberately written this function in a way that doesn’t require a lot of explanation for an R user. It’s all written in base R, I’m not using any fancy packages, and I haven’t made any efforts to make it run quickly. I don’t want any complexity on the R side: all I want for this post is to have some R code that works.\nHere’s what it does:\n\nstr &lt;- automaton(\n  linebreak = \"\\n\", \n  n_rows = 20,\n  n_cols = 60,\n  iterations = 100000\n)\ncat(str)\n\n\n█▒▒▒▒▒█▒░░░░░▓▓▓░░░░░░░░░░░░░░█░██████▒██████████████▓███▓▓▓\n▒▒▒▒▒▒▒▒░▒▓░░▓▓░░░░░░░░░░░░░░░░█░░███▒███████████░░██████▓▓▓\n█▒▒▒▒▒▒▒░░▓▓▓▓▓░░░░░░░░░░░░░░░░░░███▒████████████████▓▓▓▓▓▓▓\n█▒▒▒█▒▒▒▒░▒▓▓▓▓░░░░░░░░░░░░░░░░░████▒▒████████████████▓▓▓█▓▓\n██▒▒███▒▒▒▒▓▒▓░▒░░░░░░░░░░░░░░░░█████▒▒████████████░░█░▓▓▓██\n▒██▒▒▒▒██▒▓▓▒▓▓▓▒▒░░░░░▒░░░░░█░░░███▒▒▒████████████░░░░▓▓███\n▒▒▒▒▒▒▒▒▒▓▓▒▓▓▒▒▒▒▓░░░░░░░░░░█░█████▒▒▒████████████░░█░████░\n▒▒▒▒▒▒▒▒▒▒▓▒▓▒▓▒▒▓▒░░░▒░░░░▒▒█░███████▒██████▒▒████░░██████░\n▒▒▒▒▒▒▒▒▒▓▓▓▓▓▒▒▒▒░▓▓░▒▒▒▒░██░████████████▒██▒███████░░░░█░█\n▒▒▒▒▒▒▒▒▒▓▒▒▒▒▒▒▒█▓▓█▒▒░▒▒▒██░░███████████▒▒▒█▒█▒█▒████░█░██\n▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒█▓████░██▒▒██░░░██▒███████████▒▒███████░██░\n▒▒▒▒▒▒▒▓▒▓▓▒░▒▒▒▒▓░░░█░░█▓▓▓░██░░░▒▒▒█▒███▒█▓█████████░░█░█░\n▒▒▒▒▒▒▒▒▓▒▒▒▒░░▒▓▒░░░░░░░▓▓▓████▓▓▒▒▒▒▒██▒▒▓▒▒▒▒████░░░░░███\n▒▒▒▒▒▒▒▒▓▒▒░▒▒░░░▓░░░░░░▓▓░▓█████▓▓▓▒▒█▒█▒▓▒▒▒▒▒▒█████░░░░█░\n▒▒▒▒▒▒▒▒▒░▒▓▒▒▒░░░▒░█░░░░░█░▓████▓▓▓▓▓█▒▒▓▓▒▓░░▒▒▒█▒███░░░░░\n▒▒▒▒▒▒▒▒▒░░▒▒▒▒░▒░▒▒▒░░░░██░░░░░▓▓▓▓▓████▒▒▒▒▒░▒▒▒▒▒██░░░░░░\n▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░░█▒░░░░░░░░░░░░▓▓▓▓█▓█▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░░░░\n▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░░█░░░░░░░░░░░░░▓▓▓▓▓▓▓▓██▒▒▒▒▒▒▒▒▒▒▒▒░░░░░█░\n▒▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░█░░░░░░░░░░░░░▓▓▓▓▓▓██▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░░░█\n▒▒▒▒▒▒▒▒▒▒▒▒▒░░░░░░░░░░░░░░░░█░█▓▓▓▓▓▓▓██▒▒▒▒▒▒▒▒▒▓▒▒░░░████\n\n\nThe output here doesn’t look as pretty as the pictures shown at the start of this post, but that’s mostly because the CSS styling for this blog creates white space between lines. On the webr-automata.djnavarro.net site there’s nothing like that so all that white space disappears and the output looks like a shaded grid.\nAnyway, the key thing here is that on the website I have this code saved to an R script called automaton.R, and it is this script that webR will need to work with."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#the-html",
    "href": "posts/2023-04-09_webr/index.html#the-html",
    "title": "How to run R code in the browser with webR",
    "section": "The HTML",
    "text": "The HTML\nThe next step in the process is to create a webpage. Or, more precisely, to write an HTML document. I’m going to keep this part extremely simple. The site has an index.html file that only does two things: it creates an div called “grid” that we can modify using javascript, and imports a script called webr-app.js that takes care of any such modifications:\n\n\n\nindex.html\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n  &lt;head&gt;\n    &lt;meta charset=\"utf-8\" /&gt;\n    &lt;title&gt;Text Textures&lt;/title&gt;\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div id=\"grid\"&gt;webR is starting, please wait...&lt;/div&gt;\n    &lt;script src=\"./webr-app.js\"&gt;&lt;/script&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n\nThis HTML document provides the skeleton for the webpage, and the webr-app.js script will act as the intermediary between my R code and the HTML document."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#the-javascript",
    "href": "posts/2023-04-09_webr/index.html#the-javascript",
    "title": "How to run R code in the browser with webR",
    "section": "The Javascript",
    "text": "The Javascript\nNow comes the fun part.5 I need to write some javascript that:\n\nImports webR itself so I can execute R code\nReads my R script from the automaton.R file\nUses webR to call the automaton() R function defined in the file\nUpdates the state of the HTML document using the output of automaton()\n\nHow do I do this? As much as I love a good murder mystery, I don’t think the mystery novel format is ideal when writing technical documents, so let’s start by revealing the ending. Here’s the complete source code for webr-app.js:\n\n\n\nwebr-app.js\n\n// import the webr module and then run the code\nimport('https://webr.r-wasm.org/latest/webr.mjs').then(async ({WebR}) =&gt; {\n\n    // the HTML element showing the grid\n    let grid = document.getElementById(\"grid\");\n\n    // wait for the webR session to start\n    const webr = new WebR();\n    await webr.init();\n\n    // read the script as a string, and evaluate it in R\n    let automaton = await fetch('automaton.R');\n    await webr.evalR(await automaton.text());\n\n    // initialise the state of the grid\n    let str = await webr.evalR('automaton()')\n    grid.innerHTML = (await str.toJs()).values;\n\n    // function to update the state of the grid\n    async function grid_update() {\n        await webr.objs.globalEnv.bind('str', grid.innerHTML)\n        let str = await webr.evalR('automaton(str)')\n        grid.innerHTML = (await str.toJs()).values;\n    }\n\n    // repeatedly call the update function\n    while (true) {\n        await grid_update();\n    }\n});\n\n\nIf you’re fluent in javascript you can probably look at this and immediately work out what it does, but not every R user has had the (mis)fortune to work extensively in javascript, so I’ll be kind and try to unpack it a bit! At a high level, this code has the following structure:\nimport('https://webr.r-wasm.org/latest/webr.mjs').then(async ({WebR}) =&gt; {\n\n    // use the WebR object to do a bunch of stuff\n\n});\nFor an R user some of this is a little opaque if you don’t have the javascript concept of a thenable, but it’s not too hard to see the intent. The browser is first asked to download webR from the specified URL, and then once this operation has finished, it proceeds to execute the code that follows. More precisely, the .then() method is called once the promise returned by the previous method resolves, and it executes whatever function it is passed. In this case the code uses the =&gt; notation to define an anonymous function that takes the WebR object – the interface to webR – as an input argument. So now let’s take a look at what happens when this function is called.\nThe code starts simple. The first thing it does is define a javascript variable grid that refers to the HTML element that we want our R code to manipulate:\nlet grid = document.getElementById(\"grid\");\nNext, we start the R session:\nconst webr = new WebR();\nawait webr.init();\nThe const and new keywords aren’t interesting: new just means we’re creating a new instance from WebR, and const means it’s a constant. We aren’t going to modify the webr object itself. What we are going to do, however, is use it to initialise a new R session. That’s what webr.init() does. The await keyword is used to tell javascript to wait for this code to finish executing before starting the next instruction. That’s important in javascript because – unlike R – a lot of javascript code runs asynchronously. Javascript is perfectly happy to start a function running (returning a promise object) and moving onto the next line of code without waiting for that function to finish. When calling asynchronous functions, javascript doesn’t wait for the “promise” to resolve unless you explicitly tell it to using the await keyword. In this case, we definitely don’t want to try executing R code until we actually have an R session, so I’ve used await to tell javascript to wait until we have one.\nThe next step in the process is to have javascript read the source code from the automaton.R file as a string, and then pass that to webr to evaluate. The code below does this:\nlet automaton = await fetch('automaton.R');\nawait webr.evalR(await automaton.text());\nHere, the javascript fetch() function takes care of the file read operation, and the webr.evalR() function passes the code to R for evaluation. Once this code finishes, the R session will have the automaton() function defined in the global workspace.\nNow that the R session has access to the automaton() function, we can again use webr.evalR() to call this function. If you look back to the way I defined the automaton() function earlier in the post, you’ll notice that if we call it without passing it a string as input, it will create a random string to use as the starting point for the cellular automaton. So that’s what we do in the first line of this snippet:\nlet str = await webr.evalR('automaton()')\ngrid.innerHTML = (await str.toJs()).values;\nThe second line in this javascript takes the string that the automaton() R function outputs, converts it to a javascript string (that’s what the str.toJs() bit does), and then modifies the HTML document (specifically the grid element defined in the first line of javascript) so that the text displayed on the page corresponds to the string that the R function produced.\nThe rest of the javascript code is essentially the same trick repeated over and over ad infinitum:\n// function to update the state of the grid\nasync function grid_update() {\n    await webr.objs.globalEnv.bind('str', grid.innerHTML)\n    let str = await webr.evalR('automaton(str)')\n    grid.innerHTML = (await str.toJs()).values;\n}\n\n// repeatedly call the update function\nwhile (true) {\n    await grid_update();\n}\nWhat I’ve done here is define a javascript function grid_update() that does three things:\n\nIt reads the string currently displayed on the website (i.e. grid.innerHTML) and uses webr.objs.globalEnv.bind() to create a variable called str in the R workspace that contains the same text.\nNext, it uses webr.evalR() to call the R function automaton(), passing the R string str as input, and storing the results as a variable (also called str because I am lazy) on the javascript side.\nFinally, it converts the str object to a native javascript string and uses it to update the text displayed on the website (i.e., it modifies grid.innerHTML).\n\nHaving defined this function, I now do something extraordinarily lazy: I create an infinite loop that calls grid_update() over and over forever, thereby creating the animation that you see on the website.\nAnd with that we are done.\nWell, sort of. There are going to be a few details to sort out when we go to deploy this to the web, but that’s the core of our very simple app."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#the-finicky-bits",
    "href": "posts/2023-04-09_webr/index.html#the-finicky-bits",
    "title": "How to run R code in the browser with webR",
    "section": "The finicky bits",
    "text": "The finicky bits\nNow for the annoying fiddly bits. If you take a look at the actual source code on github.com/djnavarro/webr-automata you’ll notice that there are a few other files I’ve included. Two of them are completely irrelevant: README.md isn’t part of the web application, and neither is .gitignore. The serve.R function isn’t really part of the app either, it’s just an R script I used to serve the site locally while I was writing the code.\nThe other three files – netlify.toml, webr-serviceworker.js, and webr-worker.js – are relevant. Let’s start by taking a look at the contents of the two javascript files:\n\n\n\nwebr-worker.js\n\nimportScripts('https://webr.r-wasm.org/latest/webr-worker.js');\n\n\n\n\n\nwebr-serviceworker.js\n\nimportScripts('https://webr.r-wasm.org/latest/webr-serviceworker.js');```\n\n\nOkay, there’s actually not much there. That’s because I’ve copied this code directly from the webR documentation, and there’s no reason why you shouldn’t do likewise! You do need these files in order for the application to work, however, and while I don’t want to go into any detail about what’s going on under the hood here6 it might be helpful to quickly read up on what web workers and service workers are used for. Basically you need these things to allow webR to do its thing in the background.\nNext, let’s take a look at the netlify.toml file:\n\n\n\nnetlify.toml\n\n[[headers]]\n  for = \"/*\"\n\n  [headers.values]\n    Cross-Origin-Opener-Policy = \"same-origin\"\n    Cross-Origin-Embedder-Policy = \"require-corp\"\n\n\nAs before, I’m not planning to go into detail about what this is all about, but if you’re not familiar with cross-origin resource sharing (CORS) it’s probably helpful to take a read through the Mozilla documentation on CORS. The short version is that these headers are needed to prevent the browser from blocking webR in your application."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#the-wrap-up",
    "href": "posts/2023-04-09_webr/index.html#the-wrap-up",
    "title": "How to run R code in the browser with webR",
    "section": "The wrap up",
    "text": "The wrap up\n\nNail in my hand From my creator You gave me life Now show me how to live    –Audioslave\n\nDespite being in the very early stages of development I kind of love webR already. One of my big frustrations as someone who writes a lot of R code for data analysis and generative art, and someone who writes a lot of HTML/CSS/javascript code for the web, is that it’s really hard to bring these two worlds together. Literate programming tools like R markdown, jupyter, and quarto are all useful to some extent, because they make it easier to write about R code on the web in a natural way7, but they don’t allow you to execute the R code in an interactive way. You can solve this problem by writing Shiny apps, but it’s also a limited solution, because a Shiny app won’t work without a server running a local copy of R, and someone has to pay to keep that server running. Speaking as an unemployed woman who just shut down her kubernetes apps because I’m sure as hell not going to pay for them out of my own pocket, this is not ideal. The thing I’ve always wanted is the ability to create websites that execute R code client-side, in exactly the same way that my sites can execute javascript code within the users browser.\nIt’s early days, of course, but webR offers the tantalising prospect of making this dream a reality. At long last my R code can live in the browser, and I can really show people what you can do with R.\nI’m terribly excited."
  },
  {
    "objectID": "posts/2023-04-09_webr/index.html#footnotes",
    "href": "posts/2023-04-09_webr/index.html#footnotes",
    "title": "How to run R code in the browser with webR",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI imagine it also generated excitement on Twitter, but quite frankly there is no chance of me returning to Twitter while it remains a privately held company whose owner uses the site to flirt with anti-trans conspiracy theories at the same time that governments around the world are openly passing draconian legislation targeting us. In all honesty, I’ve been a bit… um… disheartened… to learn that this isn’t a deal-breaker for very many people. But whatever. I suppose I can’t really know why other people make the choices they do.↩︎\nThe source code is available at github.com/djnavarro/webr-automata.↩︎\nYes there are proofs that this occurs with probability 1 in the limit, but do you really need to see them? I mean, this is basically a simple random walk with absorbing states so of course it does. Sheesh.↩︎\nThis is a problem many subs have, to be honest.↩︎\nThis is “fun” in precisely the same way that it’s fun to spend time with an impact top. You’re going to love it, but tomorrow morning you’re going to have bruises.↩︎\nLargely because I’m not exactly an expert on this bit myself.↩︎\nIf you doubt me on the utility of these things, let me mention that I wrote the entirety of Learning Statistics with R in pure LaTeX… it was a fucking nightmare.↩︎"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html",
    "href": "posts/2022-04-20_porting-to-quarto/index.html",
    "title": "Porting a distill blog to quarto",
    "section": "",
    "text": "A little over a year ago I decided to start blogging again, and set up this site. At the time I made the deliberate choice to use distill as my blogging platform rather than something that would require a static site generator like hugo or jeykll, and I don’t regret that choice. However, along the way I’ve found a few things that have bothered me about using distill. It’s never been worth considering changing to something new though, because distill has so many things that I do like. Until now.\nEnter, stage left, quarto.\nNow out of stealth mode and attracting no end of attention, quarto offers the promise of being a cross-platform, cross-format, open source publishing tool based on pandoc. Intrigued, I decided to play around with it for a while, and ended up making the decision to port this blog from distill to quarto. This post outlines my process.\n(I am a little nervous: porting a blog often involves recomputing things. Will it work? Will everything turn out to be reproducible? I hope so…)"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#getting-started",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#getting-started",
    "title": "Porting a distill blog to quarto",
    "section": "Getting started",
    "text": "Getting started\nThe very first thing I do is go read Alison Hill’s wonderful We don’t talk about quarto blog post. If you’re an R markdown user considering making the jump to quarto and haven’t already read her summary of where things are at, you won’t regret it. It’s a nice high level overview. I’d also suggest Nick Tierney’s notes on making the switch, which is very helpful also.\nAfter doing my background reading, I go to the get started page on the quarto website to download the installer file. I’m on Ubuntu, so for me that’s a .deb file. I install it in the usual way from the command line:\nsudo dpkg -i quarto-0.9.282-linux-amd64.deb\nNow that I have quarto installed, I’m able to use it to create a blog. My old distill blog exists in a project folder that I’d imaginatively named distill-blog, so I decide to keep to tradition and create the quarto version in an equally boring project folder called quarto-blog.\nThere is a page on the quarto website that walks you through the process for creating a blog, which I dutifully follow. From the terminal, I use the quarto create-project command, and a variety of files are created:\nquarto create-project quarto-blog --type website:blog\nCreating project at /home/danielle/GitHub/sites/quarto-blog:\n  - Created _quarto.yml\n  - Created index.qmd\n  - Created posts/welcome/index.qmd\n  - Created posts/post-with-code/index.qmd\n  - Created about.qmd\n  - Created styles.css\n  - Created posts/_metadata.yml\nComing from an R markdown background, this is very familiar:\n\nThe files with a .qmd extension are the quarto markdown documents. These contain source code for the blog posts (the two files in the posts folder), the home page (the index.qmd file in the project root folder) and a standalone “about me” page for the blog (the about.qmd file).\nThe files with a .yml extension are the YAML files used to configure the blog. I don’t notice this at first, but the fact that there are two of them is important. The _quarto.yml file is used for settings that will apply across the entire site, but you will often want to configure settings that only apply to your blog posts. Those can be set by editing the posts/_metadata.yml file.\nThe styles.css file can be used to specify custom CSS rules that will apply to the whole site. I’ll talk more about styles later."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#rendering-posts",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#rendering-posts",
    "title": "Porting a distill blog to quarto",
    "section": "Rendering posts",
    "text": "Rendering posts\nThere are several different ways to interact with quarto. For example, later in the post I’ll talk about the quarto command line interface which allows you to work with quarto without going through R or RStudio. However, when getting started I try to keep things simple, and go with the option that was most familiar to me: I use RStudio.\nTo do this, it’s convenient to have an RStudio project for my blog. Using the RStudio file menu, I create a new project from an existing directory (i.e., my quarto-blog folder), which supplies the quarto-blog.Rproj file and other infrastructure needed to work with my new quarto blog as an RStudio project. Once that’s done, I am able to open up a quarto file in the RStudio editor and see a familiar looking interface:\n\n\n\n\n\nA blog post written in quarto markdown open in the RStudio editor. Notice that in the place where one would normally expect to see the ‘Knit’ button for an R markdown document, there is a ‘Render’ button. It serves the same function and is mapped to the same hot keys as the ‘Knit’ button\n\n\n\n\nFrom here I can click on the “Render” button to render a single page, or alternatively I can go to the RStudio build pane and select the “Render Project” option to build the entire site. By default, the blog builds into the _site folder."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#playing-around",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#playing-around",
    "title": "Porting a distill blog to quarto",
    "section": "Playing around",
    "text": "Playing around\nProgress! I am making progress. However, before trying to do any other practical things, I have some very important business to attend to: playing around. Aimlessly exploring the functionality of a new tool is always fun, and I find it’s a good way to familiarise myself with something. I’m pretty familiar with R markdown already, and I imagine most readers of this post will be too, so for the most part there are no surprises. Still it is worth asking myself the usual questions:\n\nCan I add footnotes?1\nCan they be nested?2\nCan I add comments in the margin?\n\n\n\nA comment in the margin\nLooking at the quarto article layout documentation, I discover some nice features. You can use the :::{.class} notation to apply a CSS class to a section of output, like this:\n:::{.column-margin}\nA comment in the margin\n:::\nThe .column-margin class is handy for margin asides, but there are several other useful classes that come in handy when adding images to blog posts:\n\n.column-body spans the usual body width of the post\n.column-body-outset extends slightly outside the usual width\n.column-page spans the whole page (including both margins)\n.column-screen class spans the full width of the screen\n.column-screen-inset class stops just short of the full screen width\n\nYou can set these inside a chunk option. For example, setting column: margin as a chunk option will assign the output a .column-margin class, and any resulting figure will appear in the margin rather than below the code. Similarly, setting column: screen as the chunk option will assign the output a .column-screen class, and the output will span the full width. Here’s a simple example based pretty closely on the one used in the quarto documentation:\n\nlibrary(leaflet)\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addMarkers(\n    lng=151.22251, \n    lat=-33.85943, \n    label=\"Mrs Macquarie's Chair\"\n  ) %&gt;% \n  addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\nI confess. I’m a little bit in love already."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#notes-on-yaml-headers",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#notes-on-yaml-headers",
    "title": "Porting a distill blog to quarto",
    "section": "Notes on YAML headers",
    "text": "Notes on YAML headers\nThe YAML headers used for blog posts are a little different in quarto than their equivalents in distill were, and it takes me a moment to work out how to adapt the YAML headers from my old R markdown posts for the new quarto blog. Here’s a quick overview. First, some fields require almost no changes:\n\nThe title field is unchanged. That was an easy one!\nThe date field is essentially unchanged, except for the fact there seems to be a tiny bug in date parsing for blogs that I’m sure will vanish soon. If you’re using ISO-8601 date formats like date: \"2022-04-20\" it will be fine.3\nThe categories field takes a list of values, which (I think?) is no different to what it looked like before. To be honest I don’t remember because my old blog didn’t use them. I’ve started now.\n\nOther changes are superficial:\n\nThe description field that I used on my old blog still does what it used to: it provides preview text on the listing page, and a summary at the top of the file. However, there is also a subtitle field that you can use for this purpose, and the output has the same look and field as my old descriptions, so I decide to switch all my old description fields to subtitle entries.\nTo specify a preview image associated with a blog post, use the image field (e.g., something like image: thumbnail.jpg) instead of the preview field from distill.\nThere is a new license field that replaces the creative_commons field from distill. At the bottom of this post you’ll see a “Reuse” appendix that links to a license file. To generate this, I’ve included a license: \"CC BY\" line in the YAML.\n\nOther changes are a little deeper:\n\nIn distill it is possible to specify the author field in a lot of detail, mirroring the academic convention of listing an authors affiliation alongside their employer, orcid record, and contact details. Quarto supports this also, though the tags have changed slightly: orcid_id is now orcid, for example. There’s an example of this shown a little later in the post.\nSpecifying the table of contents is slightly different. Just like in distill, you can turn on the table of contents by including toc: true as a line in the YAML header, and set the toc-depth field to determine how detailed the table of contents should be. But there are new options. You can customise the text that appears above the table of contents, and the location in which it appears. I decide to be boring and go with some standard options: toc-title: Table of contents and toc-location: left.\nOne feature in distill that I like is that it generates a citation for each post. You can do that in quarto too, and you’ll see at the bottom of this post that I’ve used that feature here. However, quarto manages this in a different way to distill, and uses a YAML version of citation style language (CSL) formatting to define the citation. To see how it works, you can read through the quarto pages on citations and creating citeable articles. It’s a little more elaborate than the distill version, but much more flexible. For this blog it’s as simple as including citation: true in the YAML, but it can be more elaborate and accommodate any pattern of academic citation you like."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#creating-a-new-post",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#creating-a-new-post",
    "title": "Porting a distill blog to quarto",
    "section": "Creating a new post",
    "text": "Creating a new post\nOkay. Time to get to work at transforming the starter blog into a quarto version of my distill blog. My first step is to delete the two posts that came with the starter blog, and then create this one.\nA folder with an index.qmd file is the bare minimum I need to get started with a new post. I suppose there are other ways do to this but what I actually do is create the the folder and an empty file from the terminal (for reasons known but to god):\nmkdir posts/2022-04-20_porting-to-quarto\ntouch posts/2022-04-20_porting-to-quarto/index.qmd\nTo be honest, using the terminal was overkill. What I could have done instead, had I been looking at RStudio rather than the terminal, is use the “New File” option in the file menu and then select the “Quarto Doc” option. That creates a new untitled quarto document that you can save to the appropriate location."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#inheriting-yaml-settings",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#inheriting-yaml-settings",
    "title": "Porting a distill blog to quarto",
    "section": "Inheriting YAML settings",
    "text": "Inheriting YAML settings\nA handy feature in quarto websites is that YAML fields are inherited. For example, this post has its own YAML header that contains the following – and only the following – fields:\ntitle: \"Porting a distill blog to quarto\"\nsubtitle: | \n  I have moved this blog from distill over to quarto, and \n  taken notes. A year after starting the blog, this promises \n  to be an interesting reproducibility test\ndate: \"2022-04-20\"\ncategories: [Quarto, Blogging, Reproducibility]\nimage: \"img/preview.jpg\"\nThat’s a little peculiar, because a lot of the metadata needed to specify this post is missing. The reason it is missing is that I’ve placed some fields in the posts/_metadata.yml file. Those fields are inherited by every blog post. This is the entire contents of my post metadata file:\n# Freeze computed outputs\nfreeze: true\n\n# Enable banner style title blocks\ntitle-block-banner: true\n\n# Enable CC licence appendix\nlicense: \"CC BY\"\n\n# Default for table of contents\ntoc: true\ntoc-title: Table of contents\ntoc-location: left\n\n# Default knitr options\nexecute:\n  echo: true\n  message: true\n  warning: true\n\n# Default author\nauthor:\n  - name: Danielle Navarro\n    url: https://djnavarro.net\n    affiliation: Voltron Data\n    affiliation-url: https://voltrondata.com\n    orcid: 0000-0001-7648-6578\n\n# Default fields for citation\ncitation: true\n\n\nThe freeze option is extremely useful in the blogging context. I’d advise reading the linked documentation page!\nThat explains a lot, but if you’re looking closely you’ll realise that there’s nothing in these fields specifying the output format! In R markdown I’d have included an output field for this, but in quarto the relevant field is called format. Because the output applies to the entire site, that part of the YAML header is in the _quarto.yml file. The relevant lines of that file are:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nI’ll come back to this later. For now it’s enough to recognise that this indicates that all pages on this site should be rendered to HTML documents, and using the ember.scss and styles.css files specify the blog style."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#converting-my-old-posts",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#converting-my-old-posts",
    "title": "Porting a distill blog to quarto",
    "section": "Converting my old posts",
    "text": "Converting my old posts\nThe time has come for a little manual labour. Although quarto is compatible with most existing R markdown and I can probably get away with leaving them untouched, in the longer term I’m expecting that I’ll be moving across languages so it appeals to me to take this opportunity to port everything over to quarto now. Renaming all the index.Rmd files to index.qmd files is easy enough, and can be done programmatically, but most of my edits require a small amount of manual tinkering with each post. Not a lot, because it is mostly a matter of renaming a few YAML fields. Given that there are only 20 or so posts that need to be ported, I decide it is easier to do it manually than to try to write a script to automate the task. I get through it in an afternoon."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#styling-the-new-blog",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#styling-the-new-blog",
    "title": "Porting a distill blog to quarto",
    "section": "Styling the new blog",
    "text": "Styling the new blog\nUp to this point, the adoption of quarto has felt very “distill-like”. The structure of the blog feels familiar from distill, the YAML headers are similar in spirit (if different in the particulars), and so on. When it comes to customising the appearance of the blog, it’s not very similar to distill at all, and feels more similar to simple R markdown sites. Quarto websites are bootstrap based, and as discussed on the quarto theming page, they come with the same bootswatch themes that you might be familiar with from R markdown. For example, if you decide like I did that you would like a very plain white theme, you could choose the “litera” theme. To apply this to your blog, all you’d have to do is make sure your _quarto.yml file contains the following lines:\nformat:\n  html:\n    theme: litera\n    css: styles.css\nWhat this does is assert that output will be rendered as HTML objects using the litera bootswatch theme, and applying any custom CSS rules that you add in the styles.css file.\nOne very nice feature of quarto, if you’re comfortable using SASS to define styles and know something about how the bootstrap SASS files are organised,4 is that it allows you to write your own .scss file to define your blog theme more precisely, giving you access to bootstrap parameters and so on. I would strongly recommend reading about the quarto theming system before tinkering with this aspect yourself, but if you are more knowledgeable (or more foolish) than I, here’s how I set my blog up. First, instead of referring to the litera theme, the YAML in my _quarto.yml file points at my own custom .scss file:\nformat:\n  html:\n    theme: ember.scss\n    css: styles.css\nThe contents of the ember.scss file are as follows:\n/*-- scss:defaults --*/\n\n// use litera as the base\n$theme: \"litera\" !default;\n\n// import google fonts\n@import 'https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:ital,wght@0,400;0,700;1,400;1,700&display=swap';\n@import 'https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500;600;700&display=swap';\n\n// use Atkinson Hyperlegible font if available\n$font-family-sans-serif:  \"Atkinson Hyperlegible\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, \"Noto Sans\", sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\" !default;\n\n/*-- scss:rules --*/\n\n// litera is serif by default: revert to san-serif\np {\n  font-family: $font-family-sans-serif;\n}\nAs you can see, right now my customisation really doesn’t do much other than make some very minor tweaks on the litera theme, but the potential is there to do so much more than I have in setting up this blog. I plan to tinker with this more later on!"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#adding-an-rss-feed",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#adding-an-rss-feed",
    "title": "Porting a distill blog to quarto",
    "section": "Adding an RSS feed",
    "text": "Adding an RSS feed\nMy old distill blog had an RSS feed, and while I acknowledge that it’s increasingly an esoteric feature that most folks don’t use, I have a fondness for RSS. Quarto supports this, but it’s not enabled by default. What I need to do is edit the YAML in the index.qmd file that corresponds to your homepage, because that’s where I have my primary listing of posts. In it, I see a listing field. All I need to do is add feed: true underneath and there is now an RSS feed for the site:\ntitle: \"Notes from a data witch\"\nsubtitle: A data science blog by Danielle Navarro\nlisting:\n  feed: true\n  contents: posts\nThe quarto section on feeds has more information on this."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#deploying-the-site",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#deploying-the-site",
    "title": "Porting a distill blog to quarto",
    "section": "Deploying the site",
    "text": "Deploying the site\nPreparing the site to be deployed is relatively painless. I found it useful to read the quarto website options page before doing this, because it mentions a lot of settings to tinker with, mostly in the _quarto.yml file. For example, I choose to customise the navigation bar, the social media preview images, and so on. Eventually, I reach the point where I am happy and move on to deployment.\nHappily, as to the deployment process itself, there’s not a lot to say. The quarto deployment page discusses several options for how you can do this. Most of my websites are deployed either through GitHub Pages or through Netlify. This one is a Netlify site, so I follow the instructions there and everything goes smoothly. However, this does bring me to another topic…"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#netlify-redirects",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#netlify-redirects",
    "title": "Porting a distill blog to quarto",
    "section": "Netlify redirects",
    "text": "Netlify redirects\nI’ve structured my blog in a particular way. Like the default quarto blog, all the posts live in the posts folder, and they’re named in a systematic way: they have an ISO-8601 formatted date first, and then a semantic slug. So the full URL for this blog post is:\nblog.djnavarro.net/posts/2022-04-20_porting-to-quarto\nThat’s convenient for archiving purposes and for keeping everything nicely organised in my project folder, but it’s also a little clunky for sharing links. In practice, the “posts” part is a bit redundant, and I’m never going to use the same slug twice, so it’s handy to set it up so that there’s also a shorter URL for the post,\nblog.djnavarro.net/porting-to-quarto\nand that this shorter URL automatically redirects to the longer one.\nSince I’m intending to deploy this blog to Netlify, what I need to do is ensure that whenever the site builds, a _redirects file is created within the _site folder. This file needs to have one line per redirect, listing the “redirect from” path first, followed by the “redirect to” path. Here’s what that line looks like for this post:\n/porting-to-quarto /posts/2022-04-20_porting-to-quarto\nI have no intention of adding these lines manually, so what I do instead is add an R chunk to the index.qmd file corresponding to the blog home page, with the following code:\n# list names of post folders\nposts &lt;- list.dirs(\n  path = here::here(\"posts\"),\n  full.names = FALSE,\n  recursive = FALSE\n)\n\n# extract the slugs\nslugs &lt;- gsub(\"^.*_\", \"\", posts)\n\n# lines to insert to a netlify _redirect file\nredirects &lt;- paste0(\"/\", slugs, \" \", \"/posts/\", posts)\n\n# write the _redirect file\nwriteLines(redirects, here::here(\"_site\", \"_redirects\"))\nEvery time this site gets rebuilt – which usually involves rebuilding the home page since that’s the one that contains the post listing – the _redirects file gets refreshed. There might be a cleaner way, but this works."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#quarto-cli",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#quarto-cli",
    "title": "Porting a distill blog to quarto",
    "section": "The quarto CLI",
    "text": "The quarto CLI\nSomething I forgot to mention earlier. About half way through the process of tinkering with my old posts to be suitable for the quarto blog, I decided to stop using RStudio for the rendering, and spent a little time familiarising myself with the quarto command line interface. I haven’t made any particular decisions about what my long term workflow with quarto is going to look like, but I did find it helpful to get a feel for the concept of quarto as a standalone install. I’m not going to go into detail here, but just briefly: at the terminal I can see that I have some help options,\n\nquarto help\n\n  Usage:   quarto \n  Version: 0.9.282\n                  \n\n  Description:\n\n    Quarto CLI\n\n  Options:\n\n    -h, --help     - Show this help.                            \n    -V, --version  - Show the version number for this program.  \n\n  Commands:\n\n    render          [input] [args...]  - Render input file(s) to various document types.                                                \n    serve           [input]            - Serve an interactive document.                                                                 \n    create-project  [dir]              - Create a project for rendering multiple documents                                              \n    preview         [file] [args...]   - Render and preview a Quarto document or website project. Automatically reloads the browser when\n    convert         [input]            - Convert documents to alternate representations.                                                \n    capabilities                       - Query for current capabilities (output formats, engines, kernels etc.)                         \n    check           [target]           - Check and verify correct functioning of Quarto installation and computation engines.           \n    inspect         [path]             - Inspect a Quarto project or input path. Inspecting a project returns its config and engines.   \n    tools           [command] [tool]   - Manage the installation, update, and uninstallation of useful tools.                           \n    help            [command]          - Show this help or the help of a sub-command.\n    \nFrom there I can check the help documentation for the quarto render command by typing the following,\n\nquarto render help\n\nand so on. Browsing this documentation in addition to all the excellent content on the quarto website is a useful way of finding additional options. If I wanted to render the current post, and my terminal was currently at the project root folder (i.e., my quarto-blog folder), I can render it as follows:\n\nquarto render posts/2022-04-20_porting-to-quarto/index.qmd\n\nThe ability to do this cleanly from the terminal seems like a handy feature of quarto, though I’ll admit I am not yet sure how I’ll use it."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#epilogue",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#epilogue",
    "title": "Porting a distill blog to quarto",
    "section": "Epilogue",
    "text": "Epilogue\nWhen I started this process I wasn’t quite sure if I was going to follow through on it and actually switch the blog over to quarto. The distill blog has served me well for the last year, and I don’t like fixing things if they aren’t broken. However, the longer I played with quarto the more I liked it, and the process was far less painful than I feared it would be. I feel like it’s retained the things I like about distill, but integrated those cleanly with other features (e.g., the bootstrap grid!) that I really missed having access to from distill. Every now and then I’ve come across little quirks where some of the rough edges to quarto are still showing – it is a new tool still – but I’m enjoying it a lot."
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#last-updated",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#last-updated",
    "title": "Porting a distill blog to quarto",
    "section": "Last updated",
    "text": "Last updated\n\n2023-05-27 17:54:09.879594"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#details",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#details",
    "title": "Porting a distill blog to quarto",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2022-04-20_porting-to-quarto/index.html#footnotes",
    "href": "posts/2022-04-20_porting-to-quarto/index.html#footnotes",
    "title": "Porting a distill blog to quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes↩︎\nNo, but they can be recursive2↩︎\nIn case you’re interested: the “Welcome to My Blog” post in the starter blog lists the date as date: \"20/04/2022\", which gets parsed as a string literal when the post gets built (i.e., the post date shows up as “20/04/2022”), sensibly enough. However, when you build the entire site it renders as “May 4, 2023”.↩︎\nWhich, frankly I do not, but I am also foolish and will try things anyway↩︎"
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "",
    "text": "It is late December, the office is quiet on the eve of the annual ritual of everyone vanishing into their own personal end of year vacations, and I have time to breathe and – heaven forfend – to think. As such, and in the spirit of trying to do something professionally useful during this strangely empty moment of the year, it is time for another post in my irregular and unsystematic series on pharmacometrics. Does anyone except Danielle care? Unlikely. Will it make the world a better place? Absolutely not. But neither of these has ever been a governing consideration on this blog.1"
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#the-matter-at-hand",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#the-matter-at-hand",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "The matter at hand",
    "text": "The matter at hand\nThe topic for today’s installment in Danielle’s ongoing public display of blog-based narcissism is deriving closed-form solutions for two-compartment pharmacokinetic (PK) models. There is absolutely nothing in the post that is not already extremely well known by pharmacometricians, and the scope of the post is somewhat narrower than the full range of two-compartment PK models. I’m mostly going to consider this model:\n\n\n\n\n\nHere we have the garden-variety two-compartment model with first-order absorption and first-order elimination. In the usual application of this model we have a drug that is administered orally. The drug is absorbed gradually from the gut into systemic circulation (central compartment), and is gradually eliminated from the body, again from the central compartment. During the time that the drug is within the body, it does not stay solely within the blood: it is distributed through various other tissues, and so the volume over which it is distributed changes over time. In a two-compartment model the complexity of this is simplified: the drug concentration in a typical study is measured only in the central compartment (e.g., by measuring plasma concentration), and so the model does not concern itself with the fine grained details of what is happening elsewhere in the body. It is often (not always) sufficient to assume that the “other” tissues in which the drug distributes comprise a single “peripheral compartment”.\nIn order to attach physiologically meaningful interpretation to this model, it’s typically parameterised in terms of the following five quantities:\n\nVolume of distribution for the central compartment \\(V_c\\) refers to the volume over which the drug amount in the central compartment is assumed to be evenly distributed. It’s a fictional quantity – and can take on values that are much larger than the actual volume of blood or plasma for a variety of reasons – but it’s a convenient one.\nVolume of distribution for the peripheral compartment \\(V_p\\): as above but for the peripheral compartment\nClearance \\(Cl\\) is the volume within the central compartment that can be completely cleared of drug per unit time, and governs the elimination of drug from the body\nIntercompartmental clearance \\(Q\\) governs the exchange of drug between the central and peripheral compartments\nThe absorption rate constant \\(k_a\\) is a scaling factor used to describe the proportion of the drug amount currently in the gut that is transferred into systemic circulation at any moment in time2\n\nThis model is probably the one I come across most in my everyday work, and these are the parameters used to interpret model behaviour. However, it’s not the most convenient form to use when working with the model mathematically, so let’s rewrite it in terms that are more convenient."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#a-two-compartment-pharmacokinetic-model",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#a-two-compartment-pharmacokinetic-model",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "A two compartment pharmacokinetic model",
    "text": "A two compartment pharmacokinetic model\nAs in every mathematical fairy tale and religious text, we begin with some notation. The state of our system will at time \\(t\\) be described in terms of three quantities: \\(x_{0t}\\) is the drug amount in the depot compartment (generally the gut) at time \\(t\\) units post-dose, \\(x_{1t}\\) is the amount in the central compartment, and \\(x_{2t}\\) is the amount in the peripheral compartment. Our state vector \\(\\mathbf{x}_t \\in \\mathbb{R}^3_{\\geq 0}\\) is the collection of these three things, and if you care deeply about such matters you might wish to assert that the drug amounts are described by a function \\(\\mathbf{x}(t)\\) that takes on such values:\n\\[\n\\mathbf{x}(t) = \\mathbf{x}_t = (x_{0t}, x_{1t}, x_{2t})\n\\] In an act of perversity, however, I’m not going to use the functional notation \\(\\mathbf{x}(t)\\) here. I’m simply going to talk about the state vector \\(\\mathbf{x}_t\\) because honestly no-one wants to see what this post would look like if I added all those extra parentheses.\nHaving established this notation and – in a Barbie-like state of best-day-ever excitement – we can now describe how the state vector changes over time using a system of ordinary differential equations (ODEs):\n\\[\n\\begin{array}{rcl}\n\\frac{d}{dt} x_{0t} & = & -k_{01} x_{0t} \\\\\n\\frac{d}{dt} x_{1t} & = & k_{01} x_{0t} - (k_{12} + k_{10}) x_{1t} + k_{21} x_{2t} \\\\\n\\frac{d}{dt} x_{2t} & = & k_{12} x_{1t} - k_{21} x_{2t}\n\\end{array}\n\\]\nHere we have four rate constant parameters (\\(k_{01}\\), \\(k_{10}\\), \\(k_{12}\\), and \\(k_{21}\\)), each of which describes the instantaneous transfer of drug quantity from one compartment to another: the notational convention is to refer to the source compartment as the first subscript and the destination compartment as the second, so for instance \\(k_{12}\\) is the rate constant associated with the movement of drug from the central compartment (compartment 1) to the peripheral compartment (compartment 2). In this notation the “zero-th” compartment is an abstraction: \\(k_{01}\\) describes absorption (generally from the gut) and \\(k_{10}\\) describes elimination (generally to urine).3\nSchematically, the exchange of drug amounts between the compartments in this formalism can be visualised in the following way:\n\n\n\n\n\nStructurally you can see it’s the same as the model described at the start, and it’s not at all difficult to convert between the physiologically-interpretable parameterisation and the mathematically-convenient parameterisation using these rate constants.4 5\nIn matrix form we can express the ODE system as:\n\\[\n\\frac{d}{dt} \\mathbf{x}_t = \\mathbf{K} \\mathbf{x}_t\n\\]\nwhere\n\\[\n\\mathbf{K} = \\left[\n\\begin{array}{ccc}\n-k_{01} &                0  &       0 \\\\\nk_{01} & -k_{12} - k_{10}  &  k_{21} \\\\\n      0 &           k_{12}  & -k_{21} \\\\\n\\end{array}\n\\right]\n\\]\nand our initial conditions for a dose administered orally at time \\(t=0\\) are:\n\\[\n\\mathbf{x}_{0} = (\\mbox{dose}, 0, 0)\n\\]\nThis particular parameterisation is most helpful when we want to think about the underlying ODE system, and it’s the one I’ll use for deriving solutions in this post, but when we want to interpret the models we usually rewrite it in terms of the parameters I described at the start of the post.6"
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#solving-linear-time-homogeneous-ode-systems",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#solving-linear-time-homogeneous-ode-systems",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Solving linear time-homogeneous ODE systems",
    "text": "Solving linear time-homogeneous ODE systems\nSo now we get to the part of the post where the mathematics begins to intrude. Per our model, we have a linear time-homogeneous ODE system that we want to solve, in the sense that we would like a nice algebraic expression that describes the state \\(\\mathbf{x}_t\\) at time \\(t\\). Of course, I want all sorts of things in life that I can’t obtain without suffering, and this is no exception. If you want to solve a linear homogeneous ODE, you’re going to have to work with matrix exponentials, and with matrix exponentials comes pain. Especially if, like me, you vaguely remember matrix exponentials from an undergrad maths class you took 30 years ago and you actually haven’t needed to use them for anything much in the decades that followed.\nPerhaps a small “refresher” will help us all then. The matrix exponential \\(e^{\\mathbf{K}}\\) of a matrix \\(\\mathbf{K}\\) is a quantity that is broadly analogous to its scalar equivalent \\(e^k\\) for scalar value \\(k\\), and has similar (but not identical) properties. In the same way that we can define a scalar exponential \\(e^k\\) via a series expansion, the matrix exponential \\(e^{\\mathbf{K}}\\) is defined as:\n\\[\n\\begin{array}{rcl}\ne^{\\mathbf{K}} &=& \\mathbf{I} + \\mathbf{K} + \\frac{1}{2} \\mathbf{K}^2 + \\frac{1}{6} \\mathbf{K}^3 + \\ldots + \\frac{1}{j!} \\mathbf{K}^j + \\ldots \\\\\n&=& \\sum_{j = 0}^\\infty \\frac{1}{j!} \\mathbf{K}^j\n\\end{array}\n\\]\nSome handy properties for matrix exponentials, most of which we’ll need:\n\nIf \\(\\mathbf{K}\\) is a matrix of zeros, \\(e^{\\mathbf{K}} = 1\\)\nIf \\(\\mathbf{K}\\) is the identity \\(\\mathbf{I}\\), \\(e^{\\mathbf{K}} = e^{\\mathbf{I}} = \\mathbf{I}\\)\nIf \\(t\\) is a scalar, \\(e^{t\\mathbf{K}} = e^t e^\\mathbf{K}\\)\nIf \\(m\\) and \\(n\\) are scalars, \\(e^{m\\mathbf{K}} e^{n \\mathbf{K}} = e^{(m + n)\\mathbf{K}}\\)\nIf \\(\\mathbf{D} = \\mbox{diag}(d_1, d_2, \\ldots)\\) is a diagonal matrix, \\(e^\\mathbf{D}\\) is the diagonal matrix \\(e^\\mathbf{D} = \\mbox{diag}(e^{d_1}, e^{d_2}, \\ldots)\\)\nThe derivative is analogous to the scalar case, \\(\\frac{d}{dt} e^{t \\mathbf{K}} = \\mathbf{K} e^{t \\mathbf{K}}\\)\nFor an invertible matrix \\(\\mathbf{H}\\) such that \\(\\mathbf{K} = \\mathbf{HMH^{-1}}\\), \\(e^{t\\mathbf{K}} = \\mathbf{H} e^{t \\mathbf{M}} \\mathbf{H^{-1}}\\)\n\nOn the basis of the derivative property above, it’s immediately clear that the solution to our ODE system is going to take the following form:7\n\\[\n\\mathbf{x}_t = e^{t \\mathbf{K}} \\mathbf{x}_0\n\\]\nSeeing that this is the form of the solution is the easy part. The hard part, of course, is finding the expression that describes the matrix exponential \\(e^{t\\mathbf{K}}\\). Fortunately, this is something that cleverer people than I have already thought about, and in any case the properties of matrix exponentials suggest a general strategy for such problems:\n\nFind the eigenvalues \\(\\lambda_1, \\lambda_2, \\ldots\\), for the matrix \\(\\mathbf{K}\\)\nFind the corresponding eigenvectors \\(\\mathbf{u}_1, \\mathbf{u}_2, \\ldots\\)\nConstruct the matrix \\(\\mathbf{U} = [\\mathbf{u}_1, \\mathbf{u}_2, \\ldots]\\) with the eigenvectors as columns, and diagonal matrix \\(\\mathbf{\\Lambda}\\) whose diagonals correspond to eigenvalues, and invert it to obtain \\(\\mathbf{U}^{-1}\\)\nNoting that we have the eigendecomposition \\(K = \\mathbf{U \\Lambda U^{-1}}\\), rewrite \\(e^{t\\mathbf{K}} = \\mathbf{U} e^{t\\mathbf{\\Lambda}} \\mathbf{U^{-1}}\\)\nSince \\(\\mathbf{\\Lambda}\\) is diagonal, \\(e^{t\\mathbf{\\Lambda}}\\) is straightforward, and we can calculate \\(e^{t\\mathbf{K}}\\) by matrix multiplication\n\nOkay then. Much like the Cylons, we have a Plan.8"
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#solve-a-simpler-problem",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#solve-a-simpler-problem",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Solve a simpler problem",
    "text": "Solve a simpler problem\nAs Jane Austen famously said, “it is a truth universally acknowledged, that a woman in possession of a three-state ODE system must be in want of a two-state ODE that is easier to solve”. And so it is here. Rather than try to work with the model as described, I’ll start with a simpler model (one that is itself quite useful) that will be easier to solve, and whose solution will make it massively easier to solve the full system. And so it is that our first step will be to retreat from a two-compartment model with oral dosing to a two-compartment model with bolus IV dosing.\nIn most cases I’ve come across in my pharmacometric work so far, the drug we’re modelling is orally administered, and the model with first-order absorption into the central compartment described above (or some variation thereof) is the one we want to use. However, it’s convenient to start with a simpler case where the drug is administered by a bolus IV dose, and of course this scenario does arise in real life. In this scenario there is no depot compartment, and entire dose appears in the central compartment at \\(t = 0\\). Or, to frame it in technical terms, we have zero-order absorption into the central compartment rather than first-order absorption. Again assuming first-order elimination, \\(\\mathbf{K}\\) is now a simpler 2x2 matrix.9 In this situation our state vector consists only of the central and peripheral compartments:\n\\[\n\\mathbf{x}(t) = \\mathbf{x}_t = (x_{1t}, x_{2t})\n\\]\nOur state transition matrix is now this:\n\\[\n\\mathbf{K} = \\left[\n\\begin{array}{cc}\n-k_{12} - k_{10}  &  k_{21} \\\\\n          k_{12}  & -k_{21} \\\\\n\\end{array}\n\\right]\n\\]\nand our initial state at time \\(t=0\\) is:\n\\[\n\\mathbf{x}_{0} = (\\mbox{dose}, 0)\n\\]\nAs before, our solution will be of the form\n\\[\n\\mathbf{x}_t = e^{t \\mathbf{K}} \\mathbf{x}_0\n\\]\nand we can find this solution by following the general strategy outlined earlier. To that end, I’ll begin by finding the eigenvalues \\(\\lambda\\) that satisfy \\(\\det(\\mathbf{K} - \\lambda \\mathbf{I}) = 0\\). Well, technically speaking, I’ll begin by taking a little trip down memory lane to 1994 and my first-year undergraduate maths classes, but I’ll spare you that traumatic recollection and jump straight to the derivation:\n\\[\n\\begin{array}{rcl}\n\\det(\\mathbf{K} - \\lambda \\mathbf{I})\n&=& \\det \\left[\n\\begin{array}{cc}\n-k_{12} - k_{10} - \\lambda  &  k_{21} \\\\\n          k_{12}  & -k_{21} - \\lambda \\\\\n\\end{array}\n\\right] \\\\\n&=& (-k_{12} - k_{10} - \\lambda)(-k_{21} - \\lambda) - k_{12} k_{21} \\\\\n&=& \\lambda^2 + (k_{10} + k_{12} + k_{21}) \\lambda + k_{10} k_{21}\n\\end{array}\n\\] The final expression doesn’t factorise into anything very pretty, so it’s conventional to simply define new variables \\(\\alpha\\) and \\(\\beta\\) such that:\n\\[\n\\begin{array}{rcl}\n\\alpha \\beta & = & k_{10} k_{21} \\\\\n\\alpha + \\beta & = & k_{10} + k_{12} + k_{21}\n\\end{array}\n\\]\nwhen written in these new variables, which were constructed specifically to make an expression that factorises easily, the left hand side of our characteristic equation turns out to be shockingly simple to factorise:\n\\[\n\\det(\\mathbf{K} - \\lambda \\mathbf{I}) = \\lambda^2 + (\\alpha + \\beta) \\lambda + \\alpha \\beta = (\\lambda + \\alpha)(\\lambda + \\beta)\n\\]\nWonders will never cease. In any case, when written in these terms, our two eigenvalues are \\(\\lambda = -\\alpha\\) and \\(\\lambda = -\\beta\\). This is – of course – a pure notational convenience since \\(-\\alpha\\) and \\(-\\beta\\) were defined such that they would end up being the eigenvalues, but it’s worth mentioning this because these are the exact variables that show up in various software systems (e.g., NONMEM) and textbooks. Nevertheless, it’s still no good to us if we don’t go through tedious business of applying the quadratic formula to express \\(\\alpha\\) and \\(\\beta\\) in terms of \\(k_{01}\\), \\(k_{10}\\), \\(k_{12}\\), and \\(k_{21}\\). So here it is:\n\\[\n\\begin{array}{rcl}\n\\alpha, \\beta\n&=& \\displaystyle\\frac{(\\alpha + \\beta) \\pm \\sqrt{(\\alpha + \\beta)^2 - 4\\alpha \\beta}}{2} \\\\\n&=& \\displaystyle\\frac{(k_{10} + k_{12} + k_{21}) \\pm \\sqrt{(k_{10} + k_{12} + k_{21})^2 - 4k_{10} k_{21}}}{2}\n\\end{array}\n\\]\nNow that we have the eigenvalues, we proceed to the eigenvectors. For each eigenvalue \\(\\lambda\\) there is a corresponding eigenvector \\(\\mathbf{u}\\) such that \\((\\mathbf{K} - \\lambda \\mathbf{I}) \\mathbf{u} = \\mathbf{0}\\). In our case:\n\\[\n\\left[\n\\begin{array}{cc}\n-k_{12} - k_{10} - \\lambda  &  k_{21} \\\\\n          k_{12}  & -k_{21} - \\lambda \\\\\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{c}\nu_1 \\\\\nu_2\n\\end{array}\n\\right]\n=\n\\left[\n\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\n\\right]\n\\]\nIf this were a bigger matrix we’d probably use Gauss-Jordan elimination to construct the row-reduced echelon form (RREF) and then read off the solutions using the RREF. That’s kind of overkill in this case because – let’s be brutally honest here – you can look at the bottom row and guess that the solution is going to have the form \\(u_1 = k_{21} + \\lambda\\), \\(u_2 = k_{12}\\).\nTo convince ourselves that this is the correct solution, we’ll substitute it into both rows and see that, shockingly, we end up with zero. We’ll start with the bottom row because that was the one we used to guess the solution:\n\\[\n\\begin{array}{rcl}\nk_{12} u_1 + (-k_{21} - \\lambda) u_2\n&=& k_{12} (k_{21} + \\lambda) + (-k_{21} - \\lambda) k_{12} \\\\\n&=& 0\n\\end{array}\n\\]\nI mean. Of course that was going to work. We literally chose expressions for \\(u_1\\) and \\(u_2\\) that would cause the two terms to cancel out. The actual test of our guess arrives when we try the same thing with the top row. This time it’s takes a little more effort:\n\\[\n\\begin{array}{rcl}\n(-k_{12} - k_{10} - \\lambda) u_1 + k_{21} u_2\n&=& (-k_{12} - k_{10} - \\lambda) (k_{21} + \\lambda) + k_{21} k_{12} \\\\\n&=& -k_{12} k_{21} - k_{10} k_{21} - \\lambda k_{21} - k_{12} \\lambda - k_{10} \\lambda - \\lambda^2 + k_{21} k_{12} \\\\\n&=& - k_{10} k_{21} - \\lambda k_{21} - k_{12} \\lambda - k_{10} \\lambda - \\lambda^2 \\\\\n&=& - k_{10} k_{21} - (k_{21} + k_{12} + k_{10}) \\lambda - \\lambda^2 \\\\\n&=& - \\alpha \\beta - (\\alpha + \\beta) \\lambda - \\lambda^2 \\\\\n&=& - (\\lambda + \\alpha) (\\lambda + \\beta) \\\\\n\\end{array}\n\\]\nOnce we have arrived at this expression, it’s clear that our guess is correct. The only two values that \\(\\lambda\\) can take are the eigenvalues \\(\\lambda = -\\alpha\\) and \\(\\lambda = -\\beta\\), and both of those yield a value of zero. So our guess was indeed correct and we have our eigenvectors. Awesome.10\nNow that we are in possession of eigenvalues and eigenvectors we can construct the matrices \\(\\mathbf{U}\\) and \\(\\mathbf{\\Lambda}\\) that will give us the eigendecomposition \\(\\mathbf{K} = \\mathbf{U \\Lambda U}^{-1}\\). Here they are:\n\\[\n\\begin{array}{rcl}\n\\mathbf{\\Lambda} &=&\n\\left[\n\\begin{array}{cc}\n-\\alpha & 0 \\\\\n0 & -\\beta\n\\end{array}\n\\right]\n\\\\\n\\mathbf{U} &=&\n\\left[\n\\begin{array}{cc}\nk_{21} -\\alpha & k_{21} - \\beta \\\\\nk_{12} & k_{12}\n\\end{array}\n\\right]\n\\end{array}\n\\]\nThe last thing we need is the inverse \\(\\mathbf{U}^{-1}\\), which is also fairly easy to derive. It’s a 2x2 matrix, after all, and inverting a 2x2 matrix isn’t even undergrad level maths: they taught us that one in high school. Noting first that determinant \\(\\det \\mathbf{U}\\) is as follows,\n\\[\n\\det \\mathbf{U} = (k_{21} - \\alpha) k_{12} - k_{12} (k_{21} - \\beta) = k_{12} (\\beta - \\alpha)\n\\]\nwe then write down the inverse as follows:\n\\[\n\\mathbf{U}^{-1} =\n\\frac{1}{k_{12}(\\beta - \\alpha)}\n\\left[\n\\begin{array}{cc}\nk_{12} & \\beta - k_{21} \\\\\n-k_{12} & k_{21} -\\alpha\n\\end{array}\n\\right]\n\\]\nSo now we have \\(\\mathbf{U}\\), \\(\\mathbf{U}^{-1}\\), and \\(\\mathbf{\\Lambda}\\), and we could proceed straight to doing the matrix multiplication but for the sake of what is left of my sanity I’m going to simplify my notation a bit and define two new constants \\(a\\) and \\(b\\):\n\\[\n\\begin{array}{rcl}\na & = & (k_{21} - \\alpha) / k_{12} \\\\\nb & = & (k_{21} - \\beta) / k_{12}\n\\end{array}\n\\]\nWith the help of these two constant, the expressions for \\(\\mathbf{U}\\) and \\(\\mathbf{U}^{-1}\\) are now considerably less unpleasant on the eye:\n\\[\n\\begin{array}{rcl}\n\\mathbf{U} &=&\nk_{12} \\left[\n\\begin{array}{cc}\na & b \\\\\n1 & 1\n\\end{array}\n\\right]\n\\\\\n\\mathbf{U}^{-1} &=&\n\\displaystyle\\frac{1}{\\beta - \\alpha} \\left[\n\\begin{array}{cc}\n1 & -b \\\\\n-1 & a\n\\end{array}\n\\right]\n\\end{array}\n\\]\nNow that we have the eigendecomposition \\(\\mathbf{K} = \\mathbf{U \\Lambda U}^{-1}\\), we can solve our matrix exponential. Using the last of the matrix exponential properties in the potted list I provided earlier in the pose, we can express the matrix exponential \\(e^{t \\mathbf{K}}\\) as the matrix product \\(\\mathbf{U} e^{t \\mathbf{\\Lambda}} \\mathbf{U}^{-1}\\). Conveniently, \\(\\Lambda\\) is a diagonal matrix which makes \\(e^{t \\mathbf{\\Lambda}}\\) trivially easy, and so we obtain this:11\n\\[\n\\begin{array}{rcl}\ne^{t \\mathbf{K}} & = &\n\\mathbf{U} e^{t \\mathbf{\\Lambda}} \\mathbf{U}^{-1} \\\\\n& = &\n\\displaystyle\\frac{k_{12}}{\\beta - \\alpha}\n\\left[\n\\begin{array}{cc}\na & b \\\\\n1 & 1\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\ne^{-\\alpha t} & 0 \\\\\n0 & e^{-\\beta t}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\n1 & -b \\\\\n-1 & a\n\\end{array}\n\\right]\n\\\\\n& = &\n\\displaystyle\\frac{k_{12}}{\\beta - \\alpha}\n\\left[\n\\begin{array}{cc}\na e^{-\\alpha t} & b e^{-\\beta t} \\\\\ne^{-\\alpha t} & e^{-\\beta t}\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\n1 & -b \\\\\n-1 & a\n\\end{array}\n\\right] \\\\\n&=&\n\\displaystyle\\frac{k_{12}}{\\beta - \\alpha}\n\\left[\n\\begin{array}{cc}\n(a e^{-\\alpha t} - b e^{-\\beta t}) & -ab (e^{-\\alpha t} - e^{-\\beta t}) \\\\\n-(e^{-\\alpha t} - e^{-\\beta t}) & (-b e^{-\\alpha t} + a e^{-\\beta t})\n\\end{array}\n\\right]\n\\end{array}\n\\]\nSo now we can turn to our solution:\n\\[\n\\begin{array}{rcl}\n\\mathbf{x}_t & = & e^{t \\mathbf{K}} \\ \\mathbf{x}_0 \\\\\n& = &\n\\displaystyle\\frac{k_{12}}{\\beta - \\alpha}\n\\left[\n\\begin{array}{cc}\n(a e^{-\\alpha t} - b e^{-\\beta t}) & -ab (e^{-\\alpha t} - e^{-\\beta t}) \\\\\n-(e^{-\\alpha t} - e^{-\\beta t}) & (-b e^{-\\alpha t} + a e^{-\\beta t})\n\\end{array}\n\\right]\n\\left[\n\\begin{array}{cc}\n\\mbox{dose} \\\\\n0\n\\end{array}\n\\right] \\\\\n& = &\n\\mbox{dose} \\times \\displaystyle\\frac{k_{12}}{\\beta - \\alpha} \\left[\n\\begin{array}{cc}\na e^{-\\alpha t} - b e^{-\\beta t} \\\\\ne^{-\\alpha t} - e^{-\\beta t}\n\\end{array}\n\\right]\n\\end{array}\n\\] Recalling that the central compartment corresponds to the first element of the state vector (top row), we can focus on this and compute the drug amount (not concentration) in the central compartment at time \\(t\\):\n\\[\n\\begin{array}{rcl}\nx_{1t}\n&=&\n\\mbox{dose} \\times \\displaystyle\\frac{k_{12}}{\\beta - \\alpha} \\times \\left(  a e^{-\\alpha t} - b e^{-\\beta t} \\right) \\\\\n&=&\n\\mbox{dose} \\times \\displaystyle\\frac{k_{12}}{\\beta - \\alpha} \\times \\left( \\left(\\frac{k_{21} - \\alpha}{k_{12}} \\right)  e^{-\\alpha t} - \\left(\\frac{k_{21} - \\beta}{k_{12}} \\right) e^{-\\beta t} \\right) \\\\\n&=&\n\\mbox{dose} \\times \\left( \\left(\\displaystyle\\frac{\\alpha - k_{21}}{\\alpha - \\beta} \\right) e^{-\\alpha t} - \\left(\\displaystyle\\frac{\\beta - k_{21}}{\\alpha - \\beta} \\right) e^{-\\beta t} \\right) \\\\\n&=&\n\\mbox{dose} \\times \\left(A e^{-\\alpha t} +  B e^{-\\beta t} \\right)\n\\end{array}\n\\]\nand thus we have a model that can be expressed as a sum of two exponentials where:\n\\[\n\\begin{array}{rcl}\nA & = & \\displaystyle\\frac{\\alpha - k_{21}}{\\alpha - \\beta} \\\\ \\\\\nB & = & \\displaystyle\\frac{\\beta - k_{21}}{\\alpha - \\beta}\n\\end{array}\n\\]\nIf I’m being completely honest, it’s precisely this biexponential12 form that motivated me to suck it up and derive the solution myself. One of the textbooks I was reading at work – in order to familiarise myself with some of the pharmacokinetic background that I need – introduced the two-compartment model by defining it formally in terms of this biexponential expression,13 but then drew this model schematically using a state-transition diagram similar to the ones I used at the start of this post. I was baffled, because it was not at all obvious to me from inspection how the diagram and the equation were linked, and the book did not present a derivation.\nSo yes, as with most things I do on this blog, my true motivation was in fact pig-headed stubbornness. I’m terribly bad at taking certain things on faith, and felt a deep – and let’s be honest, pathological – need to derive the solution myself. It is the very essence of spite-driven mathematics."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#returning-to-the-original-problem",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#returning-to-the-original-problem",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Returning to the original problem",
    "text": "Returning to the original problem\nNow that we have a closed form solution for a two-compartment model with zero-order absorption into the central compartment (i.e., bolus IV dosing), we can return to the oral dosing model (assuming first-order absorption) that we started with. It’s a relatively straightforward solution at this point since we have a continuous influx from the gut, so we can convolve this time-dependent influx with the zero-order solution. Since I’m assuming bioavailability \\(F = 1\\) for this post14 I’ll happily act as if the drug amount arriving in the central compartment from the gut at time \\(t\\) is the same as the amount that left the gut at time \\(t\\):\n\\[\n-\\frac{d}{dt} x_{0t} = \\mbox{dose} \\times k_{01} \\times e^{-k_{01} t}\n\\]\nThus the drug amount in the central compartment at time \\(t\\) is given:\n\\[\n\\begin{array}{rcl}\nx_{1t}\n&=& \\mbox{dose} \\times k_{01} \\times \\displaystyle\\int_0^t e^{-k_{01} u} \\left( A e^{-\\alpha (t-u)} + B e^{-\\beta (t-u)} \\right) \\ du \\\\\n&=& \\mbox{dose} \\times k_{01} \\times \\displaystyle\\int_0^t A e^{-\\alpha (t-u) -k_{01} u} + B e^{-\\beta (t-u) -k_{01} u}  \\ du \\\\\n&=& \\mbox{dose} \\times k_{01} \\times \\left( Ae^{-\\alpha t} \\left[ \\frac{1}{\\alpha - k_{01}} e^{(\\alpha - k_{01})u} \\right]_0^t + Be^{-\\beta t} \\left[ \\frac{1}{\\beta - k_{01}} e^{(\\beta - k_{01})u} \\right]_0^t   \\right) \\\\\n&=& \\mbox{dose} \\times k_{01} \\times \\left( \\displaystyle\\frac{Ae^{-\\alpha t} (e^{(\\alpha - k_{01})t} - 1)}{\\alpha - k_{01}} + \\displaystyle\\frac{Be^{-\\beta t} (e^{(\\beta - k_{01})t} - 1)}{\\beta - k_{01}}   \\right) \\\\\n\\end{array}\n\\]\nAnd just like that we have a solution."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#does-it-work",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#does-it-work",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Does it work?",
    "text": "Does it work?\nI’m not so arrogant as to simply assume I got it right. It’s reassuring that all the expressions that came out along the way bear a striking resemblance to those I’ve seen in the textbooks, but I still want to compare to a numerical method that I trust. In an earlier post I talked about using the rxode2 package to simulate from pharmacokinetic models, and while I could certainly use some other tool for this purpose (e.g., the deSolve package would be totally fine here) I might as well use rxode2 here as well. Here’s an R function that solves the problem numerically:\n\nnumeric_solution &lt;- function(k01, k12, k21, k10, time) {\n  \n  mod &lt;- rxode2::rxode({\n    d/dt(A0) = -k01 * A0;\n    d/dt(A1) = k01 * A0 - (k12 + k10) * A1 + k21 * A2;\n    d/dt(A2) = k12 * A1 - k21 * A2;\n  })\n  inits &lt;- c(A0 = 1, A1 = 0, A2 = 0)\n  \n  ev &lt;- rxode2::eventTable()\n  ev$add.sampling(time = time)\n  pars &lt;- data.frame(k01 = k01, k12 = k12, k21 = k21, k10 = k10)\n  dat &lt;- mod$solve(pars, ev, inits)\n  dat &lt;- as.data.frame(dat)\n  \n  out &lt;- data.frame(\n    time = dat$time, \n    amount = dat$A1, \n    solution = \"numeric\"\n  )\n  return(out)\n}\n\nHere’s the corresponding function implementing the analytic solution:\n\nanalytic_solution &lt;- function(k01, k12, k21, k10, time) {\n  \n  ks &lt;- k10 + k12 + k21\n  alpha &lt;- (ks + sqrt(ks^2 - (4 * k10 * k21))) / 2\n  beta  &lt;- (ks - sqrt(ks^2 - (4 * k10 * k21))) / 2\n  \n  A &lt;- (alpha - k21) / (alpha - beta)\n  B &lt;- -(beta - k21) / (alpha - beta) \n  \n  A_term &lt;- A * exp(-alpha * time) * (exp(time * (alpha - k01)) - 1) \n  B_term &lt;- B * exp(-beta  * time) * (exp(time * (beta  - k01)) - 1) \n\n  A_term &lt;- A_term * k01 / (alpha - k01)  \n  B_term &lt;- B_term * k01 / (beta  - k01)\n  \n  out &lt;- data.frame(\n    time = time, \n    amount = A_term + B_term, \n    solution = \"analytic\"\n  )\n  return(out)\n}\n\nNow let’s compare the two:\n\nk01 &lt;- .3\nk12 &lt;- .2\nk21 &lt;- .1\nk10 &lt;- .3\ntime &lt;- seq(0, 24, .1)\n\ndat_numb &lt;- numeric_solution(k01, k12, k21, k10, time)\ndat_anal &lt;- analytic_solution(k01, k12, k21, k10, time)\ndat &lt;- rbind(dat_numb, dat_anal)\n\nlibrary(ggplot2)\nggplot(dat, aes(time, amount, colour = solution)) +\n  geom_line(show.legend = FALSE) +\n  facet_wrap(~solution) +\n  theme_bw()\n\n\n\n\n\n\n\n\nLooks good to me? The differences between the two are small enough that we can attribute them to simulation precision etc…\n\nmax(abs(dat_numb$amount - dat_anal$amount))\n\n[1] 1.920889e-08\n\n\n…and yes, you get similar agreement between the two versions15 if you feed in other parameter values. That’s good enough for me."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#was-it-worth-it",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#was-it-worth-it",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Was it worth it?",
    "text": "Was it worth it?\nAnd so we come to the end. The problem is solved, Danielle has convinced herself that she understands the formalism properly, and a great many pieces of scrap paper were sacrificed to the dark gods of mathematics in the process. Was it all worthwhile? I mean… in one sense, probably not. The analytic solutions I’ve derived here are highly unoriginal, and of course they have already been implemented and incorporated into standard tools used in pharmacometric modelling. Nothing new has been added to the world by me doing this. But also, it’s worth highlighting that it was a very good thing that these solutions exist thanks to the hard work of those that have come before us,16 because they do make a massive difference in terms of computational performance:\n\nmicrobenchmark::microbenchmark(\n  numeric_solution(k01, k12, k21, k10, time),\n  analytic_solution(k01, k12, k21, k10, time)\n)\n\nUnit: microseconds\n                                        expr       min         lq       mean     median         uq      max neval\n  numeric_solution(k01, k12, k21, k10, time) 39558.780 45899.2650 51754.4014 49391.4505 55292.3945 92581.17   100\n analytic_solution(k01, k12, k21, k10, time)   137.652   170.0235   314.0824   237.3915   313.8525  5412.48   100\n\n\nThe units here are microseconds, so yeah okay they’re both fast. They’re both fast enough that I have no reason at all to care when running a small simulation: RK45 and BDF exist for a reason, and as Dan Simpson reminded me the other day, generations of numerical analysts have suffered so that I don’t have to.\nHowever, a speedup of a this magnitude makes a very big difference in the context of model fitting. Even my lazy R implementation of the analytic solution is hundreds of times faster than the very efficient numerical solution implemented by rxode2, and of course the speed up would be even more extreme if I could be bothered writing it in a compiled language like C++ or Rust or whatever. But of course I have no need to do that because that’s already been done in software. All I really care about for this post is deriving the solution and verifying that it works.\nI’ve suffered enough."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#resources",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#resources",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Resources",
    "text": "Resources\nOpen access resources:\n\nAlex Best has an open textbook Introducing Mathematical Biology, and chapter 20 derives the solution for the two-compartment bolus IV model. It doesn’t go into quite as much detail as I do in this post (it spares the reader from the pain of matrix exponentials, for example) but I found it very helpful.\nJiří Lebl and Trefor Bazett have an open resouce Introduction to Differential Equations whose section on matrix exponentials I found useful when trying to “refresh my memory” (i.e., learn something that I kind of ignored 30 years ago when it came up in my undergrad maths classes). Relatedly, the list of properties for matrix exponentials is mostly sourced from the wikipedia page on matrix exponentials.\n\nOther resources:\n\nIt’s not open access, and it doesn’t dive into the derivations, but one of the books I’ve been reading at work is Pharmacokinetic and Pharmacodynamic Data Analysis (5th ed) by Johan Gabrielsson and Daniel Weiner: chapter 2 presents the bi-exponential model using the “macro” parameters (\\(A\\), \\(B\\), \\(\\alpha\\), \\(\\beta\\)) and the formulas for converting to the “micro” parameters (the fractional rate constants \\(k_{01}\\), \\(k_{10}\\), \\(k_{21}\\), \\(k_{12}\\)), along with the more general scientific considerations around the model.\nThe other book I’m reading at work is Introduction to Population Pharmacokinetic / Pharmacodynamic Analysis with Nonlinear Mixed Effects Models by Joel S. Owen and Jill Fiedler-Kelly. It provides a good coverage of compartmental models in the context of the NONMEM software package, and is somewhat relevant insofar as the different parameterisations (i.e., TRANS subroutines17) for the ADVAN3 and ADVAN4 subroutines appear in this post (and make more sense to me now that I’ve derived the solutions)."
  },
  {
    "objectID": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#footnotes",
    "href": "posts/2023-12-19_solving-two-compartment-pk-models/index.html#footnotes",
    "title": "Closed form solutions for a two-compartment pharmacokinetic model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf she were to only write posts when she thought someone cared or when it would somehow improve the state of world affairs, she’d never write anything at all. No, she writes blog posts for the same reason twinks make sex tapes in senate hearing rooms: for the pure and unsullied chaos of the thing itself.↩︎\nThis is of course not quite true: for simplicity I’m not considering bioavailability in this post, nor am I considering lag time. I’m certainly not considering transit compartments and the like. The focus of the post is about the ODE system used to model what happens to the drug once it arrives in systemic circulation.↩︎\nBy convention we also don’t count the “depot” compartment (usually the gut) as one of the compartments: although the state \\(\\mathbf{x}_t\\) is a vector of length 3, only two of the compartments (central and peripheral) are used to model the disposition (a term of art referring to both “distribution” and “elimination”) of the drug, so this is a two-compartment model.↩︎\nYes, I do realise that there are 4 rate constants and 5 physiological parameters. In the rate-constants version there would also be a fifth parameter: when fitting models we need a volumetric scaling parameter to convert between drug amount and drug concentration in the central compartment (where the concentration is typically measured), and as such we would include \\(V_c\\) as a parameter in the rate-constant version too. I’ve omitted that in this post for the same reason I have ignored the bioavailability \\(F\\) and the lag time to absorption: none of these parameters are super-relevant to solving the ODE. \\(V_c\\) and \\(F\\) are both scaling factors that multiply various terms by a constant factor, the lag-time is a shift parameter applied to \\(t\\). None of them affect the general form of the solution, which is the thing I care about for the purposes of the post.↩︎\nFor example, to obtain the rate constants from the physiological parameters, we use \\(k_{10} = Cl / V_c\\), \\(k_{01} = k_a\\), \\(k_{12} = Q/V_c\\), and \\(k_{21} = Q/V_p\\). It’s pretty similar going the other way.↩︎\nIn the Gabrielsson and Weiner textbook I’m reading at work, the parameterisation in terms of the fractional rate constants \\(k_{01}\\), \\(k_{10}\\), \\(k_{21}\\) and \\(k_{12}\\) is referred to as the “micro-” parameterisation, taking its name from the fact that the parameters describe the low-level operation of the ODE. This is in contrast to the “physiological” parameterisation in terms of \\(Cl\\), \\(Q\\), \\(V_c\\), \\(V_p\\) and \\(k_a\\) that attempts to ascribe biological interpretation to the quantities. There is also a third “macro-” parameterisation in terms of \\(\\alpha\\), \\(\\beta\\), \\(A\\), and \\(B\\) in which the parameters correspond to the coefficients of the concentration-time curve. As we’ll see later in the post, something like the “macro-” parameterisation you see in the textbooks emerges more or less naturally from the solution to the ODE system. The version that shows up in this post isn’t 100% identical to the version in the textbook (it’s off by a scaling factor because I don’t bother to account for \\(V_c\\) or to fold the dose into the coefficients \\(A\\) and \\(B\\)) but honestly nobody should care about this because the macro-scale parameterisation doesn’t have any scientific meaning. It’s just a convenient description of a biexponential curve that makes you sound fancy at very boring parties when you tell people that the exponents are eigenvalues of the ODE state transition matrix.↩︎\nYes, I know. If I were being rigorous here I’d be precise about how I ended up with this as the exact expression, but I am tired and this is neither a journal article nor a textbook.↩︎\nAs has been so often noted in the BSG fandom, the Cylons quite clearly did not have a Plan. The writers did not ever think of a coherent Plan, and badly tried to retcon a Plan onto the plot in the TV movie by the same name. As Mike Moore later explained, it was just something that looked cool in the opening credits. Much the same could be said for me pretending I ever have a plan when trying to derive something: the Plan is the thing you make up after the fact after you accidentally end up with the answer.↩︎\nFormally speaking, I suppose I ought to be using subscripts to distinguish the 3x3 oral-dosing matrix \\(\\mathbf{K}_{o}\\) from the 2x2 bolus IV dosing matrix \\(\\mathbf{K}_{iv}\\), but I’ll refrain from doing so in this post because it’s always clear from context which one I’m referring to.↩︎\nFor extremely specific values of “awesome”.↩︎\nNo I do not know why those stray $$ fences are showing up in the rendered document. The equations are as-intended, but those should not be included in the output. I kinda think I might have broken the parser with my extremely ugly latex, which is normally a thing I only expect to get told at kink parties↩︎\nWhere of course a biexponential is a model that is sexually attracted to two exponents.↩︎\nTechnically – per. my. earlier. footnote – the one in the book differed from this one by a multiplicative scale factor since it was expressed in terms of concentrations, but whatever. That’s not germane to the post.↩︎\nI mean, multiplying everything by \\(F\\) is not exactly difficult right? If you can follow the rest of this solution you absolutely know how to generalise it to other values of \\(F\\). But okay, if you care deeply about the niceties I will be like Bart and say the line: without loss of generality, I set \\(F=1\\) in this post. I’m sure that makes everyone happier.↩︎\nYes those variable names were deliberate. Of course those variable names were deliberate, whose blog do you think you are reading girl? Now, let’s review our safety tips for numbing lubric…↩︎\nShe pauses, wondering if this is the right moment to link to her favourite Lily Allen song? Yes, it is.↩︎\nTechnically speaking they only count as trans subroutines in the UK legal system if they have a fortran recognition certificate and a permission slip from a doctor that despises them.↩︎"
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html",
    "title": "R scripts for twitter mutes and blocks",
    "section": "",
    "text": "Twitter is a complicated place. I’ve met some of my closest friends through twitter, it’s the place where I keep in touch with my professional community, and it’s an important part of my work as a developer advocate at Voltron Data. But it’s also a general purpose social media site, and there is a lot of content out there I prefer not to see. In particular, because I’m transgender and have absolutely no desire to participate in or even view the public debate that surrounds trans lives, I want to keep that kind of content off my twitter feed. This is particularly salient to me today as a member of the Australian LGBT community. Most people who follow me on twitter probably wouldn’t be aware of it, but it’s been a rough week for LGBT folks in Australia courtesy of a rather intense political fight over LGBT rights and the ostensible (and in my personal opinion, largely fictitious) conflict with religious freedom. The details of Australian politics don’t matter for this post, however. What matters is that these kinds of political disputes necessarily spill over into my twitter feed, and it is often distressing. Events like this one are quite commonplace in my online life, and as a consequence I’ve found it helpful to partially automate my use of twitter safety features such as muting and blocking. Because my experience is not unique, I thought it might be useful to write a short post talking about the the scripts I use to manage twitter safety features from R using the rtweet package."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#warning-off-label-usage",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#warning-off-label-usage",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Warning: off-label usage",
    "text": "Warning: off-label usage\nLet’s make one thing very clear at the outset. A lot of what I’m going to be doing in this blog post is “off label” use of the rtweet package. You’ll see me use it in ways that the writers of the package didn’t really intend it to be used (I think), and you’ll see me dig into the internals of the package and rely on unexported functions.\nThis is almost always a bad idea.\nIf you haven’t seen it, Hadley Wickham gave a very good talk about maintaining R packages as part the rstudio::global 2021 conference. At about the 19 minute mark he talks about the “off label” metaphor. In the context of medication, “off label” refers to any use of a medication that it’s not officially approved for. It might work, but there could be unknown consequences because maybe it hasn’t been fully explored in this context. When applied to software, “off label” use means you’re doing something with a function or package that the designer doesn’t really intend. Your code might work now, but if you’re relying on “incidental” properties of the function to achieve your desired ends, you’re taking a risk. Package maintainers will usually go to considerable lengths to make sure that updates to their packages don’t break your code when you’re using it for its intended purpose… but if you’re doing something “off label” there’s a good chance that the maintainers won’t have thought about your particular use case and they might unintentionally break your code.\nIn short: you go off-label at your own risk. In this particular instance it is a risk I’ve chosen to take and I’m perfectly happy to fix my scripts if (or, let’s be realistic, when) future updates to rtweet cause them to break. Or possibly abandon my scripts. I knew the risks when I went off-label.\nBut if you follow me along this path you need to be aware of the risk too… don’t go off-label lightly! In my case I didn’t do this on a whim: I chose this path about a year ago out of personal desperation, and I’ve had to rewrite the scripts a lot in that time. So, please be careful."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#setting-up",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#setting-up",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Setting up",
    "text": "Setting up\nThe first step is to make sure you have the developer version of rtweet: the scripts I’ve been using rely on the dev version of the package and won’t work with the current CRAN version. To be precise, I’m currently using rtweet version 0.7.0.9011. If you don’t have it, this is the command to install:\n\nremotes::install_github(\"ropensci/rtweet\")\n\nThe second step is to authenticate so that rtweet can access private information about your twitter account. The good news here is that the authentication mechanism in the dev version of rtweet is a little more streamlined than it used to be. You only need to authenticate once on your machine, and the command is as simple as this:\n\nauth_setup_default()\n\nWith that, you should be ready to start!"
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#write-a-blockmute-function",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#write-a-blockmute-function",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Write a block/mute function",
    "text": "Write a block/mute function\nOur first task will be to write a function that can be used either to block or to mute a twitter account. A little whimsically I decided to call it cancel_user(). Quite obviously the name is a personal joke, since it does not “cancel” anyone: the only thing blocking or muting accomplishes is to give you a little distance from the account you’re muting or blocking.\nThe reason for wanting one function that can switch between muting and blocking is that I typically run every process twice, once on my primary account (where, with one exception, I don’t block anyone but mute extensively) and once on my private account (where I block quite aggressively). I’d like to be able to reuse my code in both contexts, so I’ll design the core function to handle both blocks and mutes. Here’s the code:\n\ncancel_user &lt;- function(user, type) {\n  api &lt;- c(\n    \"block\" = \"/1.1/blocks/create\",\n    \"mute\" = \"/1.1/mutes/users/create\"\n  )\n  rtweet:::TWIT_post(\n    token = NULL,\n    api = api[type],\n    params = list(user_id = user)\n  )\n}\n\nThere’s quite a bit to unpack here.\nFirst notice that I have called the internal function rtweet:::TWIT_post(). This is the clearest indication that I’m working off-label. If I were interested only in muting users and never blocking, I’d be able to do this without going off-label because rtweet has an exported function called post_mute() that you can use to mute an account. However, there is no corresponding post_block() function (possibly for good reasons) so I’ve written cancel_user() as my personal workaround.\nSecond, let’s look at the interface to the function. Unlike the more sophisticated functions provided by rtweet this is a bare-bones interface. The user argument must be the numerical identifier corresponding to the account you want to block/mute, and type should be either be \"mute\" or \"block\" depending on which action you wish to take.\nFinding the numeric user id code for any given user is straightforward with rtweet. It provides a handy lookup_users() function that you can employ for this. The actual output of the function is a data frame containing a lot of public information about the user, but the relevant information is the user_id variable. So, if you hate me enough to want to mute or block me on twitter, I’ll make it easy on you. Here’s my numeric user id:\n\nlookup_users(\"djnavarro\")$user_id\n\n\n\n[1] \"108899342\"\n\n\nAs it turns out, for the particular scripts I use, I rarely need to rely on lookup_users() but it is a very handy thing to know about."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#preparing-to-scale",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#preparing-to-scale",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Preparing to scale",
    "text": "Preparing to scale\nAs written there’s nothing particularly wrong with the cancel_user() function, but it’s also not very useful. I can use it to block or mute an individual account, sure, but if that were the problem I wanted to solve it would be a lot easier to do that using the block/mute buttons on twitter. I don’t need to write an R function to do that.\nThe only real reason to implement this as an R function is if you intend to automate it in some fashion and repeat the operation on a scale that would be impossible to do manually. To give a sense of the scale at which I’ve had to implement this I currently have about 220000 accounts blocked from my private account, and a similar number muted from my main account. There’s no way I could possibly do that manually, so I’m going to need to be a little more thoughtful about my cancel_user() function.\n\nPractice safe cancellation\nThe first step in making sure the function works well “at scale”1 is error handling. If I have a list of 50000 account I want to block but for one reason or another cancel_user() throws an error on the 5th account, I don’t want to prevent R from attempting to block the remaining 49995 accounts. Better to catch the error and move on. My preferred way to do this is to use purrr::safely():\n\ncancel_safely &lt;- purrr::safely(cancel_user)\n\nThe cancel_safely() function operates the same way as cancel_user() with one exception. It never throws an error: it always returns a list with two elements, result and error. One of these is always NULL. If cancel_user() throws an error then result is NULL and error contains the error object; if it doesn’t then error is null and result contains the output from cancel_user().\nNot surprisingly, the cancel_safely() function is much more useful when we’re trying to block or mute large numbers of accounts on twitter.\n\n\nCheck your quotas my loves\nOne thing that has always puzzled me about the twitter API is that it places rate limits on how many mutes you can post in any 15 minute period, but doesn’t seem to impose any limits on the number of blocks you can post. I’m sure they have their reasons for doing it, but it’s inconvenient. One consequence of this is that there are lots of tools that exist already for blocking large numbers of accounts. You don’t actually need to write a custom R script for that! But if you want to mute large numbers of accounts, it’s a lot harder: you have to write a script that keeps posting mutes until the rate limits are exceeded, then pauses until they reset, and then starts posting mutes again. Speaking from experience, this takes a very long time. As a purely practical matter, you don’t want to be in the business of muting large numbers of accounts unless you are patient and have a very good reason. In my case, I did.\nIn any case, one thing we’ll need to write a rate_exceeded() function that returns TRUE if we’ve hit the rate limit and FALSE if we haven’t. That’s actually pretty easy to do, as it turns out, because any time our attempt to mute (or block) fails, the cancel_safely() function will catch the error and capture the error message. So all we have to do to write a rate_exceeded() function is to check to see if there’s an error message, and if there is a message, see if that message informs us that the rate limite has been exceeded. This function accomplishes that goal:\n\nrate_exceeded &lt;- function(out) {\n  if(is.null(out$error)) return(FALSE)\n  if(grepl(\"limit exceeded\", out$error$message)) return(TRUE)\n  return(FALSE)\n}\n\nBecause blocks are not rate limited, in practice this function only applies when you’re trying to mute accounts.\n\n\nBe chatty babes\nThe last step in preparing the cancellation function to work well at scale is to make it chatty. In practice, a mass block/mute operation is something you leave running in its own R session, so you want it to leave behind an audit trail that describes its actions. A moderately verbose function is good here. You could make this as sophisticated as you like, but I find this works nicely for me:\n\ncancel_verbosely &lt;- function(user, type) {\n\n  # notify user attempt has started\n  msg &lt;- c(\n    \"block\" = \"blocking user id\",\n    \"mute\" = \"muting user id\"\n  )\n  withr::local_options(scipen = 14)\n  cli::cli_process_start(paste(msg[type], user))\n\n  # make the attempt; wait 5 mins if rate limits \n  # exceeded and try again\n  repeat {\n    out &lt;- cancel_safely(user, type)\n    if(rate_exceeded(out)) {\n      Sys.sleep(300)\n    } else {\n      break\n    }\n  }\n\n  # notify user of the outcome\n  if(is.null(out$result)) {\n    cli::cli_process_failed()\n  } else{\n    cli::cli_process_done()\n  }\n}\n\nHere’s what the output looks like when it successfully blocks a user. Not fancy, but it shows one line per account, specifies whether the action was a block or a mute, and makes clear whether the attempt succeeded or failed:\n✓ blocking user id 15xxxx66 ... done\n(where, in the real output the user id for the blocked account is of course not censored). In this function I’ve used the lovely cli package to create pretty messages at the R command line, but there’s nothing stopping you from using simpler tools if you’d prefer."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#scaling-up",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#scaling-up",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Scaling up",
    "text": "Scaling up\nNow that we have a version of our block/mute function that is suitable for use on a larger scale, it’s time to put it into practice. Let’s say I have a list of 50000 users represented as numeric ids and I want to block (or mute) all these accounts. To do this, I’ll need a vectorised version of cancellation function. Thanks to the functional programming tools in the purrr package, this is not too difficult. Here’s the cancel_users() function that I use:\n\ncancel_users &lt;- function(users, type) {\n  msg &lt;- c(\"block\" = \"blocking \", \"mute\" = \"muting \")\n  cat(msg[type], length(users), \" users\\n...\")\n  purrr::walk(users, cancel_verbosely, type = type)\n}\n\nWhen given a vector of user ids, the cancel_users() function will attempt to block/mute them all one at a time. When rate limits are exceeded it will pause and wait for them to reset, and then continue with the process. For mass muting in particular it can take a long time, so it’s the kind of thing you run in its own session while you go do something else with your life. If you want to be clever about it you can make it a background job and sink the output to a log file but honestly I’m usually too lazy to bother with that: all I’m trying to do is sanitise my twitter experience, I’m not deploying production code here.\nThe trickier question is “where do I get this block list from?”\nHere, I’m not going to be too specific, for a couple of reasons. Firstly, I don’t want to be in the business of teaching people how to track down hidden networks of users embedded in social media. That’s not something I’m comfortable doing. Secondly, if you’re doing this defensively (i.e., you’re protecting yourself from attack) then you probably already know something about where the attacks are coming from. You already have your own list of key names, because they’re the people who keep harassing you. Really, your only goal is to block them and their followers, because the thing that’s happening is they’re targeting you and they’re using their follower base as a weapon. Right? I mean if that’s not the situation you’re in, and what you’re actually trying to do is seek out a hidden population to potentially target them… yeah I’m not sure I want to be telling you the other tricks I know. So let’s keep it very simple.\nThe easiest trick in the book (and, honestly, one of the most powerful when you’re trying to block out harassment from a centralised “hub-and-spokes” network), is simply to find every account that follows more than \\(k\\) of the \\(n\\) of the key actors, and block/mute them. Actually, in the case of “astroturf” organisations that don’t have real grassroots support, you can probably just pick a few of the big names and block (or mute) all their followers. That will eliminate the vast majority of the horrible traffic that you’re trying to avoid. (Yes, I am speaking from experience here!)\nThe rtweet package makes this fairly painless courtesy of the get_followers() function. Twitter makes follower lists public whenever the account itself is public, so you can use get_followers() to return a tibble that contains the user ids for all followers of a particular account.2 Here’s an example showing how you an write a wrapper around get_followers() and use it to block/mute everyone who follows a particular account:\n\ncancel_followers &lt;- function(user, type = \"block\", n_max = 50000, precancelled = numeric(0)) {\n\n  followers &lt;- get_followers(user, n = n_max, retryonratelimit = TRUE)\n  followers &lt;- followers$from_id\n\n  uncancelled &lt;- setdiff(followers, precancelled)\n  uncancelled &lt;- sort(as.numeric(uncancelled))\n\n  cancel_users(uncancelled, type = type)\n}\n\nNote the precancelled argument to this function. If you have a vector of numeric ids containing users that you’ve already blocked/muted, there’s no point wasting time and bandwidth trying to block them again, so the function will ignore anything on that list. You could use the same idea to build a whitelist of accounts that would never get blocked or muted regardless of who they follow.\nWe’re almost at the end of the post. There’s only one other thing I want to show here, and that’s how to extract a list of all the accounts you currently have muted or blocked. Again this particular bit of functionality isn’t exposed by rtweet directly, so you’ll notice that I’ve had to go off-label again and call an unexported function!\n\nlist_cancelled &lt;- function(type, n_max, ...) {\n  api &lt;- c(\n    \"block\" = \"/1.1/blocks/ids\",\n    \"mute\" = \"/1.1/mutes/users/ids\"\n  )\n  params &lt;- list(\n    include_entities = \"false\",\n    skip_status = \"true\"\n  )\n  resp &lt;- rtweet:::TWIT_paginate_cursor(NULL, api[type], params, n = n_max, ...)\n  users &lt;- unlist(lapply(resp, function(x) x$ids))\n  return(users)\n}\n\nI’m not going to expand on this one, other than to mention that when you get to the point where you have hundreds of thousands of blocked or muted accounts, it’s handy to use a function like this from time to time, and to save the results locally so that you can be a little more efficient whenever you next need to refresh your block/mute lists."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#epilogue",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#epilogue",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Epilogue",
    "text": "Epilogue\nI wrote this post in two minds. On the one hand, the rtweet developers made a decision not to support blocklists for a reason, and presumably the twitter developers have some reason for making it difficult to mute large numbers of accounts. It’s very rarely a good idea to write code that works against the clear intent of the tools you’re relying on. It is almost certain to break later on.\nOn the other hand, this is functionality that I personally need. On my primary account I’ve made the deliberate decision not to block anyone3 but to keep my twitter feed free of a particular type of content I have had to mute an extremely large number of accounts. Twitter makes that difficult to do, but with the help of these scripts I managed to automate the process. After a month or two, with a little manual intervention, the problematic content was gone from my feed, and I was able to get back to doing my job. So, if anyone else finds themselves in a similar situation, hopefully this blog post will help."
  },
  {
    "objectID": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#footnotes",
    "href": "posts/2022-02-11_r-scripts-for-twitter-blocks/index.html#footnotes",
    "title": "R scripts for twitter mutes and blocks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI mean, what exactly do we mean by “at scale” here? In the context of data wrangling, a hundred thousand anything is rarely considered “at scale”. But mute/blocks on twitter are usually measured in the tens or hundreds at most. Doing things at the hundreds of thousands scale is a big step up from the typical use case, and as noted later, because of how the twitter API handles mutes it’s something that can take months to complete.↩︎\nThis is not unrelated to the reason why I keep my follows hidden behind private lists. In public it looks like I don’t follow anyone but using private lists I actually follow several hundred people! Some time ago I had some unpleasant experiences with people using that information to target me, so now I use private lists exclusively. Being a trans woman on the internet is fuuuuuuuun.↩︎\nThere is one exception to this rule, but that’s a personal matter.↩︎"
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html",
    "href": "posts/2023-04-12_metropolis-hastings/index.html",
    "title": "The Metropolis-Hastings algorithm",
    "section": "",
    "text": "This morning I received an email from a stranger, writing to say thank you for a document I wrote almost 13 years ago. It’s a weird feeling every time I get one of those,1 but a pleasant one. This time around, the document in question was a note on the Metropolis-Hastings algorithm that I threw together in a rush for a computer science class I taught back in 2010.2 While drinking my second coffee of the morning and feeling the usual sense of dread I feel when I know that today I have to put some effort into looking for a job, yet again, I arrived at an excellent procrastination strategy…\nWhy don’t I start rescuing some of the content that I wrote all those years ago and currently have hidden away in pdf files in the dark corners of the internet, and put them up on my blog? Okay sure it won’t get me a job, but it feels less demeaning than yet again trying to get tech company recruiters to believe that yes actually the middle aged lady with a psychology PhD does in fact know how something about statistics and does know how to code.\nAnyway. Without further self-pity, here’s a quick primer on the Metropolis-Hastings algorithm. The target audience for this is someone who has a little bit of probability theory, can write code in R (or similar), but doesn’t have any background in Markov chain Monte Carlo methods. It doesn’t dive deep into the mathematics – i.e., you won’t find any discussions of detailed balance, ergodicity, and other things a statistics class would cover – but it does try to go deep enough to give a beginner an understanding of what the algorithm is doing."
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html#the-problem",
    "href": "posts/2023-04-12_metropolis-hastings/index.html#the-problem",
    "title": "The Metropolis-Hastings algorithm",
    "section": "The problem",
    "text": "The problem\nThe Metropolis-Hastings algorithm is perhaps the most popular example of a Markov chain Monte Carlo (MCMC) method in statistics. The basic problem that it solves is to provide a method for sampling from some arbitrary probability distribution, which I’ll denote \\(p(x)\\).3 The idea is that in many cases, you know how to write out the equation for the probability \\(p(x)\\), but you don’t know how to generate a random number from this distribution, \\(x \\sim p(x)\\). This is the situation where MCMC is handy. For example, suppose I’ve become interested – for reasons known but to the gods – in the probability distribution shown below:\n\n\n\n\n\n\n\n\n\nThe probability density function4 \\(p(x)\\) for this distribution is given by the following equation:\n\\[\np(x) = \\frac{\\exp(-x^2) \\left(2 + \\sin(5x) + \\sin(2x)\\right)}{\\int_{-\\infty}^\\infty \\exp(-u^2) \\left(2 + \\sin(5u) + \\sin(2u)\\right) \\ du}\n\\]\nMy problem is that I either don’t know how to solve the integral in the denominator, or I’m simply too lazy to try.5 So this means in truth, I only know the distribution “up to some unknown constant”. That is, all I really know how to calculate is the numerator. Given this, a more realistic way to express my knowledge about the target distribution is captured by this equation:\n\\[\np(x) \\propto \\exp(-x^2) \\left(2 + \\sin(5x) + \\sin(2x)\\right)\n\\] How can I generate samples from this distribution?"
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html#the-metropolis-hastings-algorithm",
    "href": "posts/2023-04-12_metropolis-hastings/index.html#the-metropolis-hastings-algorithm",
    "title": "The Metropolis-Hastings algorithm",
    "section": "The Metropolis-Hastings algorithm",
    "text": "The Metropolis-Hastings algorithm\nThe basic idea behind MCMC is very simple. The idea is to define a Markov chain6 over possible \\(x\\) values, in such a way that the stationary distribution of the Markov chain is in fact \\(p(x)\\). That is, what we’re going to do is use a Markov chain to generate a sequence of \\(x\\) values, denoted \\((x_0, x_1, x_2, \\ldots, x_n)\\), in such a way that as \\(n \\rightarrow \\infty\\), we can guarantee that \\(x_n \\sim p(x)\\). There are many different ways of setting up a Markov chain that has this property. The Metropolis-Hastings algorithm is one of these.\n\n\nThe proposal step\nHere’s how it works. Suppose that the current state of the Markov chain is \\(x_n\\), and we want to generate \\(x_{n+1}\\). In the Metropolis-Hastings algorithm, the generation of \\(x_{n+1}\\) is a two-stage process.\nThe first stage is to generate a candidate, which we’ll denote \\(x^∗\\). The value of \\(x^∗\\) is generated from the proposal distribution that we already know how to sample from. We denote this proposal distribution \\(q(x^∗ | x_n)\\). Notice that the distribution we sample from depends on the current state of the Markov chain, \\(x_n\\). There are some technical constraints on what you can use as a proposal distribution, but for the most part it can be anything you like.7 A very typical way to do this is to use a normal distribution centered on the current state \\(x_n\\). More formally, we write this as:\n\\[\nx^* | x_n \\sim \\mbox{Normal}(x_n, \\sigma^2)\n\\] for some standard deviation \\(\\sigma\\) that we select in advance (more on this later!)\n\n\n\nThe accept-reject step\nThe second stage is the accept-reject step. Firstly, what you need to do is calculate the acceptance probability, denoted \\(A(x_n \\rightarrow x_∗)\\), which is given by:\n\\[\nA(x_n \\rightarrow x_∗) = \\min \\left(1, \\frac{p(x^*)}{p(x^n)} \\times \\frac{q(x_n | x^*)}{q(x^* | x_n)} \\right)\n\\] There are two things to pay attention to here. Firstly, notice that the ratio \\(p(x^*) / p(x^n)\\) doesn’t depend on the normalising constant for the distribution. Or, to put it in a more helpful way, that integral in the first equation is completely irrelevant and we can ignore it. As a consequence, for our toy problem we can write this:\n\\[\n\\frac{p(x^*)}{p(x^n)} = \\frac{\\exp(-{x^*}^2) \\left(2 + \\sin(5x^*) + \\sin(2x^*)\\right)}{\\exp(-{x_n}^2) \\left(2 + \\sin(5x_n) + \\sin(2x_n)\\right)}\n\\] That’s a nice simple thing to compute with no need for any numerical integration or, gods forbid, solving the integral analytically.\nThe second thing to pay attention to is the behaviour of the other term, \\(q(x_n | x^*) / q(x^* | x_n)\\). What this term does is correct for any biases that the proposal distribution might induce. In this expression, the denominator \\(q(x^∗ | x_n)\\) describes the probability with which you’d choose \\(x^*\\) as the candidate if the current state of the Markov chain is \\(x_n\\). The numerator, however, describes the probability of a transition that goes the other way: that is, if the current state had actually been \\(x^∗\\), what is the probability that you would have generated \\(x^n\\) as the candidate value? If the proposal distribution is symmetric, then these two probabilities will turn out to be equal. For example, if the proposal distribution is normal, then:\n\\[\n\\begin{array}{rcl}\nq(x^* | x_n) & = & \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left(x_n - x^*\\right)^2 \\right) \\\\\nq(x_n | x^*) & = & \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left(x^* - x_n \\right)^2 \\right)\n\\end{array}\n\\]\nClearly, \\(q(x^* | x_n) = q(x_n | x^*)\\) for all choices of \\(x_n\\) and \\(x^*\\), and as a consequence the ratio \\(q(x_n | x^*) / q(x^* | x_n)\\) is always 1 in this case.\nThis special case of the Metropolis-Hastings algorithm, in which the proposal distribution is symmetric, is referred to as the Metropolis algorithm.\nOkay. Having proposed the candidate \\(x^∗\\) and calculated the acceptance probability, \\(A(x_n \\rightarrow x^∗)\\), we now either decide to “accept” the candidate and set \\(x_{n+1} = x^∗\\) or we “reject” the candidate and set \\(x_{n+1} = x_n\\). To make this decision, we generate a uniformly distributed random number between 0 and 1, denoted \\(u\\). Then:\n\\[\nx_{n+1} = \\left\\{\n\\begin{array}{rl}\nx^* & \\mbox{ if } u \\leq A(x_n \\rightarrow x^∗) \\\\\nx_n & \\mbox{ otherwise}\n\\end{array}\n\\right.\n\\] In essence, this is the entirety of the Metropolis-Hastings algorithm! True, there are quite a few technical issues that attach to this, and if you’re interested in using the algorithm for practical purposes I strongly encourage you to do some further reading to make sure you understand the traps in detail, but for now I’ll just give you some examples of things that work and things that don’t, to give you a bit of a feel for how it works in practice."
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html#implementing-the-sampler",
    "href": "posts/2023-04-12_metropolis-hastings/index.html#implementing-the-sampler",
    "title": "The Metropolis-Hastings algorithm",
    "section": "Implementing the sampler",
    "text": "Implementing the sampler\nOkay. That’s enough mathematics. Let’s have a look at some R code implementing the Metropolis-Hastings algorithm for the toy problem: can we write an R function that draws samples from the weird looking probability distribution I plotted at the start of this post?\nTo do this, I’ll define the sampler in terms of three functions. First, we have a target() function that calculates the probability of a given sample x, or – more precisely – calculates the numerator term in the equation describing the probability density:\n\ntarget &lt;- function(x) {\n  exp(-x^2) * (2 + sin(5 * x) + sin(2 * x))\n}\n\nEstoy emocionada. 😐\nNext we define a metropolis_step() function that takes some value x corresponding to the current state of the Markov chain, and a parameter sigma that describes the standard deviation of the proposal distribution:\n\nmetropolis_step &lt;- function(x, sigma) {\n  proposed_x &lt;- rnorm(1, mean = x, sd = sigma)\n  accept_prob &lt;- min(1, target(proposed_x) / target(x))\n  u &lt;- runif(1)\n  if(u &lt;= accept_prob) {\n    value &lt;- proposed_x\n    accepted &lt;- TRUE\n  } else {\n    value &lt;- x\n    accepted &lt;- FALSE\n  }\n  out &lt;- data.frame(value = value, accepted = accepted)\n  out\n}\n\nFinally we can write a metropolis_sampler() function that runs the Metropolis algorithm for some number of steps:\n\nmetropolis_sampler &lt;- function(initial_value, \n                               n = 1000, \n                               sigma = 1, \n                               burnin = 0, \n                               lag = 1) {\n  \n  results &lt;- list()\n  current_state &lt;- initial_value\n  for(i in 1:burnin) {\n    out &lt;- metropolis_step(current_state, sigma)\n    current_state &lt;- out$value\n  }\n  for(i in 1:n) {\n    for(j in 1:lag) {\n      out &lt;- metropolis_step(current_state, sigma)\n      current_state &lt;- out$value\n    }\n    results[[i]] &lt;- out\n  }\n  results &lt;- do.call(rbind, results)\n  results\n}\n\nWe have several arguments here. Three of them follow intuitively from the mathematical description I gave in the last section:\n\ninitial_value is the start point for the Markov chain\nn is the number of samples we want to draw from the target distribution\nsigma is the standard deviation of the normal distribution we use to propose candidate values at each step\n\nThe other two are:\n\nburnin is the number of iterations we run the sampler for before recording results8\nlag is the number of iterations we run the sampler for between successive samples\n\nI’ll talk more about these input arguments later, but for now let’s take a look at what happens when we call our metropolis_sampler() function:\n\nset.seed(1234)\nout &lt;- metropolis_sampler(initial_value = 0)\nout[1:10, ]\n\n        value accepted\n1   0.3143686    FALSE\n2   0.3500985     TRUE\n3   0.3500985    FALSE\n4   0.3500985    FALSE\n5   0.3500985    FALSE\n6  -0.1665712     TRUE\n7  -0.9428251     TRUE\n8   0.4271852     TRUE\n9   0.3168997     TRUE\n10  0.3352164     TRUE\n\n\nAs you can see it returns a data frame with n rows and two columns. The first column records the numeric value that was sampled, and accepted is a logical variable indicating whether this value arose from accepting the proposal (rather than rejecting it).\n\n\nThe effect of proposal width\nIn practice when you’re implementing a Metropolis algorithm, the choice of proposal distribution matters a lot. If it’s too narrow, your sampler will have an extremely high acceptance rate on average, but it will move around extremely slowly. To use the technical term, it has a very low mixing rate. This distorts your estimate of the target distribution. However, if it’s too wide, the acceptance rate becomes too low and the chain gets stuck on specific values for long periods of time. This also distorts your estimate of the target distribution. Yes, technically, if you run your accursed sampler long enough despite it’s poor choice of proposal distribution it will eventually produce the right answer… but wouldn’t you prefer a sampler that gives you the right answer quickly rather than slowly?\nThis idea is illustrated in the plots below, in which I’ve run a Metropolis sampler with n = 10000 on our toy problem three times. On the left, I’ve made a good choice of proposal distribution, setting sigma = 1. In the middle, my proposal distribution is too narrow (I set sigma = .025 and it doesn’t work). On the right, my proposal distribution is too wide: it turns out that sigma = 50 isn’t a good idea for this problem either:\n\n\nSource code for plots\nset.seed(1234)\npar(mar = c(4, 4, 2, 2))\nlayout(matrix(1:6, nrow = 2, ncol = 3))\nsigma &lt;- c(1, 0.025, 50)\nn &lt;- 10000\nfor(i in 1:3) {\n  out &lt;- metropolis_sampler(\n    initial_value = -1, \n    n = n, \n    sigma = sigma[i]\n  )\n  hist(\n    out$value, \n    breaks = seq(-3, 3, .05),\n    main = paste(\"Sample values: sigma =\", sigma[i]),\n    xlab = \"Value, x\",\n    ylab = \"Frequency\"\n  )\n  x &lt;- seq(-3, 3, .05)\n  p &lt;- target(seq(-3, 3, .05))\n  expected &lt;- n * p / sum(p) \n  lines(\n    x = x, \n    y = expected, \n    type = \"l\", \n    col = \"red\",\n    lwd = 2\n  )\n  plot(\n    x = out$value, \n    y = 1:n, \n    type = \"l\", \n    xlim = c(-3, 3),\n    xlab = \"Value, x\",\n    ylab = \"Time\"\n  )\n}\n\n\n\n\n\n\n\n\n\nThe way to read these plots is as follows: for all three values of sigma, we have two plots. The top one shows a histogram of the samples values obtained using the Metropolis sampler (that’s the black bars). Superimposed on this is a red line showing the distribution of values you’d expect to obtain when sampling from the true distribution. The lower panel plots the Markov chain itself: the sequence of generated values.9\nIn the leftmost plots, we see what happens when we choose a good proposal distribution: the chain shown in the lower panel moves rapidly across the whole distribution, without getting stuck in any one place. In the far right panel, we see what happens when the proposal distribution is too wide: the chain gets stuck in one spot for long periods of time. It does manage to make big jumps, covering the whole range, but because the acceptance rate is so low that the distribution of samples is highly irregular. Finally, in the middle panel, if we set the proposal distribution to be too narrow, the acceptance rate is very high so the chain doesn’t get stuck in any one spot, but it doesn’t cover a very wide range. This simple example should give you an intuition for why you need to “play around” with the choice of proposal distribution. A good proposal distribution can make a huge difference!10\n\n\n\nThe role of the burn-in period\nUp to this point I haven’t really explained what the burnin and lag parameters are there for. To be honest I don’t plan to go into details, but here’s the basic idea. First, let’s think about the burn-in issue. Suppose you started the sampler at a very bad location… say initial_value = -3, and – just so that we can exaggerate the problem – we’ll use a proposal distribution that is too narrow, say sigma = .1. The image below shows three runs of this sampler:\n\n\nSource code for plots\nset.seed(1234)\npar(mar = c(4, 4, 2, 2))\nlayout(matrix(1:6, nrow = 2, ncol = 3))\nn &lt;- 1000\nfor(i in 1:3) {\n  out &lt;- metropolis_sampler(\n    initial_value = -3, \n    n = n, \n    sigma = .1\n  )\n  hist(\n    out$value, \n    breaks = seq(-4, 4, .05),\n    main = NULL,\n    xlab = \"Value, x\",\n    ylab = \"Frequency\"\n  )\n  x &lt;- seq(-4, 4, .05)\n  p &lt;- target(seq(-4, 4, .05))\n  expected &lt;- n * p / sum(p) \n  lines(\n    x = x, \n    y = expected, \n    type = \"l\", \n    col = \"red\",\n    lwd = 2\n  )\n  plot(\n    x = out$value, \n    y = 1:n, \n    type = \"l\", \n    xlim = c(-4, 4),\n    xlab = \"Value, x\",\n    ylab = \"Time\"\n  )\n  abline(h = 200, lty = \"dotted\")\n}\n\n\n\n\n\n\n\n\n\nAs you can see, the sampler spends the first 200 or so iterations slowly moving rightwards towards the main body of the distribution. Once it gets there, the samples start to look okay, but notice that the histograms are biased towards the left (i.e., towards the bad start location). A simple way to fix this problem is to let the algorithm run for a while before starting to collect actual samples. The length of time that you spend doing this is called the burn in period.\nTo illustrate how it helps, the figure below shows what would have happened if I’d used a burn in period of 200 iterations for the same sampler:\n\n\nSource code for plots\nset.seed(1234)\npar(mar = c(4, 4, 2, 2))\nlayout(matrix(1:6, nrow = 2, ncol = 3))\nn &lt;- 1000\nfor(i in 1:3) {\n  out &lt;- metropolis_sampler(\n    initial_value = -3, \n    burnin = 200,\n    n = n, \n    sigma = .1\n  )\n  hist(\n    out$value, \n    breaks = seq(-4, 4, .05),\n    main = NULL,\n    xlab = \"Value, x\",\n    ylab = \"Frequency\"\n  )\n  x &lt;- seq(-4, 4, .05)\n  p &lt;- target(seq(-4, 4, .05))\n  expected &lt;- n * p / sum(p) \n  lines(\n    x = x, \n    y = expected, \n    type = \"l\", \n    col = \"red\",\n    lwd = 2\n  )\n  plot(\n    x = out$value, \n    y = 1:n, \n    type = \"l\", \n    xlim = c(-4, 4),\n    xlab = \"Value, x\",\n    ylab = \"Time\"\n  )\n}\n\n\n\n\n\n\n\n\n\nIt’s still not ideal – largely because we don’t have many samples, we haven’t set a lag, and the value of sigma isn’t very well chosen – but you can see that the bias caused by the poor choice of starting value has disappeared.\n\n\n\nThe role of the lag parameter\nFinally, I’ll mention in passing the role played by the lag parameter.11 In some situations you can be forced into using a proposal distribution that has a very low acceptance rate. When that happens, you’re left with an awkward Markov chain that gets stuck in one location for long periods of time. One thing that people often do in that situation is allow several iterations of the sampler to elapse in between successive samples. This is the lag between samples. The effect of this is illustrated in below, which shows the behaviour of a sampler with a very wide proposal distribution (sigma = 50) with n = 1000 samples drawn, at lag = 0 (left), lag = 10 (middle) and lag = 100 (right).\n\n\nSource code for plots\nset.seed(1234)\npar(mar = c(4, 4, 2, 2))\nlayout(matrix(1:6, nrow = 2, ncol = 3))\nn &lt;- 1000\nlag &lt;- c(0, 10, 100)\nfor(i in 1:3) {\n  out &lt;- metropolis_sampler(\n    initial_value = 0,\n    lag = lag[i],\n    n = n, \n    sigma = 50\n  )\n  hist(\n    out$value, \n    breaks = seq(-4, 4, .05),\n    main = paste(\"Lag:\", lag[i]),\n    xlab = \"Value, x\",\n    ylab = \"Frequency\"\n  )\n  x &lt;- seq(-4, 4, .05)\n  p &lt;- target(seq(-4, 4, .05))\n  expected &lt;- n * p / sum(p) \n  lines(\n    x = x, \n    y = expected, \n    type = \"l\", \n    col = \"red\",\n    lwd = 2\n  )\n  plot(\n    x = out$value, \n    y = 1:n, \n    type = \"l\", \n    xlim = c(-4, 4),\n    xlab = \"Value, x\",\n    ylab = \"Time\"\n  )\n}\n\n\n\n\n\n\n\n\n\nFormally speaking, the thing we’re trying to do here by increasing lag is reduce the autocorrelation between successive samples in our chain. In an ideal world we want our sampled values to be independent samples from the target distribution \\(p(x)\\). The more our samples are correlated with each other, the more potential there is for the histogram of sampled values to depart systematically from the target distribution. Introducing a lag between successive samples is a simple way to achieve this."
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html#a-word-of-warning",
    "href": "posts/2023-04-12_metropolis-hastings/index.html#a-word-of-warning",
    "title": "The Metropolis-Hastings algorithm",
    "section": "A word of warning",
    "text": "A word of warning\nThe discussion in this post is heavily oversimplified. It doesn’t talk at all about the conditions required to make Metropolis sampling work, it doesn’t talk about diagnostics, and it certainly doesn’t talk about what happens when we move this into higher dimensional problems. I haven’t made any attempt to implement this efficiently, either: there’s little point in me doing that when there are already many very efficient tools already in existence. My goal when I wrote this was so that my class could have an example that was simple enough that they could implement it themselves and get a sense of what MCMC algorithms do. At one point I had this idea that I’d gradually expand on these materials so that students could work their way up from introductory materials like this to more realistic examples, but I never really found the time to do that. Maybe one day!"
  },
  {
    "objectID": "posts/2023-04-12_metropolis-hastings/index.html#footnotes",
    "href": "posts/2023-04-12_metropolis-hastings/index.html#footnotes",
    "title": "The Metropolis-Hastings algorithm",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBizarrely, this actually happens to me a lot. It’s totally surreal.↩︎\nAnother surreal experience that I’ve had quite a bit lately is getting rejected from data science jobs because I don’t have a computer science degree and my qualifications are technically in psychology. Apparently I’m considered skilled enough to teach computational statistics to university computer science students, but still considered less skilled at those tools than the students that I taught? I mean, it’s either that or tech company recruiters don’t actually read the résumés that they get sent… but that couldn’t possibly be true, right? Efficient market hypothesis and all that…↩︎\nOkay. So, I just know that a serious statistician will read this at some point, and – from bitter experience – I’ve learned that when a lady person writes about mathematics and is unclear on even the smallest thing her credibility is immediately subject to question. So, in an attempt to forestall any misapprehensions, I most certainly understand the difference between a density function and a distribution function, and equally I know that it is conventional to use lower case to refer to densities and upper case to refer to distributions and probability mass functions. However, I wrote this note for undergraduate computer science students, and accordingly I chose to keep my notation simple, even if it is a little imprecise.↩︎\nSome asides: my experience teaching this class is that it’s quite common for people new to statistics to struggle with the concept of probability density. It’s not super important for the purposes of this post, and to a first approximation it’s totally okay to think of \\(p(x)\\) as “the probability of observing \\(x\\) (sort of)”. It is of course the case that the difference between a pdf, a pmf, and a cdf matters when you start doing mathematical statistics, but I would never be so cruel as to inflict that stuff on my undergraduate computer science students! Not once did refer to sigma algebras in my undergrad teaching, honest. Though, looking back, I did once inflict Kolmogorov complexity on a class. It was… not a good decision.↩︎\nIt’s worth mentioning that, at this point in the class my students had all been shown several examples of the curse of dimensionality problem. So even though yes, of course it’s easy to solve this problem in one-dimension, they were all acutely aware that using brute force numerical integration methods to compute the denominator isn’t going to work we started trying to do something more realistic than my toy example.↩︎\nAt this point in the class students had most certainly encountered Markov chains!↩︎\nThat said, it’s worth mentioning one important property of the proposal distribution: the sampler won’t work if there are some values of \\(x\\) that never get proposed!↩︎\nThe “burn in” period also goes by the name “warm up” period.↩︎\nThis is usually referred to as a “trace plot”. Eyeballing the trace plot is a useful thing to do when examining the behaviour of your MCMC algorithm, but beware… it is a very crude method. The devil is very much in the details here.↩︎\nAt this point she asks herself… does she want to talk about methods for automatically choosing a good proposal? No. No she does not.↩︎\nThis is often referred to as the “thinning” rate. ↩︎"
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html",
    "href": "posts/2021-07-08_generative-art-in-r/index.html",
    "title": "Generative art in R",
    "section": "",
    "text": "A little while ago I was invited by Sara Mortara to contribute art as part of an exhibit to be presented at the 2021 useR! conference, along with several artists who I admire greatly. I could hardly say no to that, now could I? So I sent some pieces that I’m fond of, most of which are posted somewhere on my art website. I realised later though that I was going to have to talk a little about my art too, and Sara suggested an informal Q&A during the timeslot allocated to the exhibit. Naturally, I agreed since that meant I didn’t have to prepare anything formal, and like all artists I am extremely lazy. Later though, it occurred to me that it actually wouldn’t be terrible if I wrote a blog post to accompany my contribution to the exhibit, loosely based on the questions Sara suggested. And so here we are…\nWhen did you start using R for art? Do you remember your first piece?\nI started making art in R some time in late 2019. I’d discovered some of the art that Thomas Lin Pedersen had been making – at the time he was posting pieces from his Genesis series – and at the same time I found the ambient package that he was using to create the pieces. Thomas famously does not post source code for his art, and being stubborn and curious I wanted to work out how he was doing it, so I started playing with ambient to see if I could reverse engineer his system. My very first piece was Constellations, shown below. It’s certainly not the prettiest thing I’ve created, and there are a lot of things I’d like to change about it now, but it’s nice to have your early work lying around to see how you’ve changed since then:\nIf you follow the link above and look at Thomas’ Genesis pieces you can tell that it’s not even remotely close to the mark, but I did eventually get the hang of it and managed to produce a few pieces like Rainbow Prisms which are closer to the kind of work he was producing:\nIt’s still not quite the same as Thomas’ in style, but by the time I’d worked out how to produce these I decided it was time to change my approach and branch out a bit. I love Thomas’ work of course, but I didn’t want my art to be just a low quality imitation of his! And besides, by that point I’d started discovering a whole lot of other people making generative art in R, such as Will Chase, Antonio Sánchez Chinchón, Marcus Volz, and (somewhat later) Ijeamaka Anyene. Each has their own style and – following the famous advice that art is theft – have shamelessly taken ideas and inspiration from each at different times.\nSome of those early pieces are still around, as part of the Rosemary gallery.\nWere you an artist before making generative art in R?\nNot really. I always wanted to do more artistic and creative things, but the only thing I’d ever done that required any kind of mix of aesthetic sensibility and craftwork was gardening. I used to have a lovely garden in Adelaide with a mix of Mediterranean and Australian native plants, and I had the same kind of enthusiasm for gardening then as I do for art now. Maybe one day I’ll garden again but there’s no space for that in my Sydney apartment!\nCan you talk about your creative process? Do you begin from code or from the outcome you are looking for? Do you start with the color palette in mind, or is it an iterative process?\nI’m honestly not sure I have a consistent process? I spend a lot of time browsing artwork by other people on twitter and instagram, and from time to time I read posts about the techniques that they use. Whenever I do this I end up thinking a bit about how I might use this technique or wondering what methods other artists use to create their work, but I don’t usually act on that information until I think of something I want to do with it. That kind of technical or stylistic information is like background knowledge that lies dormant until I need it.\nMost of the time the starting point for my art is an emotion. I might be angry or lonely or tired, or just in need of something to occupy my mind and distract me from something else. When I start implementing a new system it’s often (though not always) a modification of a previous one. In principle this modification process could go in any direction, but my aesthetic sensibilities depend a lot on my state of mind, and that imposes a bias. I tweak the code one way, and see what it produces. If I like it, I keep the change, if I don’t I reject it. It’s a lot like a Metropolis-Hastings sampler that way, but my mood strongly shapes the accept/reject decision, so the same starting point can lead to different outcomes. As a concrete example, the Pollen, Bursts and Embers series are all based on the same underlying engine, the fractal flame algorithm created by Scott Draves, but my emotional state was very different at the time I coded each version. For example, the Pollen Cloud piece I contributed to the useR exhibit is soft and gentle largely because I was feeling peaceful and relaxed at the time:\nBy way of contrast, the Geometry in a Hurricane piece from Bursts is layered in jagged textures with a chaotic energy because I was angry at the time I was coding:\nThe Soft Ember piece below (also included in the exhibit) has a different feel again. There’s more energy to it than the pollen pieces, but it’s not as chaotic as the bursts series. Again, that’s very much a reflection of my mood. I wasn’t angry when I coded this system, but I wasn’t relaxed either. At the time, something exciting had happened in my life that I wasn’t quite able to do anything about, but I was indulging in the anticipation of a new thing, and some of that emotion ended up showing through in the pieces that I made at the time:\nTo bring all this back to the question, it’s very much an iterative process. The driver behind the process is usually an emotion, and the colour choices, the shapes, and the code are all adapted on the fly to meet with how I’m feeling.\nWhat is your inspiration?\nTo the extent that my art is driven by emotion, the inspiration for it tends to be tied to sources of strong emotion in my life. Sometimes that emotion comes from the sources of love and joy: family, intimate partners, and so on. The Heartbleed series is one of those. The background texture to these images is generated by simulating a simple Turing machine known as a turmite and the swirly hearts in the foreground are generated using the toolkit provided by the ambient package. This system is very much motivated from emotional responses to the loved ones in my life. One of the pieces in the exhibit is from this series:\nOther times the emotional motivation comes from sources of pain - sometimes things that were physically painful, sometimes that were psychologically painful. The Orchid Thorn piece I included in the exhibit is one of those, linked to an intense physically painful experience.\nThe Bitterness piece below, which I haven’t done much with other than post to my instagram, is strongly tied to the psychological stresses associated with my gender transition. Yes, there’s a softness to the piece, but there’s also a sandpaper-like texture there that makes me think of abrasion. The colour shifts make me think about transitions, but the roughness at some of the boundaries reminds me that change is often painful.\nOne odd property of the art, at least from my point of view, is that looking at a given piece recalls to mind the events and emotions that inspired the work, and to some extent that recollection becomes a way of re-experiencing the events. Sometimes that’s a good thing. Not always though.\nWhat is your advice for people who wants to create art in R?\nI think I’d suggest three things. Find artists you like, read about their processes. Sometimes they’ll show source code or link to algorithms like I’ve done in a few places in this piece, and it can be really valuable to try to retrace their steps. There’s nothing wrong with learning technique by initially copying other artists and then developing your own style as you go.\nThe second thing I’d suggest, for R folks specifically, is to take advantage of the skills you already have. Most of us have skills in simulation, data wrangling, and data visualisation, and those skills can be repurposed for artistic work quite easily. A lot of my pieces are created using that specific combination. I’ll often define a stochastic process and sample data from it using tools in base R, use dplyr to transform and manipulate it, then use ggplot2 to map the data structure onto a visualisation. One of the nice things about dplyr and ggplot2 being compositional grammars is the fact that you can “reuse” their parts for different purposes. I get a lot of artistic mileage out of geom_point() and geom_polygon(), and quite frankly purrr is an absolute godsend when the generative process you’re working with is iterative in nature.\nThe other thing would be try not to put pressure on yourself to be good at it immediately. I wasn’t, and I don’t think anyone else was either. Earlier I showed the Constellations piece and referred to it as the first piece I created. In a way that’s true, because it was the first time I reached a level that I felt comfortable showing to other people. But I made a lot of junk before that, and I made a lot of junk after that. I make some good art now (or so people tell me) precisely because I made a lot of bad art before. Even now, though, I can’t tell which systems will end up good and which will end up bad. It’s a bit of a lottery, and I’m trying my best not to worry too much about how the lottery works. I like to have fun playing with visual tools, and sometimes the tinkering takes me interesting places.\nAnything to add about your pieces in the exhibit?\nNot a lot. Several of the pieces I’ve contributed are already linked above, but I might just say a little about the other pieces and how they were made. The Silhouette in Teal piece uses the flametree generative art package to create the tree shown in silhouette in the foreground, and a simple random walk to generate the texture in the background:\nIt has also been surprisingly popular on my Society6 store, which you can visit if you want some of my art on random objects. I am not sure why, but I have sold a lot more shower curtains and yoga mats than I would have expected to sell in my lifetime.\nLeviathan emerged from my first attempt to create simulated watercolours in R using this guide written by Tyler Hobbs. I was in a dark mood at the time and the ominous mood to the piece seems quite fitting to me.\nThe Floral Effect piece is an odd one. It’s part of the Viewports series that I created by applying Thomas Lin Pedersen’s ggfx package over the top of the output of the same system I used to create the Native Flora series, which in turn is an application of the flametree system I mentioned earlier. I quite like it when these systems build on top of one another.\nThe last piece I included, Fire and Ice, is a little different from the others in that it’s not a “pure” generative system. It works by reading an image file into R, using Chris Marcum’s halftoner package to convert it to a halftone image, and then manipulate that image using the tools provided in the ambient package. The end result is something that still resembles the original image but has more of a painted feel:"
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html#last-updated",
    "href": "posts/2021-07-08_generative-art-in-r/index.html#last-updated",
    "title": "Generative art in R",
    "section": "Last updated",
    "text": "Last updated\n\n2023-05-27 18:10:41.906728"
  },
  {
    "objectID": "posts/2021-07-08_generative-art-in-r/index.html#details",
    "href": "posts/2021-07-08_generative-art-in-r/index.html#details",
    "title": "Generative art in R",
    "section": "Details",
    "text": "Details\n\nsource code, R environment"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html",
    "href": "posts/2021-08-08_git-credential-helpers/index.html",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "",
    "text": "There are days when I regret switching to linux as an R user. It’s not that I’m particularly enamoured of Apple or Microsoft, and I do enjoy the freedom to tinker that linux systems provide, but without the same resourcing that underpins Windows or Mac OS, I do spent a disproportionate amount my time trying to make my long-suffering Ubuntu laptop do something that would “just work” if I’d gone with one of the more traditional options. But such is life, and besides, there’s a case to be made that the time I spend on these things is not wasted: usually, I end up learning something useful.\nThis is one of those stories."
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#the-story-is-quite-short",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#the-story-is-quite-short",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "The story is quite short…",
    "text": "The story is quite short…\n\nUsing GitHub credentials with R\nFor some years now I have been using git repositories for version control, with some ambivalence to my feelings. I absolutely love version control, and I think GitHub is a fabulous tool, but git itself gives me headaches. It feels counterintuitive and untidy, and I am resistant to learning new git tricks because of that. However, now that GitHub is moving to end password authentication for git operations, I find myself needing to do precisely that. Sigh.\nLike many R users, whenever I encounter a git problem my first impulse is to see whether Happy Git and GitHub for the useR (Bryan 2018) can help me out, and true to form, it can. Having decided that I will revert to being an https girl, renouncing my flirtation with ssh, I’ve found the chapter on caching https credentials extremely useful. The usethis article on git credentials is also worth the read.\nThe problem can be broken into three parts:\n\nHow do I set up an authentication token on my GitHub account?\nHow do I configure my git installation to use the authentication token?\nHow do I ensure that R detects these credentials?\n\nThanks to the fabulous work of the tidyverse team, it’s possible for R users to solve the problem in a fairly painless way. The solution has been documented repeatedly, but for the sake of completeness I’ll repeat the advice here.\n\n\nSetting up the credentials\nThe first thing you’ll need to do is set up a GitHub token. You can do this on the GitHub website, but for an R user it’s probably easiest to use the usethis package (Wickham and Bryan 2021):\n\nusethis::create_github_token()\n\nThis will open GitHub in a browser window, take you to the “create a new token page”, and pre-populate all the fields with sensible default values. After accepting these values, the token is created and you’ll be given a PAT, a “personal authentication token”. It’ll look something like this…\nghp_dgdfasdklfjsdklfjsadfDKFJASDLKFJ3453\n…and you should immediately save this in a secure password manager, like 1password, lastpass, etc, because GitHub will only show it to you this one time. You did save it to your password manager, right? Right? I mean, you might need it again. You really might. Yes, you. All right then. I’ll trust you’ve taken sensible precautions now, so let’s keep going. The next step in the process is to configure your git installation to use your token. This is, once again, quite easy to do with gitcreds (Csárdi 2020):\n\ngitcreds::gitcreds_set()\n\nWhen you call this function interactively, R will ask for your PAT. Paste it into the console, hit enter, and you are done. Your git installation is now configured to use the token. Yay! Let’s move onto the third step, which is to ensure that R will recognise and use these credentials. As it turns out, step three doesn’t require you to do anything, because it happens automatically! Functions like usethis::pr_push() recognise your credentials as soon as gitcreds sets them up, and everything works perfectly…\n\n\n Quinn. (Figure from giphy.com)"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#unless-youre-on-linux",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#unless-youre-on-linux",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "… unless you’re on linux",
    "text": "… unless you’re on linux\nIf you’re on linux, you might find yourself in the same boat I was. The credentials you just set up work flawlessly for about 15 minutes, at which time R complains that it cannot find any credentials and you spend the next 15 minutes crying melodramatically.\nWhen this happened to me I assumed the problem was my R environment. I tried updating gitcreds, usethis, and every other R package I could think of that might possibly be involved in communicating with git. Nothing worked. The reason nothing worked is that the problem wasn’t with R at all… it was git, and in hindsight I realise that the problem is specific to git on linux. All those beautiful people with their fancy Windows and Mac machines won’t run into the problem I encountered. They won’t spend an entire Saturday trying to teach themselves git credential management. They will never know my pain. Curse them and their superior purchasing decisions.\n\n\n Daria. (Figure from giphy.com)\nJust kidding. I love my quirky little Ubuntu box and I have a lot of fun learning how to fix her up every time she sets herself on fire.\n\nWhere did I leave my config?\nOkay, I’m going to need to make changes to my git configuration. Although git makes it possible to store configuration locally, at the repository level, I rarely need this flexibility. The relevant information is stored in the global configuration file: on my machine, this is located at /home/danielle/.gitconfig. I can use git config to list these configuration settings, like this\n\ngit config --global --list\n\nand at the start of this exercise the output would have looked like this:\nuser.name=Danielle Navarro\nuser.email=d.navarro@unsw.edu.au\nI’m not sure why this is, but I always feel slightly more reassured when I’m able to inspect the configuration file itself. Opening my .gitconfig file shows the same information, but the formatting is slightly different in the raw file:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\nTo solve the git credential problem, we’re going to need to edit this configuration information. Depending on which solution you go with, you might need to install new software too.\n\n\nDon’t forget to update git\nBefore starting, it’s a good idea to make sure you have the latest version of git: older versions may not have the tools you need. As it happens, I had already updated git to the most recent version (2.32.0 at the time of writing), but in case anyone ends up relying on this post, here’s how you do it:\nsudo add-apt-repository ppa:git-core/ppa\nsudo apt update\nsudo apt install git"
  },
  {
    "objectID": "posts/2021-08-08_git-credential-helpers/index.html#three-solutions",
    "href": "posts/2021-08-08_git-credential-helpers/index.html#three-solutions",
    "title": "Managing GitHub credentials from R, difficulty level linux",
    "section": "Three solutions",
    "text": "Three solutions\n\n1. Set a long timeout for the git cache\nRecent versions of git are released with a credential cache that retains your credentials in memory temporarily. The information is never written to disk, and it expires after a time. You can tell git to use this cache as your “credential helper” by typing the following command at the terminal:\n\ngit config --global credential.helper cache\n\nAfter doing this, my .gitconfig file now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache\nUnfortunately this isn’t an ideal solution, because the cache expires after 900 seconds (15 minutes). As soon as the cache expires, git loses track of your GitHub credentials and so does R. So you have to set the credentials again by calling gitcreds::gitcreds_set() and entering the PAT again. That’s annoying, but you did store the PAT in a password manager right? You were smart. You definitely aren’t going to be foolish like me, forget to store your PAT every time, and end up needing to create a new GitHub token every 15 minutes.\nA simple solution to this problem is to ask git to store information in the cache for just a teeny tiny little bit longer. Instead of having the cache expire after the default 900 seconds, maybe set it to expire after 10 million seconds. That way, you’ll only have to refresh the cache using gitcreds::gitcreds_set() once every four months instead of four times an hour. Implementing this solution requires only one line of code at the terminal:\n\ngit config --global credential.helper 'cache --timeout=10000000'\n\nAfter typing this, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = cache --timeout=10000000\nIn some ways this is a bit of a hack. If cache expiry normally happens every 15 minutes, there’s something a little odd about dragging it out and making it hang around for 16 weeks. That being said, I’ve done many stranger things than this in my life. It may not be the most elegant way to solve the problem, but it works.\n\n\n Trent. (Figure from giphy.com)\n\n\n2. Use libsecret credential manager\nIt puzzled me slightly that this problem only exists for linux computers, so I did a little more reading on how git manages credentials. It turns out you don’t have to rely on the in-memory cache: you can tell git to use some other program to supply the credentials. This is what all those swanky Mac and Windows people have been doing all along. On Macs, for example, git defaults to using the OS X keychain to store credentials safely on disk. It’s possible to do the same thing on linux using libsecret (source on gitlab) and thankfully it’s not much harder to set this up than to use the “long cache” trick described in the previous section.\nThe first step is ensuring libsecret is installed on your machine. It probably is (or at least, it was on my Ubuntu 20.04 box), but in case it isn’t here’s the command you need\n\nsudo apt install libsecret-1-0 libsecret-1-dev\n\nIt helps to realise that libsecret isn’t an application designed to work with git (i.e., it’s not the credential manager), nor is it the keyring where the passwords are stored. Rather, it’s a library that communicates with the keyring: I found this post useful for making sense of it. So if we want to use libsecret to access the keyring, we’re going to need a git credential manager that knows how to talk to libsecret. As it turns out, git comes with one already, you just have to build it using make:\n\ncd /usr/share/doc/git/contrib/credential/libsecret\nsudo make\n\nThis will build the git-credential-libsecret application for you and now all you have to do is tell git to use this as the “credential helper” application that supplies the GitHub credentials:\n\ngit config --global credential.helper \\\n  /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n\nAfter typing that, my .gitconfig file looks like this…\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = /usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\n… and I’m all set and ready to go.\nOne thing I found handy during this step is to check that R was reading the correct configuration information. It’s possible to do this with gitcreds:\n\ngitcreds::gitcreds_list_helpers()\n\n\n\n[1] \"/usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\"\n\n\nIn any case, if all the applications are talking to each other properly, the next time you call gitcreds::gitcreds_set() they’ll all send the message along: R will pass your PAT to git, git will pass it to git-credential-libsecret, git-credential-libsecret will pass it to libsecret, and the PAT will end up in your linux keychain. Whenever you need to authenticate and push some commits up to GitHub from R, it should find the credentials using the same communication channel. Everything should work swimmingly.\n\n\n Quinn et al. (Figure from giphy.com)\n\n\n3. Use GCM core\nAs far as I can tell, the libsecret credential manager is a perfectly good solution to the problem, but in the end I made a different choice: I decided to go with “git credential manager core”, or GCM Core. It’s developed by Microsoft and, perhaps unsurprisingly, it is what GitHub currently recommends. It’s slightly more painful to set up, and the installation instructions are different depending on what flavour of linux you’re running. Because I’m on Ubuntu 20.04, I downloaded the .deb file associated with the most recent release of GCM core, and then installed the application using the dpkg command:\n\nsudo dpkg -i &lt;path-to-deb-file&gt;\n\nThis will build GCM core on your system, and once that’s done you can ask it to take care of the git configuration for you:\n\ngit-credential-manager-core configure\n\nThis will edit the .gitconfig file, so for me it now looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\nIn a happier world you would be done at this point, but we don’t live in a happy world. We live in a sick sad world that has global pandemics and pineapple on pizzas. So there’s still one job left to do.\nMuch like the libsecret credential manager I built in the previous section, GCM core is “just” a git credential manager: it communicates with git, but it isn’t a password manager or a keyring, and it doesn’t store the PAT itself. Instead, it offers you several different options for how the PAT is to be stored. If you click through and take a look at the list, the first suggested option is to connect to a secret service API. As far as I can tell “secret service” isn’t an application, it’s a specification, and in practice it’s just a fancy way of referring to a linux keychain. Just as the libsecret credential manager needs some way of communicating with the keychain (i.e., the libsecret library itself), GCM core needs an intermediary. In fact, it turns out GCM core also uses libsecret to talk to the keychain. So that’s the option I went with. The terminal command to set this up is this:\n\ngit config --global credential.credentialStore secretservice\n\nAfter running the command, my .gitconfig file looks like this:\n[user]\n    name = Danielle Navarro\n    email = d.navarro@unsw.edu.au\n[credential]\n    helper = \n    helper = /usr/bin/git-credential-manager-core\n    credentialStore = secretservice\n[credential \"https://dev.azure.com\"]\n    useHttpPath = true\n\n\n Jane. (Figure from giphy.com)\nAs before, I can check that R is reading the correct configuration information…\n\ngitcreds::gitcreds_list_helpers()\n\n[1] \"/usr/share/doc/git/contrib/credential/libsecret/git-credential-libsecret\"\n\n\n…and now I’m ready to go. My problems are solved. The sun is shining, the birds are singing, and git is working properly from R again. All is well in heaven and earth. Oh the sheer excitement of it all. I hope I can contain my boundless enthusiasm and joy.\n\n\n\n\n\n\n Daria. (Figure from giphy.com)"
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html",
    "href": "posts/2023-12-27_seedcatcher/index.html",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "",
    "text": "Ah fuck it. So. Earlier this morning1 I posted on mastodon about the sense of sadness I have about the death of turn-of-the-century-yes-this-century blog culture:\nOnce upon a much happier time, we had a blogging culture where writing a blog post didn’t have to be “A Very Serious Blog Post By A Very Serious Person”. The craft of blogging wasn’t built around the idea that blog posts are miniature journal articles. Back then it was understood that a blog post was an inherently ephemeral and rarely serious thing. You’d have an idle thought, spend a small amount of time developing the idea, write it up, and ET FUCKING VOILA BITCHES I HAVE A BLOG POST.\nI kind of loved that culture. It’s precisely in that spirit that I decided, in my last post, to cobble together an absolutely-cursed rethinking of the blogdown R package and write an unapologetically-unhinged post about it. The “eleventy plus knitr” system I built in an afternoon – following the Bob Katter principle in which “I ain’t spending any time on it, because in the meantime, every three months a person’s torn to pieces by a crocodile in North Queensland” – was a fun toy, and nothing more than that. This is exactly what blogs are for, and precisely the reason why the subtitle on that post is “Because you know what? I am here to fuck spiders”. The entire purpose of blogging is to have some fun. It’s not a public relations exercise.2 3 4\nSo let’s fuck some spiders."
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#managing-computational-state-when-generating-pseudo-random-numbers",
    "href": "posts/2023-12-27_seedcatcher/index.html#managing-computational-state-when-generating-pseudo-random-numbers",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "Managing computational state when generating pseudo-random numbers",
    "text": "Managing computational state when generating pseudo-random numbers\nThe spider I’m thinking about today relates to the problem of generating pseudo-random numbers in a reproducible way. Generating a sequence of numbers that satisfy formal definitions of randomness is an inherently tricky business and programming languages have a very, ummmmm, mixed track record in finding ways to do it sanely. The core of the problem lies in the fact that computers are Turing machines, and as such are deterministic systems. You can’t make a deterministic system behave “randomly” without doing quite a bit of mathematical work to (a) decide what “randomly” means in this context and, (b) constructing algorithms that produce behaviour that we are willing to describe as “random”. Fortunately for us, this part of the problem was solved a long time ago, and I have no desire whatsoever to use this post to discuss the Mersenne Twister in relation to Martin-Löf randomness.5 The algorithm is good enough for my purposes, it’s implemented as a random number generator (usually one of many) in various language, and that is fine.\nThe tricky part, from a practical perspective, is that pseudo-random number generators are stateful entities that depend on a “random number generator seed”, and – by design! – they are spectacularly sensitive to the seed. If you do even the tiniest thing in your code that touches the RNG seed, every subsequent action that uses that RNG will be changed in fundamental ways. If you want to program carefully around random number generators, you need to be super careful with managing the RNG seed.\nAh fuck it. Dua Lipa already said it better:\n\nI come and I go  Tell me all the ways you need me  I’m not here for long  Catch me or I go Houdini  I come and I go  Prove you got the right to please me  Everybody knows  Catch me or I go Houdini\n\nFrom a reproducible computing perspective, you’d better catch the RNG state and work carefully with it, or else it will be gone forever."
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#how-do-different-languages-manage-rng-state",
    "href": "posts/2023-12-27_seedcatcher/index.html#how-do-different-languages-manage-rng-state",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "How do different languages manage RNG state?",
    "text": "How do different languages manage RNG state?\nHow should we manage the RNG state in a programming language? It’s a difficult problem, and I am absolutely not the person to resolve the question. I’m basically an idiot, and I don’t even pretend to know what the right answer to this is. That being said, I think there’s essentially three categories of solution that exist in the wild:\n\nThe javascript style: The solution in vanilla javascript is basically a “fuck you” to the user. The core random number generator is Math.random() and it doesn’t let you specify the seed at all. If you want reproducible sequences of random numbers in javascript you can go fuck yourself.6\nThe C++ style: The solution in C++ is to use the random library, in which the RNG state is itself an object that must be passed to a probabilistic function, creating an object that can then be used to generate random numbers using the RNG state. It’s somewhat rigorous, but it leads to code like this, which is so obnoxiously painful that I barely even have words:\n#include &lt;iostream&gt;\n#include &lt;random&gt;\n\nint main() {\n    // set seed using time, define PRNG with Mersenne Twister\n    long unsigned int seed = static_cast&lt;long unsigned int&gt;(time(0));\n    std::mt19937_64 mersenne {seed};\n\n    // sample_poisson() draws from Poisson(4.1) and returns an integer.\n    std::poisson_distribution&lt;int&gt; sample_poisson(4.1);\n\n    // draw poisson sample (passing the PRNG as argument) and write to stdout\n    std::cout &lt;&lt; \"poisson sample: \" &lt;&lt; sample_poisson(mersenne) &lt;&lt; std::endl;\n    return 0;\n}\nHoney I just wanted some Poisson variates I didn’t want your life story.\nThe R style: Okay what if we secretly placed the RNG state into a .Random.seed variable that exists in the global environment but made it invisible so a typical user will never see it or think about it, and then have a set.seed() function to control it in ways that 99% of users won’t ever think about?\n\nUm. There is, as the young people say, a lot to unpack here."
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#on-the-particulars-of-the-r-approach",
    "href": "posts/2023-12-27_seedcatcher/index.html#on-the-particulars-of-the-r-approach",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "On the particulars of the R approach",
    "text": "On the particulars of the R approach\nOkay yes, that little summary is a bit of rhetorical largesse on my part. It should be obvious to anyone who knows me that the primary focus I have in writing about this topic is thinking about how R solves this. The whole purpose of talking about the “three styles” in the previous section is that I want to contrast the core approach in R with two other styles I’ve seen in other languages: compared to R, javascript is utterly lacking in rigour on this topic and as a consequence is utterly useless for analysts, whereas – by way of deliberately constructed contrast – C++ has rigour but is utterly lacking in practicality for everyday data analysis. The set of analysts who are going to put up with C++ bullshit when trying to simulate from a model is perilously close to measure zero. There is a reason why R adopts the peculiar solution it does.7\nSo let’s unpack it a tiny bit. We’ll start by looking at the .Random.seed object itself.\n\nWhat’s in the .Random.seed babe?\nAs I mentioned, what R does when you call set.seed() is create a hidden variable called .Random.seed that exists in the users global workspace, and is used to specify the state of the random number generator.8 Here’s what it looks like when we call set.seed() with seed = 1:\n\nset.seed(1)\nstate &lt;- .Random.seed\nstate\n\n  [1]       10403         624  -169270483  -442010614  -603558397\n  [6]  -222347416  1489374793   865871222  1734802815    98005428\n [11]   268448037    63650722 -1754793285 -2135275840  -779982911\n [16]  -864886130  1880007095   463784588  1271615005  1390544442\n....\n\n\nI’ve truncated the output because the actual state variable here is quite long and we don’t need all that clutter.9 It’s noticeable, when you look at this thing, that the first two elements of the .Random.seed seem to be rather different from the others. Let’s test that by calling set.seed() with seed = 2:\n\nset.seed(2)\nstate &lt;- .Random.seed\nstate\n\n  [1]       10403         624 -1619336578  -714750745  -765106180\n  [6]   158863565 -1093870294   242367651 -1691232888 -1538791959\n [11]   438890646  -141146593   721730004  1779208901   575310018\n [16]  -949789349   329933024  -952437919  2079445422  1509473879\n....\n\n\nYeah okay, there’s something going on here. The first two values in this vector are clearly different in some sense from the rest of the numbers. Let’s start with the first element of our state vector, the 10403 value. This one is not part of the random number generator itself. Rather, it’s used to encode the kind of random number generator in use. The way to decode what means is to split it up into three numbers, like this 1 04 03. From the help documentation:\n\nThe lowest two decimal digits are in 0:(k-1) where k is the number of available RNGs. The hundreds represent the type of normal generator (starting at 0), and the ten thousands represent the type of discrete uniform sampler.\n\nTo help make sense of this, it helps to realise that set.seed() has more arguments to it than just the seed value. There are in fact four arguments, as shown below:\nset.seed(seed, kind = NULL, normal.kind = NULL, sample.kind = NULL)\nThe kind argument specifies which RNG algorithm should be used to generate uniform random numbers (e.g., Mersenne Twister, the default), which is usually the thing we’re interested in, at least to the extent that most probabilistic process require that we have a generator for uniform random numbers. This is what the 03 part of that 10403 number refers to. However. There are two special cases that come up so often that R allows you to customise them. The normal.kind argument to set.seed() specifies the algorithm to by used when generating normally distributed numbers (e.g., Box-Muller), and this is is what the 04 part of 10403 references. The sample.kind argument refers to the algorithm used when sampling from a discrete set (e.g., as in the sample() function), and the 1 part of 10403 refers to that.\nAs to what the different options are, what defaults are used, and how those defaults have changed across different versions of R, I’ll just refer the interested reader to the help documentation, because honestly that’s not the point of this post. For now, it’s enough to recognise that the first element of .Random.seed specifies the kind of RNG, and that by default we’re using the Mersenne Twister any time we need uniform random numbers.\nOkay, what about that second element? Much like the 10403 value in the first position, the 624 number in the second position seems to be screaming out “hello I am not actually a part of the RNG state” too, and indeed that’s correct. It’s specific to the Mersenne Twister, and is used to indicate that the actual Mersenne Twister RNG state is an integer vector of length 624. And shockingly, if we take a look at how long our state variable is\n\nlength(state)\n\n[1] 626\n\n\nwe get an answer of 626: there are 624 integers used to specify the state of the Mersenne Twister, one integer used to indicate that yes the Mersenne Twister state has length 624, and one more integer used to indicate that (among other things) we’re using the Mersenne Twister.\nThat checks out.\n\n\nLet’s be random\nWell that was tiresome. I seem to be pathologically incapable of writing a short blog post without going off on bizarre yak-shaving tangents. Sorry. Anyway, let’s get back on track and do something that relies on the state of the RNG, shall we? First, we’ll reset the value of .Random.seed and capture its initial value:\n\nset.seed(1)\nstate &lt;- .Random.seed\n\nNext, I’ll do something that requires the random number generator:\n\nsample(10)\n\n [1]  9  4  7  1  2  5  3 10  6  8\n\n\nWhen I do this, there are two things that happen. Most obviously, by calling sample() I now have a random permutation of the numbers between 1 to 10. But as a hidden side effect, the value of .Random.seed has changed.10 Because the RNG state has changed, if I repeat the exercise I get a different random permutation:\n\nsample(10)\n\n [1]  3  1  5  8  2  6 10  9  4  7\n\n\nThis is of course the desired behaviour, but the only reason it works is by relying on the .Random.seed vector. If I restore the original state of the RNG before calling sample(), I get the exact same result as the first time:\n\n.Random.seed &lt;- state\nsample(10)\n\n [1]  9  4  7  1  2  5  3 10  6  8\n\n\nAgain, this is expected and desired behaviour.\n\n\nStrengths and weaknesses of the R approach\nThe approach used in R reflect as a specific philosophy that emerges from the core purpose of the language: R is a scripting language designed to support scientific data analysis. This core goal leads to two key features:\n\nScientists care about computational reproducibility, so (unlike javascript) base R comes with the set.seed() function that allows you to initialise the state of the RNG in a reproducible way. In fact, R goes one step further and provides a RNGversion() function that supports backward-compatibility across R versions, because the low level details of how R implements random number generation have changed over the years.\nData analysts need simple, practical solutions. The C++ style where you have to construct an RNG object and then explicitly pass it as an argument when you want to sample from a distribution is awkward and frustrating, and rarely helpful when doing everyday data analysis.\n\nThese twin considerations lead to the R solution: there’s one RNG state variable in R, tucked away in a hidden variable in the user workspace, and you rarely have to think about it in any more detail than remembering to include set.seed() in your analysis script. In some ways it’s an inelegant solution, but it’s shockingly effective from a practical standpoint.\nHowever.\nThere are edge cases when the R solution doesn’t quite work as well as you’d hope, and I’ve encountered them more than once. Because R relies on a single .Random.seed variable to manage state, there’s no easy way for the analyst to make a distinction between “things I’m doing that incidentally require some random numbers”, and “other probabilistic things I’m doing that are utterly essential to a simulation”. Everything you do in an R script relies on the same random number generator, and uses the same seed to manage that state. This can sometimes be fragile, because any line of code that “incidentally” touches the RNG will affect the results from any “essential” probabilistic code you write later in the script. That happens a lot with code that has this structure:\n\nset the RNG seed\ndo some essential probabilistic simulations\ndo something that incidentally calls the RNG\ndo some more essential probabilistic simulation\n\nWhen you write the code, what you sort of have in your head is that “I’m setting the RNG seed in part 1 in order to ensure that the simulations in part 2 and 4 are reproducible”, but you have a hidden dependence on the code in part 3. Often times, you don’t even realise that the code in part 3 is affecting the RNG state because there are lots of R functions that incidentally use the RNG without you realising it.\nOften what people do to address this, when they are aware of this issue, is to set the seed multiple times, at key points in the code:\n\nset the RNG seed\ndo some essential probabilistic simulations\ndo something that incidentally calls the RNG\nset the RNG seed again\ndo some more essential probabilistic simulation\n\nBy setting the seed in multiple places, you have a solution that is more robust. If, for example, there are package updates that change the manner in which the code in part 3 touches the RNG, your simulation in part 5 won’t be affected. It’s a defensive coding trick to minimise your exposure to unexpected changes to RNG state, and it works pretty well.11"
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#creating-an-isolated-rng-seed",
    "href": "posts/2023-12-27_seedcatcher/index.html#creating-an-isolated-rng-seed",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "Creating an “isolated” RNG seed",
    "text": "Creating an “isolated” RNG seed\nAs you can probably guess, I am actually a huge fan of the R solution. Yes, it’s an unprincipled hack where the language “cheats” by creating a hidden global state variable, but it really does work for the vast majority of use cases and it doesn’t waste the analysts time by making them do all the dirty work managing the RNG themselves. From its inception R has never been a language that cares about ideological purity: as Hadley Wickham once noted,12 R is first and foremost a language for “getting shit done”.\nThat being said, sometimes I find myself wishing there was a way of creating an “isolated” RNG seed. The idea here is that as the data analyst, I know perfectly well which parts of my code are essential to my probabilistic simulations, and what I really want to do is “protect” those parts of the code by executing them with a dedicated RNG. All my incidental code can use the global RNG state, but nothing I do in the incidental code would affect the output of the protected simulation code, not one hair out of place.\n\nWatch me dance, dance the night away  My heart could be burnin’, but you won’t see it on my face  Watch me dance, dance the night away  I’ll still keep the party runnin’, not one hair out of place\n\nOn the face of it, this seems hard to accomplish with R because the .Random.seed variable is aggressively unique. The documentation makes it very clear that the only place R will look for the RNG state is the .Random.seed variable in the user global environment, so you cannot solve this problem by creating a new .Random.seed variable in another environment. However, the documentation also makes clear that you are absolutely allowed to save the value of .Random.seed and restore it later.13 In other words, you totally could do something like this:\n\nUse set.seed() to create the “to-be-isolated” RNG, and then do something like protected_state &lt;- .Random.seed to store the state of that RNG\nUse set.seed() again to set the “global” RNG state\nDo some “incidental” random things (implicitly using the global RNG state)\nIn preparation for the protected step, cache the global state in a temporary global_state &lt;- .Random.seed\nRestore the protected RNG with .Random.seed &lt;- protected_state\nRun your “protected” simulation code\nCapture the updated state protected_state &lt;- .Random.seed\nRestore the global RNG with .Random.seed &lt;- global_state\n\nThis approach works perfectly well, actually. There is absolutely nothing stopping you from caching the state of a protected RNG separately from the global RNG, and occasionally restoring it when you specifically want to use the protected RNG. The only problem with the solution is that I am absolutely not willing to faff about writing code that does this in my everyday analysis work. It’s time-consuming and annoying, and I have deadlines to meet.\nEnter, stage left, the R6 package. It is almost obnoxiously easy to design a stateful R6 class that solves this problem. Here’s how you do it:\n\nSeed &lt;- R6::R6Class(\"Seed\",\n  public = list(\n    initialize = function(...) {\n      old &lt;- eval(.Random.seed, envir = .GlobalEnv)\n      set.seed(...)\n      self$state &lt;- eval(.Random.seed, envir = .GlobalEnv)\n      assign(\".Random.seed\", old, envir = .GlobalEnv)\n    },\n    state = NULL,\n    use = function(expr, envir = parent.frame()) {\n      old &lt;- eval(.Random.seed, envir = .GlobalEnv)\n      assign(\".Random.seed\", self$state, envir = .GlobalEnv)\n      x &lt;- eval(substitute(expr), envir = envir)\n      self$state &lt;- eval(.Random.seed, envir = .GlobalEnv)\n      assign(\".Random.seed\", old, envir = .GlobalEnv)\n      return(x)\n    }\n  )\n)\n\nThe Seed class is terribly simple. When you initialise a new Seed object, it temporarily caches the global .Random.seed state, then calls set.seed() to create the protected RNG state. This protected state is then cached within the Seed object itself as the $state field.14 Finally, it restores the global .Random.seed variable to its original state.\nUsing the protected seed is pretty straightforward: the Seed class has a $use() method to which you pass an R expression. All code in that expression is evaluated using the protected RNG state rather than the global state. The mechanism here is exatly the same: the $use() method caches the global RNG state, copies the $state field to the .Random.seed, then executes the R code. After the code has executed, the new value of .Random.seed is copied back to the $state field, and then the global state is restored.\nLet’s have a look at how it works. First, I’ll set the “usual” RNG state using set.seed() with seed = 123. Then, I’ll create two new isolated RNG seeds, both of which use seed = 1:\n\nset.seed(123)\nstate &lt;- .Random.seed\nx &lt;- Seed$new(1)\ny &lt;- Seed$new(1)\n\nNext, I’ll call sample() using these isolated seeds:\n\nx$use(sample(10))\ny$use(sample(10))\n\n [1]  9  4  7  1  2  5  3 10  6  8\n [1]  9  4  7  1  2  5  3 10  6  8\n\n\nNotice that both of these produce identical output (as they should, since they were both initialised using the same seed value), and the output is exactly the same as the results we saw earlier when I used set.seed(1). So far, so good. Okay, now let’s use these isolated seeds a second time:\n\nx$use(sample(10))\ny$use(sample(10))\n\n [1]  3  1  5  8  2  6 10  9  4  7\n [1]  3  1  5  8  2  6 10  9  4  7\n\n\nAgain, the results are identical to each other, and they’re also identical to the results we saw earlier when I called sample() a second time after using set.seed(1). Also what we’re expecting. Yay! Finally, let’s check that using these isolated RNG seeds has left the state of .Random.seed in the global workspace unchanged:\n\nall.equal(state, .Random.seed)\n\n[1] TRUE\n\n\nYup. It works.15\nImportantly for the desired functionality, the protection runs the other way too. RNG-sensitive code executed using the global RNG doesn’t affect the behaviour of code executed using one of the protected generators. This is actually the key feature, so let’s take a look. As before, we’ll set up our global generator and two identical protected generators.\n\nset.seed(123)\nstate &lt;- .Random.seed\nx &lt;- Seed$new(1)\ny &lt;- Seed$new(1)\n\nNext, I’ll use the x generator to do something that we might imagine is part of an “essential” simulation exercise:\n\nx$use(sample(10))\n\n [1]  9  4  7  1  2  5  3 10  6  8\n\n\nUnsurprisingly, it produces the same output. Now here’s the key part. What would have happened if I ran some incidental code beforehand? Well, let’s do exactly that:\n\nrunif(1)\n\n[1] 0.2875775\n\n\nIn my hypothetical scenario, this would be something that happens during the incidental code (e.g., maybe a function I called on the side – in order to explore something that came up during the scientific reasoning process, because analysis code is not production code and it has an inherently different logic16 – happened to generate a random number in order to break a tie or whatever). In the normal course of events, this would alter the state of the RNG for all subsequent code. But, if we now repeat the “essential” line of code using the y generator, we see that it still produces the exact same answer:\n\ny$use(sample(10))\n\n [1]  9  4  7  1  2  5  3 10  6  8\n\n\nWell that’s a relief. I mean, this is what we should expect because x and y were both created to be identical generators and they were both designed to be isolated from the global RNG state, so of course state changes in the global RNG are entirely irrelevant to the behaviour of code that uses one of the protected generators, but it’s nice to confirm.\nThis is the behaviour I wish I had easy access to in R. There are times when I have “special” code that I really, really, really want to be executed with its very own RNG, completely isolated from the global RNG. It actually irritates me that the solution to the problem can be implemented in R6 with a mere 19 lines of code. Annoyed that I didn’t think of this years ago tbqh."
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#wrapping-up",
    "href": "posts/2023-12-27_seedcatcher/index.html#wrapping-up",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "Wrapping up",
    "text": "Wrapping up\nSo okay, I solved the problem I was idly thinking about when I decided to fuck this particular spider. What now?\nIt would be pretty easy to wrap something like this in a package I suppose,17 but (a) I’m too lazy to write it myself, and (b) I think the use case for it is pretty narrowly confined to situations when you are writing a very long script that performs an “essential” simulation and also contains incidental code isn’t supposed to affect the simulation itself. Plus, and perhaps most importantly, (c) remember how I said this was a spider-fucking post? I said it and I bloody well meant it. I’m not trying to Solve A Big Problem here. I’m just playing around with code and enjoying the act of writing about it.\nThat being said, I have to admit I’ve encountered a few situations in my professional life where I really wished there were a package that implemented something like the Seed class. I had one experience a little while back where I’d inherited a long simulation script that did the right thing insofar as it called set.seed() at the top of the script, but it had lots of essential simulation code interleaved between other code that was used for non-essential purposes and incidentally modified the RNG state. It was a nightmare to try to refactor the code without breaking reproducibility. Eventually I just had to give up. The code absolutely did need to be refactored because of the future use that we had in mind, and – despite the original programmers laudable effort to do the right thing – it was absolutely impossible to do so without changing the results of the simulations. It would have been a lot easier to do this if the “essential” simulation code had been properly isolated from the incidental code. Situations like this are exactly the ones where you want something like the Seed class.\nAnyway. Whatever. This was supposed to be an exercise in fucking a spider not shaving a yak, and frankly there has been altogether too much yak shaving going on in this post. So let’s leave it there, yes?"
  },
  {
    "objectID": "posts/2023-12-27_seedcatcher/index.html#footnotes",
    "href": "posts/2023-12-27_seedcatcher/index.html#footnotes",
    "title": "Fine-grained control of RNG seeds in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOkay fine it was yesterday morning, because instead of finishing this blog post last night as I’d intended I went out for cocktails. Sue me.↩︎\nOne of the most cursed things that has happened to public tech culture is the idea of corporate-style “community”. Oh look at me, I’m a tHouGHt lEaDer iN tEcH blah blah blah. Honey, if I wanted to masturbate in public there are much easier ways to make men pay to watch me do it.↩︎\nSomewhat relatedly, I often think to myself that the reason why a lot of technical blog posts end up with very bland writing is that the author feels obligated to “act professionally” on their blog, for fear that their employer might see it and react negatively if they ever use the word “fuck”. I understand and share that sentiment but also… I’ve worked as an academic, I’ve worked in tech, and I now work in pharma. Anyone who knows me professionally knows that (especially as I’ve gotten older) I don’t ever talk like this at work. Professionalism is important, in a professional context. But my blog is not my job, and in much the same way that trying to be a professional artist sucked all the joy out of making art for me, trying to be professional in my blog posts sucks all the joy out of writing. In my professional life I have to be restrained, and things that are essential to my very character – my queerness, for instance – are inadmissable and unspeakable in a work context. I’m frankly unwilling to extend that level of self-imposed closeting to my personal life. This blog is part of my personal life, not my professional life. So I get to be me here. If that bothers people they are free to not read my blog.↩︎\nAs Dan Simpson once remarked, it is extremely homophobic of quarto not to support nested footnotes. Like, what the hell are queers supposed to do when we can’t turn our blog posts into a tangled web of unhinged footnotes? This is our primary defence mechanism to ensure that the straights on linkedin won’t ever try to interact with us, damn it.↩︎\nNo seriously. I spent a solid six months of my mid-20s life reading journal articles about algorithmic randomness and its relationships to Kolmogorov complexity and Bayesian inference, when instead I could have spent that time doing literally anything else and it was a terrible fucking decision.↩︎\nYes I know about seedrandom, shut up.↩︎\nIn defence of both C++ and javascript, you could probably argue the same for those languages: C++ is a systems language, and you’re not really supposed to use it for everyday data analysis. The tedious verbosity of C++ code in this context reflects the function of the language. Similarly, javascript was designed to support scripting for web pages, and while there are now libraries that support data analysis in javascript, it wasn’t originally designed for that purpose and so “vanilla” javascript doesn’t come with the same level of careful thought on this topic that you see in base R. My point in using those two as contrasts to R is not to call them bad languages, but to highlight the fact that different languages make different choices that reflect the primary function those languages were designed to support.↩︎\nNote that the .Random.seed vector doesn’t actually exist at start up: it is created explicitly when the user calls set.seed(), but it will also be created for you if you do something that requires the RNG without previously calling set.seed(), using the current time and the process ID as the input.↩︎\nI swear to the almighty femme top above, every single time I have to write a knitr hook I have to spend 20 minutes googling to find this page again. I don’t know why this specific thing is so hard to search for, but I’m about this close to writing a pointless blog post on my own site that just copies the damn code line for line, just so that I don’t have to search for it again.↩︎\nParenthetically, if you want to configure R so that you get notified every time .Random.seed changes, you can set up a callback handler to do this. Henrik Bengtsson has a nice post showing you how to do this. I have something similar set up in my .Rprofile.↩︎\nMore generally, though, if you want to be completely safe you’d probably need to use tools like Docker, renv, and rig to control the computational environment. But that’s beyond the scope.↩︎\nI’m too lazy to track down the original citation or the exact quote, but I think he said it during an rstudio::conf / posit::conf talk. The specifics don’t matter very much.↩︎\nThe exact phrasing in the documentation says that .Random.seed “can be saved and restored, but should not be altered by the user”, i.e., it’s totally fine to copy the RNG state, just don’t try to modify the values stored in the vector yourself because you’ll almost certainly mess it up.↩︎\nI should probably have made this a private field rather than a public field, and then written a public accessor method like $get_state() or whatever. But this is a toy example, I’m not trying to be rigorous here.↩︎\nIt should go without saying that this isn’t guaranteed to work properly if we’re doing a multi-threaded execution thing. But that’s true for normal random number generation anyway: you need special tools when doing random number generation in parallel. One of these days I want to do a deep dive on that topic, but it’s totally something for a future post.↩︎\nOne of these days I want to write a post about what counts as “best practice” for writing analysis code that doesn’t go into prod but might be sent to a regulator, because seriously my babes that is a fucking different beast altogether. But that’s for another time.↩︎\nOn the off chance anyone does go down this path, I propose the name “seedcatcher” so that all the stats gays can make “no loads refused” jokes about it. See also, “lubridate”.↩︎"
  },
  {
    "objectID": "posts/2023-06-14_ansi-tricks/index.html",
    "href": "posts/2023-06-14_ansi-tricks/index.html",
    "title": "A little ANSI trickery",
    "section": "",
    "text": "On mastodon today I wrote cute little post showing what my R startup message currently looks like:\nIn the replies, I included the code showing how to generate the rainbow strip that appears at the bottom of the startup message. Calling this rainbow_strip() function from the R console will give you the result you’re looking for:\nrainbow_strip &lt;- function() {\n  c(\"#e50000\", \"#ff8d00\", \"#ffee00\", \"#028121\", \"#004cff\", \"#770088\") |&gt;\n    purrr::map(cli::make_ansi_style) |&gt;\n    purrr::walk(~ cat(.x(paste0(rep(\"\\u2583\", 6), collapse = \"\"))))\n}\nThe code isn’t very complicated, but it does rely on a few tricks. The most important trick is the one that occurs on the second line, in which I use the cli package to define a set of six styles, each of which colours the text in one of the colours from the LGBTIQ+ pride flag.1 Each of these six styles is then applied to the UTF-8 character2 string \"▃▃▃▃▃▃\" and the results are concatenated in the output. It’s very pretty. But suppose I wanted to reproduce the style out output within a quarto document like this one. Alas, it does not work, because information about text colour is not preserved in the output when the document is rendered:\nrainbow_strip()\n\n▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\nHm. Well that’s annoying, but hardly surprising to anyone familiar with R markdown or quarto. But just to be sure, let’s try it again, shall we?\nrainbow_strip()\n\n▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\nSay what? It works the second time, but not the first? Peculiar. Someone – possibly me?3 – must be engaged in some trickery that isn’t obvious from first inspection."
  },
  {
    "objectID": "posts/2023-06-14_ansi-tricks/index.html#the-trick-revealed",
    "href": "posts/2023-06-14_ansi-tricks/index.html#the-trick-revealed",
    "title": "A little ANSI trickery",
    "section": "The trick revealed",
    "text": "The trick revealed\nIf you were to take a look at the source code for this quarto document, the thing you’d immediately notice is that while both of these two code chunks contain R code, one of them is explicitly an R chunk and the other is… not. Here’s the first one again, with the quarto code fencing shown:\n\n```{r}\nrainbow_strip()\n```\n\n▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n\n\nThe second one, however, is tagged as an “asciicast” code chunk. It still executes R code, but something extra is going on when the code runs, because now the output now preserves the text colour:\n\n```{asciicast}\nrainbow_strip()\n```\n\n▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃                                            \n\n\n\nTo be clear, there’s nothing special about my rainbow_strip() function here. You can see the same thing happening with any R function that produces coloured text in the output. For example, when printing a tibble at the R console you would normally expect to see the less important parts of the output printed in a muted grey colour. However, when we create a tibble in a quarto document the shading disappears:\n\n# this is an R chunk\ntibble::tibble(x = 1:3, y = letters[1:3])\n\n# A tibble: 3 × 2\n      x y    \n  &lt;int&gt; &lt;chr&gt;\n1     1 a    \n2     2 b    \n3     3 c    \n\n\nThe shading reappears when the same code is executed with the asciicast knitr engine:\n\n# this is an asciicast chunk\ntibble::tibble(x = 1:3, y = letters[1:3])\n\n# A tibble: 3 × 2                                                               \n      x y                                                                       \n  &lt;int&gt; &lt;chr&gt;                                                                   \n1     1 a                                                                       \n2     2 b                                                                       \n3     3 c                                                                       \n\n\n\nThe thing that makes this all work is the asciicast package. Typically, the asciicast package is used to create screencast from R code. I’ve used it before on this blog, actually. When I wrote the “pretty little CLIs” post about the cli package, I used asciicast to create all the animations that appear in the post. However, you can also use asciicast in conjunction with the knitr engine that powers the code execution in quarto.4 Here’s the code chunk I used to set it up for the current post:\n\n```{r}\nknitr::opts_chunk$set(\n  collapse = FALSE,\n  comment = \"\",\n  out.width = \"100%\",\n  cache = TRUE,\n  asciicast_knitr_output = \"html\"\n)\n\nasciicast::init_knitr_engine(\n  echo = TRUE,\n  echo_input = FALSE,\n  same_process = TRUE,\n  startup = quote({\n    library(cli)\n    options(\n      cli.num_colors = cli::truecolor,\n      asciicast_theme = list(background = c(255, 255, 255))\n    )\n    set.seed(1)\n  })\n)\n```\n\nThe first command sets the knitr options for R code chunks. The important line here is the one that sets asciicast_knitr_output = \"html\", which ensures that asciicast produces HTML output. If you don’t set this, the output from asciicast chunks will be rendered as images rather than HTML. The second line does what you might expect: calling asciicast::init_knitr_engine() initialises the asciicast knitr engine."
  },
  {
    "objectID": "posts/2023-06-14_ansi-tricks/index.html#the-trick-explained",
    "href": "posts/2023-06-14_ansi-tricks/index.html#the-trick-explained",
    "title": "A little ANSI trickery",
    "section": "The trick explained",
    "text": "The trick explained\nIn case you don’t already know this stuff, it’s probably worth explaining why the text colour in R output usually vanishes when R code is executed within a quarto or R markdown document. In fact, when I wrote the “pretty little CLIs” post I talked about precisely this:\n\nThe R console is a terminal, and its behaviour doesn’t always translate nicely to HTML. Part of the magic of the rmarkdown package is that most of the time it is able to capture terminal output and translate it seamlessly into HTML, and we mere mortal users never notice how clever this is. However, when dealing with cli output, we run into cases where this breaks down and the law of leaky abstractions comes into play: text generated at the R console does not follow the same rules as text inserted into an HTML document, and R Markdown sometimes needs a little help when transforming one to the other.\n\nAs regards colour:\n\nThe colours and symbols used by cli, and supported in the R console, rely on ANSI escape codes, but those escape codes aren’t recognised in HTML\n\nIn that post I used the fansi package to write a knitr hook that translated the relevant ANSI characters into HTML, thereby preserving the colour information. In essence, the asciicast package allows me to do the same thing here.\nFun!\n\n\nrainbow_flag &lt;- function() {\n  c(\"#e50000\", \"#ff8d00\", \"#ffee00\", \"#028121\", \"#004cff\", \"#770088\") |&gt;\n    purrr::map(cli::make_ansi_style) |&gt;\n    purrr::walk(~ cat(.x(c(paste0(rep(\"\\u2588\", 18), collapse = \"\"), \"\\n\"))))\n}\n\nrainbow_flag()\n\n██████████████████                                                              \n██████████████████                                                              \n██████████████████                                                              \n██████████████████                                                              \n██████████████████                                                              \n██████████████████"
  },
  {
    "objectID": "posts/2023-06-14_ansi-tricks/index.html#footnotes",
    "href": "posts/2023-06-14_ansi-tricks/index.html#footnotes",
    "title": "A little ANSI trickery",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the post on Mastodon I used crayon::make_style () rather than cli::make_ansi_style(), but they do the same thing. After I wrote the post I remembered that the crayon package has been superseded by cli, so it’s generally better to use the cli version instead.↩︎\nIn the source code I use \"\\u2583\" to produce the UTF-8 block character \"▃\".↩︎\nDefinitely me.↩︎\nI mean, that’s assuming you’re using knitr as the engine, which I am in this post. There’s nothing stopping you from using jupyter as the execution engine, which I’ve done in the past for python-focused posts.↩︎"
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html",
    "title": "Getting started with Apache Arrow",
    "section": "",
    "text": "If you’re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but you’re not really sure what they’re about and are curious? If that’s you, then–\nSo we’re just writing obvious “I want a job in tech, please hire me!” blog posts pitched at potential employers now?\nOh shush. It’s fun and useful too, you know.\nOkay fine, but could you at least be transparent about what you’re doing? Because it’s sort of obnoxious otherwise\nSheesh, what do you think this fake dialogue is for if not making the subtext blatant? Now could you please stop interrupting me and let me talk about Apache Arrow? It is in fact a more interesting subject than our pending unemployment.\nYeah, see how you feel about that in December babe…\nSigh.\nArrow image by Tim Mossholder. It has nothing whatsoever to do with the Apache Software Foundation. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#introduction",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#introduction",
    "title": "Getting started with Apache Arrow",
    "section": "Introduction",
    "text": "Introduction\nOkay, where was I? Ah yes…\nIf you’re like me and spend far too much time talking about R on Twitter you may have come across people talking about how to work with large data sets in R. Perhaps you hear people talking about Parquet files, Apache Arrow, and the arrow package for R, but you’re not really sure what they’re about and are curious? If that’s you, then then this blog post is designed to help you get started.\n\nWait… do I actually care?\nLet’s start at the beginning, with the most important question of all: do you actually need to care about this? This might be a long post (or possibly the first post in a long series), so let’s make sure you’re reading for the right reasons!\nFor a lot of people, the answer to the “do I care?” question is going to be “probably not – or at least not right now”. For example, if all your data sets are small and rectangular, then you’re probably working with CSV files and not encountering a lot of problems. Your current workflow uses read.csv() or readr::read_csv() to import data, and everything is fine. Sure, the CSV format has some problems, but it’s simple and it works. If that is you, then right now you don’t need to worry about this.\nBut perhaps that’s not you, or maybe that won’t be you forever. You might be working with larger data sets, either now or in the future, and when that happens you might need to care.\n\n\nOkay… so what’s the problem?\nThanks for a great question! Here are a few scenarios to think about.\n\nScenario 1: Let’s suppose you have a big rectangular data set. An enormous table, basically, and currently it’s stored as a file on your disk. The format of that file could be a plain CSV, a compressed CSV, or it could be something fancier like a Parquet file (I’ll come back to those in a later post, I suspect). It might be a couple of billion rows or so, the kind of thing that you can store on disk but is too big to fit into memory, so it’s not going to be very easy to read this thing into R as a data frame! But your boss wants you to analyse it in R anyway. That’s awkward. R likes to store things in memory. Eek.\nScenario 2: Okay, maybe your data isn’t that big and it fits in memory, but it’s still pretty big, and you need to do something complicated with it. Maybe your analysis needs to start in R but then continue in Python. Or something like that. In your head, you’re thinking okay first I have to read the whole dataset into memory in R, and then it has to be transferred to Python which will have to read its own copy, and… gosh that sounds slow and inefficient. Ugh.\nScenario 3: Honestly, you’re just tired of having to deal with the fact that every language has its own idiosyncratic way of storing data sets in memory and it’s exhausting to have to keep learning new things and you really wish there were some standardised way that programming languages represent data in memory and you’d like a single toolkit that you can use regardless of what language you’re in. Sigh…\n\nIn any of these scenarios, Arrow might be useful to you.\n\n\nFiiiiiine, I’ll keep reading… tell me what Arrow is\nYaaaaay! Green Arrow is a superhero in the DC Comics universe, whose real name is Oliver Queen. He was the subject of an unintentionally hilarious TV show, and–\n\n\nSigh. Apache Arrow please?\nOh right. Apache Arrow is a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead.\n\n\nI hate you\nSorry. Let’s unpack each of those terms:\n\nArrow is a standardised and language-independent format. It’s the same thing regardless of what programming language you’re using: a data set accessed from R with Arrow has the same format as the a data set accessed in Python.\nArrow is used to store table-like data, very similar to a data frame or tibble.\nArrow refers to the in-memory format: it’s not talking about how the data are stored on disk, and it’s not talking about file formats. It’s all about how a loaded data set is represented in memory.1\nArrow uses columnar format. Unlike a CSV file, which stores the data row-wise, it represents the data column-wise: this turns out to be a much more efficient way to represent data when you need to subset the data (e.g., by using dplyr::filter() in R or the WHERE clause in SQL).\nArrow supports zero-copy reads without serialisation overhead, which… um… yeah, what the heck does that mean?\n\nSo yeah. Serialisation is one of those terms that those fancy data people know all about, but a regular R user might not be quite as familiar with. It’s worth unpacking this a bit because it’s helpful for understanding the problem that Arrow solves…\n…Hey!\nWait a second, I already wrote a blog post about serialisation! I don’t need to write another one.2 The TL;DR, for folks who quite reasonably don’t want to do a deep dive into how R objects are written to RDS files, is that serialisation is the process of taking an in-memory data structure (like a data frame), and converting it into a sequence of bytes. Those bytes can either be written to disk (when you’re saving a file) or they can be transmitted over some other channel. Regardless of what you want to do with the serialised data, this conversion takes time and resources, and at some point the data will need to be unserialised later. The resources expended in doing so are referred to as the “serialisation overhead”.\nFor small data sets, it doesn’t take R very long to serialise or unserialise. The “serialisation overhead” isn’t a big deal. But when the data set is very large, this is not a trivial operation and you don’t want to do this very often. That’s a problem when a large data set needs to be passed around between multiple platforms. Loading the a CSV into R incurs a serialisation cost; transferring a copy of the data from R to Python incurs a serialisation cost. This happens because R and Python have different structured representations: a data frame in R is a different kind of thing to a panda in Python, so the data has to be serialised, transferred, and then unserialised at the other end in order to pass the data from one to another.\nWouldn’t it be nice if we could avoid that? What if there was just one data structure representing the table in-memory, and R and Python could both agree to use it? That would remove the need to copy and transfer the data, right? And in doing so, it would eliminate those pesky serialisation costs incurred every time. It would be a “zero-copy” mechanism.\nIf only there were a standardised, language-independent format for storing table-like data in-memory, using a columnar format and supporting zero-copy reads without serialisation overhead…\n\n\n\n\n\nArrow image by Possessed Photography. It also has nothing whatsoever to do with the Apache Software Foundation. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#overview-of-arrow",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#overview-of-arrow",
    "title": "Getting started with Apache Arrow",
    "section": "Overview of Arrow",
    "text": "Overview of Arrow\nHere’s one of the two big ideas: standardisation prevents wasteful copying. The current situation that most of us are working in looks something like this. Every application and programming language defines its own format for storing data in memory (and often on disk too), and so any time multiple applications require access to the same data, there’s a serialisation cost. The bigger the data, the higher that cost will be. The more applications you connect to the same data, the more times you incur the cost:\n\n\n\n\n\n\n\n\n\nArrow solves this problem by allocating its own memory to store the data, and providing tools that allow you to access this from any language you like. The goal is to make those tools feel “natural” in whatever language you’re using. For example, if you’re an R user, you may already be familiar with the dplyr grammar for data manipulation and you’d like to be able to manipulate an Arrow Table using dplyr, in exactly the same way you would manipulate a data frame. The arrow R package allows you to do precisely this, and there’s a similar story that applies on the Python side. This allows you to write code that feels natural for the language you’re working in.\nIn this approach, R and Python both have a toolkit that plays nicely with Arrow and feels native to that language. Applications written in R and applications written in Python can both work with the same underlying data (because it’s in Arrow), so you don’t have to serialise the data in order for them to talk to each other:\n\n\n\n\n\n\n\n\n\nSo that’s the first big idea.\nThe second big idea is that Arrow organises data column-wise in memory and as consequence it can support cool single instruction multiple data (or SIMD) operations that you can do with modern CPUs, which I totally understand 100% and am not just paraphrasing Wikipedia. Anyway, it doesn’t really matter at the user level. All we care about there is that manipulating data with Arrow can be very fast. There’s a very brief discussion of this on the Arrow overview page. (It also has prettier versions of my crappy handwritten diagrams)\n\n\n\n\n\nArrow image by Denise Johnson. Yet again, it has nothing whatsoever to do with the Apache Software Foundation but it is very pretty. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#installing-arrow",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#installing-arrow",
    "title": "Getting started with Apache Arrow",
    "section": "Installing Arrow",
    "text": "Installing Arrow\nInstalling Apache Arrow on your local machine as an R user is either extremely easy or mildly tiresome, depending almost entirely on whether you’re on Linux. If you’re using Windows or Mac OS, you shouldn’t need to do anything except install the arrow package in the usual way. It just works:\n\ninstall.packages(\"arrow\")\n\nIf you’re on Linux, there may not be any precompiled C++ binaries for your system, so you’ll have to do it yourself. On my system this was quite time consuming, and the first couple of times I tried it I was convinced that nothing was actually happening because I wasn’t seeing a progress bar or anything, and being impatient I killed the install process before it was finished. If you’re like me and need visual confirmation that something is happening, there’s an ARROW_R_DEV environment variable you can set that will make the process more verbose:\n\nSys.setenv(ARROW_R_DEV = TRUE)\ninstall.packages(\"arrow\")\n\nThis way you get to see all the C++ build information scrolling by on the screen during the installation process. It doesn’t make for very exciting viewing, but at least you have visual confirmation that everything is working!\nThere are quite a few ways you can customise the installation process, and they’re all documented on the installation page. One particularly useful thing to do is to set LIBARROW_MINIMAL to false, which ensures that arrow will install a bunch of optional features like compression libraries and AWS S3 support. It takes longer but you get more stuff! So the actual installation code I used was this:\n\nSys.setenv(\n  ARROW_R_DEV = TRUE,\n  LIBARROW_MINIMAL = FALSE\n)\ninstall.packages(\"arrow\")\n\nThis may take quite a long time if you’re compiling from source so you may want to go make a cup of tea or something while it installs. At the end, hopefully, you’ll have a working version of the package:\n\nlibrary(arrow)\n\nYou can use the arrow_info() function to obtain information about your installation:\n\narrow_info()\n\nArrow package version: 12.0.0\n\nCapabilities:\n               \nacero      TRUE\ndataset    TRUE\nsubstrait FALSE\nparquet    TRUE\njson       TRUE\ns3         TRUE\ngcs        TRUE\nutf8proc   TRUE\nre2        TRUE\nsnappy     TRUE\ngzip       TRUE\nbrotli     TRUE\nzstd       TRUE\nlz4        TRUE\nlz4_frame  TRUE\nlzo       FALSE\nbz2        TRUE\njemalloc   TRUE\nmimalloc   TRUE\n\nMemory:\n                  \nAllocator jemalloc\nCurrent    0 bytes\nMax        0 bytes\n\nRuntime:\n                          \nSIMD Level          avx512\nDetected SIMD Level avx512\n\nBuild:\n                           \nC++ Library Version  12.0.0\nC++ Compiler            GNU\nC++ Compiler Version 11.3.0\n\n\nYaaas queen! We are ready to go.\n\n\n\n\n\nArrow image by Frank Busch. Now there are two! There are two arrows. Available by CC0 licence on unsplash."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#does-it-work",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#does-it-work",
    "title": "Getting started with Apache Arrow",
    "section": "Does it work?",
    "text": "Does it work?\nMy goal in this post is fairly modest. I wanted to understand why everyone I talk to seems so excited about Arrow, and try to get it configured to work on my machine. Assuming I can be bothered continuing this series, the next step would be to start playing with Arrow and do a proper exploration. For now though, I’ll try something simple, using the diamonds data from the ggplot2 package\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(ggplot2)\ndiamonds\n\n# A tibble: 53,940 × 10\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# ℹ 53,930 more rows\n\n\n\nExample 1: Arrow data sets aren’t stored in R memory\nOkay, so the first thing I want to investigate is this idea that Arrow holds the data in its own memory, not in the memory allocated to R. As things currently stand the diamonds tibble has 53940 rows stored in R memory, and that occupies about 3.3MB of memory:\n\nlobstr::obj_size(diamonds)\n\n3.46 MB\n\n\nWhat happens when we move the data into Arrow? To do this we would construct a “Table” object using the arrow_table() function, like this:\n\ndiamonds2 &lt;- arrow_table(diamonds)\ndiamonds2\n\nTable\n53940 rows x 10 columns\n$carat &lt;double&gt;\n$cut &lt;dictionary&lt;values=string, indices=int8, ordered&gt;&gt;\n$color &lt;dictionary&lt;values=string, indices=int8, ordered&gt;&gt;\n$clarity &lt;dictionary&lt;values=string, indices=int8, ordered&gt;&gt;\n$depth &lt;double&gt;\n$table &lt;double&gt;\n$price &lt;int32&gt;\n$x &lt;double&gt;\n$y &lt;double&gt;\n$z &lt;double&gt;\n\n\nIt’s printed a little differently, but it’s the same tabular data structure consisting of 53940 rows and 10 columns. So how much R memory does diamonds2 occupy?\n\nlobstr::obj_size(diamonds2)\n\n284.64 kB\n\n\nOnly 279KB. The reason why it occupies so little memory is that diamonds2 doesn’t contain all the data. The data are stored elsewhere, using memory allocated to Arrow. If a Python program wanted to access the diamonds2 data, it could do so without having to serialise the data again. It can link to the same data structure in Arrow memory that I just created. Neat!\n\n\nExample 2: Arrow plays nicely with dplyr\nOne neat thing about dplyr is that it cleanly separates the API from the backend. So you can use the dbplyr package to work with databases using dplyr code, or the dtplyr package to use a data.table backend, and so on. The arrow package does the same thing for Apache Arrow.\nHere’s an example. If I were working with the original diamonds tibble, I might write a simple dplyr pipe to tabulate the clarity of premium-cut diamonds:\n\ndiamonds %&gt;% \n  filter(cut == \"Premium\") %&gt;% \n  count(clarity)\n\n# A tibble: 8 × 2\n  clarity     n\n  &lt;ord&gt;   &lt;int&gt;\n1 I1        205\n2 SI2      2949\n3 SI1      3575\n4 VS2      3357\n5 VS1      1989\n6 VVS2      870\n7 VVS1      616\n8 IF        230\n\n\nCan I do the same thing using the diamonds2 Table? Let’s try:\n\ndiamonds2 %&gt;% \n  filter(cut == \"Premium\") %&gt;% \n  count(clarity)\n\nTable (query)\nclarity: dictionary&lt;values=string, indices=int8, ordered&gt;\nn: int64\n\nSee $.data for the source Arrow object\n\n\nOkay, perhaps not what we were expecting. In order to optimise performance, the query doesn’t get evaluated immediately (more on this in a later post perhaps) You have to tell it either to compute() the result, which will return another Table, or to collect() the result into a data frame\n\ndiamonds2 %&gt;% \n  filter(cut == \"Premium\") %&gt;% \n  count(clarity) %&gt;% \n  collect()\n\n# A tibble: 8 × 2\n  clarity     n\n  &lt;ord&gt;   &lt;int&gt;\n1 SI1      3575\n2 VS2      3357\n3 SI2      2949\n4 I1        205\n5 VS1      1989\n6 VVS1      616\n7 VVS2      870\n8 IF        230\n\n\nAt no point has the full data set been loaded into R memory. The diamonds2 object doesn’t contain any new information. It’s still the same size:\n\nlobstr::obj_size(diamonds2)\n\n284.64 kB\n\n\nMy example is trivial, of course, because the diamonds data set isn’t very big. But if you start reading the Arrow documentation, they give an example using the NYC taxi data which is about 37GB in size. That’s… a teeensy bit bigger than I’d want to try loading into memory on my laptop, so I wouldn’t be able to load it into R at all much less use dplyr. However, because Arrow supplies a dplyr back end, it is possible to write dplyr code for the NYC taxi data.\nOld and jaded though I may be, I have to admit that’s pretty cool.\n\n\n\n\n\nOkay yeah, this one actually does have something to do with the Apache Software Foundation. It’s, like, a registered trademark or something. I’m guessing this counts as fair use though."
  },
  {
    "objectID": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#footnotes",
    "href": "posts/2021-11-19_starting-apache-arrow-in-r/index.html#footnotes",
    "title": "Getting started with Apache Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically speaking, there’s a little ambiguity here. Usually when we’re talking about Arrow we’re talking about the in memory specification, but the term is also used to refer to the software implementing it, which includes a lot of compute functionality that goes beyond what the specification states. Similarly, the Arrow in-memory format doesn’t have to imply any particular serialisation format, but in practice it’s tightly connected to the IPC (“interprocess communication”) streaming and file format, and to the parquet file format. As a consequence, the term “Arrow” is sometimes used to refer to that broader suite of tools.↩︎\nOkay, I’ll be honest, the RDS serialisation post came about because I was thinking about Arrow and serialisation costs, and got slightly distracted!↩︎"
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html",
    "href": "posts/2023-05-22_santoku/index.html",
    "title": "Santoku",
    "section": "",
    "text": "Once upon a time in a blog that has vanished into the mists of the internet past, I used to have a habit of writing short posts talking about R packages I liked. The idea behind that habit was to spend a bit of time diving a little deeper into tool than I otherwise might have done. The intent wasn’t to write tutorials or anything quite that ambitious, it was simply to spend a bit of time with the package and commit my thoughts about it to paper, metaphorically speaking. It was quite a lot of fun.\nToday I find myself having a few hours of free time on my hands, and thought I might revive the habit. I’ll pick one R package, play with it for a bit, and then write about it for no reason except for the joy I get in doing so.\nWith that in mind today’s post is about the santoku package authored by David Hugh-Jones. Why santoku,1 I hear you ask? Honestly, no particular reason except that I like it. That, and the fact that it has a cool hex sticker:\nlibrary(santoku)\nlibrary(tibble)"
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#what-problem-does-santoku-solve",
    "href": "posts/2023-05-22_santoku/index.html#what-problem-does-santoku-solve",
    "title": "Santoku",
    "section": "What problem does santoku solve?",
    "text": "What problem does santoku solve?\nIn my academic days as a computational social scientist, I’d very often find myself needing to bin continuous data into discrete (and usually ordered) categories. It was one of those little data wrangling tasks that popped up a lot. When working with survey data, for instance, I might need to take raw test scores on some psychometric scale and carve them up into easily interpreted groups. The original data would be quantitative in nature but the raw numbers often didn’t mean much: it was often convenient to bin the respondents into “high”, “medium”, and “low” scores on… whatever it was about. Sometimes I’d have external guidance on what constitutes “high” and what constitutes “low”. For example, clinical measurement tools generally have some rough guidelines about what scores on a questionnaire roughly match up to “mild depression” versus “severe depression”. But sometimes there would be no external guidelines to rely on so I’d just have to make choices based on the data themselves.\nMaking these choices in real world data analysis can sometimes be fraught, particularly when someone later wants to use those categories as input to some subsequent analysis.2 Because it’s a decision that needs to be made with care, you really do want to have versatile tools that make the process painless and even – dare I say it – fun.\nSo, how do you do this in R? The way I have traditionally done this is to use the cut() function. For example, if I wanted to take some numbers that vary from 1 to 100 and bin them into intervals of size 25, I would do this:\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = cut(value, breaks = c(0, 25, 50, 75, 100))\n)\n\n# A tibble: 10 × 2\n   value group   \n   &lt;dbl&gt; &lt;fct&gt;   \n 1 27.3  (25,50] \n 2 37.8  (25,50] \n 3 57.7  (50,75] \n 4 90.9  (75,100]\n 5 21.0  (0,25]  \n 6 89.9  (75,100]\n 7 94.5  (75,100]\n 8 66.4  (50,75] \n 9 63.3  (50,75] \n10  7.12 (0,25]  \n\n\nThere’s nothing wrong with this workflow at all. The cut() function does the job it was designed for, and a little bit of extra work allows me to assign labels:\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = cut(\n    value,\n    breaks = c(0, 25, 50, 75, 100),\n    labels = c(\"grp 1\", \"grp 2\", \"grp 3\", \"grp 4\")\n  )\n)\n\n# A tibble: 10 × 2\n   value group\n   &lt;dbl&gt; &lt;fct&gt;\n 1  21.4 grp 1\n 2  18.5 grp 1\n 3  69.0 grp 3\n 4  39.0 grp 2\n 5  77.2 grp 4\n 6  50.3 grp 3\n 7  72.0 grp 3\n 8  99.2 grp 4\n 9  38.6 grp 2\n10  78.0 grp 4\n\n\nI could extend this workflow and use the quantile() function to carve the data set up into two groups of roughly equal size:\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = cut(\n    value,\n    breaks = quantile(value, c(0, .5, 1))\n  ))\n\n# A tibble: 10 × 2\n   value group      \n   &lt;dbl&gt; &lt;fct&gt;      \n 1 93.5  (36.8,93.5]\n 2 22.0  (2.33,36.8]\n 3 65.5  (36.8,93.5]\n 4 13.4  (2.33,36.8]\n 5 27.5  (2.33,36.8]\n 6 39.2  (36.8,93.5]\n 7  2.33 &lt;NA&gt;       \n 8 38.9  (36.8,93.5]\n 9 87.1  (36.8,93.5]\n10 34.7  (2.33,36.8]\n\n\nOkay that doesn’t quite work, because the smallest value in the data doesn’t get included: the zeroth quantile of the empirical data is always equal to the smallest value in the data, and by default the intervals constructed by cut() are left-open, right-closed. Fortunately, the cut() function has an include.lowest argument that I can use to fix this:\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = cut(\n    value,\n    breaks = quantile(value, c(0, .5, 1)),\n    include.lowest = TRUE\n  ))\n\n# A tibble: 10 × 2\n   value group      \n   &lt;dbl&gt; &lt;fct&gt;      \n 1  48.7 [11.7,55.1]\n 2  60.4 (55.1,82.9]\n 3  49.9 [11.7,55.1]\n 4  19.4 [11.7,55.1]\n 5  82.9 (55.1,82.9]\n 6  67.2 (55.1,82.9]\n 7  79.6 (55.1,82.9]\n 8  11.7 [11.7,55.1]\n 9  72.6 (55.1,82.9]\n10  41.7 [11.7,55.1]\n\n\nThe automated labels helpfully make clear that the lowest interval is closed on both sides. It isn’t always pretty, but it works.\nGiven that the base R tools work, it’s not obvious from afar why there would be any need or desire to have a specialised R package that provides a replacement for cut(). To that I would answer thusly: if discretising a continuous variable is something you do rarely, you probably don’t need a specialised package. The tools in base R are perfectly functional, and the “pain points” aren’t hard to work around: the include.lowest issue I mentioned above is a good example. On the other hand, if this is a data wrangling task you have to perform a lot – as I used to – these pain points start to become really annoying. Over and over I’d forget the workarounds, and I’d have to reinvent solutions and rewrite code that I’d written a dozen times before. When revisiting code that I’d written months before, I’d find myself wondering “why did I do it that way?” because I’d forgotten the specific edge case I was trying to work around.\nWhenever you find yourself in that situation – where very small pain points in the standard tooling are adding up to a substantial annoyance because you happen to be one of those people who encounters them a lot – you might very well be in the market for a more specialised tool.\nEnter, stage left, the santoku package…"
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#specialist-chop-functions",
    "href": "posts/2023-05-22_santoku/index.html#specialist-chop-functions",
    "title": "Santoku",
    "section": "Specialist chop functions",
    "text": "Specialist chop functions\nLet’s start with the basics. As you might hope, the santoku package provides a drop-in replacement for cut(), called chop(). If I re-run my first example using the chop() function, I’d hope to get the same results and indeed – random sampling notwithstanding – I more or less do…\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = chop(value, breaks = c(0, 25, 50, 75, 100))\n)\n\n# A tibble: 10 × 2\n   value group    \n   &lt;dbl&gt; &lt;fct&gt;    \n 1 82.3  [75, 100]\n 2 65.1  [50, 75) \n 3 78.5  [75, 100]\n 4 55.8  [50, 75) \n 5 53.4  [50, 75) \n 6 79.1  [75, 100]\n 7  3.31 [0, 25)  \n 8 48.2  [25, 50) \n 9 73.5  [50, 75) \n10 69.6  [50, 75) \n\n\nLooking carefully at the output we can see some minor differences: the default behavior of chop() is to create intervals that are left-closed and right-open, whereas cut() is left-open and right-closed. Both functions have arguments that allow you to switch between these two forms, and to my mind there’s not a strong argument to prefer one default over the other.3\nHowever, let’s have a look at what happens when I try my second example with chop():\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = chop(\n    value,\n    breaks = quantile(value, c(0, .5, 1))\n  ))\n\n# A tibble: 10 × 2\n   value group\n   &lt;dbl&gt; &lt;fct&gt;\n 1 48.3  50%  \n 2 86.3  50%  \n 3 44.4  50%  \n 4 25.2  0%   \n 5  8.00 0%   \n 6 10.8  0%   \n 7 32.3  0%   \n 8 52.3  50%  \n 9 66.5  50%  \n10 41.3  0%   \n\n\nOoh. Something nice has happened. Notice that there are no NA values: chop() has a close_end argument that is analogous to the include.lowest argument to cut(), but it defaults to TRUE rather than FALSE. Though mathematically inelegant,4 the chop() default is nicely aligned with the needs of the analyst.\nA second interesting thing to note is that if the user doesn’t explicitly specify labels, but the breaks argument is a named vector – like the ones produced by quantile() – the discretised variable will inherit those names. That’s usually a good thing, in my experience, though it’s a little clunky in this instance because the names produced by quantile() aren’t precisely the labels I’d like to have when chopping the data into two equally sized groups.\nActually… you know what? Chopping the data into \\(n\\) equally sized groups is something that happens so bloody often in practice that it deserves to have a specialist function. Rather than tediously passing \\(n+1\\) quantiles to the quantile() function and then passing the result to cut(), it would be nice to have a specialised function that handles this case. That’s the job of chop_equally():\n\ntibble(\n  value = runif(10, min = 1, max = 100), \n  group = chop_equally(value, 2)\n)\n\n# A tibble: 10 × 2\n   value group         \n   &lt;dbl&gt; &lt;fct&gt;         \n 1 91.4  [47.41, 91.37]\n 2 30.1  [9.34, 47.41) \n 3 46.4  [9.34, 47.41) \n 4 33.9  [9.34, 47.41) \n 5 65.4  [47.41, 91.37]\n 6 26.5  [9.34, 47.41) \n 7 48.4  [47.41, 91.37]\n 8 76.9  [47.41, 91.37]\n 9  9.34 [9.34, 47.41) \n10 87.7  [47.41, 91.37]\n\n\nLovely. I mean, compare the pair…\n\n# base R\ncut(\n  value,\n  breaks = quantile(value, c(0, .5, 1)),\n  include.lowest = TRUE\n)\n\n# santoku\nchop_equally(value, groups = 2)\n\nIf you have to do the task on a daily basis, you really want to use the second version. It’s easy to write, easy to read, and it “just works”. True, if you’re a package developer or an R user who doesn’t have to discretise continuous data regularly, it might not be worth the effort of learning santoku. But for everyday data wrangling? As someone who has5 to do this over and over again… yes please.\nFundamentally, this is the value of specialised tools. I honestly don’t think there’s anything wrong with the cut() function in base R. Most languages wouldn’t even supply something like cut() as part of the core language because most programmers don’t have a need for it, but R is not a typical language: it is a language for data analysts and it is designed to allow data analysts to – and I’m going to quote Hadley Wickham here – “get shit done”. The R core devs know that this matters in data analysis so there is a cut() function that sits at this sweet spot that is simple and convenient for a typical data analyst who has to very occasionally solve this problem. But if you’re doing it a lot… oh my the specialist functions provided by santoku really make a difference."
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#you-had-me-at-hello-but",
    "href": "posts/2023-05-22_santoku/index.html#you-had-me-at-hello-but",
    "title": "Santoku",
    "section": "You had me at “hello” but…",
    "text": "You had me at “hello” but…\nAt this point, I have to be honest. I liked santoku when I first looked at the defaults for chop(), and I was already in love when I saw chop_equally(). Anything else beyond those two functions is icing on the cake that I was already salivating over. As it happens, however, santoku goes quite a long way past these things. There are so many gorgeous details in how it works that you just know that the author has suffered some version of the same fucking pain I have suffered when trying to discretise a variable that is “technically” continuous but kinda isn’t really.\nWant an example? Of course you do.\nOne of the many weird topics that I spent far too much of my former life working on is human probability judgments. Like, in some experiment, the researcher would present participants with some scenario or proposition, and then actual human beings would be asked to provide a numeric rating corresponding to the probability that the scenario will actually happen, or that the proposition is actually true.\nWhen you think about this as a mathematical or inferential question, it’s well posed. “What is the probability of rain tomorrow?” is a perfectly sensible question and the set of admissable answers corresponds to the unit interval: 0 means impossible, 1 means definite, and every number in between is a quantified measure of uncertainty. Literally this is the sort of question that the discipline of meteorology exists to answer. Meteorologists, in fact, are shockingly good at answering these questions.\nThe rest of us, however, are not so good.\nOne of the nastiest features of probability judgment data arises because the natural language semantics of chance don’t map cleanly onto the mathematics of probability. In everyday usage, people often use “fifty-fifty” as a non-literal expression to that rougly means “I have no fucking idea what will happen”. So when somebody responds with “50%” to a probability question, the interpretation of that answer is ambiguous. Sometimes it means what it says it means: “the event is exactly as likely to happen as not happen”. But at other times it means: “fucked if I know”. Those aren’t equivalent statements, not by a long shot. And what happens in practice when you analyse data from those kinds of studies is that your notionally-continuous variable has a massive spike at 0.5, because people are doing something with the question that isn’t quite in alignment with the formal logic of uncertainty. That is to say, there is a qualitative difference between 49% and 50%. “50%” is a special category.\nOkay, so let’s create a silly vector that captures this intuition.\n\nsigh &lt;- c(.1234, .45634, .5, .5, .5, .5842, .95234)\n\nIn real life, human participants don’t give responses to several decimal places, but you get the idea. There are three people who have responded with .5 because that’s a special value, and there are four people who have given other answers.\nHow would I carve this into three categories using cut()? It’s clunky but doable. You create intervals that are a tiny bit above and a tiny bit below the special value:\n\ntibble(\n  value = sigh,\n  group = cut(value, breaks = c(0, .49999, .50001, 1))\n)\n\n# A tibble: 7 × 2\n  value group            \n  &lt;dbl&gt; &lt;fct&gt;            \n1 0.123 (0,0.49999]      \n2 0.456 (0,0.49999]      \n3 0.5   (0.49999,0.50001]\n4 0.5   (0.49999,0.50001]\n5 0.5   (0.49999,0.50001]\n6 0.584 (0.50001,1]      \n7 0.952 (0.50001,1]      \n\n\nIt works, but it’s kind of weird. It feels like a workaround because it is: cut() doesn’t allow you to specify a specific value as a category:\n\ntibble(\n  value = sigh,\n  group = cut(value, breaks = c(0, .5, .5, 1))\n)\n\nError in cut.default(value, breaks = c(0, 0.5, 0.5, 1)): 'breaks' are not unique\n\n\nAgain, this is behaviour that makes sense mathematically if you are dealing with strictly continuous data: a set that has measure zero in the domain will necessarily include zero observations when presented with a finite sample from a continuous distribution specified over that domain. But… real life is not like that. You absolutely can observe data from distributions that are best modelled as a mixture of discrete point masses blended with an underlying continuous distribution,6 and so you need data chopping tools that accommodate continuous ranges and discrete points at the same time.\nHappily, chop() does precisely this:\n\ntibble(\n  value = sigh,\n  group = chop(value, breaks = c(0, .5, .5, 1))\n)\n\n# A tibble: 7 × 2\n  value group   \n  &lt;dbl&gt; &lt;fct&gt;   \n1 0.123 [0, 0.5)\n2 0.456 [0, 0.5)\n3 0.5   {0.5}   \n4 0.5   {0.5}   \n5 0.5   {0.5}   \n6 0.584 (0.5, 1]\n7 0.952 (0.5, 1]\n\n\nNotice how the labels for the intervals automatically adjust to accommodate this specification. The point mass interval in the middle is labelled in discrete set notation as {0.5}, and the continuous intervals that surround it are both open on that side, leading to [0, 0.5) as the label on the lower side and (0.5, 1] as the label on the upper side. It’s the exactly correct way to refer to these categories, and while cut() probably doesn’t need this level of nuance, it is deeply appreciated when you have to work with “fuck ugly”7 data that arise when you run a behavioural experiment with probability judgments as an outcome variable.\nYou love to see it, honestly."
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#want-more",
    "href": "posts/2023-05-22_santoku/index.html#want-more",
    "title": "Santoku",
    "section": "Want more?",
    "text": "Want more?\nWhen I started writing this post earlier this morning I promised myself I wouldn’t turn it into a whole thing the way I usually do. This is not meant to be a comprehensive treatment or a deep dive or whatever. Today is a day off for me and I’m having fun playing with a thing that I enjoy. So I shall resist the temptation to dive too much deeper. Nevertheless, I’ll add a few other remarks that give a sense of why I love the santoku package. First, it has convenience functions that allow you to automatically tabulate. Let’s say I have a bunch of dull, normally distributed observations:\n\nx &lt;- rnorm(100)\n\nAs described above, I can chop them into 5 equally sized groups like this:\n\nchop_equally(x, groups = 3)\n\n  [1] [-1.805, -0.3243) [-1.805, -0.3243) [-0.3243, 0.3842)\n  [4] [0.3842, 2.402]   [0.3842, 2.402]   [-0.3243, 0.3842)\n  [7] [-0.3243, 0.3842) [0.3842, 2.402]   [0.3842, 2.402]  \n [10] [-1.805, -0.3243) [-1.805, -0.3243) [-0.3243, 0.3842)\n [13] [0.3842, 2.402]   [-0.3243, 0.3842) [0.3842, 2.402]  \n [16] [0.3842, 2.402]   [-1.805, -0.3243) [-0.3243, 0.3842)\n [19] [-1.805, -0.3243) [0.3842, 2.402]   [0.3842, 2.402]  \n [22] [-1.805, -0.3243) [-1.805, -0.3243) [0.3842, 2.402]  \n [25] [-0.3243, 0.3842) [0.3842, 2.402]   [-0.3243, 0.3842)\n [28] [0.3842, 2.402]   [-0.3243, 0.3842) [-1.805, -0.3243)\n [31] [-0.3243, 0.3842) [-1.805, -0.3243) [0.3842, 2.402]  \n [34] [-0.3243, 0.3842) [0.3842, 2.402]   [0.3842, 2.402]  \n [37] [-1.805, -0.3243) [0.3842, 2.402]   [-1.805, -0.3243)\n [40] [-1.805, -0.3243) [-0.3243, 0.3842) [-1.805, -0.3243)\n [43] [-0.3243, 0.3842) [-0.3243, 0.3842) [-1.805, -0.3243)\n [46] [-1.805, -0.3243) [-0.3243, 0.3842) [0.3842, 2.402]  \n [49] [-1.805, -0.3243) [0.3842, 2.402]   [-0.3243, 0.3842)\n [52] [0.3842, 2.402]   [-0.3243, 0.3842) [-0.3243, 0.3842)\n [55] [-0.3243, 0.3842) [-1.805, -0.3243) [0.3842, 2.402]  \n [58] [0.3842, 2.402]   [0.3842, 2.402]   [0.3842, 2.402]  \n [61] [0.3842, 2.402]   [-1.805, -0.3243) [-1.805, -0.3243)\n [64] [-1.805, -0.3243) [-1.805, -0.3243) [-1.805, -0.3243)\n [67] [-0.3243, 0.3842) [-1.805, -0.3243) [-0.3243, 0.3842)\n [70] [-1.805, -0.3243) [0.3842, 2.402]   [0.3842, 2.402]  \n [73] [0.3842, 2.402]   [0.3842, 2.402]   [0.3842, 2.402]  \n [76] [-1.805, -0.3243) [-1.805, -0.3243) [0.3842, 2.402]  \n [79] [-1.805, -0.3243) [-0.3243, 0.3842) [-1.805, -0.3243)\n [82] [-0.3243, 0.3842) [-0.3243, 0.3842) [0.3842, 2.402]  \n [85] [-0.3243, 0.3842) [-1.805, -0.3243) [0.3842, 2.402]  \n [88] [-0.3243, 0.3842) [-0.3243, 0.3842) [-0.3243, 0.3842)\n [91] [0.3842, 2.402]   [-0.3243, 0.3842) [-0.3243, 0.3842)\n [94] [-1.805, -0.3243) [-0.3243, 0.3842) [-0.3243, 0.3842)\n [97] [-1.805, -0.3243) [0.3842, 2.402]   [-1.805, -0.3243)\n[100] [-0.3243, 0.3842)\nLevels: [-1.805, -0.3243) [-0.3243, 0.3842) [0.3842, 2.402]\n\n\nBut what if I want to tabulate them? That’s a spectacularly common task for discretised data, and in base R I have the table() function for this purpose, so I could turn this into a pipeline and write my code like this if I wanted:\n\nx |&gt;\n  chop_equally(groups = 3) |&gt;\n  table()\n\n\n[-1.805, -0.3243) [-0.3243, 0.3842)   [0.3842, 2.402] \n               33                33                34 \n\n\nExcept, because santoku has anticipated this as a need and provided me with a tab_equally() function that tabulates the data but otherwise mirrors chop_equally(), I don’t have to:\n\ntab_equally(x, groups = 3)\n\n[-1.805, -0.3243) [-0.3243, 0.3842)   [0.3842, 2.402] \n               33                33                34 \n\n\nAh, lovely.\nBetter yet, there are quite a lot of specialised chop_*() functions that have corresponding tab_*() functions.\n\nchop_equally() splits the data into equally frequent groups\nchop_evenly() splits the data into groups of equal width\nchop_mean_sd() splits the data into groups of equal numbers of standard deviations around the mean\n… and so on.\n\nThere’s several of these and they all have handy tab_*() analogs. So, on the off chance I’ve forgotten the horrible lessons drummed into me from first year stats, let’s have a look at what I can expect from 100K normally distributed observations…\n\ntab_mean_sd(rnorm(100000), sds = 1:3)\n\n[-Inf sd, -3 sd)   [-3 sd, -2 sd)   [-2 sd, -1 sd)    [-1 sd, 0 sd) \n             134             2161            13666            33934 \n    [0 sd, 1 sd)     [1 sd, 2 sd)     [2 sd, 3 sd)   [3 sd, Inf sd] \n           34292            13487             2196              130 \n\n\nSuffice it to say, if you’ve ever been forced to memorise the “X% of data falls within Y standard deviations of the mean” dogma that surrounds the tyranny of the normal,8 these numbers will be painfully unsurprising.\nMore generally though, it can be super helpful when doing exploratory data analysis to rely on these tab_*() functions. For example, the basic chop() function has a tab() analog, and if I return to my annoying sigh vector, I can use tab() to quickly check if there was actually a good reason for me to treat 50% as a “special” value:\n\ntab(sigh, breaks = c(0, .5, .5, 1))\n\n[0, 0.5)    {0.5} (0.5, 1] \n       2        3        2 \n\n\nYeah, there really was.\nBut wait! There’s more!9 The santoku package also comes with some handy labeller functions that make it easy to construct sensible labels for your categories. For instance, lbl_glue():\n\ntab(sigh, breaks = c(0, .5, .5, 1), labels = lbl_glue(\"from {l} to {r}\"))\n\n  from 0 to 0.5 from 0.5 to 0.5   from 0.5 to 1 \n              2               3               2 \n\n\nOoh pretty."
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#summary",
    "href": "posts/2023-05-22_santoku/index.html#summary",
    "title": "Santoku",
    "section": "Summary",
    "text": "Summary\nAaaanyway, I promised myself I wasn’t going to go overboard in this post. I was going to throw it together quickly and then shove it out even if it wasn’t perfect. Life is like that. Really the take home message in all this is that santoku is a really nice, specialised package. You don’t need it until you need it, but when you do… you know.\nI’ll leave you to explore the whole thing should you feel the need. The point here is really that the santoku package really does capture all the little bits and pieces that you might need when addressing the “discretising a continuous variable” problem. In any specific data pipeline you probably wouldn’t expect to call it more than once or twice in the final version, but it’s extremely helpful in getting you to that final version, and you might find you end up using it a lot when the same damn problems show up over and over."
  },
  {
    "objectID": "posts/2023-05-22_santoku/index.html#footnotes",
    "href": "posts/2023-05-22_santoku/index.html#footnotes",
    "title": "Santoku",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe package takes its name from the santoku bōchō, a Japanese kitchen knife. The name, I’m told, translates to “three virtues”, indicating that the knife is used for cutting, slicing, and chopping.↩︎\nIndeed there is a whole mini-literature about if and when it is safe to discretise a continuous variable in the context of statistical inference. I’m not going to talk about it in this post, but suffice it to say that if you’re a psychologist thinking of forwarding me “that paper” by Kris Preacher and others, trust me I’m fully aware of it. Kris and I go way back.↩︎\nI have a mild personal preference for the chop() default, because I tend to think that break points specify lower bounds rather than upper bounds, but that’s probably a me thing.↩︎\nThe cut() default is to always use half-closed intervals, which makes sense as a mathematical operation, but it’s irritating from a practical data analysis perspective because we almost never want to exclude one extremum of the data solely because the technical definition of an empirical quantile leads to the exclusion of one end point.↩︎\nOkay I should probably use past tense here: I don’t have this problem so much these days, being unemployed and all.↩︎\nStatistics has had formal tools for dealing with this for a long time. I imagine it existed earlier, but for me at least the oldest tool I’ve used comes from Ferguson 1973 paper introducing the Dirichlet process. If your observations are sampled from a DP with an absolutely continuous base distribution, the posterior will be a DP whose base distribution is mixture of the continuous base distribution and the discrete base distribution. Life has continuous and discrete parts, so of course we see this in our data.↩︎\nThat’s a technical term.↩︎\nI’m so annoyed that there isn’t an obvious wikipedia page I can link to here. The central limit theorem is lovely, honest. I fucking love it. But, alas, not all measurable things in this world are U-statistics, and sometimes you can’t fucking rely on the CLT as a justification for your laziness. Extreme value distributions and other more fucked-up things have an obnoxious tendency to appear in the wild and you bloody well have to deal with it.↩︎\nOkay, I’m sort of assuming that you know the old trope of late night hour-long paid advertisement tv shows that would inevitably end with the announcer saying “but wait there’s more” and throwing in a set of steak knives for free, which I totally want when I’m buying a vacuum cleaner. It occurs to me that my children have literally never seen one of these obnoxious things, and would have no idea what I’m alluding to in this, a post about chopping things with santoku.↩︎"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html",
    "title": "Unpredictable paintings",
    "section": "",
    "text": "Almost two years (2020-01-15) ago I wrote this blog post as an introduction to generative art in R. The idea behind the post was to start making a new generative art system from scratch, and write the blog post at the same time. By doing it that way the reader can see how the process unfolds and how many false starts and discarded ideas a generative artist tends to go through, even for a simple system like this one. The post disappeared when I moved my blog to its own subdomain and its own repository, but I’ve now managed to rescue it! Hope it’s helpful…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#introduction",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#introduction",
    "title": "Unpredictable paintings",
    "section": "Introduction",
    "text": "Introduction\nOver the last year or so I’ve taken up generative artwork as a hobby, and I am occasionally asked to write an introduction to the subject… how does one get started in generative art? When I started posting the code for my generative art to my “rosemary” repository I has this to say about my subjective experience when making artwork,\n\nMaking generative artwork reminds me a lot of gardening. Both are aesthetic exercise, creating a pleasant and relaxing evironment that the artist/gardener can enjoy no less than anyone visiting the space. Both are hard work, too. Learning how to prune, learning which plants will thrive in the land that you have, knowing what nutrients differnt plants need, taking care of the garden in hard times, et cetera, none of these are easy. At the end you might have a sustainable native garden that blends in seamlessly with the environment and brings you joy, but growing the garden is itself a technical and sometimes physically demanding exercise. The analogy between gardening and generative artwork feels solid to me, but it’s not amazingly helpful if you want to start making this kind of art. If you want to start gardening, you probably don’t really want a fancy gardener to talk about their overall philosophy of gardens, you’d like a few tips on what to plant, how often to water and so on. This post is an attempt to do that, and like so many things in life, it is entirely Mathew Ling’s fault.\n\nThe first thing to say about generative artwork is that it’s really up to you how you go about it. I do most of my programming using R, so that’s the language I use for my artwork. Most of the artwork I’ve been making lately has relied on the ambient package for the “generative” component, but to be honest you don’t have to rely on fancy multidimensional noise generators or anything like that. You can use the standard pseudorandom number generators built into R to do the work. Since the point of this post is to talk about “how to get started”, this is exactly what I’ll do!\nIn fact, what I’m going to do in this post is build a new system for generative art… I’m not sure what I’m going to end up with or if it will be any good, but let’s see where it goes! For the purposes of this post I’m assuming that you’re somewhat familiar with the tidyverse generally and ggplot2 specifically, and are comfortable writing functions in R. There’s a couple of spots where I do something slightly more complex, but I’ll explain those when they pop up. So, here goes…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#do-something-anything",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#do-something-anything",
    "title": "Unpredictable paintings",
    "section": "Do something, anything",
    "text": "Do something, anything\nTruth be told, I almost never have a plan when I start building a new system. What I do is start playing with pictures that visualise random data in some fashion, and see where that takes me. So, okay… I’ll start out creating a data frame that contains random numbers: each row in the data frame is a single “point”, and each column specifies an attribute: an x variable specifying a horizontal co-ordinate, a y variable specifying the vertical location, and a g variable that randomly assigns each of point to a “group” of some kind. At this point in time I have no idea how I’m going to use this information:\n\nlibrary(tidyverse)\nset.seed(1)\nobj &lt;- tibble(\n  x = rnorm(100), \n  y = rnorm(100), \n  g = sample(10, 100, TRUE)\n)\nobj\n\n# A tibble: 100 × 3\n        x       y     g\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 -0.626 -0.620      1\n 2  0.184  0.0421     3\n 3 -0.836 -0.911     10\n 4  1.60   0.158      7\n 5  0.330 -0.655      4\n 6 -0.820  1.77       1\n 7  0.487  0.717      9\n 8  0.738  0.910      7\n 9  0.576  0.384      6\n10 -0.305  1.68       4\n# ℹ 90 more rows\n\n\nSomething to note about this code is that I used set.seed(1) to set the state of the random number generator in R. This will ensure that every time I call the same “random” code I will always get the same output. To get a different output, I change the seed to something different.\nSo I guess the first thing I’ll do is try a scatterplot:\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_point(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\n\n\n\n\nOkay, yeah that’s scatterplot. I’m not feeling inspired here, but it does occur to me that I’ve seen some very pretty hexbin plots in the past and maybe there’s some fun I could have playing with those?\n\nggplot(obj, aes(x, y)) +\n  geom_hex(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\n\n\n\n\nHm. Interesting? Maybe I could split this by group and try overlaying different hexagonal shapes? That sometimes makes for a neat three-dimensional feel when two hexagonal grids are offset from one another… okay let’s pursue that for a bit…\n[an hour passes in which I draw many boring plots]\n…yeah, okay I’ve got nothing. It seemed like a good idea but I couldn’t make anything I really liked. This is, in my experience, really common. I go down quite a few blind alleys when making a generative system, discard a lot of things that don’t seem to do what I want. It’s an exploration process and sometimes when you explore you get lost. Oh well, let’s try something else. Instead of drawing a scatterplot, let’s connect the dots and draw some lines:\n\nggplot(obj, aes(x, y, colour = g)) +\n  geom_path(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\n\n\n\n\nHm. A bit scribbly, but there’s something aesthetically pleasing there. Okay, what if I decided to turn the paths into polygons?\n\nggplot(obj, aes(x, y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void()\n\n\n\n\n\n\n\n\nOkay, this feels promising. It reminds me a bit of the time I accidentally drew some really pretty pictures by setting axis limits inappropriately when drawing kernel density estimates with ggplot2, and ended up using it as a way to explore the scico package. Let’s run with this…"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#mix-it-up-a-bit",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#mix-it-up-a-bit",
    "title": "Unpredictable paintings",
    "section": "Mix it up a bit",
    "text": "Mix it up a bit\nTo try to get a sense of what you can do with a particular approach, it’s usually helpful to try out some variations. For example, the previous plot uses the ggplot2 default palette, which isn’t the most appealing colour scheme. So let’s modify the code to use palettes from the scico package. One of my favourites is the lajolla palette:\n\nlibrary(scico)\nggplot(obj, aes(x,y, fill = g, group = g)) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\n\n\n\n\nThis is definitely neat. I do like the “jagged little polygons” feel to this, but to be honest I’m getting a bit bored. I’ve done a few different art pieces that exploit this effect before, and this isn’t the most exciting thing for me, so I want to push things in a different direction. Speaking of which, I’m not sure I want all the polygons to lie on top of each other so much, so what I’ll do is create a small tibble called grp that specifies a random “offset” or “shift” for each group, and then using full_join() from dplyr to merge it into the data object:\n\ngrp &lt;- tibble(\n  g = 1:10,\n  x_shift = rnorm(10),\n  y_shift = rnorm(10)\n)\nobj &lt;- full_join(obj, grp)\nobj\n\n# A tibble: 100 × 5\n        x       y     g x_shift y_shift\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.626 -0.620      1   1.13   -1.00 \n 2  0.184  0.0421     3   0.741   0.945\n 3 -0.836 -0.911     10  -0.581   1.78 \n 4  1.60   0.158      7  -0.408   0.376\n 5  0.330 -0.655      4  -1.32    0.434\n 6 -0.820  1.77       1   1.13   -1.00 \n 7  0.487  0.717      9  -0.701  -1.43 \n 8  0.738  0.910      7  -0.408   0.376\n 9  0.576  0.384      6   0.398  -0.390\n10 -0.305  1.68       4  -1.32    0.434\n# ℹ 90 more rows\n\n\nSo now I can adjust my ggplot2 code like this. Instead of defining each polygon in terms of the x and y columns, I’ll add the x_shift and y_shift values so that each polygon gets moved some distance away from the origin. This is kind of helpful, because now I can see more clearly what my objects actually look like!\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\n\n\n\n\nVery pretty! But as I said, I’m bored with the “jagged little polygon” look, so what I want to do is find some way of changing the appearance of the shapes."
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#cran-is-a-girls-best-friend",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#cran-is-a-girls-best-friend",
    "title": "Unpredictable paintings",
    "section": "CRAN is a girl’s best friend",
    "text": "CRAN is a girl’s best friend\nAt this point in my process I was a bit lost for ideas. I want to do something different, and I think what I want to do is turn each point set into more of a regular shape, something without holes in it. It then occurred to me that way back in 1998 I did my honours thesis on combinatorial optimisation problems in neuropsychological testing and had played around with things like the Travelling Salesperson Problem (TSP) and remembered that the solutions to two-dimensional planar TSPs can sometimes be quite pretty. A few minutes on google uncovers the TSP package, and a few more minutes playing around with the API gives me a sense of what I need to do in order to work out what order to connect the points in order to generate a TSP solution:\n\nlibrary(TSP)\ntour &lt;- function(obj) {\n  obj$tour &lt;- unname(c(solve_TSP(ETSP(obj[, c(\"x\", \"y\")]))))\n  arrange(obj, order(tour))\n}\n\nThe code here is very ugly because I wrote it in a rush. The gist of it is that what you want to do normally is feed a data frame to the ETSP() function, which creates the data structure needed to solve the corresponding optimisation problem. The output is then passed to solve_TSP() which can produce an approximate solution via one of many different algorithms, and that then returns a data structure (as an S3 object) that specifies the order in which the points need to be connected, along with some handy metadata (e.g., the length of the tour). But I don’t want any of that information, so I use c() and unname() to strip all that information out, append the resulting information to the data object, and then use the arrange() function from the dplyr package to order the data in the desired fashion.\nNext, because I want to apply the tour() function separately to each group rather than to compute a TSP solution for the overall data structure, I use group_split() to split the data set into a list of data frames, one for each group, and then map_dfr() to apply the tour() function to each element of that list and bind the results together into a data frame:\n\nobj &lt;- obj %&gt;%\n  group_split(g) %&gt;%\n  map_dfr(~tour(.x))\nobj\n\n# A tibble: 100 × 6\n         x      y     g x_shift y_shift  tour\n     &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;\n 1 -0.626  -0.620     1    1.13   -1.00     1\n 2 -0.165  -1.91      1    1.13   -1.00     9\n 3  0.267  -0.926     1    1.13   -1.00     3\n 4 -0.103  -0.589     1    1.13   -1.00     8\n 5  0.370  -0.430     1    1.13   -1.00     4\n 6  0.557  -0.464     1    1.13   -1.00     5\n 7  2.17    0.208     1    1.13   -1.00     2\n 8  0.821   0.494     1    1.13   -1.00    10\n 9 -0.820   1.77      1    1.13   -1.00     7\n10 -0.0162 -0.320     1    1.13   -1.00     6\n# ℹ 90 more rows\n\n\nNow when I apply the same plotting code to the new data object, here’s what I get:\n\nggplot(\n  data = obj, \n  mapping = aes(\n    x = x + x_shift, \n    y = y + y_shift, \n    fill = g, \n    group = g\n  )\n) +\n  geom_polygon(show.legend = FALSE) + \n  coord_equal() + \n  theme_void() + \n  scale_fill_scico(palette = \"lajolla\")\n\n\n\n\n\n\n\n\nOoh, I like this."
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#formalise-a-system",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#formalise-a-system",
    "title": "Unpredictable paintings",
    "section": "Formalise a system…",
    "text": "Formalise a system…\nThe next step in the process is to take all the moving parts and write a system. The exact details of what consitutes a generative art system is a little vague, but I usually think of it as a collection of functions that capture the essence of the process. If I’m being fancy I’ll convert this set of functions to a full-fledged R package, but let’s not bother with that for this simple system. So what do we need? First, I’ll state the dependencies:\n\nlibrary(tidyverse)\nlibrary(scico)\nlibrary(TSP)\n\nNext, let’s keep the tour() function as a separate thing. It’s one way of organising the points that belong to the same group, but there might be others:\n\ntour &lt;- function(obj) {\n  tsp &lt;- ETSP(obj[,c(\"x\",\"y\")])\n  obj$tour &lt;- unname(c(solve_TSP(tsp)))\n  arrange(obj, order(tour))\n}\n\nMy personal style is to separate the “builder” functions that generate the underlying data structure from the “styling” functions that render that data structure as an image. For the current project, our builder function is build_art() and defined as follows:\n\nbuild_art &lt;- function(\n  points = 100,   # total number of points\n  groups = 10,    # number of groups\n  polygon = tour, # function used to organise points\n  gap = 1,        # standard deviation of the \"shift\" separating groups\n  seed = 1        # numeric seed to use\n) {\n  \n  # set the seed\n  set.seed(seed)\n  \n  # create the initial data frame\n  obj &lt;- tibble(\n    x = rnorm(points), \n    y = rnorm(points), \n    g = sample(groups, points, TRUE)\n  )\n  \n  # create the offset for each group\n  grp &lt;- tibble(\n    g = 1:groups,\n    x_shift = rnorm(groups) * gap,\n    y_shift = rnorm(groups) * gap\n  )\n  \n  # merge obj with grp\n  obj &lt;- full_join(obj, grp, by = \"g\") \n  \n  # split obj by group and apply the \"polygon\" mapping\n  # function separately to each group\n  obj &lt;- obj %&gt;%\n    group_split(g) %&gt;%\n    map_dfr(~polygon(.x))\n  \n  return(obj) # output\n}\n\nAs you can see, it’s more or less the same as the code I developed for my original example, just written with a little more abstraction so that I can feed in different parameter values later. The draw_art() function takes this object as input, and creates a plot using the same ggplot2 code. The only free “parameter” here is the ... that I can use to pass arguments to the palette function:\n\ndraw_art &lt;- function(obj, ...) {\n  ggplot(\n    data = obj, \n    mapping = aes(\n      x = x + x_shift, \n      y = y + y_shift, \n      fill = g, \n      group = g\n    )\n  ) +\n    geom_polygon(show.legend = FALSE) + \n    coord_equal() + \n    theme_void() + \n    scale_fill_scico(...)\n}\n\nNow we’re ready to go! Because I set it up so that every parameter has a default value that corresponds to the same parameters I used to draw the original picture, this code reproduces the original image:\n\nbuild_art() %&gt;% \n  draw_art()\n\n\n\n\n\n\n\n\n… well, almost!"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#vary-parameters",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#vary-parameters",
    "title": "Unpredictable paintings",
    "section": "Vary parameters…",
    "text": "Vary parameters…\nOkay, the one thing that I didn’t do is specify the default palette. In the scico package the default palette is “bilbao”, and the original artwork I produced used the “lajolla” palette. So the default output of the system is identical to this:\n\nbuild_art(seed = 1) %&gt;% \n  draw_art(palette = \"bilbao\")\n\n\n\n\n\n\n\n\nIf I’d set palette = \"lajolla\" I’d have obtained exactly the same result as before. But let’s play around a little bit. If I switch to the “vik” palette I get output with the same shapes, just with a different colours scheme:\n\nbuild_art(seed = 1) %&gt;% \n  draw_art(palette = \"vik\")\n\n\n\n\n\n\n\n\nHowever, if I modify the seed argument as well I get different random points, and so the resulting shapes are different.\n\nbuild_art(seed = 2) %&gt;% \n  draw_art(palette = \"vik\")\n\n\n\n\n\n\n\n\nMore generally, I can play around with my new system and find out what it is capable of. Here’s a version with 1000 points divided into 5 groups with a fairly modest offset:\n\nbuild_art(\n  points = 1000, \n  groups = 5,\n  gap = 2\n) %&gt;% \n  draw_art(\n    palette = \"vik\", \n    alpha = .8\n  )\n\n\n\n\n\n\n\n\nThe shapes aren’t quite what I was expecting: I’m not used to seeing TSP solutions rendered as polygons, because they’re usually drawn as paths, and they make me think of crazy shuriken or maybe really screwed up snowflakes. Not as organic as I thought it might look, but still neat. Notice that I’ve also made the shapes slightly transparent by setting the alpha argument that gets passed to scale_fill_scico(). Okay, let’s play around a bit more:\n\nbuild_art(\n  points = 5000, \n  groups = 20,\n  gap = 7\n) %&gt;% \n  draw_art(\n    palette = \"bamako\", \n    alpha = .8\n  )\n\n\n\n\n\n\n\n\nThis is kind of neat too, but I want to try something different. The general pattern for a TSP solution is that they take on this snowflake/shuriken look when there are many points, but not when there are fewer data points. So this time I’ll have 10000 points in total, but divide them among 1000 groups so that on average each polygon is defined by 10 vertices. I’ll space them out a little bit more too, and…\n\nbuild_art(\n  points = 10000, \n  groups = 1000,\n  gap = 15, \n  seed = 10\n) %&gt;% \n  draw_art(palette = \"tokyo\")\n\n\n\n\n\n\n\n\nI kind of love it!"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#have-fun-exploiting-loopholes",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#have-fun-exploiting-loopholes",
    "title": "Unpredictable paintings",
    "section": "Have fun exploiting loopholes",
    "text": "Have fun exploiting loopholes\nAt the beginning, when I created the system, I set tour() to be the default polygon function used to modify each polygon. My original plan was that this function was really just supposed to be used to order the points, but there’s actually nothing in the system that prevents me from doing something fancier. For example, here’s a sneaky trick where the function calls dplyr::mutate() before passing the data for that group to the tour() function. In this case, what I’ve done is a dilation transformation: the overall size of each group is multiplied by the group number g, so now the shapes will lie on top of each other with different scales. It also, in another slightly sneaky trick, flips the sign of the group number which will ensure that when the data gets passed to draw_art() the order of the colours will be reversed. The result…\n\nshift_tour &lt;- function(obj) {\n  obj %&gt;% \n    mutate(\n      x = x * g, \n      y = y * g, \n      g = -g\n    ) %&gt;%\n    tour()\n}\n\nbuild_art(\n  points = 5000,\n  groups = 200,\n  gap = 0,\n  polygon = shift_tour\n) %&gt;% draw_art(palette = \"oslo\")\n\n\n\n\n\n\n\n\n… is really quite lovely. Later on, I might decide that this little trick is worth bundling into another function, the system gains new flexibility, and the range of things you can do by playing around with it expands. But I think this is quite enough for now, so it’s time to move on to the most important step of all …"
  },
  {
    "objectID": "posts/2021-11-01_unpredictable-paintings/index.html#tweet-it",
    "href": "posts/2021-11-01_unpredictable-paintings/index.html#tweet-it",
    "title": "Unpredictable paintings",
    "section": "Tweet it!",
    "text": "Tweet it!\nBecause what’s the point of making art if you can’t share it with people?"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html",
    "title": "Visualising the hits of Queen Britney",
    "section": "",
    "text": "I’ve never participated in Tidy Tuesday before, but because I’ve now joined a slack that does, it is high time I did something about that poor track record. I wasn’t sure what I wanted to do with this week’s “Billboard” data, other than I wanted it to have something to do with Britney Spears (because she’s awesome). After going back and forward for a while, I decided what I’d do is put together a couple of plots showing the chart performance of all her songs and – more importantly – write it up as a blog post in which I try to “over-explain” all my choices. There are a lot of people in our slack who haven’t used R very much, and I want to “unpack” some of the bits and pieces that are involved. This post is pitched at beginners who are hoping for a little bit of extra scaffolding to explain some of the processes…"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-the-data-on-github",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-the-data-on-github",
    "title": "Visualising the hits of Queen Britney",
    "section": "Finding the data on GitHub",
    "text": "Finding the data on GitHub\nEvery week the Tidy Tuesday data are posted online, and the first step in participating is generally to import the data. After a little bit of hunting online, you might discover that the link to the billboard data looks like this:\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nLet’s start by unpacking this link. There is a lot of assumed knowledge buried here, and while it is entirely possible for you to get started without understanding it all, for most of us in the slack group the goal is to learn new data science skills. At some point you are probably going to want to learn the “version control” magic. This post is not the place to learn this sorcery, but I am going to start foreshadowing some important concepts because they will be useful later.\n\nGitHub repositories\nThe place to start in understanding this link is the peculiar bit at the beginning: what is this “github” nonsense? The long answer is very long, but the short answer is that https://github.com is a website that programmers use to store their code. GitHub is one of several sites (e.g., https://gitlab.org, https://bitbucket.com) that are all built on top of a version control system called “git”. Git is a powerful tool that lets you collaborate with other people when writing code, allows you to keep track of the history of your code, and to backup your code online in case your laptop mysteriously catches on fire.\n\n\nIn the R community, “laptop fires” are universally understood to be a reference to what happens to you when you foolishly ignore the wise advice of Jenny Bryan\nGit is a complicated tool and it takes quite some time to get the hang of (I’m still learning, quite frankly), but it is worth your effort. When you have time, I recommend starting a free GitHub account. You can sign up using an email address, and if you have a university email address you get the educational discount (basically you get the “pro” version for free). My username on GitHub is djnavarro, and you can find my profile page here:\nhttps://github.com/djnavarro\nThe Tidy Tuesday project originated in the “R for data science” learning community, and there is a profile page for that community too:\nhttps://github.com/rfordatascience\n\n\nR for data science is a wonderful book by Hadley Wickham and Garrett Grolemund\nOkay, so that’s part of the link explained. The next thing to understand is that when you create projects using git and post them to GitHub, they are organised in a “repository” (“repo” for short). Each repo has its own page. The Tidy Tuesday repo is here:\nhttps://github.com/rfordatascience/tidytuesday\nIf you click on this link, you’ll find that there’s a nice description of the whole project, links to data sets, and a whole lot of other things besides.\nMost of the work organising this is done by Thomas Mock, and it’s very very cool.\n\n\nRepositories have branches\nWhenever someone creates a git repository, it will automatically have at least one “branch” (usually called “master” or “main”). The idea behind it is really sensible: suppose you’re working on a project and you think “ooooh, I have a cool idea I want to try but maybe it won’t work”. What you can do is create a new “branch” and try out all your new ideas in the new branch all without ever affecting the master branch. It’s a safe way to explore: if your new idea works you can “merge” the changes into the master branch, but if it fails you can switch back to the master branch and pick up where you left off. No harm done. If you have lots of branches, you effectively have a “tree”, and it’s a suuuuuuper handy feature. Later on as you develop your data science skills you’ll learn how to do this yourself, but for now this is enough information. The key thing is that what you’re looking at when you visit the Tidy Tuesday page on GitHub is actually the master branch on the tree:\nhttps://github.com/rfordatascience/tidytuesday/tree/master\n\n\nRepositories are usually organised\nThe Tidy Tuesday repository has a lot of different content, and it’s all nicely organised into folders (no different to the folders you’d have on your own computer). One of the folders is called “data”, and inside the “data” folder there is a “2021” folder:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021\nInside that folder you find lots more folders, one for every week this year. If you scroll down to the current week and click on the link, it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-09-14\nBeing the kind soul that he is, Thomas has included a “readme” file in this folder: it’s a plain markdown file that gets displayed in a nice human readable format on the github page. Whenever you’re doing a Tidy Tuesday analysis, it’s super helpful to look at the readme file, because it will provide you a lot of the context you need to understand the data. Whenever doing your own projects, I’d strongly recommend creating readme files yourself: they’re reeeeaaaaaally helpful to anyone using your work, even if that’s just you several months later after you’ve forgotten what you were doing. Over and over again when I pick up an old project I curse the me-from-six-months ago when she was lazy and didn’t write one, or feel deeply grateful to her for taking the time to write one.\n\n\nReadme files are your best friend. Seriously\nIn any case, one of the things you’ll see on that page is a link to the “billboard.csv” data. If you click on that link it will take you here:\nhttps://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-09-14/billboard.csv\nNotice that this doesn’t take you to the data file itself: it goes to a webpage! Specifically, it takes you to the “blob” link that displays some information about the file (notice the “blob” that has sneakily inserted itself into the link above?). In this case, the page won’t show you very much information at all because the csv file is 43.7MB in size and GitHub doesn’t try to display files that big! However, what it does give you is a link that tells you where they’ve hidden the raw file! If you click on it (which I don’t recommend), it will take you to the “raw” file located at…\nhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\nThis is the link that you might have discovered if you’d been googling to find the Billboard data. It’s a GitHub link, but GitHub uses the “raw.githubusercontent.com” site as the mechanism for making raw files accessible, which is why that part of the link has changed.\n\n\nI didn’t intend for this section to be this long, honest\n\n\nThe anatomy of the data link\nAll of this tedious exposition should (I hope) help you make sense of what you’re actually looking at when you see this link. In real life I would never bother to do this, but if you wanted to you could decompose the link into its parts. In the snippet below I’ll create separate variables in R, one for each component of the link:\n\nsite &lt;- \"https://raw.githubusercontent.com\"\nuser &lt;- \"rfordatascience\"\nrepo &lt;- \"tidytuesday\"\nbranch &lt;- \"master\"\nfolder1 &lt;- \"data\"\nfolder2 &lt;- \"2021\" \nfolder3 &lt;- \"2021-09-14\"\nfile &lt;- \"billboard.csv\"\n\nArgh. Wait. There’s something slightly off-topic that I should point out… one thing you might be wondering when you look at this snippet, is where that pretty “arrow” character comes from. Don’t be fooled. It’s not a special arrow character, it’s two ordinary characters. What I’ve actually typed is &lt;-, but this blog uses a fancypants font that contains a special ligature that makes &lt;- appear to be a single smooth arrow. The font is called “Fira Code”, and a lot of programmers use it on their blogs. Once you know the trick, it’s really nice because it does make the code a little easier to read, but it can be confusing if you’re completely new to programming! It’s one of those little things that people forget to tell you about :-)\nAnyway, getting back on topic. The URL (a.k.a. “link”) for the Billboard data is what you get when you paste() all these components together, separated by the “/” character:\n\ndata_url &lt;- paste(\n  site, \n  user, \n  repo, \n  branch,\n  folder1, \n  folder2, \n  folder3, \n  file, \n  sep = \"/\"\n)\n\ndata_url\n\n[1] \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-14/billboard.csv\"\n\n\nExciting stuff."
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#attaching-packages",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#attaching-packages",
    "title": "Visualising the hits of Queen Britney",
    "section": "Attaching packages",
    "text": "Attaching packages\nI’m relatively certain that everyone in the slack has been exposed to the idea of an “R package”. A package is a collection of R functions and data sets that don’t automatically come bundled with R, but are freely available online. The tidyverse, for example, is a collection of R packages that a lot people find helpful for data analysis, and you can install all of them onto your machine (or your RStudio Cloud project) by using this command:\n\ninstall.packages(\"tidyverse\")\n\nThis can take quite a while to complete because there are a lot of packages that make up the tidyverse! Once the process is completed, you will now be able to use the tidyverse tools. However, it’s important to recognise that just because you’ve “installed” the packages, it doesn’t mean R will automatically use them. You have to be explicit. There are three tidyverse packages that I’m going to use a lot in this post (dplyr, stringr, and ggplot2), so I’ll use the library() function to “attach” the packages (i.e. tell R to make them available):\n\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#importing-the-data",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#importing-the-data",
    "title": "Visualising the hits of Queen Britney",
    "section": "Importing the data",
    "text": "Importing the data\nAt this point we know where the data set is located, and we have some R tools that we can use to play around with it. The next step is reading the data into R. The readr package is part of the tidyverse, and it contains a useful function called read_csv() that can go online for you, retrive the billboard data, and load it into R. That’s cool and all but if you look at the library() commands above, I didn’t actually attach them. I didn’t want to do this because honestly I’m only going to use the readr package once, and it feels a bit silly to attach the whole package. Instead, what I’ll do is use the “double colon” notation :: to refer to the function more directly. When I write readr::read_csv() in R, what I’m doing is telling R to use the read_csv() function inside the readr package. As long as I have readr on my computer, this will work even if I haven’t attached it using library(). The technical name for this is “namespacing”, and if you hang around enough R programmers long enough that’s a word that will pop up from time to time. The way to think about it is that every package (e.g., readr) contains a collection of things, each of which has a name (e.g., “read_csv” is the name of the read_csv() function). So you can think of a “space” of these names… and hence the boring term “namespace”.\nOkay, let’s use a “namespaced” command to import the data, and assign it to a variable (i.e., give the data a name). I’ll call the data billboard:\n\nbillboard &lt;- readr::read_csv(data_url)\n\nRows: 327895 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): url, week_id, song, performer, song_id\ndbl (5): week_position, instance, previous_week_position, peak_position, wee...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe billboard data is a nice, rectangular data set. Every row refers to a specific song on a specific date, and tells you its position in the charts on that date. We can type print(billboard) to take a look at the first few rows and columns. In most situations (not all), you can print something out just by typing its name:\n\nbillboard\n\n# A tibble: 327,895 × 10\n   url                    week_id week_position song  performer song_id instance\n   &lt;chr&gt;                  &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n 1 http://www.billboard.… 7/17/1…            34 Don'… Patty Du… Don't …        1\n 2 http://www.billboard.… 7/24/1…            22 Don'… Patty Du… Don't …        1\n 3 http://www.billboard.… 7/31/1…            14 Don'… Patty Du… Don't …        1\n 4 http://www.billboard.… 8/7/19…            10 Don'… Patty Du… Don't …        1\n 5 http://www.billboard.… 8/14/1…             8 Don'… Patty Du… Don't …        1\n 6 http://www.billboard.… 8/21/1…             8 Don'… Patty Du… Don't …        1\n 7 http://www.billboard.… 8/28/1…            14 Don'… Patty Du… Don't …        1\n 8 http://www.billboard.… 9/4/19…            36 Don'… Patty Du… Don't …        1\n 9 http://www.billboard.… 4/19/1…            97 Don'… Teddy Pe… Don't …        1\n10 http://www.billboard.… 4/26/1…            90 Don'… Teddy Pe… Don't …        1\n# ℹ 327,885 more rows\n# ℹ 3 more variables: previous_week_position &lt;dbl&gt;, peak_position &lt;dbl&gt;,\n#   weeks_on_chart &lt;dbl&gt;\n\n\n\n\nFinally, some data!\nThis view helps you see the data in its “native” orientation: each column is a variable, each row is an observation. It’s a bit frustrating though because a lot of the columns get chopped off in the printout. It’s often more useful to use dplyr::glimpse() to take a peek. When “glimpsing” the data, R rotates the data on its side and shows you a list of all the variables, along with the first few entries for that variable:\n\nglimpse(billboard)\n\nRows: 327,895\nColumns: 10\n$ url                    &lt;chr&gt; \"http://www.billboard.com/charts/hot-100/1965-0…\n$ week_id                &lt;chr&gt; \"7/17/1965\", \"7/24/1965\", \"7/31/1965\", \"8/7/196…\n$ week_position          &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 14, 36, 97, 90, 97, 97, 9…\n$ song                   &lt;chr&gt; \"Don't Just Stand There\", \"Don't Just Stand The…\n$ performer              &lt;chr&gt; \"Patty Duke\", \"Patty Duke\", \"Patty Duke\", \"Patt…\n$ song_id                &lt;chr&gt; \"Don't Just Stand TherePatty Duke\", \"Don't Just…\n$ instance               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position &lt;dbl&gt; 45, 34, 22, 14, 10, 8, 8, 14, NA, 97, 90, 97, 9…\n$ peak_position          &lt;dbl&gt; 34, 22, 14, 10, 8, 8, 8, 8, 97, 90, 90, 90, 90,…\n$ weeks_on_chart         &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 10, 11, 1, 2, 3, 4, 5, 6, 1, …\n\n\nNotice that this time I just typed glimpse rather than dplyr::glimpse. I didn’t need to tell R to look in the dplyr namespace because I’d already attached it when I typed library(dplyr) earlier."
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-britney",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#finding-britney",
    "title": "Visualising the hits of Queen Britney",
    "section": "Finding Britney",
    "text": "Finding Britney\nTime to start analysing the data. I have made a decision that today I have love in my heart only for Britney. So what I want to do is find the rows in billboard that correspond to Britney Spears songs. The natural way to do this would be to pull out the “performer” column and then try to find entries that refer to Britney. The slightly tricky aspect to this is that Britney doesn’t appear solely as “Britney Spears”. For example, “Me Against The Music” features Madonna, and the entry in the performer column is “Britney Spears Featuring Madonna”. So we’re going to have to search in a slightly smarter way. Before turning this into R code, I can sketch out my plan like this:\nget the billboard data, THEN\n  pull out the performer column, THEN\n  search for britney, THEN\n  tidy up a bit\nThis kind of workflow is naturally suited to the “pipe”, which is written %&gt;%. You’ll see referred to either as the “magrittr pipe” (referring to the magrittr package where it originally came from) or the “dplyr pipe” (because dplyr made it famous!). I’m sure you’ve seen it before, but since one goal of this post is to be a refresher, I’ll explain it again. The pipe does the same job as the word “THEN” in the pseudo-code I wrote above. Its job is to take the output of one function (whatever is on the left) and then pass it on as the input to the next one (on the right). So here’s that plan re-written in an “R-like” format:\nthe_billboard_data %&gt;% \n  pull_out_the_performer_column() %&gt;% \n  search_for_britney() %&gt;% \n  tidy_it_up()\nIn fact that’s pretty close to what the actual R code is going to look like! The dplyr package has a function dplyr::pull() that will extract a column from the data set (e.g., all 327,895 listings in the performer column), and base R has a function called unique() that will ignore repeat entries, showing you only the unique elements of a column. So our code is going to look almost exactly like this\nbillboard %&gt;% \n  pull(performer) %&gt;% \n  search_for_britney() %&gt;% \n  unique()\n\nPattern matching for text data\nInexcusably, however, R does not come with a search_for_britney() function, so we’re going to have to do it manually. This is where the stringr package is very helpful. It contains a lot of functions that are very helpful in searching for text and manipulating text. The actual function I’m going to use here is stringr::str_subset() which will return the subset of values that “match” a particular pattern. Here’s a very simple example, where the “pattern” is just the letter “a”. I’ll quickly define a variable animals containing the names of a few different animals:\n\nanimals &lt;- c(\"cat\", \"dog\", \"rat\", \"ant\", \"bug\")\n\nTo retain only those strings that contain the letter \"a\" we do this:\n\nstr_subset(string = animals, pattern = \"a\")\n\n[1] \"cat\" \"rat\" \"ant\"\n\n\nAlternatively we could write this using the pipe:\n\nanimals %&gt;% \n  str_subset(pattern = \"a\")\n\n[1] \"cat\" \"rat\" \"ant\"\n\n\nI’m not sure this second version is any nicer than the first version, but it can be helpful to see the two versions side by side in order to remind yourself of what the pipe actually does!\nWe can use the same tool to find all the Britney songs. In real life, whenever you’re working with text data you need to be wary of the possibility of mispellings and other errors in the raw data. Wild caught data are often very messy, but thankfully for us the Tidy Tuesday data sets tend to be a little kinder. With that in mind can safely assume that any song by Britney Spears will include the pattern “Britney” in it somewhere.\nSo let’s do just try this and see what we get:\n\nbillboard %&gt;% \n  pull(performer) %&gt;% \n  str_subset(\"Britney\") %&gt;% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Rihanna Featuring Britney Spears\"            \n[3] \"will.i.am & Britney Spears\"                  \n[4] \"Britney Spears & Iggy Azalea\"                \n[5] \"Britney Spears Featuring G-Eazy\"             \n[6] \"Britney Spears Featuring Madonna\"            \n[7] \"Britney Spears Featuring Tinashe\"            \n[8] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\n\n\nAt this point I was sorely tempted to get distracted by Ke$ha and Rihanna, but somehow managed to stay on topic. Somehow\nOkay, so it turns out that Britney is listed in eight different ways. For the sake of this post, I’m happy to include cases where another artist features on a Britney track, but I don’t want to include the two cases where Britney is the featuring artist. Looking at the output above, it seems like I can find those cases by keeping only those rows that start with the word “Britney”.\nNow our question becomes “how do we write down a pattern like that?” and the answer usually involves crying for a bit because the solution is to use a regular expression, or “regex”.\nRegular expressions are a tool used a lot in programming: they provide a compact way to represent patterns in text. They’re very flexible, but can often be quite hard to wrap your head around because there are a lot of special characters that have particular meanings. Thankfully, for our purposes today we only need to know one of them: the ^ character is used to mean “the start of the string”. So when interpreted as a regular expression, \"^Britney\" translates to “any string that begins with ‘Britney’”. Now that we have our regular expression, this works nicely:\n\nbillboard %&gt;% \n  pull(performer) %&gt;% \n  str_subset(\"^Britney\") %&gt;% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\nRegular expressions are one of those things you’ll slowly pick up as you go along, and although they can be a huuuuuuge headache to learn, the reward is worth the effort.\n\n\nIn my mental list of “stuff I hatelove in data science”, git and regexes are tied for first place\n\n\nCreating the Britney data\nOkay so now we’re in a position to filter the billboard data, keeping only the rows that correspond to Britney songs. Most people in our slack group have taken an introductory class before, so you’ll be expecting that dplyr::filter() is the tool we need. The kind of filtering you’ve seen before looks like this:\n\nbritney &lt;- billboard %&gt;% \n  filter(performer == \"Britney Spears\")\n\nHowever, this doesn’t work the way we want. The bit of code that reads performer == \"Britney Spears\" is a logical expression (i.e., a code snippet that only returns TRUE and FALSE values) that will only detect exact matches. It’s too literal for our purposes. We can’t use the == operator to detect our regular expression either: that will only detect cases where the performer is literally listed as “^Britney”. What we actually want is something that works like the == test, but uses a regular expression to determine if it’s a match or not.\nThat’s where the str_detect() function from the stringr package is really handy. Instead of using performer == \"Britney Spears\" to detect exact matches, we’ll use str_detect(performer, \"^Britney\") to match using the regular expression:\n\nbritney &lt;- billboard %&gt;% \n  filter(str_detect(performer, \"^Britney\"))\n\n\n\nA confession. I didn’t technically need to use a regex here, because stringr has a handy str_starts() function. But half the point of our slack group is to accidentally-on-purpose reveal new tools and also I forgot that str_starts() exists so… regex it is\nThis version works the way we want it to, but it’s usually a good idea in practice to check that we haven’t made any mistakes. Perhaps I have forgotten what str_detect() actually does or I’ve made an error in my use of filter(), for example. So let’s take a look at the performer column in the britney data and check that it contains the same six unique strings:\n\nbritney %&gt;% \n  pull(performer) %&gt;% \n  unique()\n\n[1] \"Britney Spears\"                              \n[2] \"Britney Spears & Iggy Azalea\"                \n[3] \"Britney Spears Featuring G-Eazy\"             \n[4] \"Britney Spears Featuring Madonna\"            \n[5] \"Britney Spears Featuring Tinashe\"            \n[6] \"Britney Spears Featuring Nicki Minaj & Ke$ha\"\n\n\nThat’s reassuring. So let’s take a quick peek at the results of our data wrangling:\n\nglimpse(britney)\n\nRows: 468\nColumns: 10\n$ url                    &lt;chr&gt; \"http://www.billboard.com/charts/hot-100/2000-0…\n$ week_id                &lt;chr&gt; \"4/22/2000\", \"10/24/2009\", \"12/20/2008\", \"12/2/…\n$ week_position          &lt;dbl&gt; 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ song                   &lt;chr&gt; \"Oops!...I Did It Again\", \"3\", \"Circus\", \"Stron…\n$ performer              &lt;chr&gt; \"Britney Spears\", \"Britney Spears\", \"Britney Sp…\n$ song_id                &lt;chr&gt; \"Oops!...I Did It AgainBritney Spears\", \"3Britn…\n$ instance               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position &lt;dbl&gt; NA, NA, NA, NA, NA, 45, NA, NA, NA, NA, 27, NA,…\n$ peak_position          &lt;dbl&gt; 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ weeks_on_chart         &lt;dbl&gt; 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1,…\n\n\nThat looks good to me…\n\n\nFixing the dates\n…or does it? Looking at the week_id column is enough to make any data analyst sigh in mild irritation. This column encodes the date, but the first two entries are \"4/22/2000\" and \"10/24/2009\". They are encoded in a “month/day/year” format. Nobody on this planet except Americans writes dates this way. Most countries use “day/month/year” as their standard way of writing dates, and most programming style guides strongly recommend “year/month/day” (there are good reasons for this, mostly to do with sorting chronologically). Worse yet, it’s just a character string. R doesn’t know that this column corresponds to a date, and unlike Excel it is smart enough not to try. Trying to guess what is and is not a date is notoriously difficult, so R makes that your job as the data analyst. Thankfully, the lubridate package exists to make it a little bit easier. In this case, where we have data in month/day/year format, the lubridate::mdy() function will do the conversion for us. You’ll be completely unsurprised to learn that there are lubridate::dmy() and lubridate::ymd() functions that handle other kinds of date formats.\nSo let’s do this. I’ll use the dplyr::mutate() function to modify the britney data, like so:\n\nbritney &lt;- britney %&gt;% \n  mutate(week_id = lubridate::mdy(week_id))\n\nglimpse(britney)\n\nRows: 468\nColumns: 10\n$ url                    &lt;chr&gt; \"http://www.billboard.com/charts/hot-100/2000-0…\n$ week_id                &lt;date&gt; 2000-04-22, 2009-10-24, 2008-12-20, 2000-12-02…\n$ week_position          &lt;dbl&gt; 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ song                   &lt;chr&gt; \"Oops!...I Did It Again\", \"3\", \"Circus\", \"Stron…\n$ performer              &lt;chr&gt; \"Britney Spears\", \"Britney Spears\", \"Britney Sp…\n$ song_id                &lt;chr&gt; \"Oops!...I Did It AgainBritney Spears\", \"3Britn…\n$ instance               &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ previous_week_position &lt;dbl&gt; NA, NA, NA, NA, NA, 45, NA, NA, NA, NA, 27, NA,…\n$ peak_position          &lt;dbl&gt; 67, 1, 3, 70, 70, 21, 17, 29, 76, 1, 26, 62, 65…\n$ weeks_on_chart         &lt;dbl&gt; 1, 1, 1, 1, 1, 8, 1, 1, 1, 1, 8, 1, 1, 1, 1, 1,…\n\n\nMuch better!"
  },
  {
    "objectID": "posts/2021-09-14_tidy-tuesday-billboard/index.html#visualising-a-queen",
    "href": "posts/2021-09-14_tidy-tuesday-billboard/index.html#visualising-a-queen",
    "title": "Visualising the hits of Queen Britney",
    "section": "Visualising a queen",
    "text": "Visualising a queen\nI’m now at the point that I have a britney data set I can visualise. However, being the queen she is, Britney has quite a few songs that appear in the Billboard Top 100, so the first thing I’ll do is specify a few favourites that we’ll highlight in the plots:\n\nhighlights &lt;- c(\"Work B**ch!\", \"...Baby One More Time\", \"Toxic\")\n\nMost people in our slack will probably have encountered the ggplot2 package before, and at least have some experience in creating data visualisations using it. So we might write some code like this, which draws a plot showing the date on the horizontal axis (the mapping x = week_id) and the position of the song on the vertical axis (represented by the mapping y = week_position). We’ll also map the colour to the song by setting colour = song. Then we’ll add some points and lines:\n\nggplot(\n  data = britney,\n  mapping = aes(\n    x = week_id,\n    y = week_position,\n    colour = song\n  )\n) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE)\n\n\n\n\n\n\n\n\nThe reason I’ve included show.legend = FALSE here is that there are quite a few different songs in the data, and if they were all added to a legend it wouldn’t leave any room for the data!\nWe can improve on this in a couple of ways. First up, let’s use scale_y_reverse() to flip the y-axis. That way, a top ranked song appears at the top, and a 100th ranked song appears at the bottom:\n\nbritney %&gt;% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line(show.legend = FALSE) + \n  geom_point(show.legend = FALSE) + \n  scale_y_reverse()\n\n\n\n\n\n\n\n\nNotice that I’ve switched to using the pipe here. I take the britney data, pipe it with %&gt;% to the ggplot() function where I set up the mapping, and then add things to the plot with +. It’s a matter of personal style though. Other people write their code differently!\nOkay, it’s time to do something about the lack of labels. My real interest here is in the three songs I listed in the highlights so I’m going to use the gghighlight package, like this:\n\nbritney %&gt;% \n  ggplot(aes(\n    x = week_id, \n    y = week_position, \n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\n\n\n\n\nWhen the data are plotted like this, you get a strong sense of the chronology of Britney’s career, but the downside is that you can’t easily see how the chart performance of “…Baby One More Time” compares to the performance of “Toxic” and “Work B**ch!“. To give a better sense of that, it’s better to plot weeks_on_chart on the horizontal axis:\n\nbritney %&gt;% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line() + \n  geom_point() + \n  scale_y_reverse() + \n  gghighlight::gghighlight(song %in% highlights)\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\nTried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\n\n\n\n\nShown this way, you get a really strong sense of just how much of an impact “…Baby One More Time” had. It wasn’t just Britney’s first hit, it was also her biggest. It’s quite an outlier on the chart!\nIf we’re doing exploratory data analysis, and the only goal is to have a picture to show a colleague, that’s good enough. However, if we wanted to share it more widely, you’d probably want to spend a little more time fiddling with the details, adding text, colour and other things that actually matter a lot in real life!\n\nbritney %&gt;% \n  ggplot(aes(\n    x = weeks_on_chart, \n    y = week_position, \n    group = song,\n    colour = song\n  )) + \n  geom_line(size = 1.5) + \n  scale_y_reverse() + \n  scale_color_brewer(palette = \"Dark2\") + \n  gghighlight::gghighlight(song %in% highlights, \n    unhighlighted_params = list(size = .5)) + \n  theme_minimal() +\n  labs(\n    title = \"Britney Spears' first hit was also her biggest\",\n    subtitle = \"Chart performance of Britney Spears' songs\",\n    x = \"Weeks in Billboard Top 100\",\n    y = \"Chart Position\"\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Tried to calculate with group_by(), but the calculation failed.\nFalling back to ungrouped filter operation...\n\n\nlabel_key: song\n\n\n\n\n\n\n\n\n\nIf I were less lazy I would also make sure that the chart includes a reference to the original data source, and something that credits myself as the creator of the plot. That’s generally good etiquette if you’re planning on sharing the image on the interwebs. There’s quite a lot you could do to tinker with the plot to get it to publication quality, but this is good enough for my goals today!\n\n\n\n\n\nHer Royal Highness Britney Spears, performing in Las Vegas, January 2014. Figure from wikimedia commons, released under a CC-BY-2.0 licence by Rhys Adams"
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html",
    "href": "posts/2022-12-22_queue/index.html",
    "title": "Queue",
    "section": "",
    "text": "Okay. So I wrote a simple package for multi-threaded tasks queues in R this week. It wasn’t intentional, I swear. I was just trying to teach myself how to use the callr package,1 and making sure I had a solid grasp of encapsulated object-oriented programming with R6. Things got a little out of hand. Sorry.\nAnd let’s be very clear about something at the outset. If you want to do parallel computing in R correctly, you go look at futureverse.org. The future package by Henrik Bengtsson provides a fabulous way to execute R code asynchronously and in parallel. And there are many excellent packages built on top of that, so there’s a whole lovely ecosystem there just waiting for you.2 Relatedly, if the reason you’re thinking about parallel computing is that you’ve found yourself with a burning need to analyse terabytes of data with R then babe it might be time to start learning some R workflows using Spark, Arrow, Kubernetes. It may be time to learn about some of those other eldritch words of power that have figured rather more prominently in my life than one might expect for a simple country girl.3\nMy little queue package is a personal project. I happen to like it, but you should not be looking at it as an alternative to serious tools.\nThat’s been said now. Good. We can put aside all pretension."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#what-does-it-do",
    "href": "posts/2022-12-22_queue/index.html#what-does-it-do",
    "title": "Queue",
    "section": "What does it do?",
    "text": "What does it do?\nLet’s say I have a generative art function called donut(), based loosely on a teaching example from my art from code workshop. The donut() function takes an input seed, creates a piece of generative art using ggplot2, and writes the output to an image file. This process takes several seconds to complete on my laptop:\n\nlibrary(tictoc)\ntic()\ndonut(seed = 100)\ntoc()\n\n5.277 sec elapsed\n\n\nHere’s the piece, by the way:\n\nThat’s nice and I do like this piece, but generative art is an iterative process and I like to make many pieces at once to help me get a feel for the statistical properties of the system. Waiting five or six seconds for one piece to render is one thing: waiting 8-10 minutes for 100 pieces to render is quite another. So it’s helpful if I can do this in parallel.\n\nlibrary(queue)\n\nHere’s how I might do that using queue. I designed the package using R6 classes – more on that later – so we’ll be working in the “encapsulated” object oriented programming style that is more common in other programming languages. The first step is to initialise a Queue object, specifying the number of workers we want to use. I’ll use six:\n\nqueue &lt;- Queue$new(workers = 6)\n\nWhen I do this, the queue package starts six R sessions for us, and all my computations will be done in those R sessions. Under the hood, all the hard work of managing the R sessions is being done by the wonderful callr package by Gábor Csárdi4 – the only thing that queue does is provide a layer of abstraction and automation to the whole process.\nNext, I’ll add some tasks to the queue. Queue objects have an add() method that take a function and a list of arguments, so I can do this to push a task to the queue:\n\nqueue$add(donut, args = list(seed = 100))\n\nWhen the queue executes, it will be in a “first in, first out” order,5 so this task will be the first one to be assigned to a worker. Though of course that’s no guarantee that it will be the first one to finish!\nAnyway, let’s load up several more tasks. There’s some weird aversion out there to using loops in R, but this isn’t one of those situations where we need to worry about unnecessary copying, so I’m going to use a loop:\n\nfor(s in 101:108) queue$add(donut, list(seed = s))\n\nSo now we have nine tasks loaded onto a queue with six workers. To start it running I call the run() method for the queue. By default, all you’d see while the queue is running is a spinner with a progress message telling you how many tasks have completed so far, how many are currently running, and how many are still waiting. But I’ll ask it to be a bit more chatty. I’ll call it setting message = \"verbose\" so that we can see a log showing the order in which the tasks completed and time each task took to complete, in addition to the total time elapsed on my system while the queue was running:\n\nout &lt;- queue$run(message = \"verbose\")\n\n→ Done: task_5 finished in 3.18 secs\n\n\n→ Done: task_2 finished in 5.78 secs\n\n\n→ Done: task_6 finished in 5.78 secs\n\n\n→ Done: task_4 finished in 7.34 secs\n\n\n→ Done: task_3 finished in 8.09 secs\n\n\n→ Done: task_1 finished in 9.46 secs\n\n\n→ Done: task_7 finished in 7.76 secs\n\n\n→ Done: task_9 finished in 6.09 secs\n\n\n→ Done: task_8 finished in 6.92 secs\n\n\n✔ Queue complete: 9 tasks done in 12.7 secs\n\n\nHere are the nine pieces that popped off the queue in 13 seconds:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo it’s a three-step process: (1) create the queue, (2) load up the tasks, (3) execute the tasks. In practice I would probably simplify the code to this:\n\nqueue &lt;- Queue$new(workers = 6)\nfor(s in 100:108) queue$add(donut, list(seed = s))\nout &lt;- queue$run()\n\nTrue, I could simplify it further. For example, if I know that I’m always calling the same function and always passing the same the same arguments – just with different values – this could be wrapped up in purrr style syntax, but honestly I’m not sure why I would bother doing that when furrr already exists? I’m not planning to reinvent the wheel, especially not when Davis Vaughn already offers a fully-operational mass-transit system free of charge."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#what-does-it-store",
    "href": "posts/2022-12-22_queue/index.html#what-does-it-store",
    "title": "Queue",
    "section": "What does it store?",
    "text": "What does it store?\nThe output object out stores quite a lot of information about the tasks, the results, and the events that occurred during task execution, but most of it isn’t immediately interesting to us (especially when things actually work!) So let’s keep things simple for the moment and just look at the first five columns:\n\nout[, 1:5]\n\n  task_id worker_id state        result       runtime\n1  task_1    577202  done donut_100.png 9.457713 secs\n2  task_2    577226  done donut_101.png 5.782191 secs\n3  task_3    577239  done donut_102.png 8.087054 secs\n4  task_4    577251  done donut_103.png 7.336802 secs\n5  task_5    577263  done donut_104.png 3.183247 secs\n6  task_6    577275  done donut_105.png 5.780861 secs\n7  task_7    577263  done donut_106.png 7.763061 secs\n8  task_8    577226  done donut_107.png 6.921016 secs\n9  task_9    577275  done donut_108.png 6.093996 secs\n\n\nThe columns are pretty self-explanatory I think?\n\ntask_id is a unique identifier for the task itself\nworker_id is a unique identifier for the worker that completed the task (it’s also the process id for the R session)\nstate summarises the current state of the task (they’re all \"done\" because the queue is finished)\nresult is a list column containing the output from each task\nruntime is a difftime column telling you how long each task took to finish\n\nAs for the the full output… well… here it is…\n\nout\n\n  task_id worker_id state        result       runtime                                                                          fun args\n1  task_1    577202  done donut_100.png 9.457713 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  100\n2  task_2    577226  done donut_101.png 5.782191 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  101\n3  task_3    577239  done donut_102.png 8.087054 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  102\n4  task_4    577251  done donut_103.png 7.336802 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  103\n5  task_5    577263  done donut_104.png 3.183247 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  104\n6  task_6    577275  done donut_105.png 5.780861 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  105\n7  task_7    577263  done donut_106.png 7.763061 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  106\n8  task_8    577226  done donut_107.png 6.921016 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  107\n9  task_9    577275  done donut_108.png 6.093996 secs function (seed) , {,     source(\"donut.R\", local = TRUE),     donut(seed), }  108\n              created              queued            assigned             started            finished code\n1 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:03  200\n2 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00  200\n3 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:02  200\n4 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:01  200\n5 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:57  200\n6 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00  200\n7 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:27:57 2022-12-23 12:27:57 2022-12-23 12:28:05  200\n8 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00 2022-12-23 12:28:00 2022-12-23 12:28:06  200\n9 2022-12-23 12:27:54 2022-12-23 12:27:54 2022-12-23 12:28:00 2022-12-23 12:28:00 2022-12-23 12:28:06  200\n                             message stdout stderr error\n1 done callr-rs-result-8ce3e320ef539                NULL\n2 done callr-rs-result-8ce3e6e032153                NULL\n3 done callr-rs-result-8ce3e5d32c7ba                NULL\n4  done callr-rs-result-8ce3e72346af                NULL\n5 done callr-rs-result-8ce3e4193129d                NULL\n6 done callr-rs-result-8ce3e42c3653c                NULL\n7 done callr-rs-result-8ce3e6ecedbfa                NULL\n8 done callr-rs-result-8ce3e41e0f9e6                NULL\n9 done callr-rs-result-8ce3e5a3c4630                NULL\n\n\nOkay so there’s a bit more to unpack here. Let’s take a look…\n\nThe fun and args columns contain the functions and arguments that were originally used to specify the task\nThe created, queued, assigned, started, and finished columns contain POSIXct timestamps indicating when the task was created, added to a queue, assigned to a worker, started running on a worker, and returned from the worker\ncode is a numeric code returned by the callr R session: of particular note 200 means it returned successfully, 500 means the session exited cleanly, and 501 means the session crashed\nmessage is a message returned by callr\nstdout and stderr are the contents of the output and error streams from the worker session while the task was running\nerror currently is NULL because I haven’t implemented that bit yet lol."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#surviving-a-crash",
    "href": "posts/2022-12-22_queue/index.html#surviving-a-crash",
    "title": "Queue",
    "section": "Surviving a crash",
    "text": "Surviving a crash\nI’m going to be honest. Sometimes6 I write bad code when I am exploring a new generative art system. Code that crashes the R session unpredictably. So it would be nice if the queue had a little bit of robustness for that. To be honest, the queue package isn’t very sophisticated in detecting sessions that have crashed,7 but it does have some ability to recover when a task crashes its thread. Let’s keep this simple. I’ll define a perfectly safe function that waits for a moment and then returns, and another function that always crashes the R session as soon as it is called:\n\nwait &lt;- function(x) {\n  Sys.sleep(x)\n  x\n}\ncrash &lt;- function(x) .Call(\"abort\")\n\nNow let’s define a queue that has only two workers, but has no less than three tasks that are guaranteed to crash the worker the moment the tasks are started:\n\nqueue &lt;- Queue$new(workers = 2)\nqueue$add(wait, list(x = .1))\nqueue$add(crash)\nqueue$add(crash)\nqueue$add(crash)\nqueue$add(wait, list(x = .1))\n\nThe queue allocates task in a first-in first-out order, so the three “crash tasks” are guaranteed to be allocated before the final “wait task”. Let’s take a look at what happens when the queue runs:\n\nqueue$run()\n\n✔ Queue complete: 5 tasks done in 4.3 secs\n\n\n# A tibble: 5 × 17\n  task_id worker_id state result    runtime        fun    args            \n  &lt;chr&gt;       &lt;int&gt; &lt;chr&gt; &lt;list&gt;    &lt;drtn&gt;         &lt;list&gt; &lt;list&gt;          \n1 task_1     401728 done  &lt;dbl [1]&gt; 0.1239407 secs &lt;fn&gt;   &lt;named list [1]&gt;\n2 task_2     401740 done  &lt;NULL&gt;    2.1264157 secs &lt;fn&gt;   &lt;list [0]&gt;      \n3 task_3     401728 done  &lt;NULL&gt;    3.0233221 secs &lt;fn&gt;   &lt;list [0]&gt;      \n4 task_4     401773 done  &lt;NULL&gt;    1.9341383 secs &lt;fn&gt;   &lt;list [0]&gt;      \n5 task_5     401792 done  &lt;dbl [1]&gt; 0.1577439 secs &lt;fn&gt;   &lt;named list [1]&gt;\n# ℹ 10 more variables: created &lt;dttm&gt;, queued &lt;dttm&gt;, assigned &lt;dttm&gt;,\n#   started &lt;dttm&gt;, finished &lt;dttm&gt;, code &lt;dbl&gt;, message &lt;chr&gt;,\n#   stdout &lt;list&gt;, stderr &lt;list&gt;, error &lt;list&gt;\n\n\nIt’s a little slower than we’d hope, but it does finish both valid tasks and returns nothing for the tasks that crashed their R sessions. What has happened in the background is that the queue runs a simple check to see if any of the R sessions have crashed, and attempts to replace them with a new worker whenever it detects that this has happened. It’s not in any sense optimised, but it does sort of work."
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#design",
    "href": "posts/2022-12-22_queue/index.html#design",
    "title": "Queue",
    "section": "Design",
    "text": "Design\nAlthough my confidence in my ability to have a career in tech is at an all-time low, I have to admit that the work I’ve done over the last year has made me a better programmer. I didn’t much effort into writing queue, but the code feels cleaner and more modular than the code I was writing a year ago. Good practices have become habits, I suppose. That’s a nice feeling. I automatically write proper unit tests as I go, knowing that those tests will save me when I need to make changes later. I document properly as I go, knowing that I won’t remember a bloody thing about how my own code works six hours later – never mind six months. And, maybe most importantly of all, my code now seems to have this habit of organising itself into small, manageable abstractions. I have no idea when that happened, because I wasn’t actually part of a software engineering team. I was just the girl who wrote some docs and few little blog posts.8\nHere’s what I mean. If you take a look at the source code for the Queue object, it’s actually not very long: the file is mostly devoted to the documentation, and the object doesn’t have very many methods. Honestly, we’ve already seen most of them:\n\nnew() creates a new queue\nadd() adds a task to a queue\nrun() sets the queue running\n\nIf everything works smoothly you don’t need anything else, so why burden the user with extra details? Sure, there’s a little complexity to these methods which is of course documented on the relevant pkgdown page because I’m not a jerk, but this isn’t a complicated package…\n…when it’s working.\nOf course, when things start to break, you start to care a lot more about the internals. Fair enough. There are two important data structures within the Queue:\n\nInternally, a Queue manages a WorkerPool comprised of one or more Worker objects. As you’d expect given the names, these provide abstractions for managing the R sessions. A Worker object provides a wrapper around a callr R session, and tools that automate the interaction between that session and a task.\nThe Queue also holds a TaskList comprised of one or more Task objects. Again, as you might expect from the names, these are the storage classes. A Task object is a container that holds a function, its arguments, any results it might have returned, and any logged information about the process of its execution.\n\nIn some situations it can be awfully handy to have access to these constituent data structures, particularly because those objects expose additional tools that I deliberately chose not to make available at the Queue level. From the Queue itself what you can do is return the objects:\n\nworkers &lt;- queue$get_workers()\ntasks &lt;- queue$get_tasks()\n\nThese objects are R6 classes: they have reference semantics so anything I do with workers and tasks will have corresponding effects on queue. For this blog post I don’t intend to dive into details of what I did when designing the WorkerPool and TaskList classes – especially because queue is only at version 0.0.2 and I don’t yet know what I’m going to do with this cute little package – but I’ll give one example.\nLet’s take the workers. By default, a Queue cleans up after itself and closes any R sessions that it started. The WorkerPool object associated with a Queue has a get_pool_state() method that I can use to check the state of the workers, and some other methods to modify the workers if I so choose. Let’s have a go. I ask workers to report on the status of the R sessions, this is what I get:\n\nworkers$get_pool_state()\n\n    401792     401804 \n\"finished\" \"finished\" \n\n\nYes, as expected the workers have stopped. But I can replace them with live R sessions by calling the refill_pool() method:\n\nworkers$refill_pool()\n\n401820 401832 \n\"idle\" \"idle\" \n\n\nAnd I can shut them down again by calling shutdown_pool():\n\nworkers$shutdown_pool()\n\n    401820     401832 \n\"finished\" \"finished\" \n\n\nAlong similar lines the TaskList object has some methods that let me manipulate the data storage associated with my Queue. Normally I don’t need to. Sometimes I do. It’s handy to have those tools lying around. At the moment the toolkit feels a little light, but the nice thing about writing your own package is that I can always add more if I need them :-)"
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#epilogue",
    "href": "posts/2022-12-22_queue/index.html#epilogue",
    "title": "Queue",
    "section": "Epilogue",
    "text": "Epilogue\n\nqueue &lt;- Queue$new(workers = 6)\nqueue$add(wait, list(x = 1.3), id = toupper(\"multithreading\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"it is\"))\nqueue$add(wait, list(x = 0.7), id = toupper(\"acknowledged\"))\nqueue$add(wait, list(x = 1.0), id = toupper(\"post\"))\nqueue$add(wait, list(x = 0.5), id = toupper(\"universally\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"a truth\"))\nqueue$add(wait, list(x = 1.2), id = toupper(\"must be\"))\nqueue$add(wait, list(x = 0.9), id = toupper(\"about\"))\nqueue$add(wait, list(x = 1.6), id = toupper(\"trick\"))\nqueue$add(wait, list(x = 0.1), id = toupper(\"that a\"))\nqueue$add(wait, list(x = 0.5), id = toupper(\"in want of\"))\nqueue$add(wait, list(x = 1.0), id = toupper(\"an async\"))\nout &lt;- queue$run(message = \"verbose\")\n\n→ Done: IT IS finished in 0.172 secs\n\n\n→ Done: A TRUTH finished in 0.169 secs\n\n\n→ Done: UNIVERSALLY finished in 0.536 secs\n\n\n→ Done: ACKNOWLEDGED finished in 0.776 secs\n\n\n→ Done: THAT A finished in 0.173 secs\n\n\n→ Done: POST finished in 1.07 secs\n\n\n→ Done: ABOUT finished in 0.957 secs\n\n\n→ Done: MULTITHREADING finished in 1.37 secs\n\n\n→ Done: MUST BE finished in 1.25 secs\n\n\n→ Done: IN WANT OF finished in 0.582 secs\n\n\n→ Done: AN ASYNC finished in 1.06 secs\n\n\n→ Done: TRICK finished in 1.66 secs\n\n\n✔ Queue complete: 12 tasks done in 2.21 secs"
  },
  {
    "objectID": "posts/2022-12-22_queue/index.html#footnotes",
    "href": "posts/2022-12-22_queue/index.html#footnotes",
    "title": "Queue",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHonestly, the whole reason this exists is that I was reading the callr blog post on writing multi-worker task queues and decided to try doing it myself…↩︎\nNote to self: Learn parallelly↩︎\nkubectl auth can-i create chaos↩︎\nLongtime readers will have noticed that I have become a bit of a fangirl. I swear I’m not stalking him, but like, every time I think… gosh this is a really handy bit of infrastructure tooling, who do I have to thank for this… oh, of course it’s bloody Gábor again. Anyway.↩︎\nI am a country girl, so FIFO means “fly-in fly-out”, and I shan’t be listening to any of you computer nerds who claim it has another meaning↩︎\nOften↩︎\nI mean, it was just a fun side project I did over the weekend because I have found myself quite unexpectedly unemployed, and my self-confidence is utterly shattered at the moment, and Stella needs to get her groove back slowly okay?↩︎\nFor the record, dear potential future employer, this is what is known as “self-deprecation”. Mistake not my awareness of the absurd cultural norms to which women are expected to conform with a literal claim about competence. In point of fact I am rather good at what I do.↩︎"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html",
    "title": "Generative art with grid",
    "section": "",
    "text": "As I approach four months of unemployment I’m finding I need projects to work on purely for the sake of my mental health. One project that has helped a lot is working on the ggplot2 book (which I coauthor with Hadley Wickham and Thomas Lin Pedersen). At the moment I’m working on the book chapters that discuss the ggplot2 extension system: it’s been quite a lot of fun. One really nice thing about working on those chapters is that I’ve ended up learning a lot about the grid graphics system upon which ggplot2 is built.1\nAt this point we’re really not sure how much grid to incorporate into the book, but as a fun side-project I decided to adapt some of it and use it as the basis for a post on generative art."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#what-is-grid",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#what-is-grid",
    "title": "Generative art with grid",
    "section": "What is grid?",
    "text": "What is grid?\nThe grid package provides the underlying graphics system upon which ggplot2 is built. It’s one of two quite different drawing systems that are included in base R: base graphics and grid. Base graphics has an imperative “pen-on-paper” model: every function immediately draws something on the graphics device. Much like ggplot2 itself, grid takes a more declarative approach where you build up a description of the graphic as an object, which is later rendered. This declarative approach allows us to create objects that exist independently of the graphic device and can be passed around, analysed, and modified. Importantly, parts of a graphical object can refer to other parts, which allows you to do things like define rectangle A to have width equal to the length of text string B, and so on.\nThis blog post – and the corresponding section in the book, should we decide to include it – isn’t intended to be a comprehensive introduction to grid. But it does cover many of the core concepts and introduces key terms like grobs, viewports, graphical parameters, and units. Hopefully it will make sense even if you’re completely new to grid."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#grobs",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#grobs",
    "title": "Generative art with grid",
    "section": "Grobs",
    "text": "Grobs\nTo understand how grid works, the first thing we need to talk about are grobs. Grobs (graphic objects) are the atomic representations of graphical elements in grid, and include types like points, lines, circles, rectangles, and text. The grid package provides functions like pointsGrob(), linesGrob(), circleGrob(), rectGrob(), and textGrob() that create graphical objects without drawing anything to the graphics device.2 These functions are vectorised, allowing a single point grob to represent multiple points, for instance:\n\nlibrary(grid)\n\nset.seed(1)\nn &lt;- 8\nx &lt;- runif(n, min = .3, max = .7) # x coordinate\ny &lt;- runif(n, min = .3, max = .7) # y coordinate\nr &lt;- runif(n, min = 0, max = .15) # radius\n\ncircles &lt;- circleGrob(x = x, y = y, r = r)\n\nNotice that this does not create any output. Much like a ggplot2 plot object, this grob is a declarative description of a set of circles. To trigger a drawing operation we first call grid.newpage() to clear the current graphics device, and then grid.draw() to perform a draw operation:\n\ngrid.newpage()\ngrid.draw(circles)\n\n\n\n\n\n\n\n\nIn addition to providing geometric primitives, grid also allows you to construct composite objects that combine multiple grobs using grobTree(). Here’s an illustration:\n\nsquares &lt;- rectGrob(x = x, y = y, width = r * 2.5, height =  r * 2.5)\ncomposite &lt;- grobTree(squares, circles)\ngrid.newpage()\ngrid.draw(composite)\n\n\n\n\n\n\n\n\nIt is also possible to define your own grob classes. You can define a new primitive grob class using grob() or a new composite class using gTree(), and specify special behaviour for your new class. We’ll see an example of this in a moment."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#viewports",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#viewports",
    "title": "Generative art with grid",
    "section": "Viewports",
    "text": "Viewports\nThe second key concept in grid is the idea of a viewport. A viewport is a rectangular plotting region that supplies its own coordinate system for grobs that are drawn within it, and can also provide a tabular grid in which other viewports an be nested. An individual grob can have its own viewport or, if none is provided, it will inherit one. In the example below I’ll use viewport() to define three different viewports, one with default parameters, and two more that are rotated around the midpoint by 15 and 30 degrees respectively:\n\nvp_default &lt;- viewport()\nvp_rotate1 &lt;- viewport(angle = 15)\nvp_rotate2 &lt;- viewport(angle = 30)\n\nThis time around, when we create our composite grobs, we’ll explicitly assign them to specific viewports by setting the vp argument:\n\ncomposite_default &lt;- grobTree(squares, circles, vp = vp_default)\ncomposite_rotate1 &lt;- grobTree(squares, circles, vp = vp_rotate1)\ncomposite_rotate2 &lt;- grobTree(squares, circles, vp = vp_rotate2)\n\nWhen we plot these two grobs, we can see the effect of the viewport: although composite_default and composite_rotated are comprised of the same two primitive grobs (i.e., circles and squares), they belong to different viewports so they look different when the plot is drawn:\n\ngrid.newpage()\ngrid.draw(composite_default)\ngrid.draw(composite_rotate1)\ngrid.draw(composite_rotate2)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#graphical-parameters",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#graphical-parameters",
    "title": "Generative art with grid",
    "section": "Graphical parameters",
    "text": "Graphical parameters\nThe next concept we need to understand is the idea of graphical parameters. When we defined the circles and labels grobs, we only specified some of its properties. For example, we said nothing about colour or transparency, and so these properties are all set to their default values. The gpar() function in grid allows you to specify graphical parameters as distinct objects:\n\ngpA &lt;- gpar(fill = \"grey30\", col = \"white\", lwd = 20)\ngpB &lt;- gpar(fill = \"white\", col = \"grey30\", lwd = 20)\n\nThe gpA and gpB objects provide lists of graphical settings that can now be applied to any grob we like using the gp argument:\n\nset.seed(1)\nn &lt;- 5\ncircles &lt;- circleGrob(\n  x = runif(n, min = .2, max = .8),\n  y = runif(n, min = .2, max = .9),\n  r = runif(n, min = 0, max = .15)\n)\n\ngrob1 &lt;- grobTree(circles, vp = vp_default, gp = gpA)\ngrob2 &lt;- grobTree(circles, vp = vp_rotate1, gp = gpB)\ngrob3 &lt;- grobTree(circles, vp = vp_rotate2, gp = gpA)\n\nWhen we plot these two grobs, they inherit the settings provided by the graphical parameters as well as the viewports to which they are assigned:\n\ngrid.newpage()\ngrid.draw(grob1)\ngrid.draw(grob2)\ngrid.draw(grob3)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#units",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#units",
    "title": "Generative art with grid",
    "section": "Units",
    "text": "Units\nThe last core concept that we need to discuss is the grid unit system. The grid package allows you to specify the positions (e.g. x and y) and dimensions (e.g. length and width) of grobs and viewports using a flexible language. In the grid unit system there are three qualitatively different styles of unit:\n\nAbsolute units (e.g. centimeters, inches, and points refer to physical sizes).\nRelative units (e.g. npc which scales the viewport size between 0 and 1).\nUnits based on other grobs (e.g. grobwidth).\n\nThe unit() function is the main function we use when specifying units: unit(1, \"cm\") refers to a length of 1 centimeter, whereas unit(0.5, \"npc\") refers to a length half the size of the relevant viewport. The unit system supports arithmetic operations that are only resolved at draw time, which makes it possible to combine different types of units: unit(0.5, \"npc\") + unit(1, \"cm\") defines a point one centimeter to the right of the center of the current viewport."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#building-grob-classes",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#building-grob-classes",
    "title": "Generative art with grid",
    "section": "Building grob classes",
    "text": "Building grob classes\nNow that we have a basic understanding of grid, let’s attempt to create our own “transforming” grob class: objects that are circles if they are smaller than some threshold (1cm by default), but transform into squares whenever they are larger than the threshold.3 This is not the most useful kind of graphical object, but it’s useful for illustrating the flexibility of the grid system. The first step is to write our own constructor function using grob() or gTree(), depending on whether we are creating a primitive or composite object. We begin by creating a “thin” constructor function:\n\ntransGrob &lt;- function(x, \n                      y, \n                      size,\n                      threshold = 1,\n                      default.units = \"npc\", \n                      name = NULL, \n                      gp = gpar(), \n                      vp = NULL) {\n  \n  # Ensure that input arguments are units\n  if (!is.unit(x)) x &lt;- unit(x, default.units)\n  if (!is.unit(y)) y &lt;- unit(y, default.units)\n  if (!is.unit(size)) size &lt;- unit(size, default.units)\n  \n  # Construct the grob class as a gTree\n  gTree(\n    x = x, \n    y = y, \n    size = size, \n    threshold = threshold,\n    name = name, \n    gp = gp, \n    vp = vp, \n    cl = \"trans\"\n  )\n}\n\nThe transGrob() function doesn’t do very much on its own. All it does is ensure that the x, y, and size arguments are grid units, and sets the class name to be “trans”. To define the behaviour of our grob, we need to specify methods for one or both of the generic functions makeContext() and makeContent():\n\nmakeContext() is called when the parent grob is rendered and allows you to control the viewport of the grob. We won’t need to use that for our surprise grob.\nmakeContent() is called every time the drawing region is resized and allows you to customise the look of the grob based on the size or other aspect.\n\nBecause these generic functions use the S3 object oriented programming system, we can define our method simply by appending the class name to the end of the function name. That is, the makeContent() method for our surprise grob is defined by creating a function called makeContent.trans() that takes a grob as input and returns a modified grob as output:\n\nmakeContent.trans &lt;- function(x) {\n  x_pos &lt;- x$x\n  y_pos &lt;- x$y\n  size &lt;- convertWidth(x$size, unitTo = \"cm\", valueOnly = TRUE)\n  threshold &lt;- x$threshold\n  \n  # Work out which shapes are circles, and which are not\n  circles &lt;- size &lt; threshold\n  \n  # Create a circle grob for the small ones\n  if (any(circles)) {\n    circle_grob &lt;- circleGrob(\n      x = x_pos[circles], \n      y = y_pos[circles], \n      r = unit(size[circles] / 2, \"cm\")\n    )\n  } else {\n    circle_grob &lt;- nullGrob()\n  }\n  \n  # Create a rect grob for the large ones\n  if (any(!circles)) {\n    square_grob &lt;- rectGrob(\n      x = x_pos[!circles], \n      y = y_pos[!circles], \n      width = unit(size[!circles], \"cm\"),\n      height = unit(size[!circles], \"cm\")\n    )\n  } else {\n    square_grob &lt;- nullGrob()\n  }\n  \n  # Add the circle and rect grob as children of our input grob\n  setChildren(x, gList(square_grob, circle_grob))\n}\n\nSome of the functions we’ve called here are new, but they all reuse the core concepts that we discussed earlier. Specifically:\n\nconvertWidth() is used to convert grid units from one type to another.\nnullGrob() creates a blank grob.\ngList() creates a list of grobs.\nsetChildren() specifies the grobs that belong to a gTree composite grob.\n\nThe effect of this function is to ensure that every time the grob is rendered the absolute size of each shape is recalculated. All shapes smaller than the threshold become circles, and all shapes larger than the threshold become squares. To see how this plays out, lets call our new function:\n\nset.seed(1)\nn &lt;- 20\ntrans &lt;- transGrob(\n  x = runif(n, min = .2, max = .8),\n  y = runif(n, min = .2, max = .8),\n  size = runif(n, min = 0, max = .15),\n  threshold = 2\n)\n\nThe trans grob contains shapes whose locations and sizes have been specified relative to the size of the viewport. At this point in time we have no idea which of these shapes will be circles and which will be squares, because that depends on the size of the viewport in which the trans grob is to be drawn. Here’s what we end up with for this quarto post that defines the figure size to be 8x8 inches:\n\ngrid.newpage()\ngrid.draw(trans)\n\n\n\n\n\n\n\n\nThe exact same code, but now I’ve made the plot size smaller and as a consequence all the shapes have turned into circles:\n\ngrid.newpage()\ngrid.draw(trans)\n\n\n\n\n\n\n\n\nIf you run this code interactively and resize the plotting window you’ll see that the objects change shape based on the size of the plotting window. It’s not the most useful application of grid, but it is fun to play with."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewports",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#pushing-viewports",
    "title": "Generative art with grid",
    "section": "Pushing viewports",
    "text": "Pushing viewports\nA nice feature of grid is that viewports can be nested within other viewports. At the top level there is always “root” viewport that encompasses the entire image. By default user-created viewports are children of the root viewport, and inherit properties from it. However, there’s nothing stopping you from assigning new viewports to be children of previous user-generated viewports. In the simplest case4, we can use this to create a viewport stack in which each new viewport is the child of the previous one. The pushViewport() function allows us to do this.\nHere’s an example:\n\nvp &lt;- viewport(angle = 17, width = .8, height = .8)\nbox &lt;- rectGrob(width = 1, height = 1)\ngrid.newpage()\nfor(i in 1:20) {\n  pushViewport(vp)\n  grid.draw(box)\n}\n\n\n\n\n\n\n\n\nIn this code I define vp to be a viewport that shrinks the width and height of the current viewport to be 80% of its parent, and rotates the frame by 17 degrees.b Then I repeatedly push vp to the viewport stack, and draw a border (the box grob) showing the edges of that viewport. The effects of each push to the stack are cumulative, as the image shows.\nIt’s also quite pretty.\nThe grid package has a lot of tools for working with viewport lists, stacks, and trees. You can assign names to viewports, navigate back and forth between different viewports during plot construction, and so on. But that’s a topic for another day."
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#trans-spirals",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#trans-spirals",
    "title": "Generative art with grid",
    "section": "Trans spirals",
    "text": "Trans spirals\nI’m pleasantly surprised at how easy it is to create interesting generative art with grid. As a really simple example, here’s a system that combines two tricks: it uses the transGrob() that we defined earlier, and it uses a viewport stack to create spiraling images:\n\ntrans_spiral &lt;- function(seed) {\n  \n  set.seed(seed)\n  n &lt;- 15\n  trans &lt;- transGrob(\n    x = runif(n, min = .1, max = .9),\n    y = runif(n, min = .1, max = .9), \n    size = runif(n, min = 0, max = .15),\n    threshold = 1\n  )\n  \n  cols &lt;- sample(c(\"#5BCEFA\", \"#F5A9B8\", \"#FFFFFF\"), 30, TRUE)\n  \n  vp_spiral &lt;- viewport(width = .95, height = .95, angle = 10)\n  \n  grid.newpage()\n  for(i in 1:30) {\n    pushViewport(vp_spiral)\n    grid.draw(grobTree(trans, gp = gpar(fill = cols[i])))\n  }\n}\n\nIt produces output like this…\n\ntrans_spiral(1)\n\n\n\n\n\n\n\n\n… and this …\n\ntrans_spiral(2)\n\n\n\n\n\n\n\n\n… and this.\n\ntrans_spiral(3)"
  },
  {
    "objectID": "posts/2023-03-31_generative-art-with-grid/index.html#footnotes",
    "href": "posts/2023-03-31_generative-art-with-grid/index.html#footnotes",
    "title": "Generative art with grid",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe best text to use if you want to learn grid is R Graphics (2nd ed) by Paul Murrell. I’ve found myself relying on it quite heavily.↩︎\nThere are also functions grid.points(), grid.lines(), etc that immediately draw the corresponding grob to the graphics device, but when creating a generative art system I find it makes more separate the plot specification from the drawing process.↩︎\nThis example is based on the “surprise grob” example originally written by Thomas Lin Pedersen. I’ve adapted it a bit here because I’m the kind of person who does that.↩︎\nThe only case I’m going to consider here.↩︎"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html",
    "href": "posts/2024-03-02_julia-data-frames/index.html",
    "title": "Working with data in Julia",
    "section": "",
    "text": "This is the second of an impromptu three-part series in which, in a decision I am rapidly starting to regret as these posts get longer and longer, I decided it was time to teach myself how to use Julia. In the first part of the series I looked at some foundational concepts (types, functions, pipes, etc), though in a completely idiosyncratic way and ignoring concepts that I find boring (loops, conditionals). I mean, this is a blog where I write “notes to self”. It’s not a textbook.\nAnyway… my thought for the second part of the series is to shift away from core programming concepts and instead look at a practical task that data analysts have to work with on a daily basis: wrangling rectangular data sets. In other words, I’m going to talk about data frames and tools for manipulating them."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#creating-data-frames",
    "href": "posts/2024-03-02_julia-data-frames/index.html#creating-data-frames",
    "title": "Working with data in Julia",
    "section": "Creating data frames",
    "text": "Creating data frames\nUnlike R, Julia doesn’t come with a native class to represent data frames. Instead, there is the DataFrames package which provides the functionality needed to represent tabular data. The DataFrame() function allows you to manually construct a data frame, with a syntax that feels very familiar to an R user. Vectors passed as inputs to DataFrame() must all have one element for every row in the data frame, or else be length one.\nContinuing the vague science fiction theme that started in the previous post, I’ll start by constructing a small data frame listing the novels from William Gibson’s Sprawl trilogy, which I enjoyed considerably more than Asimov’s Foundation series that (very very loosely) inspired the TV show of the same name. Anyway, here’s how you do that:\n\nusing DataFrames\n\nsprawl = DataFrame(\n  title = [\"Neuromancer\", \"Count Zero\", \"Mona Lisa Overdrive\"],\n  published = [1984, 1986, 1988], \n  author = \"William Gibson\"\n)\n\n3×3 DataFrame\n\n\n\nRow\ntitle\npublished\nauthor\n\n\n\nString\nInt64\nString\n\n\n\n\n1\nNeuromancer\n1984\nWilliam Gibson\n\n\n2\nCount Zero\n1986\nWilliam Gibson\n\n\n3\nMona Lisa Overdrive\n1988\nWilliam Gibson\n\n\n\n\n\n\nData frames have pretty print methods so the output looks quite nice here. But internally it’s essentially a collection of vectors, one for each column. For example, sprawl.title is a vector of three strings:\n\nsprawl.title\n\n3-element Vector{String}:\n \"Neuromancer\"\n \"Count Zero\"\n \"Mona Lisa Overdrive\"\n\n\nIn real life though, you don’t usually construct a data frame manually. It’s more typical to import a data frame from a CSV file or similar. To that end, we can take advantage of the CSV package to read data from a data file:\n\nusing CSV\nstarwars_csv = CSV.File(\"starwars.csv\"; missingstring = \"NA\");\n\nThis starwars_csv object isn’t a data frame yet, it’s an object of type CSV.file. Data frames are columnar data structures (i.e., a collection of vectors, one per column), whereas a CSV.file is a rowwise data structure (i.e., a collection of CSV.row objects, one per row). You could test this for yourself by taking a look at the first element starwars_csv[1] to verify that it’s a representation of a single CSV row, but the output isn’t very interesting so I’m going to move on.\nTo convert this CSV.file object to a DataFrame object, we can simply pass it to DataFrame(), and this time around the data we end up with is a little bit richer than the last one (even if the Star Wars movies are incredibly boring compared to the infinitely superior Sprawl novels…)\n\nstarwars = DataFrame(starwars_csv)\n\n87×11 DataFrame62 rows omitted\n\n\n\nRow\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\n\n\n\nString31\nInt64?\nFloat64?\nString15?\nString31\nString15\nFloat64?\nString15?\nString15?\nString15?\nString15?\n\n\n\n\n1\nLuke Skywalker\n172\n77.0\nblond\nfair\nblue\n19.0\nmale\nmasculine\nTatooine\nHuman\n\n\n2\nC-3PO\n167\n75.0\nmissing\ngold\nyellow\n112.0\nnone\nmasculine\nTatooine\nDroid\n\n\n3\nR2-D2\n96\n32.0\nmissing\nwhite, blue\nred\n33.0\nnone\nmasculine\nNaboo\nDroid\n\n\n4\nDarth Vader\n202\n136.0\nnone\nwhite\nyellow\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\n5\nLeia Organa\n150\n49.0\nbrown\nlight\nbrown\n19.0\nfemale\nfeminine\nAlderaan\nHuman\n\n\n6\nOwen Lars\n178\n120.0\nbrown, grey\nlight\nblue\n52.0\nmale\nmasculine\nTatooine\nHuman\n\n\n7\nBeru Whitesun Lars\n165\n75.0\nbrown\nlight\nblue\n47.0\nfemale\nfeminine\nTatooine\nHuman\n\n\n8\nR5-D4\n97\n32.0\nmissing\nwhite, red\nred\nmissing\nnone\nmasculine\nTatooine\nDroid\n\n\n9\nBiggs Darklighter\n183\n84.0\nblack\nlight\nbrown\n24.0\nmale\nmasculine\nTatooine\nHuman\n\n\n10\nObi-Wan Kenobi\n182\n77.0\nauburn, white\nfair\nblue-gray\n57.0\nmale\nmasculine\nStewjon\nHuman\n\n\n11\nAnakin Skywalker\n188\n84.0\nblond\nfair\nblue\n41.9\nmale\nmasculine\nTatooine\nHuman\n\n\n12\nWilhuff Tarkin\n180\nmissing\nauburn, grey\nfair\nblue\n64.0\nmale\nmasculine\nEriadu\nHuman\n\n\n13\nChewbacca\n228\n112.0\nbrown\nunknown\nblue\n200.0\nmale\nmasculine\nKashyyyk\nWookiee\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n76\nSan Hill\n191\nmissing\nnone\ngrey\ngold\nmissing\nmale\nmasculine\nMuunilinst\nMuun\n\n\n77\nShaak Ti\n178\n57.0\nnone\nred, blue, white\nblack\nmissing\nfemale\nfeminine\nShili\nTogruta\n\n\n78\nGrievous\n216\n159.0\nnone\nbrown, white\ngreen, yellow\nmissing\nmale\nmasculine\nKalee\nKaleesh\n\n\n79\nTarfful\n234\n136.0\nbrown\nbrown\nblue\nmissing\nmale\nmasculine\nKashyyyk\nWookiee\n\n\n80\nRaymus Antilles\n188\n79.0\nbrown\nlight\nbrown\nmissing\nmale\nmasculine\nAlderaan\nHuman\n\n\n81\nSly Moore\n178\n48.0\nnone\npale\nwhite\nmissing\nmissing\nmissing\nUmbara\nmissing\n\n\n82\nTion Medon\n206\n80.0\nnone\ngrey\nblack\nmissing\nmale\nmasculine\nUtapau\nPau'an\n\n\n83\nFinn\nmissing\nmissing\nblack\ndark\ndark\nmissing\nmale\nmasculine\nmissing\nHuman\n\n\n84\nRey\nmissing\nmissing\nbrown\nlight\nhazel\nmissing\nfemale\nfeminine\nmissing\nHuman\n\n\n85\nPoe Dameron\nmissing\nmissing\nbrown\nlight\nbrown\nmissing\nmale\nmasculine\nmissing\nHuman\n\n\n86\nBB8\nmissing\nmissing\nnone\nnone\nblack\nmissing\nnone\nmasculine\nmissing\nDroid\n\n\n87\nCaptain Phasma\nmissing\nmissing\nnone\nnone\nunknown\nmissing\nfemale\nfeminine\nmissing\nHuman"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#subsetting-data-frames-i",
    "href": "posts/2024-03-02_julia-data-frames/index.html#subsetting-data-frames-i",
    "title": "Working with data in Julia",
    "section": "Subsetting data frames I",
    "text": "Subsetting data frames I\nThe core tools for working with data frames in Julia feel quite familiar coming from either Matlab or R. You can subset a data frame by passing it numeric indices, for instance:\n\nstarwars[1:6, 1:5]\n\n6×5 DataFrame\n\n\n\nRow\nname\nheight\nmass\nhair_color\nskin_color\n\n\n\nString31\nInt64?\nFloat64?\nString15?\nString31\n\n\n\n\n1\nLuke Skywalker\n172\n77.0\nblond\nfair\n\n\n2\nC-3PO\n167\n75.0\nmissing\ngold\n\n\n3\nR2-D2\n96\n32.0\nmissing\nwhite, blue\n\n\n4\nDarth Vader\n202\n136.0\nnone\nwhite\n\n\n5\nLeia Organa\n150\n49.0\nbrown\nlight\n\n\n6\nOwen Lars\n178\n120.0\nbrown, grey\nlight\n\n\n\n\n\n\nHowever, there are other methods for subsetting a data frame. You can also filter the rows of a data frame using logical expressions. Again, this is quite similar to how it works in base R. For instance, I can construct a boolean vector fair_skinned which indicates whether the corresponding row in starwars refers to a person with fair skin:1\n\nfair_skinned = starwars.skin_color .== \"fair\";\n\nNow that I have these indices, I can create a subset of the data frame containing only those rows referring to fair skinned person (or robot, or…)\n\nstarwars[fair_skinned, 1:5]\n\n17×5 DataFrame\n\n\n\nRow\nname\nheight\nmass\nhair_color\nskin_color\n\n\n\nString31\nInt64?\nFloat64?\nString15?\nString31\n\n\n\n\n1\nLuke Skywalker\n172\n77.0\nblond\nfair\n\n\n2\nObi-Wan Kenobi\n182\n77.0\nauburn, white\nfair\n\n\n3\nAnakin Skywalker\n188\n84.0\nblond\nfair\n\n\n4\nWilhuff Tarkin\n180\nmissing\nauburn, grey\nfair\n\n\n5\nHan Solo\n180\n80.0\nbrown\nfair\n\n\n6\nWedge Antilles\n170\n77.0\nbrown\nfair\n\n\n7\nJek Tono Porkins\n180\n110.0\nbrown\nfair\n\n\n8\nBoba Fett\n183\n78.2\nblack\nfair\n\n\n9\nMon Mothma\n150\nmissing\nauburn\nfair\n\n\n10\nArvel Crynyd\nmissing\nmissing\nbrown\nfair\n\n\n11\nQui-Gon Jinn\n193\n89.0\nbrown\nfair\n\n\n12\nFinis Valorum\n170\nmissing\nblond\nfair\n\n\n13\nRic Olié\n183\nmissing\nbrown\nfair\n\n\n14\nShmi Skywalker\n163\nmissing\nblack\nfair\n\n\n15\nCliegg Lars\n183\nmissing\nbrown\nfair\n\n\n16\nDooku\n193\n80.0\nwhite\nfair\n\n\n17\nJocasta Nu\n167\nmissing\nwhite\nfair\n\n\n\n\n\n\nOn the columns side, we also have more flexible options for subsetting a data frame. For example, instead of referring to columns using numerical indices, we can select the variables that we want to keep using their names:\n\nstarwars[1:6, [:name, :gender, :homeworld]]\n\n6×3 DataFrame\n\n\n\nRow\nname\ngender\nhomeworld\n\n\n\nString31\nString15?\nString15?\n\n\n\n\n1\nLuke Skywalker\nmasculine\nTatooine\n\n\n2\nC-3PO\nmasculine\nTatooine\n\n\n3\nR2-D2\nmasculine\nNaboo\n\n\n4\nDarth Vader\nmasculine\nTatooine\n\n\n5\nLeia Organa\nfeminine\nAlderaan\n\n\n6\nOwen Lars\nmasculine\nTatooine\n\n\n\n\n\n\nReferring to columns by name is very handy in practice, and there’s some hidden Julia concepts here that I didn’t talk about in the last post. So with that in mind I’ll digress slightly to talk about…"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#symbols",
    "href": "posts/2024-03-02_julia-data-frames/index.html#symbols",
    "title": "Working with data in Julia",
    "section": "Symbols",
    "text": "Symbols\nLooking at the syntax in the last code cell, it’s fairly clear that [:name, :gender, :homeworld] is a vector of three… somethings, but it’s not immediately obvious what :name actually is. Much like R (and also inherited from Lisp) Julia has extensive Metaprogramming capabilities because it has the ability to represent Julia code as data structures within the language itself. In the simplest case, we have Symbols like :name, which are constructed using the quotation operator : and used to represent object names. So as you can see, :name is an object of type Symbol:\n\ntypeof(:name)\n\nSymbol\n\n\nSymbols can be assigned to variables, and those variables can be used as part of expressions to be evaluated. In the code below I create a variable colname that stores the symbolic representation of a column name that I can invoke later:\n\ncolname = :title\n\n:title\n\n\nAs a simple example of how symbols can be used in practice, here’s a Julia implementation of something like the pull() function in the R package dplyr, which allows the user to extract a single column from a data frame:\n\nfunction pull(data::DataFrame, column::Symbol)\n  getproperty(data, column)\nend;\n\nIn this code I’m using the getproperty() function to do the same job that the . operator would do in an expression like sprawl.title. So here it is in action:\n\npull(sprawl, :title)\n\n3-element Vector{String}:\n \"Neuromancer\"\n \"Count Zero\"\n \"Mona Lisa Overdrive\"\n\n\nI know, it’s exciting right?\nOkay yeah, at the moment this pull() function isn’t very useful at all – pull(sprawl, :title) is really not an improvement on sprawl.title – but a little bit later when I get around to talking about data wrangling pipelines it might turn out to be a little less silly.\n\n\n\n\n\nMass Effect 2. By user lagota on Deviant Art, released under a CC-BY-NC-ND licence. Still the strangest of the three games: the main storyline with the Collectors is a hot mess, but it has the best side quests in the series, and the best romance too (Thane, obviously…)"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#subsetting-data-frames-ii",
    "href": "posts/2024-03-02_julia-data-frames/index.html#subsetting-data-frames-ii",
    "title": "Working with data in Julia",
    "section": "Subsetting data frames II",
    "text": "Subsetting data frames II\nAnyway, getting back on track, the key thing to realise is that when I wrote [:name, :gender, :homeworld] earlier what I was really doing is constructing a vector of symbols, and it’s those symbols that I was using to select the columns that I wanted to retain. The DataFrames package also supplies a various selector functions that can be used to extract a subset of the columns. For example, Not() will select every column except the ones that are passed to Not(). So if I want to drop the hair color, eye color, sex, and homeworld columns, I could do this:\n\nstarwars[1:6, Not([:hair_color, :eye_color, :sex, :homeworld])]\n\n6×7 DataFrame\n\n\n\nRow\nname\nheight\nmass\nskin_color\nbirth_year\ngender\nspecies\n\n\n\nString31\nInt64?\nFloat64?\nString31\nFloat64?\nString15?\nString15?\n\n\n\n\n1\nLuke Skywalker\n172\n77.0\nfair\n19.0\nmasculine\nHuman\n\n\n2\nC-3PO\n167\n75.0\ngold\n112.0\nmasculine\nDroid\n\n\n3\nR2-D2\n96\n32.0\nwhite, blue\n33.0\nmasculine\nDroid\n\n\n4\nDarth Vader\n202\n136.0\nwhite\n41.9\nmasculine\nHuman\n\n\n5\nLeia Organa\n150\n49.0\nlight\n19.0\nfeminine\nHuman\n\n\n6\nOwen Lars\n178\n120.0\nlight\n52.0\nmasculine\nHuman\n\n\n\n\n\n\nThe Between() selector does what you’d think. It returns all columns in between two named columns:\n\nstarwars[1:6, Between(:sex, :homeworld)]\n\n6×3 DataFrame\n\n\n\nRow\nsex\ngender\nhomeworld\n\n\n\nString15?\nString15?\nString15?\n\n\n\n\n1\nmale\nmasculine\nTatooine\n\n\n2\nnone\nmasculine\nTatooine\n\n\n3\nnone\nmasculine\nNaboo\n\n\n4\nmale\nmasculine\nTatooine\n\n\n5\nfemale\nfeminine\nAlderaan\n\n\n6\nmale\nmasculine\nTatooine\n\n\n\n\n\n\nThere’s also an All() selector that returns all columns, but that’s not super exciting. More interesting, I think, is the Cols() selector which takes a predicate function as input.2 The column names are passed to the function, and they are included in the output if that function returns true. So, for example, if I want to extract the columns in the data whose name ends in \"color\" I can do this:\n\nstarwars[1:6, Cols(x -&gt; endswith(x, \"color\"))]\n\n6×3 DataFrame\n\n\n\nRow\nhair_color\nskin_color\neye_color\n\n\n\nString15?\nString31\nString15\n\n\n\n\n1\nblond\nfair\nblue\n\n\n2\nmissing\ngold\nyellow\n\n\n3\nmissing\nwhite, blue\nred\n\n\n4\nnone\nwhite\nyellow\n\n\n5\nbrown\nlight\nbrown\n\n\n6\nbrown, grey\nlight\nblue\n\n\n\n\n\n\nI find myself liking these selector functions. Coming from the tidyverse style in R where tidyselect is used to govern column selection it feels… not terribly different. Superficially different, perhaps, but the combination of All(), Not(), Between(), and Cols() seems to provide a fairly powerful and (I think?) user-friendly way to select columns.\n\n\n\n\n\nMass Effect 3. By user lagota on Deviant Art, released under a CC-BY-NC-ND licence. No, I will not be drawn into expressing a comment on the ending. I love ME3, in part because every Shepard I’ve ever played comes into this game already completely broken and makes unhinged choices because of it…"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-i-groupby-combine",
    "href": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-i-groupby-combine",
    "title": "Working with data in Julia",
    "section": "Data wrangling I: groupby, combine",
    "text": "Data wrangling I: groupby, combine\nUp to this point I haven’t really done any data wrangling with the starwars data. Okay, yeah, to some extent there’s some data wrangling implied by the discussion of subsetting in the previous sections, but in truth none of that is how you’d normally go about it in a more real-world context. So to that end I’ll talk about some of the data wrangling functions that DataFrames supplies.\nLet’s start with something simple, and not very useful. Suppose what I want to do here is group the data by gender and sex, and then for every unique combination of gender and sex that appears in the data set have Julia pick one row at random and report the corresponding mass. To do that is a two step operation. First, I need to use groupby() to describe the groups, and then I need to call combine() to tell Julia what function to apply separately for each group. This does the trick:\n\ncombine(groupby(starwars, [:gender, :sex]), :mass =&gt; rand) \n\n6×3 DataFrame\n\n\n\nRow\ngender\nsex\nmass_rand\n\n\n\nString15?\nString15?\nFloat64?\n\n\n\n\n1\nmasculine\nmale\n74.0\n\n\n2\nmasculine\nnone\n32.0\n\n\n3\nmasculine\nhermaphroditic\n1358.0\n\n\n4\nfeminine\nnone\nmissing\n\n\n5\nfeminine\nfemale\nmissing\n\n\n6\nmissing\nmissing\n110.0\n\n\n\n\n\n\nIn the call to groupby(starwars, [:gender, :sex]) what Julia does is construct a grouped data frame (very similar to what you expect in R, really), and then this grouped data frame is passed to combine(). For each such group, we take the relevant subset of the :mass column and pass it to the rand() function, and by doing so a random mass is returned.\nThere’s some obvious limitations to note in my code here though. Firstly, I’m not using the pipe |&gt; at all, and while it’s sort of fine in this context because there’s only two steps in my data wrangling exercise, the code is going to get very ugly very quickly if I try to do something fancier.3 So let’s start by fixing this.\nAs I mentioned in the first post in this series, one way I could transform this into a pipeline is to use the Pipe package, which supplies a pipe that behaves very similarly to the base pipe in R. However, I’m not going to do that. Instead, I’m going to adopt a workflow where I use the Julia base pipe together with anonymous functions. Here’s the same code expressed in this kind of pipeline:4\n\nstarwars |&gt;\n  d -&gt; groupby(d, [:gender, :sex]) |&gt;\n  d -&gt; combine(d, :mass =&gt; rand)\n\n6×3 DataFrame\n\n\n\nRow\ngender\nsex\nmass_rand\n\n\n\nString15?\nString15?\nFloat64?\n\n\n\n\n1\nmasculine\nmale\nmissing\n\n\n2\nmasculine\nnone\n32.0\n\n\n3\nmasculine\nhermaphroditic\n1358.0\n\n\n4\nfeminine\nnone\nmissing\n\n\n5\nfeminine\nfemale\nmissing\n\n\n6\nmissing\nmissing\n85.0\n\n\n\n\n\n\nI genuinely wasn’t expecting this when I first learned about the restrictiveness of the Julia pipe, but I think I really like this syntax. Because you have to define an anonymous function at each step in the pipeline, I find myself noticing that:\n\nIt’s only slightly more verbose than the R style, and has the advantage (to my mind) that you can use this workflow without having to think too much about Julia macros\nThe input argument (in this case d) serves the same role that the placeholder (_ for the R base pipe and the Julia “Pipe-package-pipe”, or . for the R magrittr pipe)\nYou have the ability to subtly remind yourself of the internal workings of your pipeline by naming the input argument cleverly. If the input to this step in the pipeline is a data frame I tend to call the input argument d, but if – as sometimes happens in real life – at some point in the pipeline I pull out a column and do a bit of processing on that before returning the results, I might find it handy to use something else to remind myself that this step is applied to string variables.\n\nAs regards that third point, here’s an example using the pull() function that I defined earlier that does exactly this:\n\nstarwars |&gt;\n  d -&gt; subset(d, :skin_color =&gt; x -&gt; x.==\"fair\") |&gt;\n  d -&gt; pull(d, :name) |&gt;\n  n -&gt; map(x -&gt; split(x, \" \")[1], n)\n\n17-element Vector{SubString{String31}}:\n \"Luke\"\n \"Obi-Wan\"\n \"Anakin\"\n \"Wilhuff\"\n \"Han\"\n \"Wedge\"\n \"Jek\"\n \"Boba\"\n \"Mon\"\n \"Arvel\"\n \"Qui-Gon\"\n \"Finis\"\n \"Ric\"\n \"Shmi\"\n \"Cliegg\"\n \"Dooku\"\n \"Jocasta\"\n\n\nAgain, not the most exciting pipeline in the world – all I’m doing is returning the first names of all the fair-skinned characters – but it does highlight the fact that the combination of base pipe and anonymous function syntax in Julia works rather well if you’re inclined to write in this style.\nIn fact, the ability to name the input argument is especially helpful in the last line of the pipe where there are two separate functions being used, one of which is a call to map() applied to the :name column (and takes n as the input), and another that is used by map() when extracting the first name out of every name (where I’ve unimaginatively used x to name my input).\nIn any case, though it may not be to everyone’s tastes, I’ve found a pipe-centric style that I can use in Julia that I don’t mind, so it’s time to move on and look at some other functions available in the DataFrames package."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-ii-subset-select-sort",
    "href": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-ii-subset-select-sort",
    "title": "Working with data in Julia",
    "section": "Data wrangling II: subset, select, sort",
    "text": "Data wrangling II: subset, select, sort\nIn the previous section I gave an example of a workflow that uses groupby() and combine() to compute summaries of a data frame. But there are other functions that come in very handy for data wrangling: I can use subset() to choose a subset of rows5, select() to choose a subset of columns, and sort() to order the rows according to some criterion. For example, here’s how I could find all the characters from Tattooine and sort them by weight:\n\nstarwars |&gt; \n  d -&gt; select(d, [:name, :mass, :homeworld]) |&gt;\n  d -&gt; subset(d, :homeworld =&gt; h -&gt; h.==\"Tatooine\", skipmissing=true) |&gt;\n  d -&gt; sort(d, :mass)\n\n10×3 DataFrame\n\n\n\nRow\nname\nmass\nhomeworld\n\n\n\nString31\nFloat64?\nString15?\n\n\n\n\n1\nR5-D4\n32.0\nTatooine\n\n\n2\nC-3PO\n75.0\nTatooine\n\n\n3\nBeru Whitesun Lars\n75.0\nTatooine\n\n\n4\nLuke Skywalker\n77.0\nTatooine\n\n\n5\nBiggs Darklighter\n84.0\nTatooine\n\n\n6\nAnakin Skywalker\n84.0\nTatooine\n\n\n7\nOwen Lars\n120.0\nTatooine\n\n\n8\nDarth Vader\n136.0\nTatooine\n\n\n9\nShmi Skywalker\nmissing\nTatooine\n\n\n10\nCliegg Lars\nmissing\nTatooine\n\n\n\n\n\n\nNotice that this time I’ve been a little smarter about handling missing values. In the call to subset() I specified skipmissing=true to drop all cases where the homeworld is missing. The sort() function doesn’t have a skipmissing argument, so the results include the two cases where someone from Tatooine doesn’t have a stated weight. But hopefully it’s clear that I could easily subset the data again to remove any cases with missing values on the :mass column if I wanted to. In fact, the DataFrames package supplies functions dropmissing(), allowmissing(), and completecases() that could be used for that purpose. For example:\n\nstarwars |&gt; \n  d -&gt; select(d, [:name, :mass, :homeworld]) |&gt;\n  d -&gt; subset(d, :homeworld =&gt; h -&gt; h.==\"Tatooine\", skipmissing=true) |&gt;\n  d -&gt; sort(d, :mass, rev=true) |&gt;\n  d -&gt; dropmissing(d, :mass)\n\n8×3 DataFrame\n\n\n\nRow\nname\nmass\nhomeworld\n\n\n\nString31\nFloat64\nString15?\n\n\n\n\n1\nDarth Vader\n136.0\nTatooine\n\n\n2\nOwen Lars\n120.0\nTatooine\n\n\n3\nBiggs Darklighter\n84.0\nTatooine\n\n\n4\nAnakin Skywalker\n84.0\nTatooine\n\n\n5\nLuke Skywalker\n77.0\nTatooine\n\n\n6\nC-3PO\n75.0\nTatooine\n\n\n7\nBeru Whitesun Lars\n75.0\nTatooine\n\n\n8\nR5-D4\n32.0\nTatooine\n\n\n\n\n\n\nThe missing :mass rows are now gone, and – just for my own personal amusement – this time I’ve sorted the results in order of descending weight by setting rev=true.\n\n\n\n\n\nStar Wars. By user lagota on Deviant Art, released under a CC-BY-NC-ND licence. If I had more Mass Effect images to use here I would but alas, I do not."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-iii-stack",
    "href": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-iii-stack",
    "title": "Working with data in Julia",
    "section": "Data wrangling III: stack",
    "text": "Data wrangling III: stack\nOkay, now it’s time to start thinking about how to reshape a data frame in Julia. Consider this, as the beginnings of a pipeline:\n\nstarwars |&gt;\n  d -&gt; select(d, [:name, :eye_color, :skin_color, :hair_color])\n\n87×4 DataFrame62 rows omitted\n\n\n\nRow\nname\neye_color\nskin_color\nhair_color\n\n\n\nString31\nString15\nString31\nString15?\n\n\n\n\n1\nLuke Skywalker\nblue\nfair\nblond\n\n\n2\nC-3PO\nyellow\ngold\nmissing\n\n\n3\nR2-D2\nred\nwhite, blue\nmissing\n\n\n4\nDarth Vader\nyellow\nwhite\nnone\n\n\n5\nLeia Organa\nbrown\nlight\nbrown\n\n\n6\nOwen Lars\nblue\nlight\nbrown, grey\n\n\n7\nBeru Whitesun Lars\nblue\nlight\nbrown\n\n\n8\nR5-D4\nred\nwhite, red\nmissing\n\n\n9\nBiggs Darklighter\nbrown\nlight\nblack\n\n\n10\nObi-Wan Kenobi\nblue-gray\nfair\nauburn, white\n\n\n11\nAnakin Skywalker\nblue\nfair\nblond\n\n\n12\nWilhuff Tarkin\nblue\nfair\nauburn, grey\n\n\n13\nChewbacca\nblue\nunknown\nbrown\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n76\nSan Hill\ngold\ngrey\nnone\n\n\n77\nShaak Ti\nblack\nred, blue, white\nnone\n\n\n78\nGrievous\ngreen, yellow\nbrown, white\nnone\n\n\n79\nTarfful\nblue\nbrown\nbrown\n\n\n80\nRaymus Antilles\nbrown\nlight\nbrown\n\n\n81\nSly Moore\nwhite\npale\nnone\n\n\n82\nTion Medon\nblack\ngrey\nnone\n\n\n83\nFinn\ndark\ndark\nblack\n\n\n84\nRey\nhazel\nlight\nbrown\n\n\n85\nPoe Dameron\nbrown\nlight\nbrown\n\n\n86\nBB8\nblack\nnone\nnone\n\n\n87\nCaptain Phasma\nunknown\nnone\nnone\n\n\n\n\n\n\nSuppose what I want to do is transform this into a data set that has variables :name, :body_part, and :color. In other words I want to pivot this into a long-form data set where each character is represented by three rows, and has one row that specifies the colour of the relevant body part. We can do this with the stack() function:\n\nstarwars |&gt;\n  d -&gt; select(d, [:name, :eye_color, :skin_color, :hair_color]) |&gt;\n  d -&gt; stack(d, [:eye_color, :skin_color, :hair_color],\n    variable_name=:body_part,\n    value_name=:color\n  )\n\n261×3 DataFrame236 rows omitted\n\n\n\nRow\nname\nbody_part\ncolor\n\n\n\nString31\nString\nString31?\n\n\n\n\n1\nLuke Skywalker\neye_color\nblue\n\n\n2\nC-3PO\neye_color\nyellow\n\n\n3\nR2-D2\neye_color\nred\n\n\n4\nDarth Vader\neye_color\nyellow\n\n\n5\nLeia Organa\neye_color\nbrown\n\n\n6\nOwen Lars\neye_color\nblue\n\n\n7\nBeru Whitesun Lars\neye_color\nblue\n\n\n8\nR5-D4\neye_color\nred\n\n\n9\nBiggs Darklighter\neye_color\nbrown\n\n\n10\nObi-Wan Kenobi\neye_color\nblue-gray\n\n\n11\nAnakin Skywalker\neye_color\nblue\n\n\n12\nWilhuff Tarkin\neye_color\nblue\n\n\n13\nChewbacca\neye_color\nblue\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n250\nSan Hill\nhair_color\nnone\n\n\n251\nShaak Ti\nhair_color\nnone\n\n\n252\nGrievous\nhair_color\nnone\n\n\n253\nTarfful\nhair_color\nbrown\n\n\n254\nRaymus Antilles\nhair_color\nbrown\n\n\n255\nSly Moore\nhair_color\nnone\n\n\n256\nTion Medon\nhair_color\nnone\n\n\n257\nFinn\nhair_color\nblack\n\n\n258\nRey\nhair_color\nbrown\n\n\n259\nPoe Dameron\nhair_color\nbrown\n\n\n260\nBB8\nhair_color\nnone\n\n\n261\nCaptain Phasma\nhair_color\nnone"
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-iv-unstack",
    "href": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-iv-unstack",
    "title": "Working with data in Julia",
    "section": "Data wrangling IV: unstack",
    "text": "Data wrangling IV: unstack\nWe can also go the other way. Let’s start with a slightly different data frame called census, one that counts the number of characters of each species on each homeworld\n\ncensus = starwars |&gt; \n  d -&gt; dropmissing(d, [:homeworld, :species]) |&gt;\n  d -&gt; groupby(d, [:homeworld, :species]) |&gt; \n  d -&gt; combine(d, :name =&gt; (n -&gt; length(n)) =&gt; :count)\n\n51×3 DataFrame26 rows omitted\n\n\n\nRow\nhomeworld\nspecies\ncount\n\n\n\nString15\nString15\nInt64\n\n\n\n\n1\nTatooine\nHuman\n8\n\n\n2\nTatooine\nDroid\n2\n\n\n3\nNaboo\nDroid\n1\n\n\n4\nAlderaan\nHuman\n3\n\n\n5\nStewjon\nHuman\n1\n\n\n6\nEriadu\nHuman\n1\n\n\n7\nKashyyyk\nWookiee\n2\n\n\n8\nCorellia\nHuman\n2\n\n\n9\nRodia\nRodian\n1\n\n\n10\nNal Hutta\nHutt\n1\n\n\n11\nNaboo\nHuman\n5\n\n\n12\nKamino\nHuman\n1\n\n\n13\nTrandosha\nTrandoshan\n1\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n40\nGeonosis\nGeonosian\n1\n\n\n41\nMirial\nMirialan\n2\n\n\n42\nSerenno\nHuman\n1\n\n\n43\nConcord Dawn\nHuman\n1\n\n\n44\nZolan\nClawdite\n1\n\n\n45\nOjom\nBesalisk\n1\n\n\n46\nKamino\nKaminoan\n2\n\n\n47\nSkako\nSkakoan\n1\n\n\n48\nMuunilinst\nMuun\n1\n\n\n49\nShili\nTogruta\n1\n\n\n50\nKalee\nKaleesh\n1\n\n\n51\nUtapau\nPau'an\n1\n\n\n\n\n\n\nSo now, if I wanted a version of this data set with one row per :homeworld and a column for each :species that contains the :count of the number of characters of that species on the corresponding world, I could use unstack() to pivot from long-form to wide-form data like this:\n\nunstack(census, :species, :count, fill=0) \n\n46×37 DataFrame21 rows omitted\n\n\n\nRow\nhomeworld\nHuman\nDroid\nWookiee\nRodian\nHutt\nTrandoshan\nMon Calamari\nEwok\nSullustan\nNeimodian\nGungan\nToydarian\nDug\nZabrak\nTwi'lek\nAleena\nVulptereen\nXexto\nToong\nCerean\nNautolan\nTholothian\nIktotchi\nQuermian\nKel Dor\nChagrian\nGeonosian\nMirialan\nClawdite\nBesalisk\nKaminoan\nSkakoan\nMuun\nTogruta\nKaleesh\nPau'an\n\n\n\nString15\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nTatooine\n8\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\nNaboo\n5\n1\n0\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\nAlderaan\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\nStewjon\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n5\nEriadu\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n6\nKashyyyk\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n7\nCorellia\n2\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n8\nRodia\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n9\nNal Hutta\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n10\nKamino\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n\n\n11\nTrandosha\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n12\nSocorro\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n13\nBespin\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n35\nChampala\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n36\nGeonosis\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n37\nMirial\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n2\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n38\nSerenno\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n39\nConcord Dawn\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n40\nZolan\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n41\nOjom\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n42\nSkako\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n43\nMuunilinst\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n44\nShili\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n45\nKalee\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n46\nUtapau\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\nHere I’ve specified fill=0 to indicate missing values should be replaced with zeros, which is very sensible in this case because if there are no characters with a particular species/homeworld combination there wouldn’t be a row in census. Also, because I can, here’s a version that appears in a pipeline where I return only a subset of species, and – in act of appalling xenophobia – consider only planets inhabited by at least one human character:\n\ncensus|&gt;\n  d -&gt; unstack(d, :species, :count, fill=0) |&gt;\n  d -&gt; select(d, [:homeworld, :Human, :Droid, :Ewok, :Wookiee, :Hutt]) |&gt;\n  d -&gt; subset(d, :Human =&gt; h -&gt; h .&gt; 0)\n\n14×6 DataFrame\n\n\n\nRow\nhomeworld\nHuman\nDroid\nEwok\nWookiee\nHutt\n\n\n\nString15\nInt64\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nTatooine\n8\n2\n0\n0\n0\n\n\n2\nNaboo\n5\n1\n0\n0\n0\n\n\n3\nAlderaan\n3\n0\n0\n0\n0\n\n\n4\nStewjon\n1\n0\n0\n0\n0\n\n\n5\nEriadu\n1\n0\n0\n0\n0\n\n\n6\nCorellia\n2\n0\n0\n0\n0\n\n\n7\nKamino\n1\n0\n0\n0\n0\n\n\n8\nSocorro\n1\n0\n0\n0\n0\n\n\n9\nBespin\n1\n0\n0\n0\n0\n\n\n10\nChandrila\n1\n0\n0\n0\n0\n\n\n11\nCoruscant\n2\n0\n0\n0\n0\n\n\n12\nHaruun Kal\n1\n0\n0\n0\n0\n\n\n13\nSerenno\n1\n0\n0\n0\n0\n\n\n14\nConcord Dawn\n1\n0\n0\n0\n0\n\n\n\n\n\n\nHm. Not sure what those results say about the willingness of humans to mix with other species in the Star Wars universe but it’s probably not good to reflect on it too much.\n\n\n\n\n\nAlso Star Wars. By user lagota on Deviant Art, released under a CC-BY-NC-ND licence."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-v-transform",
    "href": "posts/2024-03-02_julia-data-frames/index.html#data-wrangling-v-transform",
    "title": "Working with data in Julia",
    "section": "Data wrangling V: transform",
    "text": "Data wrangling V: transform\nIn all honesty I am getting exhausted with this post, and mildly irrited at the fact that I’ve spent so much time in the Star Wars universe rather than in a fictional universe that I actually enjoy. So it’s time to start wrapping this one up. There’s only one more topic I really want to mention and that’s the transform() function which you can use to add new columns to a data frame.\n\nstarwars[1:6, [:name]] |&gt; \n  d -&gt; transform(d, :name =&gt; (n -&gt; n.==\"Darth Vader\") =&gt; :lukes_father)\n\n6×2 DataFrame\n\n\n\nRow\nname\nlukes_father\n\n\n\nString31\nBool\n\n\n\n\n1\nLuke Skywalker\nfalse\n\n\n2\nC-3PO\nfalse\n\n\n3\nR2-D2\nfalse\n\n\n4\nDarth Vader\ntrue\n\n\n5\nLeia Organa\nfalse\n\n\n6\nOwen Lars\nfalse\n\n\n\n\n\n\nThere. It’s done."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#wrap-up",
    "href": "posts/2024-03-02_julia-data-frames/index.html#wrap-up",
    "title": "Working with data in Julia",
    "section": "Wrap up",
    "text": "Wrap up\nNo. Just no. There was already a first post and there’s about to be a third post. I am not being paid for this and I do not have the energy to think of a witty and erudite way to wrap up the unloved middle child of the trilogy. So let us never speak of this again."
  },
  {
    "objectID": "posts/2024-03-02_julia-data-frames/index.html#footnotes",
    "href": "posts/2024-03-02_julia-data-frames/index.html#footnotes",
    "title": "Working with data in Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs an aside, notice that I’ve used .== rather than == as the equality test. This is because == is a scalar operator: it doesn’t work for vectors unless you broadcast it using .↩︎\nIn this context, a predicate function is just one that returns true or false.↩︎\nThe other issue is that my code doesn’t handle missing data gracefully, but that will come up later so I’m ignoring it for now.↩︎\nAt some point I want to take a look at Tidier.jl, but that’s a topic for the future.↩︎\nThere is also filter() which has a slightly different syntax.↩︎"
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html",
    "href": "posts/2022-09-09_reticulated-arrow/index.html",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "",
    "text": "As the 21st century gears up for its quarter-life crisis, the trend in data science is toward multi-language tools. I use quarto to write this blog, a document preparation system that supports code evaluation in R, Python, Julia, and more. My work revolves around Apache Arrow, a toolbox for data analysis and interchange with implementations in multiple languages. You get the idea. In one sense this new development is fantastic – your language of choice is much more likely to be supported in the future than it ever was in the past. In another sense it is daunting – it sometimes feels like we need to learn all the things in order to get by in this brave new world. Meanwhile we all have our actual jobs to do and we don’t have the time. In the immortal words of Bob Katter commenting on same sex marriage legislation in Australia,\nI mean, he makes a good point? Or at least, it’s a good point about data science: I’m not convinced it was a stellar contribution to the discussion of LGBT rights in the antipodes.1 There’s a lot going on in the data science world, none of us can keep pace with all of it, and we’re all trying our best not to be eaten by crocodiles.\n[R code]\n\nlibrary(tidyverse)\nlibrary(tictoc)"
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-in-a-polyglot-world",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-in-a-polyglot-world",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Data interchange in a polyglot world",
    "text": "Data interchange in a polyglot world\nIn the spirit of saving you from at least one reptilian threat, this post is a primer on how to efficiently pass control of a large data set between R and Python without making any wasteful copies of the data.\nThe idea to write this post emerged from a recent discussion on Twitter started by Cass Wilkinson Saldaña about passing control of a data set from R to Python, and a comment in that discussion by Jon Keane mentioning that with the assistance of Apache Arrow this handover can be made very smooth, and incredibly efficient too. Unfortunately, to be able to do this you need to know the trick, and as they regretfully mentioned in the thread, the trick isn’t well documented yet.\nIn time the documentation will of course improve, but in the here-and-now it seems like a good idea to explain how the magic trick works…\n\n\nThe reticulate trick\nThe “trick” is simple: if your data are stored as an Arrow Table, and you use the reticulate package to pass it from R to Python (or vice versa), only the metadata changes hands. Because an Arrow Table has the same structure in-memory when accessed from Python as it does in R, the data set itself does not need to be touched at all. The only thing that needs to happen is the language on the receiving end needs to be told where the data are stored. Or, to put it another way, we just pass a pointer across. This all happens invisibly, so if you know how to use reticulate,2 you already know almost everything you need to know and can skip straight to the section on passing Arrow objects. If you’re like Danielle-From-Last-Month and have absolutely no idea how reticulate works, read on…\n\n\n\nManaging the Python environment from R\nIf reticulate is not already on your system, you can install it from CRAN with install.packages(\"reticulate\"). Once installed, you can load it in the usual fashion:\n\n\n\n[R code]\n\nlibrary(reticulate)\n\n\nWhat happens next depends a little on whether you already have a Python set up. If you don’t have a preferred Python configuration on your machine and would like to let reticulate manage everything for you, then you can do something like this:\n\n\n\n[R code]\n\ninstall_python()\ninstall_miniconda()\n\n\nThis will set you up with a default Python build, managed by a copy of Miniconda that it installs in an OS-specific location that you can discover by calling miniconda_path().\nThe previous approach is a perfectly sensible way to use reticulate, but in the end I took a slightly different path. If you’re like me and already have Python and Miniconda configured on your local machine, you probably don’t want reticulate potentially installing new versions and possibly making a mess of things.3 You probably want to use your existing set up and ensure that reticulate knows where to find everything. If that’s the case, what you want to do is edit your .Renviron file4 and set the RETICULATE_MINICONDA_PATH variable. Add a line like this one,\n\n\n\n[within .Renviron]\n\nRETICULATE_MINICONDA_PATH=/home/danielle/miniconda3/\n\n\nwhere you should specify the path to your Miniconda installation, not mine 😁\nRegardless of which method you’ve followed, you can use conda_list() to display a summary of all your Python environments.5 Somehow, despite the fact that I went to the effort of setting everything up, I haven’t used Python much on this machine, so my list of environments is short:\n\n\n\n[R code]\n\nconda_list()\n\n\n\n\n          name                                                 python\n1         base                   /home/danielle/miniconda3/bin/python\n2 continuation /home/danielle/miniconda3/envs/continuation/bin/python\n3 r-reticulate /home/danielle/miniconda3/envs/r-reticulate/bin/python\n\n\nFor the purposes of this post I’ll create a new environment that – in honour of Bob Katter and the reptilian terror in the north – I will call “reptilia”. To keep things neat I’ll install6 the pandas and pyarrow packages that this post will be using at the same time:\n\n\n\n[R code]\n\nconda_create(\n  envname = \"reptilia\",\n  packages = c(\"pandas\", \"pyarrow\")\n)\n\n\nWhen I list my conda environments I see that the reptilia environment exists:\n\n\n\n[R code]\n\nconda_list()\n\n\n          name                                                 python\n1         base                   /home/danielle/miniconda3/bin/python\n2 continuation /home/danielle/miniconda3/envs/continuation/bin/python\n3 r-reticulate /home/danielle/miniconda3/envs/r-reticulate/bin/python\n4     reptilia     /home/danielle/miniconda3/envs/reptilia/bin/python\n\n\nTo ensure that reticulate uses the reptilia environment throughout this post,7 I call the use_miniconda() function and specify the environment name:\n\n\n\n[R code]\n\nuse_miniconda(\"reptilia\")\n\n\nOur set up is now complete!\n\n\n\n\n\nA tree frog photographed near Cairns, because some reptiles are cute and adorable – even in Queensland. Original image freely available courtesy of David Clode via Unsplash.\n\n\n\n\n\n\nUsing reticulate to call Python from R\nNow that my environment is set up I’m ready to use Python. When calling Python code from within R, some code translation is necessary due to the differences in syntax across languages. As a simple example, let’s say I have my regular Python session open and I want to check my Python version and executable. To do this I’d import the sys library:\n\n\n\n[python code]\n\nimport sys\nprint(sys.version)\nprint(sys.executable)\n\n\n3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:15:10) \n[GCC 10.3.0]\n/home/danielle/miniconda3/envs/reptilia/bin/python\n\n\nTo execute these commands from R, the code needs some minor changes. The import() function replaces the import keyword, and $ replaces . as the accessor:\n\n\n\n[R code]\n\nsys &lt;- import(\"sys\")\nsys$version\nsys$executable\n\n\n[1] \"3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:15:10) \\n[GCC 10.3.0]\"\n[1] \"/home/danielle/miniconda3/envs/reptilia/bin/python\"\n\n\nThe code looks more R-like, but Python is doing the work.8\n\n\n\nCopying data frames between languages\nOkay, now that we understand the basics of reticulate, it’s time to tackle the problem of transferring data sets between R and Python. For now, let’s leave Arrow out of this. All we’re going to do is take an ordinary R data frame and transfer it to Python.\nFirst, let’s load some data into R. Sticking to the reptilian theme we’ve got going here, the data are taken from The Reptile Database (accessed August 31 2022), an open and freely available catalog of reptile species and their scientific classifications.9\n\n\n\n[R code]\n\ntaxa &lt;- read_csv2(\"taxa.csv\")\ntaxa\n\n\n# A tibble: 14,930 × 10\n   taxon_id family subfa…¹ genus subge…² speci…³ autho…⁴ infra…⁵ infra…⁶ infra…⁷\n   &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 2 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  alaicus ELPATJ…\n 3 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  kucenk… NIKOLS…\n 4 Ablepha… Scinc… Eugong… Able… NA      alaicus ELPATJ… subsp.  yakovl… (EREMC…\n 5 Ablepha… Scinc… Eugong… Able… NA      anatol… SCHMID… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 6 Ablepha… Scinc… Eugong… Able… NA      bivitt… (MENET… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 7 Ablepha… Scinc… Eugong… Able… NA      budaki  GÖCMEN… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 8 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   \n 9 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… subsp.  cherno… DAREVS…\n10 Ablepha… Scinc… Eugong… Able… NA      cherno… DAREVS… subsp.  eiselti SCHMID…\n# … with 14,920 more rows, and abbreviated variable names ¹​subfamily,\n#   ²​subgenus, ³​specific_epithet, ⁴​authority, ⁵​infraspecific_marker,\n#   ⁶​infraspecific_epithet, ⁷​infraspecific_authority\n\n\nCurrently this object is stored in-memory as an R data frame and we want to move it to Python. However, because Python data structures are different from R data structures, what this actually requires us to do is make a copy of the whole data set inside Python, using a Python-native data structure (in this case a Pandas DataFrame). Thankfully, reticulate does this seamlessly with the r_to_py() function:\n\n\n\n[R code]\n\npy_taxa &lt;- r_to_py(taxa)\npy_taxa\n\n\n                            taxon_id  ...    infraspecific_authority\n0                 Ablepharus_alaicus  ...                         NA\n1         Ablepharus_alaicus_alaicus  ...          ELPATJEVSKY, 1901\n2        Ablepharus_alaicus_kucenkoi  ...             NIKOLSKY, 1902\n3      Ablepharus_alaicus_yakovlevae  ...         (EREMCHENKO, 1983)\n4              Ablepharus_anatolicus  ...                         NA\n...                              ...  ...                        ...\n14925           Zygaspis_quadrifrons  ...                         NA\n14926               Zygaspis_vandami  ...                         NA\n14927     Zygaspis_vandami_arenicola  ...  BROADLEY & BROADLEY, 1997\n14928       Zygaspis_vandami_vandami  ...         (FITZSIMONS, 1930)\n14929              Zygaspis_violacea  ...                         NA\n\n[14930 rows x 10 columns]\n\n\nWithin the Python session, an object called r has been created: the Pandas DataFrame object is stored as r.py_taxa, and we can manipulate it using Python code in whatever fashion we normally might.\nIt helps to see a concrete example. To keep things simple, let’s pop over to our Python session and give ourselves a simple data wrangling task. Our goal is to count the number of entries in the data set for each reptilian family using Pandas syntax:\n\n\n\n[python code]\n\ncounts = r. \\\n  py_taxa[[\"family\", \"taxon_id\"]]. \\\n  groupby(\"family\"). \\\n  agg(len)\n  \ncounts\n\n\n                 taxon_id\nfamily                   \nAcrochordidae           3\nAgamidae              677\nAlligatoridae          16\nAlopoglossidae         32\nAmphisbaenidae        206\n...                   ...\nXenodermidae           30\nXenopeltidae            2\nXenophidiidae           2\nXenosauridae           15\nXenotyphlopidae         1\n\n[93 rows x 1 columns]\n\n\nNaturally I could have done this in R using dplyr functions, but that’s not the point of the post. What matters for our purposes is that counts is a Pandas DataFrame that now exists in the Python session, which we would like to pull back into our R session.\nThis turns out to be easier than I was expecting. The reticulate package exposes an object named py to the user, and any objects I created in my Python session can be accessed that way:\n\n\n\n[R code]\n\npy$counts\n\n\n                   taxon_id\nAcrochordidae             3\nAgamidae                677\nAlligatoridae            16\nAlopoglossidae           32\nAmphisbaenidae          206\nAnguidae                113\nAniliidae                 3\nAnomalepididae           23\nAnomochilidae             3\n...\n\n\nWhat’s especially neat is that the data structure has been automatically translated for us: the counts object in Python is a Pandas DataFrame, but when accessed from R it is automatically translated into a native R data structure: py$counts is a regular data frame:\n\n\n\n[R code]\n\nclass(py$counts)\n\n\n[1] \"data.frame\"\n\n\n\n\n\n\n\nA chameleon. I suppose there is some logic for this image, at least insofar as reticulate allows R to mimic Python and as for arrow Arrow – while it does a lot of the work in the next section — it blends seamlessly into the background. Like a chameleon. Get it? I’m so clever. Original image freely available courtesy of David Clode via Unsplash."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-with-arrow-in-the-polyglot-world",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#data-interchange-with-arrow-in-the-polyglot-world",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Data interchange with Arrow in the polyglot world",
    "text": "Data interchange with Arrow in the polyglot world\nSo far we have not touched Arrow, and you might be wondering if it’s even necessary to do so given that reticulate seems so smooth and seamless. Appearances can be a little deceiving however. The example from the last section only looks smooth and seamless because the data set is small. As I’ll show later in the post, cracks in the facade start to appear when you have to pass large data sets across languages. This happens for the very simple reason that a Pandas DataFrame is a different thing to an R data frame. It’s not possible for the two languages to share a single copy of the same data object because they don’t agree on what constitutes “a data object”. The only way we can do the handover is to make a copy of the data set and convert it to a format more suitable to the destination language. When the data set is small, this is not a problem. But as your data set grows, this becomes ever more burdensome. These copy-and-convert operations are not cheap.\nWouldn’t it be nice if R and Python could both agree to represent the data as, oh let’s say…. an Arrow Table? On the R side we could interact with it using the arrow R package, and on the Python side we could interact with it using the pyarrow module. But regardless of which language we’re using, the thing in memory would be exactly the same… handing over the data set from one language to the other would no longer require any copying. A little metadata would change hands, and that’s all.\nThat sounds much nicer.\n\n\nSetting up arrow\nI’m not going to talk much about setting up arrow for R in this post, because I’ve written about it before! In addition to the installation instructions on the arrow documentation there’s a getting started with arrow post on this blog. But in any case, it’s usually pretty straightfoward: you can install the arrow R package from CRAN in the usual way using install.packages(\"arrow\") and then load it in the usual fashion:\n\n\n\n[R code]\n\nlibrary(arrow)\n\n\nOn the Python side, I’ve already installed pyarrow earlier when setting up the “reptilia” environment. But had I not done so, I could redress this now using conda_install() with a command such as this:\n\n\n\n[R code]\n\nconda_install(\n  packages = \"pyarrow\", \n  envname = \"reptilia\"\n)\n\n\nFrom there we’re good to go. On the R side, let’s start by reading the reptiles data directly from file into an Arrow Table:\n\n\n\n[R code]\n\ntaxa_arrow &lt;- read_delim_arrow(\n  file = \"taxa.csv\", \n  delim = \";\", \n  as_data_frame = FALSE\n)\ntaxa_arrow\n\n\nTable\n14930 rows x 10 columns\n$taxon_id &lt;string&gt;\n$family &lt;string&gt;\n$subfamily &lt;string&gt;\n$genus &lt;string&gt;\n$subgenus &lt;null&gt;\n$specific_epithet &lt;string&gt;\n$authority &lt;string&gt;\n$infraspecific_marker &lt;string&gt;\n$infraspecific_epithet &lt;string&gt;\n$infraspecific_authority &lt;string&gt;\n\n\nNext let’s import pyarrow on the Python side and check the version:10\n\n\n\n[python code]\n\nimport pyarrow as pa\npa.__version__\n\n\n'8.0.0'\n\n\nEverything looks good here too!\n\n\n\nHandover to Python\nAfter all that set up, it’s almost comically easy to do the transfer itself. It’s literally the same as last time: we call r_to_py(). The taxa_arrow variable refers to an Arrow Table on the R side, so now all I have to do is use r_to_py() to create py_taxa_arrow, a variable that refers to the same Arrow Table from the Python side:\n\n\n\n[R code]\n\npy_taxa_arrow &lt;- r_to_py(taxa_arrow)\n\n\nSince we’re in Python now, let’s just switch languages and take a peek, shall we? Just like last time, objects created by reticulate are accessible on the Python side via the r object, so we access this object in Python with r.py_taxa_arrow:\n\n\n\n[python code]\n\nr.py_taxa_arrow\n\n\npyarrow.Table\ntaxon_id: string\nfamily: string\nsubfamily: string\ngenus: string\nsubgenus: null\nspecific_epithet: string\nauthority: string\ninfraspecific_marker: string\ninfraspecific_epithet: string\ninfraspecific_authority: string\n----\ntaxon_id: [[\"Ablepharus_alaicus\",\"Ablepharus_alaicus_alaicus\",\"Ablepharus_alaicus_kucenkoi\",\"Ablepharus_alaicus_yakovlevae\",\"Ablepharus_anatolicus\",...,\"Plestiodon_egregius_onocrepis\",\"Plestiodon_egregius_similis\",\"Plestiodon_elegans\",\"Plestiodon_fasciatus\",\"Plestiodon_finitimus\"],[\"Plestiodon_gilberti\",\"Plestiodon_gilberti_cancellosus\",\"Plestiodon_gilberti_gilberti\",\"Plestiodon_gilberti_placerensis\",\"Plestiodon_gilberti_rubricaudatus\",...,\"Zygaspis_quadrifrons\",\"Zygaspis_vandami\",\"Zygaspis_vandami_arenicola\",\"Zygaspis_vandami_vandami\",\"Zygaspis_violacea\"]]\nfamily: [[\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",...,\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\"],[\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",\"Scincidae\",...,\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\",\"Amphisbaenidae\"]]\nsubfamily: [[\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",\"Eugongylinae\",...,\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\"],[\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",\"Scincinae\",...,null,null,null,null,null]]\ngenus: [[\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",\"Ablepharus\",...,\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\"],[\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",\"Plestiodon\",...,\"Zygaspis\",\"Zygaspis\",\"Zygaspis\",\"Zygaspis\",\"Zygaspis\"]]\nsubgenus: [11142 nulls,3788 nulls]\nspecific_epithet: [[\"alaicus\",\"alaicus\",\"alaicus\",\"alaicus\",\"anatolicus\",...,\"egregius\",\"egregius\",\"elegans\",\"fasciatus\",\"finitimus\"],[\"gilberti\",\"gilberti\",\"gilberti\",\"gilberti\",\"gilberti\",...,\"quadrifrons\",\"vandami\",\"vandami\",\"vandami\",\"violacea\"]]\nauthority: [[\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"ELPATJEVSKY, 1901\",\"SCHMIDTLER, 1997\",...,\"BAIRD, 1858\",\"BAIRD, 1858\",\"(BOULENGER, 1887)\",\"(LINNAEUS, 1758)\",\"OKAMOTO & HIKIDA, 2012\"],[\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",\"(VAN DENBURGH, 1896)\",...,\"(PETERS, 1862)\",\"(FITZSIMONS, 1930)\",\"(FITZSIMONS, 1930)\",\"(FITZSIMONS, 1930)\",\"(PETERS, 1854)\"]]\ninfraspecific_marker: [[null,\"subsp.\",\"subsp.\",\"subsp.\",null,...,\"subsp.\",\"subsp.\",null,null,null],[null,\"subsp.\",\"subsp.\",\"subsp.\",\"subsp.\",...,null,null,\"subsp.\",\"subsp.\",null]]\ninfraspecific_epithet: [[null,\"alaicus\",\"kucenkoi\",\"yakovlevae\",null,...,\"onocrepis\",\"similis\",null,null,null],[null,\"cancellosus\",\"gilberti\",\"placerensis\",\"rubricaudatus\",...,null,null,\"arenicola\",\"vandami\",null]]\ninfraspecific_authority: [[null,\"ELPATJEVSKY, 1901\",\"NIKOLSKY, 1902\",\"(EREMCHENKO, 1983)\",null,...,\"(COPE, 1871)\",\"(MCCONKEY, 1957)\",null,null,null],[null,\"(RODGERS & FITCH, 1947)\",\"(VAN DENBURGH, 1896)\",\"(RODGERS, 1944)\",\"(TAYLOR, 1936)\",...,null,null,\"BROADLEY & BROADLEY, 1997\",\"(FITZSIMONS, 1930)\",null]]\n\n\nThe output is formatted slightly differently because the Python pyarrow library is now doing the work. You can see from the first line that this is a pyarrow Table, but nevertheless when you look at the rest of the output it’s pretty clear that this is the same table.\nEasy!\n\n\n\nHandover to R\nRight then, what’s next? Just like last time, let’s do a little bit of data wrangling on the Python side. In the code below I’m using pyarrow to do the same thing I did with Pandas earlier: counting the number of entries for each reptile family.\n\n\n\n[python code]\n\ncounts_arrow = r.py_taxa_arrow. \\\n  group_by(\"family\"). \\\n  aggregate([(\"taxon_id\", \"count\")]). \\\n  sort_by([(\"family\", \"ascending\")])\n  \ncounts_arrow\n\n\npyarrow.Table\ntaxon_id_count: int64\nfamily: string\n----\ntaxon_id_count: [[3,677,16,32,206,...,2,2,15,1,5]]\nfamily: [[\"Acrochordidae\",\"Agamidae\",\"Alligatoridae\",\"Alopoglossidae\",\"Amphisbaenidae\",...,\"Xenopeltidae\",\"Xenophidiidae\",\"Xenosauridae\",\"Xenotyphlopidae\",null]]\n\n\nFlipping back to R, the counts_arrow object is accessible via the py object. Let’s take a look:\n\n\n\n[R code]\n\npy$counts_arrow\n\n\nTable\n93 rows x 2 columns\n$taxon_id_count &lt;int64&gt;\n$family &lt;string&gt;\n\n\nThe output is formatted a little differently because now it’s the R arrow package tasked with printing the output, but it is the same Table.\nMission accomplished!\nBut… was it all worthwhile?\n\n\n\n\n\nA baby crocodile, just so that Bob Katter doesn’t feel like I completely forgot about his worries. It doesn’t look like it’s about to tear anyone to pieces but what would I know? I’m not an expert on such matters. Original image freely available courtesy of David Clode via Unsplash."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#does-arrow-really-make-a-big-difference",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#does-arrow-really-make-a-big-difference",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Does Arrow really make a big difference?",
    "text": "Does Arrow really make a big difference?\nAt the end of all this, you might want to know if using Arrow makes much of a difference. As much as I love learning new things for the sheer joy of learning new things, I prefer to learn useful things when I can! So let’s do a little comparison. First, I’ll define a handover_time() function that takes two arguments. The first argument n specifies the number of rows in the to-be-transferred data set. The second argument arrow is a logical value: setting arrow = FALSE means that an R data frame will be passed to Python as a Panda DataFrame, wheras arrow = TRUE means that an Arrow Table in R will be passed to Python and remain an Arrow Table. The actual data set is constructed by randomly sampling n rows from the taxa data set (with replacement):\n\n\n\n[R code]\n\nhandover_time &lt;- function(n, arrow = FALSE) {\n  data_in_r &lt;- slice_sample(taxa, n = n, replace = TRUE)\n  if(arrow) {\n    data_in_r &lt;- arrow_table(data_in_r)\n  }\n  tic()\n  data_in_python &lt;- r_to_py(data_in_r)\n  t &lt;- toc(quiet = TRUE)\n  return(t$toc - t$tic)\n}\n\n\nNow that I’ve defined the test function, let’s see what happens. I’ll vary the number of rows from 10000 to 1000000 for both the native data frame version and the Arrow Table version, and store the result as times:\n\n\n\n[R code]\n\ntimes &lt;- tibble(\n  n = seq(10000, 1000000, length.out = 100),\n  data_frame = map_dbl(n, handover_time),\n  arrow_table = map_dbl(n, handover_time, arrow = TRUE),\n)\n\n\nNow let’s plot the data:\n\n\n\n[R code]\n\ntimes |&gt; \n  pivot_longer(\n    cols = c(\"data_frame\", \"arrow_table\"), \n    names_to = \"type\", \n    values_to = \"time\"\n  ) |&gt; \n  mutate(\n    type = type |&gt; \n      factor(\n        levels = c(\"data_frame\", \"arrow_table\"),\n        labels = c(\"Data Frames\", \"Arrow Tables\")\n      )\n  ) |&gt;\n  ggplot(aes(n, time)) + \n  geom_point() + \n  facet_wrap(~type) + \n  theme_bw() + \n  labs(\n    x = \"Number of Rows\",\n    y = \"Handover Time (Seconds)\", \n    title = \"How long does it take to pass data from R to Python?\"\n  )\n\n\n\n\n\n\n\n\n\nOkay yeah. I’ll be the first to admit that this isn’t a very sophisticated way to do benchmarking, but when the difference is this stark you really don’t have to be sophisticated. Without Arrow, the only way to hand data from R to Python is to copy and convert the data, and that’s time consuming. The time cost gets worse the larger your data set becomes. With Arrow, the problem goes away because you’re not copying the data at all. The time cost is tiny and it stays tiny even as the data set gets bigger.\nSeems handy to me?"
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#acknowledgments",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#acknowledgments",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThank you to Marlene Mhangami and Fernanda Foertter for reviewing this post."
  },
  {
    "objectID": "posts/2022-09-09_reticulated-arrow/index.html#footnotes",
    "href": "posts/2022-09-09_reticulated-arrow/index.html#footnotes",
    "title": "Passing Arrow data between R and Python with reticulate",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThat being said, it wasn’t the worst comment on same sex marriage I saw an Australian politician make at the time, not by a long margin.↩︎\nSomething to note here is that the reticulate solution implicitly assumes R is your “primary” language and Python is the “secondary” language. That is, reticulate is an R package that calls Python, not a Python module that calls R. Simularly, this quarto document uses the knitr engine (also an R package) to integrate code from the two languages. Yes the tools are multi-language, but the setup is pretty R-centric. Arguably this is typical for how an R user would set up a multi-language project, and since R is my primary language it’s my preferred solution. However, it’s not a particularly Pythonic way of approaching the problem. But fear not, Python fans. In the next post I’m going to describe an approach that solves the same problem in a Python-centric way.↩︎\nOkay, in the spirit of total honesty… when I first started using reticulate I actually did let reticulate install its own version of Miniconda and everything was a total mess there for a while. My bash profile was set to find my original version of Miniconda, but reticulate was configured to look for the version it had installed. Hijinx ensued. As amusing as that little episode was, I’m much happier now that reticulate and bash are in agreement as to where Miniconda lives.↩︎\nThe easiest way to edit this file, if you don’t already know how, is to call usethis::edit_r_environ() at the R console.↩︎\nWell, all the Conda environments anyway↩︎\nYou can also use conda_install() to install into an existing conda environment.↩︎\nOkay, I should unpack a little. This blog is written using quarto, and in this post I’m using knitr as the engine to power the evaluation of code chunks. The knitr R package relies on reticulate when it needs to execute Python code. What that means is that code chunks labelled “Python code” in this post are implicitly executed using the same interface (reticulate) as the examples that I am explicitly calling when some of my “R code” chunks use reticulate in a more obvious way. When I call use_miniconda() here it specifies the Python environment used by reticulate in this R session, irrespective of which “path” I use. In other words, the “Python code” chunks and the explicit calls to reticulate functions are all executed with the same Python environment (reptilia) because they occur within the same R session.↩︎\nAs an aside it’s worth noting that reticulate exports an object called py, from which Python objects can be accessed: the sys object can also be referred to as py$sys.↩︎\nNote that the website does not explicitly specify a particular licence, but journal articles documenting the database written by the maintainers do refer to it as “open and freely available”. With that in mind I take it that the use of the data in this post is permitted. Naturally, should I discover that it is not I’ll immediately remove it!↩︎\nAs an aside – because I’m on on linux and life on linux is dark and full of terrors – this didn’t actually work for me the first time I tried it, and naturally I was filled with despair. Instead, I received this: libstdc++.so.6: version 'GLIBCXX_3.4.22' not found. As usual, googling the error message solved the problem. I updated with sudo apt-get install libstdc++6, and another catastrophe was thereby averted by copy/pasting into a search engine 🙃↩︎"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html",
    "href": "posts/2024-03-01_julia-foundation/index.html",
    "title": "A foundation in Julia",
    "section": "",
    "text": "After many years of procrastination and telling myself I’ll get around to it later, I’ve finally decided that now is the time for me to start learning Julia. At this point in my life I am strong in R, passable in Javascript, and can survive in SQL, C++ and Python if I need them for something. But despite my interest-from-afar in Julia, I haven’t had much of an excuse to dive into it before.\nPart of the appeal in Julia is that it’s designed to be a high-performance language for scientific computing. Like other scientific languages (e.g., R, Matlab, etc) it has 1-based indexing rather than 0-based indexing (Python, C++, etc). Julia code is automatically compiled giving you performance that is comparable to compiled languages like C++, without the hassle of actually having to deal with the compiler yourself. But we’ve all heard the sales pitch for Julia before, there’s no need for me to repeat it here, and anyway I kinda just want to dive into the code."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#getting-started",
    "href": "posts/2024-03-01_julia-foundation/index.html#getting-started",
    "title": "A foundation in Julia",
    "section": "Getting started",
    "text": "Getting started\nFirst things first. In order to get started I had to go through the process of installing Julia, which was pretty straightforward. Getting it to work within my quarto blog was a bit trickier, but there’s some fairly decent documentation on Julia for Quarto which got me there. After getting it set up it was as simple as including this line in the YAML header for this post,1\njupyter: julia-1.10\nand then creating executable Julia code cells by appending {julia} after the triple-fence used to define a block. So let’s see. Is Julia working in my quarto environment? I’ll start with my usual variant on the traditional “hello world” program using the println() (i.e., “print line”) function:\n\nprintln(\"hello cruel world\")\n\nhello cruel world\n\n\nYes, that seems to be working, as – shockingly – is the ability to do some basic calculations using aritmetic operators that seem pretty much the same as most languages I use:\n\n24 * 7\n\n168\n\n\nI can define variables, using = as the assignment operator:\n\nhours = 24;\ndays = 7;\n\nThe semicolons here are optional: they’re used as end-of-line delimiters, but the main reason I’ve used them in the code chunk above is to suppress printing the return value of these assignments.\nSo yes, we are up and running."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#object-types",
    "href": "posts/2024-03-01_julia-foundation/index.html#object-types",
    "title": "A foundation in Julia",
    "section": "Object types",
    "text": "Object types\nJulia is a dynamically typed language, so when I defined the hours variable earlier I was able to create an integer without explicitly defining it as such:\n\ntypeof(hours)\n\nInt64\n\n\nBy default Julia creates a 64-bit integer, but – unlike R and more like C++ – there are several integer types. If I’d wanted to create a 128-bit integer to represent the number of minutes in an hour (but why????) I could have done so by declaring the type explicitly:\n\nminutes::Int128 = 60;\ntypeof(minutes)\n\nInt128\n\n\nSo while minutes and hours are both integers they are different types, and – as you would expect – are represented differently internally. In an extremely strict language, it would not be possible to multiple minutes by hours without first converting at least one of them to a different type, but thankfully Julia operators will automatically promote to common type and so I can calculate the number of minutes in one day without doing the tedious type conversions myself:\n\ntypeof(minutes * hours)\n\nInt128\n\n\nYou can see the same mechanism in action when I try to calculate the number of minutes in 1.7 days. The minutes variable2 is a 64-bit integer, the hours variable is a 128-bit integer, but the value of 1.7 is represented as a 64-bit floating point numbers. So when I compute minutes * hours * 1.7, the return value is a 64-bit float:\n\ntypeof(minutes * hours * 1.7)\n\nFloat64\n\n\n\n\n\nThe big floating Vault thingy. Much more dramatic than the book version, I guess, but at the same time I kind of feel they use it as a Seldon Ex Machina a bit too much. Not my favourite innovation in the show"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#vectors",
    "href": "posts/2024-03-01_julia-foundation/index.html#vectors",
    "title": "A foundation in Julia",
    "section": "Vectors",
    "text": "Vectors\nI find myself liking the syntax Julia uses to create objects. You can create a vector using square brackets like this, which feels very much like Matlab to me:3\n\nwords = [\"hello\", \"cruel\", \"world\"];\n\nThe words variable I’ve just created is a vector of three strings:\n\ntypeof(words)\n\n\nVector{String} (alias for Array{String, 1})\n\n\n\nSubsetting uses square brackets too, and as I mentioned earlier indexing in Julia starts at 1:\n\nwords[1]\n\n\"hello\"\n\n\nA couple of other things to note here. In Julia, you need to be more careful about single versus double quotes than you would be in R (where they are interchangeable). In Julia, single quotes are used to define a single character (e.g., 'h' is a character), whereas double quotes are used to define a string (e.g. \"hello\" is a string). Strings are in fact a vector of characters, so \"hello\"[1] returns the character 'h'. But whatevs. Let’s move along."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#tuples",
    "href": "posts/2024-03-01_julia-foundation/index.html#tuples",
    "title": "A foundation in Julia",
    "section": "Tuples",
    "text": "Tuples\nI have no intention of diving too deeply into object types in Julia, but there are two more that I feel are worth mentioning at this point: tuples and dictionaries. Let’s start with tuples. A tuple is simply an ordered collection of values, and are constructed using parentheses:4\n\nfruit = (\"apple\", \"banana\", \"cherry\")\n\n(\"apple\", \"banana\", \"cherry\")"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#dictionaries",
    "href": "posts/2024-03-01_julia-foundation/index.html#dictionaries",
    "title": "A foundation in Julia",
    "section": "Dictionaries",
    "text": "Dictionaries\nIn contrast, a dictionary5 is a list of key-value pairs. There’s a few different ways to define a dictionary but I’m partial to this syntax:\n\ndanielle = Dict(\n  \"name\" =&gt; \"danielle\",\n  \"age\" =&gt; 47,\n  \"gender\" =&gt; \"female\",\n  \"boring\" =&gt; true\n)\n\nDict{String, Any} with 4 entries:\n  \"name\"   =&gt; \"danielle\"\n  \"boring\" =&gt; true\n  \"gender\" =&gt; \"female\"\n  \"age\"    =&gt; 47\n\n\nThe entries in a dictionary can be indexed using the keys:\n\ndanielle[\"gender\"]\n\n\"female\"\n\n\n\n\n\nNot even remotely a storyline in the books, obviously. I mean, the Cleonic dynasty was not a thing in the books, but the whole arc involves women playing politics, and also having sex sometimes"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#functions",
    "href": "posts/2024-03-01_julia-foundation/index.html#functions",
    "title": "A foundation in Julia",
    "section": "Functions",
    "text": "Functions\nThe syntax for defining functions in Julia comes in a couple of forms. The usual way to do it is using the function keyword, and I could define a simple greet() function like this:\n\nfunction greet(name) \n  \"hello $name, nice to meet you\"\nend;\n\nThe end keyword is required here. Note also that I’ve taken advantage of Julia’s string interpolation syntax to substitute the value of name into the string that greet() outputs:\n\ngreet(\"danielle\")\n\n\"hello danielle, nice to meet you\"\n\n\nYou can also create functions using the anonymous function syntax (e.g., x -&gt; \"hello $x\" defines an anonymous function), which is handy in the functional programming context if you want to map a vector of values onto another vector using map():\n\nmap(x -&gt; \"hello $x\", [\"amy\", \"belle\", \"chiara\"])\n\n3-element Vector{String}:\n \"hello amy\"\n \"hello belle\"\n \"hello chiara\"\n\n\nIn this case though I didn’t really need to resort to using map() because Julia also allows you to vectorise a function, using . to “broadcast” a scalar function to accept vector inputs:\n\ngreet.([\"amy\", \"belle\", \"chiara\"])\n\n3-element Vector{String}:\n \"hello amy, nice to meet you\"\n \"hello belle, nice to meet you\"\n \"hello chiara, nice to meet you\"\n\n\nI can see that being handy.\nI’ll come back to functions momentarily in order to talk about generic functions and method dispatch in Julia, but first I’ll pivot a little to talk about packages."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#packages",
    "href": "posts/2024-03-01_julia-foundation/index.html#packages",
    "title": "A foundation in Julia",
    "section": "Packages",
    "text": "Packages\nAs with any programming language, most of the power comes in Julia comes from the extensive collection of packages that other users have contributed. The usual way to install a package is via the Julia REPL.6 The Julia REPL is a little unusual in that it has several different “modes”. Normally your command prompt in the Julia REPL looks something like this:\njulia&gt;\nBut if you type ] at the REPL you’ll see it transform into something like this:7\n(@v1.10) pkg&gt;\nThis tells you that you’ve entered “package” mode, and you can type commands that can be used to install Julia packages and various other things.8 9 (If you want to get out of package mode and return to the regular REPL press “backspace”.)\nSo then, if you want to install the JSON package, the command you’d type at the REPL in package mode would simply be add JSON. And having installed the JSON package into my Julia environment, I can load it using the using keyword:\n\nusing JSON\n\nAnd now I can read the “praise.json” file that I just so happen to have sitting in my working directory by calling JSON.parsefile()\n\npraise_dict = JSON.parsefile(\"praise.json\")\n\nDict{String, Any} with 3 entries:\n  \"exclamation\" =&gt; Any[\"ah\", \"aha\", \"ahh\", \"ahhh\", \"aw\", \"aww\", \"awww\", \"aye\", …\n  \"superlative\" =&gt; Any[\"ace\", \"amazing\", \"astonishing\", \"astounding\", \"awe-insp…\n  \"adverb\"      =&gt; Any[\"beautifully\", \"bravely\", \"brightly\", \"calmly\", \"careful…\n\n\nMost convenient, because now that I have this praise_dict object I’m going to use it in the next section when I return to talking about functions…\n\n\n\nProbably a smart move by the showrunners to use pretty pictures and cool tech as a way of representing a mathematical discipline like psychohistory, but… also it’s very silly"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#methods",
    "href": "posts/2024-03-01_julia-foundation/index.html#methods",
    "title": "A foundation in Julia",
    "section": "Methods",
    "text": "Methods\nOne of my favourite little R packages is praise, which you can use to create random snippets of positive feedback that can be inserted in various places. Inspired by this, I’m going to define a cute little praise() function that does something similar.\nIn the last section I defined praise_dict, a handy dictionary that contains some adverbs, superlatives, and exclamations that you can use to construct random praise statements. So let’s define praise() such that it takes the name of a person as a string, and outputs a piece of positive feedback:\n\nfunction praise(name::String)\n    hey = rand(praise_dict[\"exclamation\"])\n    sup = rand(praise_dict[\"superlative\"])\n    adv = rand(praise_dict[\"adverb\"])\n    \"$hey $name you are $adv $sup\"\nend;\n\npraise(\"danielle\")\n\n\"wow danielle you are wisely wicked\"\n\n\nOh, that’s so sweet of you to say. Notice, however, that I’ve been a little stricter in how I’ve defined the input arguments for praise() than I was earlier when I defined greet(). The praise() function won’t work if the name argument is not a string:\n\npraise(103)\n\n\nMethodError: no method matching praise(::Int64)\n\nClosest candidates are:\n  praise(::String)\n   @ Main In[22]:1\n\n\n\n\nThat’s probably a good thing. We don’t typically provide praise to an integer, so it makes sense that the function doesn’t work when you pass it an integer!\nOn the other hand, we might want our praise() function to work if the user doesn’t pass it a name at all. To accomplish that, we can write another praise() function that doesn’t take any arguments:\n\nfunction praise()\n    hey = rand(praise_dict[\"exclamation\"])\n    sup = rand(praise_dict[\"superlative\"])\n    adv = rand(praise_dict[\"adverb\"])\n    \"$hey you are $adv $sup\"\nend;\n\nSo now this works:\n\npraise()\n\n\"mmh you are bravely good\"\n\n\nThe key thing to notice here is that though I’ve defined praise() twice, what Julia actually does in this situation is construct a single “generic” function that has two methods. In other words, praise() will work if you pass it a single string, and it will also work if you don’t pass it any arguments at all. It won’t work for any other kind of input. On the surface that seems pretty sensible, but in practice we might need a third method. Suppose I have a vector where there are a few people’s names listed, but it has missing values:10\n\npeople = [\"alex\", missing, \"fiona\"];\n\nMy praise() function isn’t inherently vectorised, but of course we can use the . syntax to praise several people at once and call praise.(people). Unfortunately this work right now because praise() doesn’t know what to do with the missing value. So if we want our praise() function to handle missing data gracefully it needs a third method:\n\nfunction praise(name::Missing)\n    hey = rand(praise_dict[\"exclamation\"])\n    sup = rand(praise_dict[\"superlative\"])\n    adv = rand(praise_dict[\"adverb\"])\n    \"$hey you are $adv $sup\"\nend;\n\nNow that we have all three methods praise() works just fine:\n\npraise.(people)\n\n3-element Vector{String}:\n \"mmhm alex you are correctly hunky-dory\"\n \"yeah you are truthfully primo\"\n \"mhm fiona you are neatly lovely\"\n\n\nAs an aside, if you ever needed to find out what methods have been defined for the praise() function, you can do so by calling methods(praise).\n\n\n\nThe jump ships are just cool. No further comment needed"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#piping",
    "href": "posts/2024-03-01_julia-foundation/index.html#piping",
    "title": "A foundation in Julia",
    "section": "Piping",
    "text": "Piping\nMuch like recent versions of R, Julia comes with a piping operator |&gt; that you can use to pass the output of one function to another one. So let’s say I have some numbers stored as vals, and I want to compute their geometric mean:\n\nvals = [10.2, 12.1, 14.3]\n\n3-element Vector{Float64}:\n 10.2\n 12.1\n 14.3\n\n\nIn Julia mean() is part of the Statistics package, so we’ll load that:\n\nusing Statistics\n\nTo compute the geometric mean, we first compute the natural logarithm for each element in vals using log(), compute the arithmetic mean of those log-values with mean(), and then exponentiate the result with exp(). Written as a series of nested function calls, it looks like this:\n\nexp(mean(log.(vals)))\n\n12.084829472557535\n\n\nAs has been noted many times in the past, one awkward feature of code written in this form is that you have to read it from the inside (innermost parentheses) to the outside in order to understand the sequence of events: first you take vals and pass it to log.(), then you take these logarithms and pass them to mean(), and then you take this mean and pass it to exp(). In this specific case it’s not terrible to read, because it just so happens that “exp mean log value” is pretty much how the formula for the geometric mean is written mathematically, but most data oriented programming isn’t structured to look exactly like an equation, and “inside out” code quickly becomes difficult to read.\nThis is where the “pipe” operator |&gt; comes in handy. You start with an object on the left hand side, and then pass it to the function named on the right hand side. When you chain a series of piping operations together you end up with code that reads left-to-right rather than inside-out:\n\nvals .|&gt; log |&gt; mean |&gt; exp\n\n12.084829472557535\n\n\nNotice that like other operators, I can use . to broadcast when using the pipe.\nMuch like R, Julia has multiple versions of the pipe. For the purpose of this post I’m only going to talk about the base pipe, which is much much stricter than the magrittr pipe %&gt;% in R, and indeed considerably stricter than the base pipe |&gt; in R.11 As you can see from the code above, the right hand side of the pipe is a function, not a call. The object supplied on the left hand side of the pipe is passed as the first argument to the function. No additional arguments can be supplied to the function on the right.\nOn the surface this seems very restrictive, but the longer I’ve been playing with Julia the more I realise it’s not as restrictive as I first thought. Because Julia makes it very easy to write anonymous functions, and because there’s very little overhead to calling one, you can write a pipeline that consists entirely of calls to anonymous functions. As a very simple example of a “split, apply, combine” style workflow constructed with the Julia pipe, here’s how you could use this to reverse each of the individual words in a string:\n\n\"hello cruel world\"  |&gt;\n  x -&gt; split(x, \" \") |&gt;\n  x -&gt; reverse.(x)   |&gt;\n  x -&gt; join(x, \" \")\n\n\"olleh leurc dlrow\"\n\n\nTo make this work I really do need to be able to specify additional arguments to split() and join(), which would not be permitted in a simpler application of the Julia pipe, but it works perfectly well here because those additional arguments are specified inside the anonymous functions to which the inputs are piped.\nHonestly, as much as I was initially like “ugh this is unwieldy”, I’m starting to appreciate the simplicity of the design and how it really does force you to start thinking about your pipelines in functional programming terms.12"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#function-composition",
    "href": "posts/2024-03-01_julia-foundation/index.html#function-composition",
    "title": "A foundation in Julia",
    "section": "Function composition",
    "text": "Function composition\nI should also mention that Julia has the function composition operator ∘ that you can use for this purpose, using much the same notation as in mathematics.13 So I could define a geomean() function as the following composition:\n\ngeomean = exp ∘ mean ∘ (x -&gt; log.(x))\ngeomean(vals)\n\n12.084829472557535\n\n\nIn this expression I’ve used an anonymous function as the third function to be composed so as to ensure that if the user passes a vector such as vals, the default behaviour of geomean() is to broadcast the call top log() (i.e., compute the log of each input individually), then pass the resulting vector of logarithms to mean() as a vector, and then pass the resulting scalar to exp().\nTo be honest, as cute as this is, I’m not sure I see much utility to this right now. So yeah, time to move onto the last “topic” in this foundations post, in which the author will mention but in no way actually explain the extensive capabilities that Julia has for allowing…\n\n\n\nGender-swapping Daneel to Demerzel was a good move in the show just for the sake of helping to mitigate the sausage-fest that the novels presented, but also I kind of think Demerzels story is just more interesting"
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#metaprogramming",
    "href": "posts/2024-03-01_julia-foundation/index.html#metaprogramming",
    "title": "A foundation in Julia",
    "section": "Metaprogramming",
    "text": "Metaprogramming\nMuch like R – and very unlike Matlab, which Julia syntax sometimes resembles – the design choices underpinning Julia has been influnced heavily by Lisp. While I have never actually learned to program in any of the major dialects of Lisp, I’ve always wanted to, and I’m a huge fan of the way that Lisp and its descendants contain programming constructs that directly represent abstract syntax trees and provide tools that let you manipulate user-supplied code.14\nBecause Julia Metaprogramming is such a powerful tool, what I’ve noticed already – even as a novice – is that most practical uses of the language end up relying on it heavily. Julia supports abstract Symbols, Expressions, and Macros, all of which start to pop up in your code once you start using it for real world data wrangling and visualisation. So it’s pretty important to understand something about how it all works. That said… it’s an advanced topic rather than a basic one, so what I think I’m going to do for now is issue a promissory note: I’ll talk more about this in later posts as those topics become relevant."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#where-to-next",
    "href": "posts/2024-03-01_julia-foundation/index.html#where-to-next",
    "title": "A foundation in Julia",
    "section": "Where to next?",
    "text": "Where to next?\nVery obviously, I skipped a lot of foundational topics in this post. This is not in any sense a coherent or complete introduction to Julia programming. I mean, I didn’t even bother to talk about control flow, which is one hell of an omission. But my goal here isn’t to provide a complete overview, and perhaps surprisingly I don’t actually use loops or if/then conditionals at all in the next two posts, so I simply haven’t bothered to write anything about those topics here. I focused on the things that popped up as I went about trying to try out a few things.\nIn any case, if you’re curious about where this is about to go, the second post in this series will talk about data frames and data wrangling, while the third post will take a look at a data visualisation tool. Which, quite frankly, is a lot more than I’d intended to do when I had this idea – which increasingly feels ill-advised – to play around with Julia and write about the experience."
  },
  {
    "objectID": "posts/2024-03-01_julia-foundation/index.html#footnotes",
    "href": "posts/2024-03-01_julia-foundation/index.html#footnotes",
    "title": "A foundation in Julia",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOkay that’s only half true. The other thing I ended up doing was creating a project environment for this blog post, and if you look at the source code for this post you can see that I’ve actually used Pkg.activate() to ensure that the Julia code in this post is executed using that environment. There’s a nice blog post on setting up a project environment here, but it’s a bit beyond the scope of what I want to cover here.↩︎\nTechnically, the value referenced by the minutes variable: values have types, variables are simply labels that point to values. But I shan’t be bothered with that distinction here.↩︎\nFun fact. Apart from a brief period in undergraduate where I learned a little bit of C, Matlab was my first programming language. But it’s been a very, very long time since I used Matlab – or GNU Octave – for anything. I imagine I could pick it up again if I had to but I honestly don’t see the point.↩︎\nA tuple is an immutable type, so the idea here is that you’re really supposed to use tuples to represent list of values that doesn’t change.↩︎\nDictionaries are mutable, so you can modify values stored in a dictionary.↩︎\nREPL = “Read-evaluate-print loop”. It’s a fancy way of referring to the command line I guess. In R we’d usually refer to the REPL as the R console, but other languages tend to use the term REPL.↩︎\nThere are other modes besides regular and package. For instance if you type ? at the REPL it takes you into help mode.↩︎\nYou don’t actually have to do it this way. The “package” mode in the REPL exposes various functions from the Pkg package, so if you have loaded Pkg then you could totally call Pkg.add() to install a package. In practice I find this a bit silly, but I suppose it has more useful applications in activating an environment via Pkg.activate() etc.↩︎\nThe syntax here is meaningful. If you are working in the base Julia environment, the bit in parentheses tells you that if you add a package it will be added to the base environment. For this blog post, however I’m using a custom environment called “sandbox” that has the packages I’m using, so the prompt I would see looks like this: (sandbox) pkg&gt;.↩︎\nThe missing value is used to define missing data in Julia, analogous to how NA is used to define missing values in R.↩︎\nIf you are interested in such things, the Pipe package supplies a pipe that is very similar to the R base pipe.↩︎\nSo much so, in fact, that while my original plan for these Julia posts was to briefly dispense of the base pipe and spend more time talking about the Pipe package, I think I’m going to skip the package entirely and just use base pipe + anonymous functions. ↩︎\nFor most editors that are configured to handle Julia syntax can type the ∘ operator by typing \\circ and then hitting tab.↩︎\nVery often I see programmers who have never worked in a Lisp-descended language (e.g., they know Python, C, etc. but not R, Julia, Scheme, etc.) react in horror and outrage at the things that you are permitted to do in languages that rely extensively on metaprogramming, but honestly I love it. I think it’s such a powerful tool for constructing domain specific languages within the confines of a more general language.↩︎"
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html",
    "href": "posts/2023-05-31_software-design-by-example/index.html",
    "title": "Software design by example",
    "section": "",
    "text": "The book I’m currently reading is Software Design by Example: A Tool-Based Introduction with JavaScript by Greg Wilson. Greg was kind enough to send me a review copy a little while back, and I’ve been slowly working my way through it.\nIn some ways I’m not the target audience for the book: it’s a book about software engineering that uses JavaScript for the worked examples, not a book designed to teach you JavaScript. I’m not the worst JavaScript coder in the world, but I’m not the strongest either, so it’s harder work for me than maybe it would have been if JavaScript were my primary language."
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#who-is-the-book-for",
    "href": "posts/2023-05-31_software-design-by-example/index.html#who-is-the-book-for",
    "title": "Software design by example",
    "section": "Who is the book for?",
    "text": "Who is the book for?\nSome years ago I took a very good course that Greg ran on how to teach technical concepts,1 and one thing he emphasised in that course is the importance of designing teaching materials with specific learning personas in mind. Having a small number of representative examples in mind when you write the content is incredibly useful when teaching, so it is no surprise that – literally on page 1 – the book states explicitly what the learner personas used to write the book were.\nI’m going to reproduce them in full in this blog post as a reminder to myself that this is the right way to construct learner personas.\n\n\n\nAïsha started writing VB macros for Excel in an accounting course and never looked back. After spending three years doing front-end JavaScript work she now wants to learn how to build back-end applications. This material will fill in some gaps in her programming knowledge and teach her some common design patterns\nRupinder is studying computer science at college. He has learned a lot about the theory of algorithms, and while he uses Git and unit testing tools in his assignments, he doesn’t feel he understands how they work. This material will give him a better understanding of these tools and how to design new ones.\nYim builds mobile apps for a living but also teaches two college courses: one on full-stack web development using JavaScript and Node and another titled “Software Design”. They are happy with the former, but frustrated that so many books about the latter subject talk about it in the abstract and use examples that their students can’t relat to. This material will fill those gaps and give them starting points for a wide variety of course assignments.\n\n\nThey’re detailed enough to make the teacher engage with the personas during the writing process, diverse enough to help catch things the writer might not have thought of, and provide a strong indication to the learner about whether the book is written for them. In my case, for instance, it’s pretty clear from the outset that I’m likely to struggle with the level of JavaScript involved. Indeed, when I look at the list of skills that the reader is expected to have (on pages 1-2) I’m fine on most things but I suspect I have a little less practical experience with JavaScript than is ideal for a reader of this book. I’m not terrible at it, I just don’t spend enough time with it to feel fluent.\nThat’s okay, of course. It’s often a good experience as a learner to read content that pushes you outside your comfort zone. What matters is that you know in advance which aspects are going to be hard work for you. Thankfully, Software Design by Example does that very well at the beginning."
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#rewarding-the-cursory-reader",
    "href": "posts/2023-05-31_software-design-by-example/index.html#rewarding-the-cursory-reader",
    "title": "Software design by example",
    "section": "Rewarding the cursory reader",
    "text": "Rewarding the cursory reader\nDespite knowing from the outset that I’m ever-so-slightly outside the target audience for the book, I’m still finding it very rewarding. To understand why, it’s worth taking a moment to look at how the book uses the glossary as the end to help readers like myself who have never received a formal education in programming… I’m basically the Aïsha persona, except with R playing the same role for me that JavaScript plays for her.\nBecause I don’t have a formal background and – to put it gently – don’t belong to a demographic that can easily acquire the “culture” of software engineering, I very commonly have the experience in conversation that software engineers will use terms that they simply assume that “everybody knows”, and never take the time to explain them. The actual concepts are often very simple things, and often I’ve had experience with them without knowing the names, but no-one ever tells you what the words mean and – sadly – many people in the field have a tendency to make you feel like an idiot if you ask.\nWith that as the real world backdrop, I’m finding the glossary to be worth its weight in gold. Here’s a little snippet from the top of page 318:\n\n\n\nquery string. The portion of a URL after the question mark ? that specfies extra parameters for the HTTP request as name-value pairs\nrace condition. A situation in which a result depend on the order in which two or more concurrent operations are carried out.\nraise (an exception). To signal that something unexpected or unusual has happened in a program by creating an exception and handling it to the error-handling system, which then tries to find a point in the program that will catch it.\nread-eval-print-loop (REPL). An interactive program that reads a command typed in by a user, executes it, prints the result, and then waits patiently for the next command. REPLs are often used to explore new ideas, or for debugging.\n\n\nI can honestly say that at no point in my life has someone explained to me what a race condition is or what a REPL actually is. Seriously. I’ve worked in tech companies and people use those terms all the time but they never explain them. Very frustrating. So when I read entries like this in the glossary I find myself going “oh, that’s what  means… okay, yes, I did already know this… but now I know what the name for it is”. I mean, race conditions are not at all unfamiliar to me – I encounter them quite a bit – but because software engineers have a tendency to refer to “race conditions” without ever saying what the term means, I’ve sat in a lot of very confusing conversations over the years that would have made a lot more bloody sense had I known the nomenclature or been in a position to “just ask” without being made to feel stupid.\nI think that’s likely to be true for a lot of self-taught programmers who never studied computer science, but instead had to learn to code in order to solve practical problems. The mere act of reading a concise definition of each thing has the effect of making my mental model more precise, and better aligned with the mental models that other people in the field adopt. It’s a helpful way to learn the culture and avoid getting caught out by the various shibboleths that are sadly pervasive the tech industry.2\nThere are other examples of this sort of thing throughout the book, historical anecdotes and other tidbits that make it a little easier for an outsider to make sense of the culture of software engineering. As an example, this little passage on page 145 makes sense of something I’ve never understood:\n\nThe coordinate systems for screens puts (0, 0) in the upper left corner instead of the lower left. X increases to the right as usual, but Y increases as we go down, rather than up [The book has a little picture here]. This convention is a holdover from the days of teletype terminals that printed lines on rolls of paper\n\nThese historical asides are really valuable. It feels a little bit like one of those “Magician’s Secrets Revealed!” shows. Knowing the terminology, conventions, and history behind a thing does so much of the work in making it all feel a bit more coherent.\nAnyway, let’s dive a little deeper, shall we?"
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#a-worked-example",
    "href": "posts/2023-05-31_software-design-by-example/index.html#a-worked-example",
    "title": "Software design by example",
    "section": "A worked example",
    "text": "A worked example\n\nMy mum always liked Delia Smith  And I drank, drank, drank just to deal with my shit  I learned to tell little white lies  When I feel inadequate almost all the time   I’d like to think I’m deep  I’d like to think I’m deep  I’d like to think I’m deep  But I just skim the pages, so I can fake my speech      - Sprints\n\nA little honesty when writing blog posts is important, I feel. When reading the book I did not, in fact, attempt all the exercises or work through all the code examples. I tinkered with a few of the examples, read some parts thoroughly, and skimmed other parts. That’s pretty representative of how I read technical books, really. I’ll pick a few parts that I want to understand properly and do a deep dive in those sections, and read the rest of it in a much more superficial way.\nThe parts that I did read fairly deeply are Chapters 7 and 8, which talk about how to parse a regular expression, and how to write the code that does pattern matching using them. Actually, if I’m super honest the part I spent most time with is Chapter 8, which shows you how to extract tokens and a parse tree for a restricted form of regular expression syntax. For that chapter, I worked through the examples and translated (most of) the code in R. In the rest of the post I’ll show the code that I wrote, tie it back to the structure of Chapter 8 in the book, and at the end I’ll say a little about what I learned from this exercise.\nThe subset of regular expression syntax that the book uses for this chapter has the following rules:\n\n\n\n\n\n\n\n\nToken Kind\nMeaning\nCharacters used\n\n\n\n\nLiteral\nA literal character\na, b, c, etc\n\n\nStart\nBeginning of string\n^\n\n\nEnd\nEnd of string\n$\n\n\nAny\nZero or more of the previous thing\n*\n\n\nOr\nEither the thing before or the thing after\n|\n\n\nGroup\nCollection of tokens to treat as one thing\n( and )\n\n\n\nIt’s a very small subset of the regular expression grammar available for pattern matching in R, JavaScript, and pretty much every language these days, but it’s a handy one. It’s certainly rich enough to make it an interesting exercise to write a parser. Not that it’s particularly important to write my own parser: the purpose of doing so is to wrap my head around the basics of how parsers work and nothing more. As Greg helpfully reminds us on page 99, if a format is commonly known there will be good tools already, and if you find yourself wanting to roll your own parser to interpret some new file format you just invented for something… there’s a good chance you shouldn’t do it. The world has enough file formats already."
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#writing-the-tokenizer",
    "href": "posts/2023-05-31_software-design-by-example/index.html#writing-the-tokenizer",
    "title": "Software design by example",
    "section": "Writing the tokenizer",
    "text": "Writing the tokenizer\nIf you want to write a regular expression parser, or any other parser for that matter, you actually have two distinct but interrelated problems to solve. Your data takes the form of an input string that you need to process. In English text, an input string might look like \"Danielle hates regular expressions\" but in a regular expression you’re more likely to have something that looks like \"^caa*t$. To interpret these strings – in a purely syntactic sense, not a semantic one – you need to\n\nCarve the string up into a sequence of distinct (and possibly labelled) tokens. My English expression could be divided into a sequence of four tokens corresponding to the four words: \"Danielle\", \"hates\", \"regular\", \"expressions\". This tokenization process is not uniquely defined. For instance, I might choose to acknowledge that the \"-s\" ending in some words is in fact a distinct unit. While this is absolutely not the post to talk about inflexional morphology in linguistics, it’s important to recognise that in some contexts you might want to tokenize my sentence as \"Danielle\", \"hate\", \"-s\", \"regular\", \"expression\", \"-s\".\nOrganise (or parse) the tokens into a structured format that explicitly acknowledges the way they relate to each other grammatically. For my English sentence this parsing might be something like this [Danielle] [[hates] [regular expressions]], where I’m using the square brackets to illustrate the organisation: \"regular expressions\" and \"Danielle\" are both noun phrases, [hates] is a verb, and [hates regular expressions] is a verb phrase.\n\nThe difference between the two is visually apparent when you try to draw them. When you tokenize the input you end up with a list of tokens:\n\n\nDanielle\nhates\nregular\nexpressions\n\n\nAfter parsing this list of tokens, you end up with a tree. There’s lots of ways you could visualise this tree, but something like this is good enough for my purposes:\n\n\n[\n  Danielle\n]\n[\n  [\n    hates\n  ]\n  [\n    regular\n    expressions\n  ]\n]\n\n\nI could probably take this a bit further and annotate each part of the tree the way that linguists like to do, but it’s not necessary to get the basic idea.\nThe key thing to realise is that these two problems aren’t independent. If you tokenize the string in a way that isn’t suited to your problem, you’re going to make life harder when you try to write the parser. As it happens, Software Design by Example gives you a very gentle example of how this happens, which I’ll get to in moment when I try to write some code that automatically parses regular expressions like ^caa*t$.\n\nPreliminaries\nIn the book, all the examples are – obviously!!! – written in JavaScript, and my code is going to be written in R. This matters somewhat since R and JavaScript are different languages with different assumptions about how you write code.3 To make my life a little easier, I’m going to define some extremely informal R classes4 that don’t actually do much. In my code, a “token” is just a list that has three fields: the kind of token referred to, the loc (location) of the token within the original string, and (optionally) a value that specifies the character (or characters) that represent the token in that string. Relatedly, the “token list” class is literally just a list:\n\n\n\ntoken_class.R\n\ntoken &lt;- function(kind, loc, value = NULL) {\n  structure(\n    list(kind = kind, loc = loc, value = value),\n    class = \"token\"\n  )\n}\n\ntoken_list &lt;- function(...) {\n  structure(\n    list(...),\n    class = \"token_list\"\n  )\n}\n\nprint.token &lt;- function(x, ...) {\n  cat(\"&lt;Token at \",  x$loc, \"&gt; \", x$kind, sep = \"\")\n  if(!is.null(x$value)) {\n    cat(\":\", x$value)\n  }\n  cat(\"\\n\")\n  return(invisible(x))\n}\n\nprint.token_list &lt;- function(x, ...) {\n  if(length(x) == 0) {\n    cat(\"&lt;Empty token list&gt;\\n\")\n  } else {\n    for(token in x) {\n      print(token)\n    }\n  }\n  return(invisible(x))\n}\n\n\nThe reason I decided to write this code for this post is that it also defines print() methods for these objects that make the printed output a little prettier:\n\ntoken(kind = \"Noun\", loc = 1, value = \"Danielle\")\n\n&lt;Token at 1&gt; Noun: Danielle\n\ntoken_list(\n  token(kind = \"Noun\", loc = 1, value = \"Danielle\"),\n  token(kind = \"Verb\", loc = 10, value = \"hates\")\n)\n\n&lt;Token at 1&gt; Noun: Danielle\n&lt;Token at 10&gt; Verb: hates\n\n\nWithout the print methods, you’d see each token printed as a list, and the list of tokens printed as a list of lists. It’s not super-important, but it does help me stay sane as I write. But whatever, let’s move on and start writing a tokenizer for (very restricted!) regular expressions…\n\n\nVersion 1\nA hallmark of good teaching – in my not-entirely-humble opinion – is when the instructor designs material in a way that gently encourages the learner to discover things on your own. Chapter 8 of Software Design by Example does this rather nicely at the beginning, by having the reader start out by writing a simple tokenizer that is later revealed to be “readable, efficient, and wrong”. At the risk of revealing the instructional magic that Greg is so terribly good at, I’ll explicitly state the flawed intuition that might lead you to write a tokenizer like this.\nSuppose I were to think about my tokenizer by considering a regular expression like \"^(cat)|(dog)$\". There’s a real trap you can fall into here, because this regular expression has natural language words, and you might be tempted to write a tokenizer that treats \"cat\" and \"dog\" as tokens. Or, to be slightly more precise, you might decide that to create tokens that allow “Literals” to contain multiple characters, like this:\n\ntokenize(\"^(cat|dog)$\")\n\n&lt;Token at 1&gt; Start\n&lt;Token at 2&gt; GroupStart\n&lt;Token at 3&gt; Literal: cat\n&lt;Token at 6&gt; Or\n&lt;Token at 7&gt; Literal: dog\n&lt;Token at 10&gt; GroupEnd\n&lt;Token at 11&gt; End\n\n\nMerging a sequence of literal characters into a single multi-character literal makes the output readable, and who doesn’t love it when the tokenizer informs you that you have a “Literal cat” in your string?\nThe book then walks you through the process of writing a tokenize() function that behaves exactly like this. Obviously, the original is in JavaScript, but here’s an R version:\n\n\n\ntokenizer_1.R\n\nsimple &lt;- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize &lt;- function(text) {\n  result &lt;- token_list()\n  n &lt;- 0\n  for (i in 1:nchar(text)) {\n    chr &lt;- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] &lt;- token(simple[[chr]], i)\n      n &lt;- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] &lt;- token(\"Start\", i)\n      n &lt;- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] &lt;- token(\"End\", i)\n      n &lt;- n + 1\n\n    # literals that follow a non-literal create a new token\n    } else if (n &gt; 0 && result[[n]]$kind != \"Literal\"){\n      result[[n + 1]] &lt;- token(\"Literal\", i, value = chr)\n      n &lt;- n + 1\n\n    # literals that follow a literal are combined with it\n    } else {\n      result[[n]]$value &lt;- paste0(result[[n]]$value, chr)\n    }\n  }\n  return(result)\n}\n\n\nI won’t recapitulate all the steps that go into writing code like this – read the book if you want to know things at this level of detail – but suffice it to say that if I were better at JavaScript I’d have found it very easy to follow.\nThe point that matters here is that the reader is being encouraged to consider what happens to you later if you tokenize the input this way. Merging a sequence of literal characters into a single multi-character literal makes the output readable, and in this specific case the token list is very convenient if I later wanted to write a regular expression matcher that builds on top of this tokenizer. Using the base R grepl() function, you can see which strings match \"^(cat|dog)$\" and which don’t:\n\ngrepl(\"^(cat|dog)$\", c(\"cat\", \"dog\", \"dag\", \"ca\"))\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nIf my tokenizer represents the literals in ^(cat|dog)$ as the two multicharacter literals \"cat\" and \"dog\" then – in this instance – it’s going to be easier for me to write a regex matcher that mirrors the behaviour of grepl().\nVery nice.\nUnfortunately, once you start thinking about regular expressions more generally, there’s a big problem with this tokenizer. It’s trying to be clever, by grouping multiple literals together without considering how those literals will be used later on by the parser, and it ends up being too greedy sometimes. Consider the regular expression \"^caa*t$\". This is a pattern that should match against \"cat\" and \"caaaaat\" but should not match \"caacaat\", as illustrated below:\n\ngrepl(\"^caa*t$\", c(\"cat\", \"caaaaat\", \"caacaat\"))\n\n[1]  TRUE  TRUE FALSE\n\n\nHowever, let’s look at the tokens produced by our tokenizer() function:\n\ntokenize(\"^caa*t$\")\n\n&lt;Token at 1&gt; Start\n&lt;Token at 2&gt; Literal: caa\n&lt;Token at 5&gt; Any\n&lt;Token at 6&gt; Literal: t\n&lt;Token at 7&gt; End\n\n\nYeah, we’re in big trouble here.\nThat’s the wrong way to tokenize this input: in this regular expression the * operator (i.e., the Any token in our token list) needs to be applied only to the preceding character, so it’s not correct to treat \"caa\" as a single literal string. To write a functioning parser on top of this tokenizer would be a nightmare, because the parser would need to inspect the internal contents of the tokens: in order to parse the * character (the Any token), it would need to grab the Literal caa token and split it into two parts, a prefix \"ca\" and a suffix \"a\", because * applies only to the suffix.\nWhat this is telling us is that we’ve chosen a poor tokenizing scheme. The book is quite gentle in leading the reader to this conclusion, but when you’re actually writing the code yourself you can’t avoid discovering it. If your parser has to break apart your tokens to organise the input, then really you should be rethinking the tokenizer.\n\n\nVersion 2\nOkay so that doesn’t work. The second approach considered in the chapter simplifies the code a little and produces a token list where every literal character is treated as a distinct token. The code is only a minor modification of the previous version:\n\n\n\ntokenizer_2.R\n\nsimple &lt;- list(\n  \"*\" = \"Any\",\n  \"|\" = \"Or\",\n  \"(\" = \"GroupStart\",\n  \")\" = \"GroupEnd\"\n)\n\ntokenize &lt;- function(text) {\n  result &lt;- token_list()\n  n &lt;- 0\n  for (i in 1:nchar(text)) {\n    chr &lt;- substr(text, start = i, stop = i)\n\n    # simple cases are always added as non-literal tokens\n    if (chr %in% names(simple)) {\n      result[[n + 1]] &lt;- token(simple[[chr]], i)\n      n &lt;- n + 1\n\n    # the ^ character is non-literal if position is 1\n    } else if (chr == \"^\" & i == 1) {\n      result[[n + 1]] &lt;- token(\"Start\", i)\n      n &lt;- n + 1\n\n    # the $ character is non-literal if it's the last character\n    } else if (chr == \"$\" & i == nchar(text)) {\n      result[[n + 1]] &lt;- token(\"End\", i)\n      n &lt;- n + 1\n\n    # literals always create a new token\n    } else {\n      result[[n + 1]] &lt;- token(\"Literal\", i, value = chr)\n      n &lt;- n + 1\n    }\n  }\n  return(result)\n}\n\n\nHere’s the sort of output we get from this version of the tokenizer:\n\ntokenize(\"^caa*t$\")\n\n&lt;Token at 1&gt; Start\n&lt;Token at 2&gt; Literal: c\n&lt;Token at 3&gt; Literal: a\n&lt;Token at 4&gt; Literal: a\n&lt;Token at 5&gt; Any\n&lt;Token at 6&gt; Literal: t\n&lt;Token at 7&gt; End\n\n\nThis output is entirely correct, as long as our goal is only to extract and label the tokens in our regular expression. The output correctly labels each literal as a literal and assigns the appropriate label to the non-literals. What is very clear, though, when you look at the output from this version of tokenize() is that it is absolutely not a parser. We’ve lost the grouping structure that we had in our original version. The tokenizer has no way of expressing the idea that \"ca\" is a syntactically coherent unit in the expression ^caa*t$, or that \"cat\" is similarly coherent within ^(cat|dog)$. It’s a good tokenizer, but a bad parser.\nThat’s okay: it’s not meant to be a parser. The tokenizer does its job perfectly well, and importantly it provides output that a good parser can work with. It requires a little thought, but it’s going to work out okay because the tokenizer is reliable as a tokenizer. Each tool does one job and only that job: you don’t need your tokenizer to parse the syntax, and you don’t want your parser to mess around with the internal contents of the tokens.\nGosh… I wonder if that’s one of those software engineering principles?\n\n\nPost-mortem\nAt this point any competent software engineer is probably screaming internally, because I have not written any unit tests for my code. This is terribly bad practice, and something I would never do when writing actual software.5 Suffice it to say Software Design by Example is at great pains to emphasize the importance of unit tests, and if you were actually following the chapter step by step you’d see that Greg does in fact introduce tests as he develops this example. I’ve been lazy in this blog post because… well, it’s a blog post. It’s neither intended to be software nor a chapter in a book on software engineering.\nAnyway… let’s return to the development of ideas in the chapter, yes?"
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#parsing-the-tokens",
    "href": "posts/2023-05-31_software-design-by-example/index.html#parsing-the-tokens",
    "title": "Software design by example",
    "section": "Parsing the tokens",
    "text": "Parsing the tokens\nThe second half of chapter 8 in Software Design by Example focuses on the parser, and again I’m not going to try to recapitulate all the logic that the book walks you through. It takes the reader through an intuitive process of thinking about how you want to write the parser, but the key thing I want to highlight is that when you’re reading the book you get a strong sense of why you want to write the parser in two parts: there’s a “forward pass” where the parser sweeps through the token list from first to last, constructing the parts of the parse tree that it can handle on the basis of what it has seen so far, and then a subsequent clean up phase where it sweeps back and fixes all the incomplete parts. When I came to implement it in R myself I made some small departures from the way Greg has done it in the book, but the essence is the same. What I’ll do here is present two versions of the parser, one that only does the forward pass (so you can see all the missing bits), and then a second version that does the clean up afterwards.\n\nPreliminaries\nAs before, I’ll do some preliminary work that isn’t really essential for the purposes of the book, but I find helpful for writing this blog post. Specifically, I’ll define a subtree() function that provides a “subtree” class. All it does is capture the fundamental structure of a tree: each node is defined by a parent element, and that parent can have zero, one, or more children. This isn’t really necessary for our parser, but it does allow me to define a print() method that makes the parse trees in this post look a little prettier:\n\n\n\nparse_class.R\n\nsubtree &lt;- function(parent, children = list()) {\n  structure(\n    list(parent = parent, children = children),\n    class = \"subtree\"\n  )\n}\n\nprint.subtree &lt;- function(x, ...) {\n  if(length(x) == 0) {\n    cat(\"&lt;Empty parse_tree&gt;\\n\")\n  } else {\n    print(x$parent)\n    if(length(x$children) &gt; 0) {\n      for(child in x$children) {\n        out &lt;- capture.output(print(child))\n        out &lt;- paste(\"    \", out)\n        cat(out, sep = \"\\n\")\n      }\n    }\n  }\n  return(invisible(x))\n}\n\n\nWhatevs. That’s not really the point of the book, and frankly if I were writing a book on software design6 I would probably make the same choice that Greg has made: for the blog post I want pretty output because it’s supposed to be an easy read. For a book that is intended to help you think about software design? For that you actually want the reader to engage deeply with the “list of lists of lists of…” data structure that my print methods are glossing over.\n\n\nVersion 1\nWith that out of the way, let’s have a look at the code for a parser that only does the forward sweep. The key insight that the book walks you through is that there are three distinct types of action the parser takes at this step:\n\nThere are kinds of token (Literal, Start, End, and GroupStart) that the parser can process simply by appending the token to the bottom of the tree the moment it encounters them.\nThere are other kinds of token (GroupEnd and Any) that require the parser to restructure the tree, but they rely only on the tokens seen so far, so the parser can reorganise the tree on the fly during this first pass\nThere is one annoying token (Or) that depends both on things the parser has already seen and on things that haven’t been observed yet as the parser sweeps forward. What we do here is a partial organisation: we process the bits we know about (e.g., the a on the left hand side of an a|b statement is known even when we’ve only read a| so far) but then leave a “Missing” placeholder token to express the fact that we know that the Or operator | has two children: we know a, but b will need to be filled in later.\n\nHere’s the R code, which supplies an update_tree() function that sequentially modifes the parse tree whenever new tokens arrive:7\n\n\n\nparser_1.R\n\nlist_reverse &lt;- function(x) {\n  x[length(x):1]\n}\n\nupdate_tree &lt;- function(tree, token) {\n\n  # For some kinds of token, we simply append them to the tree\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    tree[[length(tree) + 1]] &lt;- subtree(token)\n  }\n\n  # When GroupEnd is encountered, find the most recent GroupStart and\n  # make the tokens between them the children of a Group\n  if (token$kind == \"GroupEnd\") {\n    children &lt;- list()\n    while(TRUE) {\n      last &lt;- tree[[length(tree)]]\n      tree &lt;- tree[-length(tree)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] &lt;- last\n    }\n    tree[[length(tree) + 1]] &lt;- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # When Any is encountered, make the preceding token (or subtree)\n  # the child of the Any token\n  if (token$kind == \"Any\") {\n    last &lt;- tree[[length(tree)]]\n    tree[[length(tree)]] &lt;- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # When Or is encountered, create a subtree with two children. The\n  # first (or left) child is taken by moving it from the previous\n  # token/subtree in our list. The second child is tagged as \"Missing\"\n  # and will be filled in later\n  if (token$kind == \"Or\") {\n    last &lt;- tree[[length(tree)]]\n    tree[[length(tree)]] &lt;- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(tree)\n}\n\nparse &lt;- function(text) {\n  tree &lt;- list()\n  tokens &lt;- tokenize(text)\n  for(token in tokens) {\n    tree &lt;- update_tree(tree, token)\n  }\n  class(tree) &lt;- \"token_list\" # allows pretty printing\n  return(tree)\n}\n\n\nFor expressions like \"^caa*t$ that don’t have a |, this version of the parser constructs a completed parse tree:\n\nparse(\"^caa*t$\")\n\n&lt;Token at 1&gt; Start\n&lt;Token at 2&gt; Literal: c\n&lt;Token at 3&gt; Literal: a\n&lt;Token at 5&gt; Any\n     &lt;Token at 4&gt; Literal: a\n&lt;Token at 6&gt; Literal: t\n&lt;Token at 7&gt; End\n\n\nNotice that the ordering of the tokens has changed: to represent the subexpression a*, the parser creates an Any token corresponding to the * character and ensures that the Literal a token is a child of the Any operator.\nA similar thing happens where GroupStart and GroupEnd tokens are collapsed into a single Group token that has all tokens within the group as children:\n\nparse(\"(na)* hey yeah goodbye\")\n\n&lt;Token at 5&gt; Any\n     &lt;Token at 1&gt; Group\n          &lt;Token at 2&gt; Literal: n\n          &lt;Token at 3&gt; Literal: a\n&lt;Token at 6&gt; Literal:  \n&lt;Token at 7&gt; Literal: h\n&lt;Token at 8&gt; Literal: e\n&lt;Token at 9&gt; Literal: y\n&lt;Token at 10&gt; Literal:  \n&lt;Token at 11&gt; Literal: y\n&lt;Token at 12&gt; Literal: e\n&lt;Token at 13&gt; Literal: a\n&lt;Token at 14&gt; Literal: h\n&lt;Token at 15&gt; Literal:  \n&lt;Token at 16&gt; Literal: g\n&lt;Token at 17&gt; Literal: o\n&lt;Token at 18&gt; Literal: o\n&lt;Token at 19&gt; Literal: d\n&lt;Token at 20&gt; Literal: b\n&lt;Token at 21&gt; Literal: y\n&lt;Token at 22&gt; Literal: e\n\n\nFor expressions that contain an either/or operation, we end up with a parse tree that contains one or more Missing tokens. That’s my way of expressing the fact that the parser needs to come back and clean up afterwards:\n\nparse(\"(cat)|(dog)\")\n\n&lt;Token at 6&gt; Or\n     &lt;Token at 1&gt; Group\n          &lt;Token at 2&gt; Literal: c\n          &lt;Token at 3&gt; Literal: a\n          &lt;Token at 4&gt; Literal: t\n     &lt;Token at 0&gt; Missing\n&lt;Token at 7&gt; Group\n     &lt;Token at 8&gt; Literal: d\n     &lt;Token at 9&gt; Literal: o\n     &lt;Token at 10&gt; Literal: g\n\n\nNotice that there are some kinds of regular expressions for which this clean-up might require us to dive deep into the tree to fix the incomplete parts:\n\nparse(\"ab|((cd*)|ef)|g\")\n\n&lt;Token at 1&gt; Literal: a\n&lt;Token at 3&gt; Or\n     &lt;Token at 2&gt; Literal: b\n     &lt;Token at 0&gt; Missing\n&lt;Token at 14&gt; Or\n     &lt;Token at 4&gt; Group\n          &lt;Token at 10&gt; Or\n               &lt;Token at 5&gt; Group\n                    &lt;Token at 6&gt; Literal: c\n                    &lt;Token at 8&gt; Any\n                         &lt;Token at 7&gt; Literal: d\n               &lt;Token at 0&gt; Missing\n          &lt;Token at 11&gt; Literal: e\n          &lt;Token at 12&gt; Literal: f\n     &lt;Token at 0&gt; Missing\n&lt;Token at 15&gt; Literal: g\n\n\nAgain, there’s a kind of principle here: each part of the tool has its own job to do. The forward pass does the parts that it can do, and delegates the unfinished work to the clean-up process. When reading the book, and especially when implementing it yourself, the learner is invited to think about the importance of carving up the software into sensible parts.\nSeems like a thing worth knowing.\n\n\nVersion 2\nAt this point in the post I’m guessing that the reader is about ready to see the end product. The final version of the code adds a compress_tree() function that is called once the forward pass is complete. For simple trees all it really does is sweep up from bottom to top, but because there are regular expressions in which you can have Or tokens nested quite a long way into the parse tree it also sweeps up through the subtrees when they are encountered:\n\n\n\nparser_2.R\n\nlist_reverse &lt;- function(x) {\n  x[length(x):1]\n}\n\nupdate_tree &lt;- function(tree, token) {\n\n  # For some kinds of token, we simply append them to the tree\n  if(token$kind %in% c(\"Literal\", \"Start\", \"End\", \"GroupStart\")) {\n    tree[[length(tree) + 1]] &lt;- subtree(token)\n  }\n\n  # When GroupEnd is encountered, find the most recent GroupStart and\n  # make the tokens between them the children of a Group\n  if (token$kind == \"GroupEnd\") {\n    children &lt;- list()\n    while(TRUE) {\n      last &lt;- tree[[length(tree)]]\n      tree &lt;- tree[-length(tree)]\n      if(last$parent$kind == \"GroupStart\") {\n        break\n      }\n      children[[length(children) + 1]] &lt;- last\n    }\n    tree[[length(tree) + 1]] &lt;- subtree(\n      parent = token(\"Group\", last$parent$loc),\n      children = list_reverse(children)\n    )\n  }\n\n  # When Any is encountered, make the preceding token (or subtree)\n  # the child of the Any token\n  if (token$kind == \"Any\") {\n    last &lt;- tree[[length(tree)]]\n    tree[[length(tree)]] &lt;- subtree(\n      parent = token,\n      children = list(last)\n    )\n  }\n\n  # When Or is encountered, create a subtree with two children. The\n  # first (or left) child is taken by moving it from the previous\n  # token/subtree in our list. The second child is tagged as \"Missing\"\n  # and will be filled in later\n  if (token$kind == \"Or\") {\n    last &lt;- tree[[length(tree)]]\n    tree[[length(tree)]] &lt;- subtree(\n      parent = token,\n      children = list(last, subtree(token(kind = \"Missing\", loc = 0)))\n    )\n  }\n\n  return(tree)\n}\n\nhas_children &lt;- function(x) {\n  !is.null(x$children) && length(x$children) &gt; 0\n}\n\ncompress_or_skip &lt;- function(tree, location) {\n  if (has_children(tree[[location - 1]])) {\n    n &lt;- length(tree[[location - 1]]$children)\n    if (tree[[location - 1]]$children[[n]]$parent$kind == \"Missing\") {\n      tree[[location - 1]]$children[[n]] &lt;- tree[[location]]\n      tree[[location]] &lt;- NULL\n    }\n  }\n  return(tree)\n}\n\ncompress_tree &lt;- function(tree) {\n  if (length(tree) &lt;= 1) {\n    return(tree)\n  }\n\n  # Compress branches of the top-level tree\n  loc &lt;- length(tree)\n  while (loc &gt; 1) {\n    tree &lt;- compress_or_skip(tree, loc)\n    loc &lt;- loc - 1\n  }\n\n  # Recursively compress branches of children subtrees\n  for (loc in 1:length(tree)) {\n    tree[[loc]]$children &lt;- compress_tree(tree[[loc]]$children)\n  }\n\n  return(tree)\n}\n\nparse &lt;- function(text) {\n  tree &lt;- list()\n  tokens &lt;- tokenize(text)\n  for(token in tokens) {\n    tree &lt;- update_tree(tree, token)\n  }\n  tree &lt;- compress_tree(tree)\n  class(tree) &lt;- \"token_list\" # allows pretty printing\n  return(tree)\n}\n\n\nFor expressions that don’t have an Or token, the output is the same as before:\n\nparse(\"^caa*t$\")\n\n&lt;Token at 1&gt; Start\n&lt;Token at 2&gt; Literal: c\n&lt;Token at 3&gt; Literal: a\n&lt;Token at 5&gt; Any\n     &lt;Token at 4&gt; Literal: a\n&lt;Token at 6&gt; Literal: t\n&lt;Token at 7&gt; End\n\n\nHowever, what we see in this version is that the parse tree for expressions like \"(cat)|(dog)\" now places the right hand side of the either/or expression (i.e., the (dog) group) in the appropriate place:\n\nparse(\"(cat)|(dog)\")\n\n&lt;Token at 6&gt; Or\n     &lt;Token at 1&gt; Group\n          &lt;Token at 2&gt; Literal: c\n          &lt;Token at 3&gt; Literal: a\n          &lt;Token at 4&gt; Literal: t\n     &lt;Token at 7&gt; Group\n          &lt;Token at 8&gt; Literal: d\n          &lt;Token at 9&gt; Literal: o\n          &lt;Token at 10&gt; Literal: g\n\n\nNo more of those “Missing” tokens. And because the compress_tree() function is recursively applied to the subtrees, it can also handle uglier expressions like \"ab|((cd*)|ef)|g\":\n\nparse(\"ab|((cd*)|ef)|g\")\n\n&lt;Token at 1&gt; Literal: a\n&lt;Token at 3&gt; Or\n     &lt;Token at 2&gt; Literal: b\n     &lt;Token at 14&gt; Or\n          &lt;Token at 4&gt; Group\n               &lt;Token at 10&gt; Or\n                    &lt;Token at 5&gt; Group\n                         &lt;Token at 6&gt; Literal: c\n                         &lt;Token at 8&gt; Any\n                              &lt;Token at 7&gt; Literal: d\n                    &lt;Token at 11&gt; Literal: e\n               &lt;Token at 12&gt; Literal: f\n          &lt;Token at 15&gt; Literal: g\n\n\nNeat. I mean… the thing itself is ugly as sin, but the fact that an amateur like myself can spend a day or two working through the relevant chapters in the book and write code that can handle parse trees like these ones… yeah, that’s very neat indeed."
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#final-thoughts",
    "href": "posts/2023-05-31_software-design-by-example/index.html#final-thoughts",
    "title": "Software design by example",
    "section": "Final thoughts?",
    "text": "Final thoughts?\nBack when I was an academic, as opposed to whatever the hell it is I am in my middle aged unemployment era, there was this whole tradition of writing “critical” book reviews that would ostensibly count as scholarly output. This led to this weird thing where you’d read a book and enjoy it, but you’d feel this obligation to write a Proper Academic Review, and like most things in academia that have Prestige Points associated with them, these reviews would be extremely thorough, mind-meltingly boring, and pointlessly cruel. Because that’s what academia is designed to encourage.\nYeah nah. I’m too old for that shit, and I gave up tenure a lifetime8 ago.\nHaving read most of the book at a superficial level and done a deep dive into the parts that I felt like diving into, I have to say I rather like Software Design by Example. It’s not written the way I would write a book: anyone who knows me understands that I will never choose to write a 200 page book when I could write a 600 page book instead. Greg Wilson doesn’t write like me: the book is brief, it covers a lot of topics concisely, and yet still manages to convey a lot of the “folk knowledge” and other cultural bits and pieces that actually matter in the wild. Honestly that’s an awfully impressive achievement. I quite enjoyed it.\nTotally worth the time I spent reading it. Would recommend to others."
  },
  {
    "objectID": "posts/2023-05-31_software-design-by-example/index.html#footnotes",
    "href": "posts/2023-05-31_software-design-by-example/index.html#footnotes",
    "title": "Software design by example",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSomething that you’d think I’d have been taught back when I was an academic and had to do it for a living. Universities, however, are utterly useless at this kind of thing. They tend to throw professors in the deep end with this expectation that someone who has made a career as a good researcher will automatically work out how to be a good teacher. Suffice it to say, this widespread practice has not been the best thing for higher education.↩︎\nAs an aside, this is hardly unique to the tech world. Academia is just as bad. Probably worse, actually.↩︎\nExample: Greg’s code relies heavily on JavaScript .push() and .pop() methods that make it very easy to treat an array as a stack. I’m going to work with R lists that don’t have these little niceties. It’s going to make some of my code later a little ugly, I’m afraid. Oh well.↩︎\nThis is not the place for me to talk about my love/hate relationship with the S3 object-oriented programming system in R. Suffice it to say, I have feelings. That’s enough for now.↩︎\nOkay, I’ll be honest, I did have a set of informal tests that I wrote to accompany my code, even for this blog post: I was too lazy to bother writing a proper test suite, but the initial drafts of this blog post had a bunch of regular expressions that would be tokenized/parsed every time I rendered the draft, which let me check that the code I was writing was not “total batshit”. I do have some standards, at least when it comes to code. The same cannot be said of my taste in men, but that’s a different story…↩︎\nYeah no that will never happen. If you’re waiting for me to attempt something like what Greg has done, then – to quote Paul Kelly and Kev Carmody – you don’t stand the chance of a cinder in snow. I know my limits.↩︎\nThis is analogous to the handle() JavaScript function that appears in the book. What can I say? I’m a perverse woman who likes to use her own names for things.↩︎\nTwo years, but that’s a lifetime in tranny-years. We age fast, you know.↩︎"
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html",
    "href": "posts/2024-12-24_art-from-code-7/index.html",
    "title": "Art from code VII: Pixel filters",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the seventh post in that series.\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(ggfx)\nlibrary(flametree)\nlibrary(ambient)\nThe last technical topic I want to cover in this workshop is pixel filters, and specifically the ggfx package which makes it relatively easy to use them in conjunction with ggplot2. Phrased in the most general terms, a filter is any function that takes one image as input and returns a new (presumably modified!) image as output. As an example, think about how Instagram filters work: the original photo is used as the input, and the modified one is returned as output. This filtering takes place at the pixel level, so it’s not immediately obvious how we could do this with ggplot2. The way that the ggfx package handles this is to render render the image (or part of the image) off screen to obtain a representation that can be filtered. The filter is applied to that rendered image and then, when the final plot is constructed, the filtered versions are included in the final plot rather than the original ones.\nIn this session I’ll provide an introduction to ggfx. The API for ggfx takes a little while to wrap your head around, but once you’ve got a handle on it, it turns out to be a very powerful tool for generative art in R."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#prelude",
    "href": "posts/2024-12-24_art-from-code-7/index.html#prelude",
    "title": "Art from code VII: Pixel filters",
    "section": "Prelude",
    "text": "Prelude\nWhen we dive into ggfx, it will be handy to have some data that we can use to make interesting art. For that purpose I’ll rely on the flametree package which I’ve used in the past to make pretty pictures of trees. The flametree_grow() function generates the raw data:\n\ntree &lt;- flametree_grow(\n  seed = 1, \n  time = 9, \n  angle = c(-15, 15, 30)\n)\ntree\n\n# A tibble: 3,069 × 12\n   coord_x coord_y id_tree id_time id_path id_leaf id_pathtree id_step\n     &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;   &lt;chr&gt;         &lt;int&gt;\n 1  -0.429    0          1       1       1 FALSE   1_1               0\n 2  -0.429    0.5        1       1       1 FALSE   1_1               1\n 3  -0.429    1          1       1       1 FALSE   1_1               2\n 4  -0.429    1          1       2       2 FALSE   1_2               0\n 5  -0.429    1.45       1       2       2 FALSE   1_2               1\n 6  -0.196    1.87       1       2       2 FALSE   1_2               2\n 7  -0.429    1          1       2       3 FALSE   1_3               0\n 8  -0.429    1.3        1       2       3 FALSE   1_3               1\n 9  -0.274    1.58       1       2       3 FALSE   1_3               2\n10  -0.196    1.87       1       3       4 FALSE   1_4               0\n# ℹ 3,059 more rows\n# ℹ 4 more variables: seg_deg &lt;dbl&gt;, seg_len &lt;dbl&gt;, seg_col &lt;dbl&gt;,\n#   seg_wid &lt;dbl&gt;\n\n\nYou can render the output conveniently with the help of flametree_plot():\n\ntree |&gt; \n  flametree_plot(\n    background = \"#ffffff\", \n    palette = c(\"#222222\", \"#f652a0\")\n  )\n\n\n\n\n\n\n\n\nThe flametree_plot() function is fairly flexible and allows you to draw the tree in several different styles, but for the purposes of this session we’ll want to write our own ggplot2 (and ggfx) code, so it may be helpful to explain what you’re looking at in this tree. Each curved segment in the tree is drawn as a Bézier curve specified by three control points whose locations are given by the coord_x and coord_y variables. Each row in the tibble refers to one control point, so there are three rows per segment. Here’s the three rows that correspond to the 99th segment in the tree:\n\ntree |&gt; filter(id_path == 99)\n\n# A tibble: 3 × 12\n  coord_x coord_y id_tree id_time id_path id_leaf id_pathtree id_step\n    &lt;dbl&gt;   &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;   &lt;chr&gt;         &lt;int&gt;\n1   -1.05    2.75       1       7      99 FALSE   1_99              0\n2   -1.11    2.78       1       7      99 FALSE   1_99              1\n3   -1.19    2.75       1       7      99 FALSE   1_99              2\n# ℹ 4 more variables: seg_deg &lt;dbl&gt;, seg_len &lt;dbl&gt;, seg_col &lt;dbl&gt;,\n#   seg_wid &lt;dbl&gt;\n\n\nThe various columns here describe different aspects of the tree: there’s a seg_wid column representing the width of each segment (usually mapped to the size aesthetic), a seg_col column specifying a colour (usually mapped to the colour aesthetic), etc.\nOf particular relevance here is the id_leaf column. This column is a logical variable that is TRUE only for those control points that represent the very tips of the tree: the leaves, essentially. Later on I’m going to plot the tree trunk and leaves separately, so it’s convenient to have a leaf tibble that contains the data only for the leaf nodes:\n\nleaf &lt;- tree |&gt; filter(id_leaf == TRUE)\n\nOkay, now let’s construct a flametree image piece by piece. I’ll start with a base plot that specifies some stylistic choices but doesn’t map any aesthetics, doesn’t include any data, and doesn’t have any geoms. It’s a blank canvas:\n\nbase &lt;- ggplot() + \n  scale_linewidth_identity() + \n  scale_size_identity() + \n  theme_void() + \n  coord_equal()\n\nNow I’m going to create the individual geoms. Usually when we write ggplot2 code we just add the geoms directly to the plot, and if my only intention was to show you how flametree works I’d do that here. But later I’m going to use this in conjunction with ggfx, and the design of ggfx is such that you (usually) apply a filter to a ggplot2 layer. The ggfx code will look a lot cleaner if we store our layers as variables.\nFirst, let’s create a layer representing the leaves. The only thing we want to do with the leaf data is draw a scatter plot, and we can accomplish our goal using geom_point():\n\nleaves &lt;- geom_point(\n  mapping = aes(coord_x, coord_y),\n  data = leaf, \n  size = 1.3, \n  stroke = 0, \n  colour = \"#e38b75\"\n)\n\nNow we can do the same thing to create the trunk. What we want in this case is something similar to geom_path(), but instead of plotting paths in linear segments we want to draw Bézier curves. Conveniently for us, the ggforce package supplies the geom_bezier() function that does exactly this. All we need to do is make sure we specify the group aesthetic so that there is one curve per segment in the tree. Here’s the code for doing that:\n\ntrunk &lt;- geom_bezier(\n  mapping = aes(\n    x = coord_x, \n    y = coord_y,\n    group = id_pathtree, \n    linewidth = seg_wid\n  ),\n  data = tree, \n  lineend = \"round\", \n  colour = \"#555555\",\n  show.legend = FALSE\n)\n\nHaving done all the work in this piecewise manner, the code to draw the tree is as simple as adding the trunk and leaves to the base image:\n\nbase + trunk + leaves\n\n\n\n\n\n\n\n\nVery pretty!\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the flametree-example.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#glow",
    "href": "posts/2024-12-24_art-from-code-7/index.html#glow",
    "title": "Art from code VII: Pixel filters",
    "section": "Glow",
    "text": "Glow\n…but it could be prettier, don’t you think? Maybe we could add a glow around the leaves, to give it a twinkling look. This feels like the kind of job for a pixel filter, and ggfx should be able to do this for us. Conceptually, what we want to do is apply a “glow” filter to the leaves of our tree, but not to the trunk. We can do that in ggfx by using the with_outer_glow() filtering function: we pass the leaves layer to the filter, along with some other arguments (e.g., colour) that specify the properties of the glow. The examples below show this in action: the left plot is the original tree, the middle one adds a small white glow to the leaves, and the right one adds a much bigger glow:\nbase + trunk + leaves\nbase +   \n  trunk + \n  with_outer_glow(leaves, colour = \"blue\")\nbase +   \n  trunk + \n  with_outer_glow(leaves, colour = \"blue\", sigma = 10, expand =5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the third example, sigma is interpreted as the standard deviation of the Gaussian blur added around each point: sigma = 5 specifies that this standard deviation is 5 pixels in size. The expand = 3 argument tells ggfx to “expand” each point by 3 pixels before drawing the glow. This can be handy for very small objects that would otherwise be difficult to see even with the added glow.\nWhen written like this – with each layer stored as a named variable – ggfx code is very easy to read. I’ve quickly found myself adopting this code style any time I want to do pixel filtering. Constructing the plot piecewise and storing each layer as a variable makes it much easier to see what I’m doing!\nAs an aside, yes, there is also a with_inner_glow() function that applies the glow to the interior of each object rather than extending the glow outside it.\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the ggfx-glow.R file in the materials."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#dither",
    "href": "posts/2024-12-24_art-from-code-7/index.html#dither",
    "title": "Art from code VII: Pixel filters",
    "section": "Dither",
    "text": "Dither\nThe previous example gives you a pretty good sense of the core mechanic of ggfx: the package supplies the with_*() functions that correspond to different kinds of pixel filtering operations. There are quite a lot of filters you might want to try. Dithering is a technique in which we reduce (or quantise) an image that has many colours down to an image with fewer colours, and then add some random (or not-so-random) noise to the coarser-grained image in a way that leaves it looking fairly natural. There are a lot of ways in which you can dither an image: one of the most famous is the Floyd-Steinberg algorithm. The Wikipedia page on Floyd-Steinberg dithering gives a nice (CC-licenced!) example using a kitten picture, shown to the left.\n  \nThe original image is shown on the left. The image in the middle reduces the number of distinct colours in the image (i.e. quantises it) but doesn’t apply any dithering. It’s immediately obvious that there are artifacts in the image, and it doesn’t look like a very good approximation to the original. On the right, a dithering algorithm is applied. The image still uses the same small set of colours, but arranges the pixels in such a way that the local density of light and dark pixels gives the impression of shading.\nThe ggfx package supplies several dithering filters, including:\n\nwith_dither() uses Floyd-Steinberg dithering\nwith_halftone_dither() uses halftone dots\nwith_ordered_dither() uses ordered dithering\nwith_custom_dither() lets you build your own!\n\nSo let’s take a look. If we want to apply dithering to some generative art, it would help to have some generative art code! This time around, I’ll reuse the code that we wrote in the very first session to create “polar coordinate art” in ggplot2. To spare you the effort of revisiting, here’s the code we’re going to use to specify a base image:\n\nset.seed(1)\npolar &lt;- tibble(\n  arc_start = runif(200),\n  arc_end = arc_start + runif(200, min = -.2, max = .2),\n  radius = runif(200),\n  shade = runif(200), \n  width = runif(200)\n)\n\nbase &lt;- ggplot(\n  data = polar, \n  mapping = aes(\n    x = arc_start, \n    y = radius,\n    xend = arc_end, \n    yend = radius, \n    colour = shade, \n    linewidth = width\n  )\n) + \n  coord_polar(clip = \"off\") +\n  scale_y_continuous(limits = c(0, 1), oob = scales::oob_keep) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::oob_keep) + \n  scale_colour_viridis_c(option = \"magma\") +\n  guides(colour = guide_none(), linewidth = guide_none()) + \n  scale_linewidth(range = c(0, 10)) + \n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#aaaaaa\"))\n\nThe details to this don’t matter very much right now. All that matters is that when we add geom_segment() to base, it produces radial art like the image shown below on the left. Then, on the right, we can see the effect of the with_dither() function. I’ve reduced the image to five distinct colours in order to exaggerate the dithering effect as much as possible. Any lower than this and the image degrades too much.\nbase + geom_segment()\nbase + with_dither(geom_segment(), max_colours = 5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFloyd-Steinberg dithering gives the image a slightly grainy feel. It can be very handy when you don’t want the art to look too clean. Rendering the image in halftone gives it a more patterend feel, as the images below illustrate:\nbase + with_halftone_dither(geom_segment())\nwith_halftone_dither(base + geom_segment())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice the difference between these two examples. The with_*() functions in ggfx are quite flexible. You can pass a single layer as the input, or alternatively you can pass the whole image. In fact, if you’re comfortable with grid graphics, you can pass individual grobs too. This is discussed in the ggfx object support help page.\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the ggfx-dither.R file in the materials."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#mask",
    "href": "posts/2024-12-24_art-from-code-7/index.html#mask",
    "title": "Art from code VII: Pixel filters",
    "section": "Mask",
    "text": "Mask\nOne of my favourite filtering tricks is to use with_mask() as a way of displaying one layer of the plot only when it overlaps with a second layer (referred to as the “mask”). To see why I love this so much, let’s use the ambient package to create a textured grid that we could draw using geom_raster(). I talked about this in the session on spatial noise tricks, so I won’t bore you by repeating the explanation. Instead, here’s the code to create a layer called texture:\n\ntexture &lt;- geom_raster(\n  mapping = aes(x, y, fill = paint),\n  data = long_grid(\n    x = seq(from = -1, to = 1, length.out = 1000),\n    y = seq(from = -1, to = 1, length.out = 1000)\n  ) |&gt; \n    mutate(\n      lf_noise = gen_simplex(x, y, frequency = 2, seed = 1234),\n      mf_noise = gen_simplex(x, y, frequency = 20, seed = 1234),\n      hf_noise = gen_simplex(x, y, frequency = 99, seed = 1234),\n      paint = lf_noise + mf_noise + hf_noise\n    )\n)\n\nWe’re going to use texture as our background, but instead of plotting the whole thing we’re going mask it. Let’s create a polygon layer that will serve as the mask.\n\nhex &lt;- tibble(x = sin((0:6)/6 * 2 * pi), y = cos((0:6)/6 * 2 * pi))\nmask &lt;- geom_polygon(aes(x, y), hex, fill = \"white\", color = \"black\")\n\nAs before I’ll create a base plot to which we can add these geoms. The code isn’t very interesting, but I should be thorough and show it to you anyway:\n\nbase &lt;- ggplot() + \n  theme_void() +\n  coord_equal() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_gradientn(\n    colours = c(\"#222222\",\"#e83e8c\"), \n    guide = guide_none()\n  )\n\nOkay, so now let’s take a look the two layers individually:\nbase + texture\nbase + mask\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGosh, I wonder what we could possibly do with a texture and a hexagon shape…\n\nbase + \n  as_reference(mask, id = \"mask\") + \n  with_mask(texture, \"mask\")\n\n\n\n\n\n\n\n\nHuh. Well look at that… we have something that could quite easily turn into the design of a hex sticker. Amazing.\nBefore we move on I should explain why I called as_reference() in the previous example, and what that function does. To get a sense of it, let’s see what happens if I attempt the more “intuitive” strategy of trying to pass the mask layer directly to with_mask():\n\nbase + with_mask(texture, mask)\n\nError in UseMethod(\"as.raster\"): no applicable method for 'as.raster' applied to an object of class \"c('LayerInstance', 'Layer', 'ggproto', 'gg')\"\n\n\nThe error message here complains that R doesn’t know how to convert the mask layer to a raster object. Remember at the beginning I said that ggfx works by converting everything to a pixel representation (i.e., turn our vector graphics layer into raster graphics). What’s going on here is that the second argument to with_mask() is allowed to be a “registered filter”, or it has to actually be a raster object. I’m not doing either one! I’m trying to pass it a raw ggplot layer. That’s where the as_reference() function comes in. Its role is to take a layer and “register” it as a filter (the id argument is used to give it a name) that can subsequently used as the mask.\nThere are a few functions in ggfx that work like that. Another one is the as_group() function. Suppose I want to apply a filter to several layers at once, but not necessarily to the whole plot. For example, suppose I want some text and a pretty border on my hex sticker, represented by these two layers:\n\nborder &lt;- geom_path(aes(x, y), hex, colour = \"white\", size = 15)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\ntext &lt;- geom_text(\n  mapping = aes(x, y, label = text), \n  dat = tibble(x = 0, y = 0, text = \"ART\"), \n  size = 36,\n  colour = \"white\", \n  fontface = \"bold\"\n) \n\nWhen I construct my mask, what I really want to do is apply it to all three layers: texture, border, and text should all be masked by the mask layer. To do that I pass texture, border, and text to as_group() to define the group; then I pass mask to as_reference() to register it as a filter; and then I apply with_mask(). Here’s how that works:\nbase + texture + text + border\nbase + \n  as_group(texture, text, border, id = \"content\") +\n  as_reference(mask, id = \"mask\") + \n  with_mask(\"content\", \"mask\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s awfully convenient :)\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the ggfx-mask.R file in the materials."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#displace",
    "href": "posts/2024-12-24_art-from-code-7/index.html#displace",
    "title": "Art from code VII: Pixel filters",
    "section": "Displace",
    "text": "Displace\nDisplacement filters are conceptually simple, but extremely powerful, and can be a bit counterintuitive to work with in practice. The idea to shift all the pixels in one layer based on the pixel values in another layer (referred to as the displacement map). For example, a black pixel in the displacement map might be interpreted to mean “don’t move this pixel in the original layer”, whereas a white pixel might mean “move this 10 pixels to the right”. The idea of constructing a pixel value to displacement mapping makes conceptual sense to me, but I find it a little hard to visualise what that actually means. A worked example would probably help!\nLet’s start by creating some ggplot layers that each contain semi-transparent triangles:\n\npolygon_layer &lt;- function(x, y, fill = \"white\", alpha = .5) {\n  geom_polygon(aes(x, y), fill = fill, alpha = alpha)\n}\npoly1 &lt;- polygon_layer(x = c(1, 0, 0), y = c(0, 0, 1))\npoly2 &lt;- polygon_layer(x = c(0, 1, 1), y = c(0, 0, 1))\npoly3 &lt;- polygon_layer(x = c(.3, 1, 1), y = c(0, 0, .7))\npoly4 &lt;- polygon_layer(x = c(0, 0, .7), y = c(.3, 1, 1))\n\nNext, I’ll define a base plot and then show you what these polygons look like when drawn together:\n\nbase &lt;- ggplot() + \n  coord_equal(xlim = c(0, 1), ylim = c(0, 1)) + \n  theme_void() + \n  theme(panel.background = element_rect(fill = \"#333333\"))\n\nbase + poly1 + poly2 + poly3 + poly4\n\n\n\n\n\n\n\n\nLater on, we’re going to use this as our displacement map. The easiest way to interpret this map (I think?) is to think about each of the shaded regions separately. Let’s start by thinking about the darkest area, the region that makes up the border of the image and the diagonal stripe in the center. When we go to “fill in” that part of the final image, we won’t grab the “same” pixel in the other image: instead we’re going to grab a pixel that is below and to the left. For the brightest areas (the two bright white triangles, one to the top left of the map and the other to the bottom right), we’ll displace in the other direction.\nOkay, so if we’re going to do this, we will need an layer that can serve as the image to be displaced. I’ll keep it simple. Here’s a layer that plots the word \"ART\" using geom_text():\n\ntext &lt;- geom_text(\n  mapping = aes(0.5, 0.5, label = \"ART\"), \n  size = 60, \n  colour = \"black\", \n  fontface = \"bold\"\n)\n\nSo here’s our process:\n\nUse as_group() to convert the four polygon layers into a single group.\nUse as_reference() to register that group as a filter.\nUse with_displacement() to displace the text layer using the polygons.\n\nHere’s the code, and the results:\nbase + poly1 + poly2 + poly3 + poly4 + text\nbase + \n  as_group(poly1, poly2, poly3, poly4, id = \"polygons\", include = TRUE) +\n  as_reference(\"polygons\", id = \"displacement_map\") + \n  with_displacement(\n    text,\n    x_map = ch_alpha(\"displacement_map\"),\n    y_map = ch_alpha(\"displacement_map\"), \n    x_scale = 150,\n    y_scale = -150\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, this is very pretty, but it probably needs a little explanation:\n\nWhen I called with_displacement() I passed the text object first: that’s the “thing we will displace”. Next, I specified two displacement maps, x_map and y_map. It is, after all, quite possible to use a different displacement map to describe how we find the displacement in the horizontal direction and the vertical directions! However, I haven’t done that here because it makes my head hurt.\nWhen constructing the displacement maps I used ch_alpha() to make clear that we should use the transparency value of each pixel as the basis for the displacement. That makes sense in this case because transparency is the thing that varies across the image.\nThe x_scale and y_scale parameters specify the number of pixels to shift. Specifically, setting x_scale = 150 means that the maximum difference in transparency should correspoind to a shift of 150 pixels.\n\nTo be perfectly honest, I find it hard to visualise the effect of with_displacement(). My spatial reasoning skills just aren’t good enough. So what I usually do is tinker until I find something I like. YMMV.\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the ggfx-displace.R file in the materials."
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#blend",
    "href": "posts/2024-12-24_art-from-code-7/index.html#blend",
    "title": "Art from code VII: Pixel filters",
    "section": "Blend",
    "text": "Blend\nThe last ggfx trick I want to mention is with_blend(). Blending filters compose two images together using a particular rule. For example, we could \"darken_intensity\" by selecting the darker of the two pixel values at each location. Or we could use a \"plus\" filter that just adds the pixel values together. Blending is a powerful technique and can be used to mimic the behaviour of other filters (e.g., a mask is really just a very particular type of blend). I’m not going to try to summarise all the different kinds of blend you can do. But I will point you to the documentation page for with_blend(), which provides a pretty good place to start and has links to other resources you can follow up on.\nMeanwhile, I’ll give a simple example using an \"xor\" blend. I’ll return to our flametree plot as the basis for this example. The trunk and leaves plot layers below define the flametree image in solid black colour:\n\nleaves &lt;- geom_point(\n  data = leaf, \n  mapping = aes(coord_x, coord_y, colour = seg_col),\n  colour = \"black\",\n  size = 2, \n  stroke = 0\n)\n\ntrunk &lt;- geom_bezier(\n  data = tree,\n  mapping = aes(\n    x = coord_x, \n    y = coord_y, \n    size = seg_wid,\n    group = id_pathtree\n  ),\n  colour = \"black\",\n  lineend = \"round\"\n)\n\nNext I’ll create a triangle layer that is a solid black triangle on the lower right side of the image, but is otherwise empty:\n\ntriangle &lt;- polygon_layer(\n  x = c(-4, 2, 2), \n  y = c(0, 0, 6), \n  fill = \"black\",\n  alpha = 1\n)\n\nBecause these images are binary valued the exclusive-or \"xor\" filter is easy to visualise in this case. If a pixel is white in exactly one of the two inputs (the tree or the triangle), that pixel will also be white in the output. Otherwise that pixel will be transparent. That gives us this:\n\nbase &lt;- ggplot() + \n  theme_void() +\n  coord_equal(xlim = c(-3, 1), ylim = c(1, 5)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_size_identity(guide = guide_none())\n\nbase +\n  as_group(trunk, leaves, id = \"tree\") + \n  with_blend(triangle, \"tree\", blend_type = \"xor\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nA script containing this code is included as the ggfx-blend.R file"
  },
  {
    "objectID": "posts/2024-12-24_art-from-code-7/index.html#materials",
    "href": "posts/2024-12-24_art-from-code-7/index.html#materials",
    "title": "Art from code VII: Pixel filters",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\nflametree-example.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(flametree)\n\ntree &lt;- flametree_grow(\n  seed = 1, \n  time = 9, \n  angle = c(-15, 15, 30)\n)\n\nleaf &lt;- tree |&gt; filter(id_leaf == TRUE)\n\nbase &lt;- ggplot() + \n  scale_size_identity() + \n  theme_void() + \n  coord_equal()\n\nleaves &lt;- geom_point(\n  mapping = aes(coord_x, coord_y),\n  data = leaf, \n  size = 1.3, \n  stroke = 0, \n  colour = \"#e38b75\"\n)\n\ntrunk &lt;- geom_bezier(\n  mapping = aes(coord_x, coord_y, group = id_pathtree, size = seg_wid),\n  data = tree, \n  lineend = \"round\", \n  colour = \"#555555\",\n  show.legend = FALSE\n)\n\nplot(base + trunk + leaves)\n\n\n\n\n\n\n\n\n\n\nggfx-blend.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(flametree)\nlibrary(ggfx)\n\ntree &lt;- flametree_grow(\n  seed = 1, \n  time = 9, \n  angle = c(-15, 15, 30)\n)\n\nleaf &lt;- tree |&gt; filter(id_leaf == TRUE)\n\nleaves &lt;- geom_point(\n  data = leaf, \n  mapping = aes(coord_x, coord_y, colour = seg_col),\n  colour = \"white\",\n  size = 2, \n  stroke = 0\n)\n\ntrunk &lt;- geom_bezier(\n  data = tree,\n  mapping = aes(\n    x = coord_x, \n    y = coord_y, \n    size = seg_wid,\n    group = id_pathtree\n  ),\n  colour = \"white\",\n  lineend = \"round\"\n)\n\npolygon_layer &lt;- function(x, y, fill = \"white\", alpha = .5) {\n  geom_polygon(aes(x, y), fill = fill, alpha = alpha)\n}\n\ntriangle &lt;- polygon_layer(\n  x = c(-4, 2, 2), \n  y = c(0, 0, 6), \n  fill = \"white\",\n  alpha = 1\n)\n\nbase &lt;- ggplot() + \n  theme_void() +\n  theme(panel.background = element_rect(\n    fill = \"black\", colour = \"black\"\n  )) + \n  coord_equal(xlim = c(-3, 1), ylim = c(1, 5)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_size_identity(guide = guide_none())\n\nplot(\n  base +\n    as_group(trunk, leaves, id = \"tree\") + \n    with_blend(triangle, \"tree\", blend_type = \"xor\")\n)\n\n\n\n\n\n\n\n\n\n\nggfx-displace.R\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(ggfx)\n\npolygon_layer &lt;- function(x, y, fill = \"white\", alpha = .5) {\n  geom_polygon(aes(x, y), fill = fill, alpha = alpha)\n}\npoly1 &lt;- polygon_layer(x = c(1, 0, 0), y = c(0, 0, 1))\npoly2 &lt;- polygon_layer(x = c(0, 1, 1), y = c(0, 0, 1))\npoly3 &lt;- polygon_layer(x = c(.3, 1, 1), y = c(0, 0, .7))\npoly4 &lt;- polygon_layer(x = c(0, 0, .7), y = c(.3, 1, 1))\n\nbase &lt;- ggplot() + \n  coord_equal(xlim = c(0, 1), ylim = c(0, 1)) + \n  theme_void() + \n  theme(panel.background = element_rect(fill = \"#333333\"))\n\ntext &lt;- geom_text(\n  mapping = aes(0.5, 0.5, label = \"ART\"), \n  size = 60, \n  colour = \"black\", \n  fontface = \"bold\"\n)\n\nplot(\n  base + \n    as_group(poly1, poly2, poly3, poly4, id = \"polygons\", include = TRUE) +\n    as_reference(\"polygons\", id = \"displacement_map\") + \n    with_displacement(\n      text,\n      x_map = ch_alpha(\"displacement_map\"),\n      y_map = ch_alpha(\"displacement_map\"), \n      x_scale = 150,\n      y_scale = -150\n    )\n)\n\n\n\n\n\n\n\n\n\n\nggfx-dither.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggfx)\n\nset.seed(1)\npolar &lt;- tibble(\n  arc_start = runif(200),\n  arc_end = arc_start + runif(200, min = -.2, max = .2),\n  radius = runif(200),\n  shade = runif(200), \n  size = runif(200)\n)\n\nbase &lt;- ggplot(\n  data = polar, \n  mapping = aes(\n    x = arc_start, \n    y = radius,\n    xend = arc_end, \n    yend = radius, \n    colour = shade, \n    size = size\n  )\n) + \n  coord_polar(clip = \"off\") +\n  scale_y_continuous(limits = c(0, 1), oob = scales::oob_keep) +\n  scale_x_continuous(limits = c(0, 1), oob = scales::oob_keep) + \n  scale_colour_viridis_c(option = \"magma\") +\n  guides(colour = guide_none(), size = guide_none()) + \n  scale_size(range = c(0, 10)) + \n  theme_void() +\n  theme(panel.background = element_rect(fill = \"#aaaaaa\"))\n\nplot(base + with_halftone_dither(geom_segment()))\n\n\n\n\n\n\n\n\n\n\nggfx-glow.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggforce)\nlibrary(flametree)\nlibrary(ggfx)\n\ntree &lt;- flametree_grow(\n  seed = 1, \n  time = 9, \n  angle = c(-15, 15, 30)\n)\n\nleaf &lt;- tree |&gt; filter(id_leaf == TRUE)\n\nbase &lt;- ggplot() + \n  scale_size_identity() + \n  theme_void() + \n  coord_equal()\n\nleaves &lt;- geom_point(\n  mapping = aes(coord_x, coord_y),\n  data = leaf, \n  size = 1.3, \n  stroke = 0, \n  colour = \"#e38b75\"\n)\n\ntrunk &lt;- geom_bezier(\n  mapping = aes(coord_x, coord_y, group = id_pathtree, size = seg_wid),\n  data = tree, \n  lineend = \"round\", \n  colour = \"#555555\",\n  show.legend = FALSE\n)\n\nplot(\n  base +   \n    trunk + \n    with_outer_glow(leaves, colour = \"#555555\", sigma = 5, expand = 3)\n)\n\n\n\n\n\n\n\n\n\n\nggfx-mask.R\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggfx)\nlibrary(ambient)\n\ntexture &lt;- geom_raster(\n  mapping = aes(x, y, fill = paint),\n  data = long_grid(\n    x = seq(from = -1, to = 1, length.out = 1000),\n    y = seq(from = -1, to = 1, length.out = 1000)\n  ) |&gt; \n    mutate(\n      lf_noise = gen_simplex(x, y, frequency = 2, seed = 1234),\n      mf_noise = gen_simplex(x, y, frequency = 20, seed = 1234),\n      hf_noise = gen_simplex(x, y, frequency = 99, seed = 1234),\n      paint = lf_noise + mf_noise + hf_noise\n    )\n)\n\nhex &lt;- tibble(x = sin((0:6)/6 * 2 * pi), y = cos((0:6)/6 * 2 * pi))\nmask &lt;- geom_polygon(aes(x, y), hex, fill = \"white\")\n\nbase &lt;- ggplot() + \n  theme_void() +\n  coord_equal() +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  scale_fill_gradientn(\n    colours = c(\"#222222\",\"#e83e8c\"), \n    guide = guide_none()\n  )\n\nborder &lt;- geom_path(aes(x, y), hex, colour = \"white\", size = 15)\n\ntext &lt;- geom_text(\n  mapping = aes(x, y, label = text), \n  dat = tibble(x = 0, y = 0, text = \"ART\"), \n  size = 36,\n  colour = \"white\", \n  fontface = \"bold\"\n) \n\nplot(\n  base + \n    as_group(texture, text, border, id = \"content\") +\n    as_reference(mask, id = \"mask\") + \n    with_mask(\"content\", \"mask\")\n)"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html",
    "href": "posts/2021-09-07_water-colours/index.html",
    "title": "Art, jasmines, and the water colours",
    "section": "",
    "text": "In recent weeks I’ve been posting generative art from the Water Colours series on twitter. The series has been popular, prompting requests that I sell prints, mint NFTs, or write a tutorial showing how they are made. For personal reasons I didn’t want to commercialise this series. Instead, I chose to make the pieces freely available under a CC0 public domain licence and asked people to donate to a gofundme I set up for a charitable organisation I care about (the Lou’s Place women’s refuge here in Sydney). I’m not going to discuss the personal story behind this series, but it does matter. As I’ve mentioned previously, the art I make is inherently tied to moods. It is emotional in nature. In hindsight it is easy enough to describe how the system is implemented but this perspective is misleading. Although a clean and unemotional description of the code is useful for explanatory purposes, the actual process of creating the system is deeply tied to my life, my history, and my subjective experience. Those details are inextricably bound to the system. A friend described it better than I ever could:\n\nThe computer doesn’t make this art any more than a camera makes a photograph; art is always intimate (Amy Patterson)\n\nIn this post I’ll describe the mechanistic processes involved in creating these pieces, but this is woefully inadequate as a description of the artistic process as a whole. The optical mechanics of a camera do not circumscribe the work of a skilled photographer. So it goes with generative art. The code describes the mechanics; it does not describe the art. There is a deeply personal story underneath these pieces (one that I won’t tell here), and I would no more mint an NFT from that story than I would sell a piece of my soul to a collector."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#prelude",
    "href": "posts/2021-09-07_water-colours/index.html#prelude",
    "title": "Art, jasmines, and the water colours",
    "section": "",
    "text": "In recent weeks I’ve been posting generative art from the Water Colours series on twitter. The series has been popular, prompting requests that I sell prints, mint NFTs, or write a tutorial showing how they are made. For personal reasons I didn’t want to commercialise this series. Instead, I chose to make the pieces freely available under a CC0 public domain licence and asked people to donate to a gofundme I set up for a charitable organisation I care about (the Lou’s Place women’s refuge here in Sydney). I’m not going to discuss the personal story behind this series, but it does matter. As I’ve mentioned previously, the art I make is inherently tied to moods. It is emotional in nature. In hindsight it is easy enough to describe how the system is implemented but this perspective is misleading. Although a clean and unemotional description of the code is useful for explanatory purposes, the actual process of creating the system is deeply tied to my life, my history, and my subjective experience. Those details are inextricably bound to the system. A friend described it better than I ever could:\n\nThe computer doesn’t make this art any more than a camera makes a photograph; art is always intimate (Amy Patterson)\n\nIn this post I’ll describe the mechanistic processes involved in creating these pieces, but this is woefully inadequate as a description of the artistic process as a whole. The optical mechanics of a camera do not circumscribe the work of a skilled photographer. So it goes with generative art. The code describes the mechanics; it does not describe the art. There is a deeply personal story underneath these pieces (one that I won’t tell here), and I would no more mint an NFT from that story than I would sell a piece of my soul to a collector."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#the-water-colours-repository",
    "href": "posts/2021-09-07_water-colours/index.html#the-water-colours-repository",
    "title": "Art, jasmines, and the water colours",
    "section": "The water colours repository",
    "text": "The water colours repository\n\nWhy use version control here?\nWhen I started making generative art I didn’t think much about archiving my art or keeping it organised. I liked making pretty things, and that was as far as my thought process went. I didn’t place the code under version control, and I stored everything in my Dropbox folder. There’s nothing wrong with that: some things don’t belong on GitHub. During the development phase of any art project that’s still what I do, and I’m perfectly happy with it.\nThings become a little trickier when you want to share the art. My art website is hosted on GitHub pages, and so my initial approach was to keep the art in the website repository. Huuuuge mistake. Sometimes the image files can be quite large and sometimes a series contains a large number of images. By the time I’d reached 40+ series, Hugo took a very long time to build the site (several minutes), and GitHub took even longer to deploy it (over half an hour).\nEventually I decided it made more sense to have one repository per series. Each one uses the “series-” prefix to remind me it’s an art repo. I don’t use these repositories during development: they exist solely to snapshot the release. For example, the series-water-colours repository isn’t going to be updated regularly, it’s really just an archive combined with a “docs” folder that is used to host a minimal GitHub Pages site that makes the images public. It’s convenient for my purposes because my art website doesn’t have to host any of the images: all it does is hotlink to the images that are exposed via the series repo.\nIt may seem surprising that I’ve used GitHub for this. Image files aren’t exactly well suited to version control, but it’s not like they’re going to be updated. Plus, there are a lot of advantages. I can explicitly include licencing information in the repository, I can release source code (when I want to), and I can include a readme file for anyone who wants to use it.\n\n\nThe manifest file\nOne nice feature of doing things this way is that it has encouraged me to include a manifest file. Because the image files belong to a completely different repository to the website, I need a way to automatically inspect the image repository and construct the links I need (because I’m waaaaaay too lazy to add the links by hand). That’s the primary function of the manifest. The manifest.csv file is a plain csv file with one row per image, and one column for each piece of metadata I want to retain about the images. It might seem like organisational overkill to be this precise about the art, but I’m starting to realise that if I don’t have a proper system in place I’ll forget minor details like “what the piece is called” or “when I made it”. That seems bad :-)\nI can use readr::read_csv() to download the manifest and do a little data wrangling to organise it into a format that is handy to me right now:\n\n\nThe data wrangling code is here\n\nmanifest\n\n# A tibble: 20 × 9\n   series      sys_id img_id short_name    format long_name date       path_2000\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;    \n 1 watercolour sys02  img34  teacup-ocean  jpg    Ocean in… 2021-07-31 https://…\n 2 watercolour sys02  img31  incursions    jpg    Incursio… 2021-08-14 https://…\n 3 watercolour sys02  img32  percolate     jpg    Percolate 2021-08-21 https://…\n 4 watercolour sys02  img37  gentle-desce… jpg    Gentle D… 2021-08-21 https://…\n 5 watercolour sys02  img41  stormy-seas   jpg    Stormy S… 2021-08-22 https://…\n 6 watercolour sys02  img42  turmeric      jpg    Turmeric… 2021-08-24 https://…\n 7 watercolour sys02  img43  torn-and-fra… jpg    Torn and… 2021-08-24 https://…\n 8 watercolour sys02  img47  inferno       jpg    Seventh … 2021-08-27 https://…\n 9 watercolour sys02  img48  storm-cell    jpg    Storm Ce… 2021-08-27 https://…\n10 watercolour sys02  img49  tonal-earth   jpg    Tonal Ea… 2021-08-29 https://…\n11 watercolour sys02  img50  cold-front    jpg    Cold Fro… 2021-08-29 https://…\n12 watercolour sys02  img51  kintsugi-dre… jpg    Kintsugi… 2021-08-29 https://…\n13 watercolour sys02  img53  departure     jpg    Departure 2021-08-29 https://…\n14 watercolour sys02  img54  echo          jpg    Echo      2021-08-30 https://…\n15 watercolour sys02  img57  portal        jpg    Portal    2021-08-31 https://…\n16 watercolour sys02  img60  salt-stone-s… jpg    Gods of … 2021-08-31 https://…\n17 watercolour sys02  img61  amanecer-de-… jpg    El Últim… 2021-09-01 https://…\n18 watercolour sys02  img65  plume         jpg    Plume     2021-09-02 https://…\n19 watercolour sys02  img67  woodland-spi… jpg    Woodland… 2021-09-02 https://…\n20 watercolour sys02  img68  below-the-ho… jpg    Below th… 2021-09-03 https://…\n# ℹ 1 more variable: path_500 &lt;chr&gt;\n\n\n\n\nPreviewing the artwork\nMore to the point, the manifest data frame is nicely suited for use with the bs4cards package, so I can display some of the pieces in a neat and tidy thumbnail grid. Here are the first eight pieces from the series, arranged by date of creation:\n\nmanifest[1:8, ] %&gt;% \n  bs4cards::cards(\n    image = path_500,\n    link = path_2000,\n    title = long_name,\n    spacing = 3,\n    width = 2\n  )  \n\n\n\n\n\n\n\n\n\nOcean in a Teacup\n\n\n\n\n\n\n\n\n\nIncursions\n\n\n\n\n\n\n\n\n\nPercolate\n\n\n\n\n\n\n\n\n\nGentle Descent\n\n\n\n\n\n\n\n\n\nStormy Seas\n\n\n\n\n\n\n\n\n\nTurmeric Against Grey Tuesday\n\n\n\n\n\n\n\n\n\nTorn and Frayed\n\n\n\n\n\n\n\n\n\nSeventh Circle\n\n\n\n\n\n\n\nEach thumbnail image links to a medium resolution (2000 x 2000 pixels) jpg version of the corresponding piece, if you’d like to see the images in a little more detail."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#dependencies",
    "href": "posts/2021-09-07_water-colours/index.html#dependencies",
    "title": "Art, jasmines, and the water colours",
    "section": "Dependencies",
    "text": "Dependencies\nIn the remainder of this post I’ll walk you through the process of creating pieces “in the style of” the water colours series. If you really want to, you can take a look at the actual source, but it may not be very helpful: the code is little opaque, poorly structured, and delegates a lot of the work to the halftoner and jasmines packages, neither of which is on CRAN. To make it a little easier on you, I’ll build a new system in this post that adopts the same core ideas.\nIn this post I’ll assume you’re already familiar with data wrangling and visualisation with tidyverse tools. This is the subset of tidyverse packages that I have attached, and the code that follows relies on all these in some fashion:\n\nlibrary(magrittr)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(purrr)\nlibrary(dplyr)\n\n\n\nThe R environment is specified formally in the lockfile. It’s a story for another day, but for reproducibility purposes I have a separate renv configuration for every post\nIn addition to tidyverse and base R functions, I’ll use a few other packages as well. The magick, raster, rprojroot, fs, and ambient packages are all used in making the art. Because functions from those packages may not be as familiar to everyone, I’ll namespace the calls to them in the same way I did with bs4cards::cards() previously. Hopefully that will make it easier to see which functions belong to one of those packages."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#art-from-image-processing",
    "href": "posts/2021-09-07_water-colours/index.html#art-from-image-processing",
    "title": "Art, jasmines, and the water colours",
    "section": "Art from image processing",
    "text": "Art from image processing\n\nFinding the image file\nAs in life, the place to start is knowing where you are.\nThis post is part of my blog, and I’ll need to make use of an image file called \"jasmine.jpg\" stored alongside my R markdown. First, I can use rprojroot to find out where my blog is stored. I’ll do that by searching for a \"_quarto.yml\" file:\n\nblog &lt;- rprojroot::find_root(\"_quarto.yml\")\nblog\n\n[1] \"/home/danielle/GitHub/quarto-blog\"\n\n\nI suspect that most people reading this would be more familiar with the here package that provides a simplified interface to rprojroot and will automatically detect the .Rproj or .here file associated with your project. In fact, because the here::here() function is so convenient, it’s usually my preferred method for solving this problem. Sometimes, however, the additional flexibility provided by rprojroot is very useful. Some of my projects are comprised of partially independent sub-projects, each with a distinct root directory. That happens sometimes when blogging: there are contexts in which you might want to consider “the blog” to be the project, but other contexts in which “the post” might be the project. If you’re not careful this can lead to chaos (e.g., RStudio projects nested inside other RStudio projects), and I’ve found rprojroot very helpful in avoiding ambiguity in these situations.\nHaving chosen “the blog” as the root folder, the next step in orientation is to find the post folder. Because this is a distill blog, all my posts are stored in the _posts folder, and I’ve adopted a consistent naming convention for organising the post folders. Every name begins with the post date in year-month-day format, followed by a human-readable “slug”:\n\npost &lt;- paste(params$date, params$slug, sep = \"_\")\npost\n\n[1] \"2021-09-07_water-colours\"\n\n\nThis allows me to construct the path to the image file:\n\nfile &lt;- fs::path(blog, \"posts\", post, \"jasmine.jpg\")\nfile\n\n/home/danielle/GitHub/quarto-blog/posts/2021-09-07_water-colours/jasmine.jpg\n\n\nHere’s the image:\n\n\n\n\n\n\n\n\n\n\n\nThe photo has an emotional resonance to me: it dates back to 2011 and appeared on the cover of Learning Statistics with R. Although 10 years separate the Water Colours series from the text and the photo, the two are linked by a shared connection to events from a decade ago\n\n\nImporting the image\nOur next step is to import the image into R at a suitable resolution. The original image size is 1000x600 pixels, which is a little more than we need. Here’s a simple import_image() function that does this:\n\nimport_image &lt;- function(path, width, height) {\n  geometry &lt;- paste0(width, \"x\", height) # e.g., \"100x60\"\n  path %&gt;% \n    magick::image_read() %&gt;% \n    magick::image_scale(geometry)\n}\n\nInternally, the work is being done by the fabulous magick package that provides bindings to the ImageMagick library. In truth, it’s the ImageMagick library that is doing most the work here. R doesn’t load the complete image, it lets ImageMagick take care of that. Generally that’s a good thing for performance reasons (you don’t want to load large images into memory if you can avoid it), but in this case we’re going to work with the raw image data inside R.\nThis brings us to the next task…\n\n\nConverting the image to data\nConverting the image into a data structure we can use is a two step process. First, we create a matrix that represents the image in a format similar to the image itself. That’s the job of the construct_matrix() function below. It takes the image as input, and first coerces it to a raster object and then to a regular matrix: in the code below, the matrix is named mat, and the pixel on the i-th row and j-th column of the image is represented by the contents of mat[i, j].\n\nconstruct_matrix &lt;- function(image) {\n  \n  # read matrix\n  mat &lt;- image %&gt;% \n    as.raster() %&gt;%\n    as.matrix()\n  \n  # use the row and column names to represent co-ordinates\n  rownames(mat) &lt;- paste0(\"y\", nrow(mat):1) # &lt;- flip y\n  colnames(mat) &lt;- paste0(\"x\", 1:ncol(mat))\n  \n  return(mat)\n}\n\nA little care is needed when interpreting the rows of this matrix. When we think about graphs, the values on y-axis increase as we move our eyes upwards from the bottom, so our mental model has the small numbers at the bottom and the big numbers at the top. But that’s not the only mental model in play here. When we read a matrix or a table we don’t look at it, we read it - and we read from top to bottom. A numbered list, for example, has the smallest numbers at the top, and the numbers get bigger as we read down the list. Both of those mental models are sensible, but it’s hard to switch between them.\nThe tricky part here is that the raw image is encoded in “reading format”. It’s supposed to be read like a table or a list, so the indices increase as we read down the image. The image data returned by construct_matrix() is organised this format. However, when we draw pictures with ggplot2 later on, we’re going to need to switch to a “graph format” convention with the small numbers at the bottom. That’s the reason why the code above flips the order of the row names. Our next task will be to convert this (reading-formatted) matrix into a tidy tibble, and those row and column names will become become our (graph-formatted) x- and y-coordinates, so the row names need to be labelled in reverse order.\nTo transform the image matrix into a tidy tibble, I’ve written a handy construct_tibble() function:\n\nconstruct_tibble &lt;- function(mat) {\n  \n  # convert to tibble\n  tbl &lt;- mat %&gt;%\n    as.data.frame() %&gt;%\n    rownames_to_column(\"y\") %&gt;%\n    as_tibble() \n  \n  # reshape\n  tbl &lt;- tbl %&gt;%\n    pivot_longer(\n      cols = starts_with(\"x\"),\n      names_to = \"x\",\n      values_to = \"shade\"\n    ) \n  \n  # tidy\n  tbl &lt;- tbl %&gt;%\n    arrange(x, y) %&gt;% \n    mutate(\n      x = x %&gt;% str_remove_all(\"x\") %&gt;% as.numeric(),\n      y = y %&gt;% str_remove_all(\"y\") %&gt;% as.numeric(),\n      id = row_number()\n    )\n  \n  return(tbl)\n}\n\nThe code has the following strucure:\n\nThe first part of this code coerces the matrix to a plain data frame, then uses rownames_to_columns() to extract the row names before coercing it to a tibble. This step is necessary because tibbles don’t have row names, and we need those row names: our end goal is to have a variable y to store those co-ordinate values.\nThe second part of the code uses pivot_longer() to capture all the other variables (currently named x1, x2, etc) and pull them down into a single column that specifies the x co-ordinate. At this stage, the tbl tibble contains three variables: an x value, a y value, and a shade that contains the hex code for a colour.\nThe last step is to tidy up the values. After pivot_longer() does its job, the x variable contains strings like \"x1\", \"x2\", etc, but we’d prefer them to be actual numbers like 1, 2, etc. The same is true for the y variable. To fix this, the last part of the code does a tiny bit of string manipulation using str_remove_all() to get rid of the unwanted prefixes, and then coerces the result to a number.\n\n\n\nThe names_prefix argument to pivot_longer() can transform x without the third step, but I prefer the verbose form. I find it easier to read and it treats x and y the same\nTaken together, the import_image(), construct_matrix(), and construct_tibble() functions provide us with everything we need to pull the data from the image file and wrangle it into a format that ggplot2 is expecting:\n\njas &lt;- file %&gt;% \n  import_image(width = 100, height = 60) %&gt;% \n  construct_matrix() %&gt;% \n  construct_tibble()\n\njas\n\n# A tibble: 6,000 × 4\n       y     x shade        id\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt;\n 1     1     1 #838c70ff     1\n 2    10     1 #3c3123ff     2\n 3    11     1 #503d3dff     3\n 4    12     1 #363126ff     4\n 5    13     1 #443a30ff     5\n 6    14     1 #8a6860ff     6\n 7    15     1 #665859ff     7\n 8    16     1 #5a5d51ff     8\n 9    17     1 #535c4cff     9\n10    18     1 #944b61ff    10\n# ℹ 5,990 more rows\n\n\nA little unusually, the hex codes here are specified in RGBA format: the first two alphanumeric characters specify the hexadecimal code for the red level, the second two represent the green level (or “channel”), the third two are the blue channel, and the last two are the opacity level (the alpha channel). I’m going to ignore the alpha channel for this exercise though.\nThere’s one last thing to point out before turning to the fun art part. Notice that jas also contains an id column (added by the third part of the construct_tibble() function). It’s generally good practice to have an id column that uniquely identifies each row, and will turn out to be useful later when we need to join this data set with other data sets that we’ll generate.\n\n\nArt from data visualisation\nLet the art begin!\nThe first step is to define a helper function ggplot_themed() that provides a template that we’ll reuse in every plot. Mostly this involves preventing ggplot2 from doing things it wants to do. When we’re doing data visualisation it’s great that ggplot2 automatically provides things like “legends”, “axes”, and “scales” to map from data to visual aesthetics, but from an artistic perspective they’re just clutter. I don’t want to manually strip that out every time I make a plot, so it makes sense to have a function that gets rid of all those things:\n\nggplot_themed &lt;- function(data) {\n  data %&gt;% \n    ggplot(aes(x, y)) +\n    coord_equal() + \n    scale_size_identity() + \n    scale_colour_identity() + \n    scale_fill_identity() + \n    theme_void() \n}\n\nThis “template function” allows us to start with a clean slate, and it makes our subsequent coding task easier. The x and y aesthetics are already specified, ggplot2 won’t try to “interpret” our colours and sizes for us, and it won’t mess with the aspect ratio. In a sense, this function turns off the autopilot: we’re flying this thing manually…\nThere are many ways to plot the jas data in ggplot2. The least imaginative possibility is geom_tile(), which produces a pixellated version of the jasmines photo:\n\njas %&gt;% \n  ggplot_themed() + \n  geom_tile(aes(fill = shade)) \n\n\n\n\n\n\n\n\nOf course, if you are like me you always forget to use the fill aesthetic. The muscle memory tells me to use the colour aesthetic, so I often end up drawing something where only the borders of the tiles are coloured:\n\njas %&gt;% \n  ggplot_themed() + \n  geom_tile(aes(colour = shade)) \n\n\n\n\n\n\n\n\nIt’s surprisingly pretty, and a cute demonstration of how good the visual system is at reconstructing images from low-quality input: remarkably, the jasmines are still perceptible despite the fact that most of the plot area is black. I didn’t end up pursuing this (yet!) but I think there’s a lot of artistic potential here. It might be worth playing with at a later date. In that sense generative art is a lot like any other kind of art (or, for that matter, science). It is as much about exploration and discovery as it is about technical prowess.\nThe path I did follow is based on geom_point(). Each pixel in the original image is plotted as a circular marker in the appropriate colour. Here’s the simplest version of this idea applied to the jas data:\n\njas %&gt;% \n  ggplot_themed() + \n  geom_point(aes(colour = shade)) \n\n\n\n\n\n\n\n\nIt’s simple, but I like it.\n\n\nExtracting the colour channels\nUp to this point we haven’t been manipulating the colours in any of the plots: the hex code in the shade variable is left intact. There’s no inherent reason we should limit ourselves to such boring visualisations. All we need to do is extract the different “colour channels” and start playing around.\nIt’s not too difficult to do this: base R provides the col2rgb() function that separates the hex code into red, green, blue channels, and represents each channel with integers between 0 and 255. It also provides the rgb2hsv() function that converts this RGB format into hue, saturation, and value format, represented as numeric values between 0 and 1.\nThis technique is illustrated by the extract_channels() helper function shown below. It looks at the shade column in the data frame, and adds six new columns, one for each channel. I’m a sucker for variable names that are all the same length (often unwisely), and I’ve named them red, grn, blu, hue, sat, and val:\n\nextract_channels &lt;- function(tbl) {\n  rgb &lt;- with(tbl, col2rgb(shade))\n  hsv &lt;- rgb2hsv(rgb)\n  tbl &lt;- tbl %&gt;% \n    mutate(\n      red = rgb[1, ],\n      grn = rgb[2, ],\n      blu = rgb[3, ],\n      hue = hsv[1, ],\n      sat = hsv[2, ],\n      val = hsv[3, ]\n    )\n  return(tbl)\n}\n\nHere’s what that looks like applied to the jas data:\n\njas &lt;- extract_channels(jas)\njas\n\n# A tibble: 6,000 × 10\n       y     x shade        id   red   grn   blu    hue   sat   val\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1 #838c70ff     1   131   140   112 0.220  0.200 0.549\n 2    10     1 #3c3123ff     2    60    49    35 0.0933 0.417 0.235\n 3    11     1 #503d3dff     3    80    61    61 0      0.237 0.314\n 4    12     1 #363126ff     4    54    49    38 0.115  0.296 0.212\n 5    13     1 #443a30ff     5    68    58    48 0.0833 0.294 0.267\n 6    14     1 #8a6860ff     6   138   104    96 0.0317 0.304 0.541\n 7    15     1 #665859ff     7   102    88    89 0.988  0.137 0.4  \n 8    16     1 #5a5d51ff     8    90    93    81 0.208  0.129 0.365\n 9    17     1 #535c4cff     9    83    92    76 0.260  0.174 0.361\n10    18     1 #944b61ff    10   148    75    97 0.950  0.493 0.580\n# ℹ 5,990 more rows\n\n\nA whole new world of artistic possibilities has just emerged!\n\n\nArt from channel manipulation\nOne way to use this representation is in halftone images. If you have a printer that contains only black ink, you can approximate shades of grey by using the size of each dot to represent how dark that pixel should be:\n\nmap_size &lt;- function(x) {\n  ambient::normalise(1-x, to = c(0, 2))\n}\n\njas %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(val)),\n    colour = \"black\", \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\n\n\nIn this code the ambient::normalise() function is used to rescale the input to fall within a specified range. Usually ggplot2 handles this automatically, but as I mentioned, we’ve turned off the autopilot…\nFor real world printers, this approach is very convenient because it allows us to construct any shade we like using only a few different colours of ink. In the halftone world shades of grey are merely blacks of different size, pinks are merely sizes of red (sort of), and so on.\nBut we’re not using real printers, and in any case the image above is not a very good example of a halftone format: I’m crudely mapping 1-val to the size aesthetic, and that’s not actually the right way to do this (if you want to see this done properly, look at the halftoner package). The image above is “inspired by” the halftone concept, not the real thing. I’m okay with that, and abandoning the idea of fidelity opens up new possibilities. For example, there’s nothing stopping us retaining the original hue and saturation, while using dot size to represent the intensity value. That allows us to produce “halftonesque” images like this:\n\njas %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val)\n    ), \n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nIn this code, the hsv() function takes the hue and saturation channels from the original image, but combines them with a constant intensity value: the output is a new colour specified as a hex code that ggplot2 can display in the output. Because we have stripped out the value channel, we can reuse the halftone trick. Much like a halftone image, the image above uses the size aesthetic to represent the intensity at the corresponding pixel."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#intermission",
    "href": "posts/2021-09-07_water-colours/index.html#intermission",
    "title": "Art, jasmines, and the water colours",
    "section": "Intermission",
    "text": "Intermission\nUp to this point I’ve talked about image manipulation, and I hope you can see the artistic potential created when we pair image processing tools like magick with data visualisation tools like ggplot2. What I haven’t talked about is how to choose (or generate!) the images to manipulate, and I haven’t talked about how we might introduce a probabilistic component to the process. I’m not going to say much about how to choose images. The possibilities are endless. For this post I’ve used a photo I took in my garden many years ago, but the pieces in Water Colours series have a different origin: I dripped some food colouring into a glass of water and took some photos of the dye diffusing. Small sections were cropped out of these photos and often preprocessed in some fashion by changing the hue, saturation etc. These manipulated photos were then passed into a noise generation process, and the output produced images like this:\n\n\n\n\n\n\n\n\n\n\nStorm Cell / Air Elemental\n\n\n\n\n\n\n\n\n\nTonal Earth\n\n\n\n\n\n\n\n\n\nCold Front\n\n\n\n\n\n\n\n\n\nKintsugi Dreams"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#art-from-noise-generators",
    "href": "posts/2021-09-07_water-colours/index.html#art-from-noise-generators",
    "title": "Art, jasmines, and the water colours",
    "section": "Art from noise generators",
    "text": "Art from noise generators\n\nMultidimensional noise generation\nHow can we generate interesting noise patterns in R? As usual, there are many different ways you can do this, but my favourite method is to use the ambient package that provides bindings to the FastNoise C++ library. A proper description of what you can do with ambient is beyond what I can accomplish here. There are a lot of things you can do with a tool like this, and I’ve explored only a small subset of the possibilities in my art. Rather than make a long post even longer, what I’ll do is link to a lovely essay on flow fields and encourage you to play around yourself.\nTo give you a sense of what the possibilities are, I’ve written a field() function that uses the ambient package to generate noise. At its heart is ambient::gen_simplex(), a function that generates simplex noise (examples here), a useful form of multidimensional noise that has applications in computer graphics. In the code below, the simplex noise is then modified by a billow fractal that makes it “lumpier”: that’s the job of ambient::gen_billow() and ambient::fracture(). This is then modified one last time by the ambient::curl_noise() function to avoid some undesirable properties of the flow fields created by simplex noise.\nIn any case, here is the code. You’ll probably need to read through the ambient documentation to understand all the moving parts here, but for our purposes the main things to note are the arguments. The points argument takes a data frame or tibble that contains the x and y coordinates of a set of points (e.g., something like the jas data!). The frequency argument controls the overall “scale” of the noise: does it change quickly or slowly as you move across the image? The octaves argument controls the amount of fractal-ness (hush, I know that’s not a word) in the image. How many times do you apply the underlying transformation?\n\nfield &lt;- function(points, frequency = .1, octaves = 1) {\n  ambient::curl_noise(\n    generator = ambient::fracture,\n    fractal = ambient::billow,\n    noise = ambient::gen_simplex,\n    x = points$x,\n    y = points$y,\n    frequency = frequency,\n    octaves = octaves,\n    seed = 1\n  )\n}\n\nInterpreting the output of the field() function requires a little care. The result isn’t a new set of points. Rather, it is a collection of directional vectors that tell you “how fast” the x- and y-components are flowing at each of the locations specified in the points input. If we want to compute a new set of points (which is usually true), we need something like the shift() function below. It takes a set of points as input, computes the directional vectors at each of the locations, and then moves each point by a specified amount, using the flow vectors to work out how far to move and what direction to move. The result is a new data frame with the same columns and the same number of rows:\n\nshift &lt;- function(points, amount, ...) {\n  vectors &lt;- field(points, ...)\n  points &lt;- points %&gt;%\n    mutate(\n      x = x + vectors$x * amount,\n      y = y + vectors$y * amount,\n      time = time + 1,\n      id = id\n    )\n  return(points)\n}\n\nIt’s worth noting that the shift() function assumes that points contains an id column as well as the x and y columns. This will be crucial later when we want to merge the output with the jas data. Because the positions of each point are changing, the id column will be the method we use to join the two data sets. It’s also worth noting that shift() keeps track of time for you. It assumes that the input data contains a time column, and the output data contains the same column with every value incremented by one. In other words, it keeps the id constant so we know which point is referred to by the row, but modifies its position in time and space (x and y). Neat.\n\n\nArt from the noise\nTo illustrate how this all works, I’ll start by creating a regular 50x30 grid of points:\n\npoints_time0 &lt;- expand_grid(x = 1:50, y = 1:30) %&gt;% \n  mutate(time = 0, id = row_number())\n\nggplot_themed(points_time0) + \n  geom_point(size = .5)\n\n\n\n\n\n\n\n\nNext, I’ll apply the shift() function three times in succession, and bind the results into a single tibble that contains the the data at each point in time:\n\npoints_time1 &lt;- shift(points_time0, amount = 1)\npoints_time2 &lt;- shift(points_time1, amount = 1)\npoints_time3 &lt;- shift(points_time2, amount = 1)\n\npts &lt;- bind_rows(\n  points_time0, \n  points_time1, \n  points_time2,\n  points_time3\n)\n\nThen I’ll quickly write a couple of boring wrapper functions that will control how the size and transparency of the markers changes as a function of time…\n\nmap_size &lt;- function(x) {\n  ambient::normalise(x, to = c(0, 2))\n}\nmap_alpha &lt;- function(x) {\n  ambient::normalise(-x, to = c(0, .5))\n}\n\n…and now we can create some art:\n\npts %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      size = map_size(time), \n      alpha = map_alpha(time)\n    ),\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nSo pretty!\n\n\nAccumulating art with purrr\n… but also so ugly. The code I used above is awfully inelegant: I’ve “iteratively” created a sequence of data frames by writing the same line of code several times. That’s almost never the right answer, especially when the code doesn’t know in advance how many times we want to shift() the points! To fix this I could write a loop (and contrary to folklore, there’s nothing wrong with loops in R so long as you’re careful to avoid unnecessary copying). However, I’ve become addicted to functional programming tools in the purrr package, so I’m going to use those rather than write a loop.\nTo solve my problem I’m going to use the purrr::accumulate() function, which I personally feel is an underappreciated gem in the functional programming toolkit. It does precisely the thing we want to do here: it takes one object (e.g., points) as input together with a second quantity (e.g., an amount), and uses the user-supplied function (e.g., shift()) to produce a new object that can, once again, be passed to the user-supplied function (yielding new points). It continues with this process, taking the output of the last iteration of shift() and using it as input to the next iteration, until it runs out of amount values. It is very similar to the better-known purrr::reduce() function, except that it doesn’t throw away the intermediate values. The reduce() function is only interested in the destination; accumulate() is a whole journey.\nSo let’s use it. The iterate() function below gives a convenient interface:\n\niterate &lt;- function(pts, time, step, ...) {\n  bind_rows(accumulate(\n    .x = rep(step, time), \n    .f = shift, \n    .init = pts,\n    ...\n  ))\n}\n\nHere’s the code to recreate the pts data from the previous section:\n\npts &lt;- points_time0 %&gt;% \n  iterate(time = 3, step = 1)\n\nIt produces the same image, but the code is nicer!"
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#assembling-the-parts",
    "href": "posts/2021-09-07_water-colours/index.html#assembling-the-parts",
    "title": "Art, jasmines, and the water colours",
    "section": "Assembling the parts",
    "text": "Assembling the parts\n\nAdding noise to jasmines coordinates\nThe time has come to start assembling the pieces of the jigsaw puzzle, by applying the flow fields from the previous section to the data associated with the jasmines image. The first step in doing so is to write a small extract_points() function that will take a data frame (like jas) as input, extract the positional information (x and y) and the identifier column (id), and add a time column so that we can modify positions over time:\n\nextract_points &lt;- function(data) {\n  data %&gt;% \n    select(x, y, id) %&gt;% \n    mutate(time = 0)\n}\n\nHere’s how we can use this. The code below extracts the positional information from jas and then use the iterate() function to iteratively shift those positions along the paths traced out by a flow field:\n\npts &lt;- jas %&gt;% \n  extract_points() %&gt;% \n  iterate(time = 20, step = .1)\n\nThe pts tibble doesn’t contain any of the colour information from jas, but it does have the “right kind” of positional information. It’s also rather pretty in its own right:\n\nmap_size &lt;- function(x) {\n  ambient::normalise(x^2, to = c(0, 3.5))\n}\n\npts %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(size = map_size(time)),\n    alpha = .01,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\n\n\nJoining the noise with jasmine colours\nWe can now take the pixels from the jasmines image and make them “flow” across the image. To do this, we’ll need to reintroduce the colour information. We can do this using full_join() from the dplyr package. I’ve written a small convenience function restore_points() that performs the join only after removing the original x and y coordinates from the jas data. The reason for this is that the pts data now contains the positional information we need, so we want the x and y values from that data set. That’s easy enough: we drop those coordinates with select() and then join the two tables using only the id column. See? I promised it would be useful!\n\nrestore_points &lt;- function(jas, pts) {\n  jas %&gt;% \n    select(-x, -y) %&gt;% \n    full_join(pts, by = \"id\") %&gt;% \n    arrange(time, id) \n}\n\nThe result is a tibble that looks like this:\n\njas &lt;- restore_points(jas, pts)\njas\n\n# A tibble: 126,000 × 11\n   shade        id   red   grn   blu    hue   sat   val     x     y  time\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 #838c70ff     1   131   140   112 0.220  0.200 0.549     1     1     0\n 2 #3c3123ff     2    60    49    35 0.0933 0.417 0.235     1    10     0\n 3 #503d3dff     3    80    61    61 0      0.237 0.314     1    11     0\n 4 #363126ff     4    54    49    38 0.115  0.296 0.212     1    12     0\n 5 #443a30ff     5    68    58    48 0.0833 0.294 0.267     1    13     0\n 6 #8a6860ff     6   138   104    96 0.0317 0.304 0.541     1    14     0\n 7 #665859ff     7   102    88    89 0.988  0.137 0.4       1    15     0\n 8 #5a5d51ff     8    90    93    81 0.208  0.129 0.365     1    16     0\n 9 #535c4cff     9    83    92    76 0.260  0.174 0.361     1    17     0\n10 #944b61ff    10   148    75    97 0.950  0.493 0.580     1    18     0\n# ℹ 125,990 more rows\n\n\nMore importantly though, it produces images like this:\n\nmap_size &lt;- function(x, y) {\n  ambient::normalise((1 - x) * y^2, to = c(0, 5))\n}\n\njas %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = hsv(hue, sat, .5), \n      size = map_size(val, time)\n    ), \n    alpha = .03,\n    show.legend = FALSE\n  )\n\n\n\n\n\n\n\n\nWhen colouring the image, we’re using the same “halftonesque” trick from earlier. The colours vary only in hue and saturation. The intensity values are mapped to the size aesthetic, much like we did earlier, but this time around the size aesthetic is a function of two variables: it depends on time as well as val. The way I’ve set it up here is to have the points get larger as time increases, but there’s no reason we have to do it that way. There are endless ways in which you could combine the positional, temporal, and shading data to create interesting generative art. This is only one example.\n\n\nThe last chapter\nAt last we have the tools we need to create images in a style similar (though not identical) to those produced by the Water Colours system. We can import, reorganise, and separate the data:\n\njas &lt;- file %&gt;% \n  import_image(width = 200, height = 120) %&gt;% \n  construct_matrix() %&gt;% \n  construct_tibble() %&gt;% \n  extract_channels()\n\nWe can define flow fields with different properties, move the pixels through the fields, and rejoin the modified positions with the colour information\n\npts &lt;- jas %&gt;% \n  extract_points() %&gt;% \n  iterate(\n    time = 40, \n    step = .2, \n    octaves = 10, \n    frequency = .05\n  )\n\njas &lt;- jas %&gt;%\n  restore_points(pts)\n\njas\n\n# A tibble: 984,000 × 11\n   shade        id   red   grn   blu    hue    sat   val     x     y  time\n   &lt;chr&gt;     &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 #9c8178ff     1   156   129   120 0.0417 0.231  0.612     1     1     0\n 2 #81b564ff     2   129   181   100 0.274  0.448  0.710     1    10     0\n 3 #8b7870ff     3   139   120   112 0.0494 0.194  0.545     1   100     0\n 4 #eedfdbff     4   238   223   219 0.0351 0.0798 0.933     1   101     0\n 5 #c29aa3ff     5   194   154   163 0.962  0.206  0.761     1   102     0\n 6 #d5e1c3ff     6   213   225   195 0.233  0.133  0.882     1   103     0\n 7 #bde8beff     7   189   232   190 0.337  0.185  0.910     1   104     0\n 8 #b3dfbcff     8   179   223   188 0.367  0.197  0.875     1   105     0\n 9 #b2dcbdff     9   178   220   189 0.377  0.191  0.863     1   106     0\n10 #b3d9bfff    10   179   217   191 0.386  0.175  0.851     1   107     0\n# ℹ 983,990 more rows\n\n\nWe can write customised helpers to guide how information is used:\n\nmap_size &lt;- function(x, y) {\n  12 * (1 - x) * (max(y)^2 - y^2) / y^2\n}\n\nAnd we can render the images with ggplot2:\n\npic &lt;- jas %&gt;% \n  ggplot_themed() +  \n  geom_point(\n    mapping = aes(\n      colour = shade, \n      size = map_size(val, time)\n    ), \n    alpha = 1,\n    stroke = 0,\n    show.legend = FALSE\n  ) \n\npic\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe colour bleeding over the edges here is to be expected. Some of the points created with geom_point() are quite large, and they extend some distance beyond the boundaries of the original jasmines photograph. The result doesn’t appeal to my artistic sensibilities, so I’ll adjust the scale limits in ggplot2 so that we don’t get that strange border:\n\npic +\n  scale_x_continuous(limits = c(11, 190), expand = c(0, 0)) +\n  scale_y_continuous(limits = c(7, 114), expand = c(0, 0))\n\nWarning: Removed 198656 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe end result is something that has a qualitative similarity to the Water Colours pieces, but is also possessed of a style that is very much its own. This is as it should be. It may be true that “all art is theft” – as Picasso is often misquoted as saying – but a good artistic theft is no mere replication. It can also be growth, change, and reconstruction.\nA happy ending after all."
  },
  {
    "objectID": "posts/2021-09-07_water-colours/index.html#epilogue",
    "href": "posts/2021-09-07_water-colours/index.html#epilogue",
    "title": "Art, jasmines, and the water colours",
    "section": "Epilogue",
    "text": "Epilogue\n\nI find it so amazing when people tell me that electronic music has no soul. You can’t blame the computer. If there’s no soul in the music, it’s because nobody put it there (Björk, via Tim de Sousa)\n\n\n\n\n\n\n\n\n\n\n\nDeparture\n\n\n\n\n\n\n\n\n\nEcho\n\n\n\n\n\n\n\n\n\nPortal\n\n\n\n\n\n\n\n\n\nGods of Salt, Stone, and Storm\n\n\n\n\n\n\n\n\n\nEl Último Amanecer de Invierno\n\n\n\n\n\n\n\n\n\nPlume\n\n\n\n\n\n\n\n\n\nWoodland Spirits\n\n\n\n\n\n\n\n\n\nBelow the Horizon"
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html",
    "href": "posts/2024-12-21_art-from-code-4/index.html",
    "title": "Art from code IV: Shading tricks",
    "section": "",
    "text": "A couple of years ago I gave an invited workshop called art from code at the 2022 rstudio::conf (now posit::conf) conference. As part of the workshop I wrote a lengthy series of notes on how to make generative art using R, all of which were released under a CC-BY licence. For a while now I’d been thinking I should do something with these notes. I considered writing a book, but in all honesty I don’t have the spare capacity for a side-project of that scale these days. I can barely keep up with the workload at my day job as it is. So instead, I’ve decided that I’d port them over to this site as a series of blog posts. In doing so I’ve made a deliberate decision not to modify the original content too much (nobody loves it when an artist tries to “improve” the original, after all). All I’ve done is update the code to accommodate package changes since 2022, and some minor edits so that the images are legible when embedded in this blog (which is light-themed, and the original was dark-theme). Other than that, I’ve left it alone. This is the fourth post in that series.\nlibrary(rayshader)\nlibrary(tibble)\nlibrary(ambient)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(tictoc)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html#rayshader-art",
    "href": "posts/2024-12-21_art-from-code-4/index.html#rayshader-art",
    "title": "Art from code IV: Shading tricks",
    "section": "Rayshader art",
    "text": "Rayshader art\nThe rayshader package is a tool used to generate 2D and 3D visualisations in R. It is designed primarily to work with elevation data: you can use it to create beautiful shaded maps in two and three dimensions. You don’t have to restrict yourself to mapping applications though. For example, you can use it to create 3D ggplot images if you want. More importantly for our purposes, generative artists in the R community have begun exploring the artistic possibilities inherent in the package. It’s a relatively new addition to my repertoire: I’ve only built a few generative art systems this way, and I’m still a novice user of the package. However, it’s too much fun not to talk about it here, so let’s take rayshader for a spin.\nTo help get us started, I’ll build a very simple generative art system. All it does is overlay a few circles on top of one another. To make this system work, I’ll define a helper function is_within_circle that takes coordinate vectors x_coord and y_coord as inputs, and returns a logical vector that is TRUE whenever those coordinates fall within a circle specified by the radius, x_center, and y_center values.\n\nis_within_circle &lt;- function(x_coord, y_coord, x_center, y_center, radius) {\n  (x_coord - x_center)^2 + (y_coord - y_center)^2 &lt; radius^2\n}\n\nThe additive_circles() function generates n circles at random (defaulting to 5 circles), and returns a long grid that defines a canvas with coordinate columns x and y, and a value column paint indicating the proportion of circles that each point falls in. If a particular point falls within every circle, the corresponding paint value is 1; if it falls within none of the circles the value is 0:\n\nadditive_circles &lt;- function(n = 5, pixels = 1000, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  # setup canvas\n  art &lt;- long_grid(\n    x = seq(0, 1, length.out = pixels),\n    y = seq(0, 1, length.out = pixels)\n  )\n  art$paint &lt;- 0\n  \n  for(i in 1:n) {\n    \n    # sample a random circle\n    x_center &lt;- runif(1, min = .3, max = .7)\n    y_center &lt;- runif(1, min = .3, max = .7)\n    radius &lt;- runif(1, min = .05, max = .25)\n    \n    # add +1 to all points inside the circle\n    art &lt;- art |&gt;\n      mutate(\n        paint = paint + is_within_circle(\n          x, y, x_center, y_center, radius\n        )\n      )\n  }\n  \n  # normalise paint to [0, 1] range and return\n  art$paint &lt;- normalise(art$paint)\n  return(art)\n}\n\nHere’s what happens when we generate output from the system and then use geom_raster() to plot it:\n\ncircle_art &lt;- additive_circles(seed = 99)\ncircle_art\n\n# A tibble: 1,000,000 × 3\n       x       y paint\n   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1     0 0           0\n 2     0 0.00100     0\n 3     0 0.00200     0\n 4     0 0.00300     0\n 5     0 0.00400     0\n 6     0 0.00501     0\n 7     0 0.00601     0\n 8     0 0.00701     0\n 9     0 0.00801     0\n10     0 0.00901     0\n# ℹ 999,990 more rows\n\nggplot(circle_art, aes(x, y, fill = paint)) +\n  geom_raster(show.legend = FALSE) + \n  theme_void()\n\n\n\n\n\n\n\n\nExperienced ggplot2 users might wonder why I’m generating art in this fashion. Why go to all the trouble of defining a raster when ggplot2 already has a geom_polygon() function that I could have used to draw the same image? The answer to this is that the rayshader package likes to deal with matrices (and other arrays). Instead of representing the data in a tibble with x and y coordinates that just happen to define a grid, it expects inputs in the form of a matrix where each row corresponds to a y coordinate and each column corresponds to an x coordinate, and the values in each cell correspond to (in our case) the paint values. Conveniently for us, the object we used to store our artwork isn’t a regular tibble, it’s a long grid provide by the ambient package. The ambient package knows how to convert this to an array quickly and painlessly:\n\ncircle_array &lt;- circle_art |&gt;\n  as.array(value = paint) \n\ncircle_array[1:10, 1:10]\n\n       x\ny       [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n   [1,]    0    0    0    0    0    0    0    0    0     0\n   [2,]    0    0    0    0    0    0    0    0    0     0\n   [3,]    0    0    0    0    0    0    0    0    0     0\n   [4,]    0    0    0    0    0    0    0    0    0     0\n   [5,]    0    0    0    0    0    0    0    0    0     0\n   [6,]    0    0    0    0    0    0    0    0    0     0\n   [7,]    0    0    0    0    0    0    0    0    0     0\n   [8,]    0    0    0    0    0    0    0    0    0     0\n   [9,]    0    0    0    0    0    0    0    0    0     0\n  [10,]    0    0    0    0    0    0    0    0    0     0\n\n\nThe ability to flip back and forth between a tibble-like representation and a matrix-like representation is very handy! Anyway, the important point is that circle_array is now a matrix. I can plot this matrix directly using image():\n\ncircle_array |&gt; \n  image(axes = FALSE, asp = 1, useRaster = TRUE)\n\n\n\n\n\n\n\n\nLet’s imagine for a moment that this image is actually a terrain map, and the values stored in circle_array refer to the height of the terrain at each point on the grid. If that were true, and we placed an illumination source above the terrain, what pattern of shadows would be cast? We can solve this using ray shading algorithms, and unsurprisingly the rayshader package contains a function called ray_shade() that does this for us. We pass our data matrix as the heightmap argument, provide sunaltitude and sunangle arguments to specify the position of the illumination source, and use the zscale argument to specify the scale of the z-axis (the values) relative to the x- and y-axes.\nHere’s what that looks like:\n\ncircle_shadow &lt;- ray_shade(\n  heightmap = circle_array,\n  sunaltitude = 15, \n  sunangle = 135,\n  zscale = .01,\n  multicore = TRUE\n)\n\nplot_map(circle_shadow, rotate = 270)\n\n\n\n\n\n\n\n\nThe results lack colour because this is only a map of the intensity of the shadow at each point. It’s not a map of the terrain. If we want to construct that map we need something like a hill shading algorithm supplied by sphere_shade(), but that requires us to supply a texture. That’s probably overkill for our initial application. Alternatively, if all we want is a height-to-colour mapping, we can use height_shade() to create the texture, and then use add_shadow() to add the shadow:\n\ncircle_scape &lt;- circle_array |&gt; \n  height_shade() |&gt;\n  add_shadow(\n    shadowmap = circle_shadow,\n    max_darken = .1\n  )\n\nplot_map(circle_scape, rotate = 270)\n\n\n\n\n\n\n\n\nIn the final line I called plot_map() to draw the final image, using the rotate argument so that the final image has the same orientation as the image I created with geom_raster() at the beginning of this page.\n\n\n\n\n\n\nExercise\n\n\n\nAt this point in the workshop, I hope that you’re starting to get a sense for how to go about tweaking a generative system and how to create new things using it. So from this point the “exercises” are going to be a bit less structured. What I’ll do is point you to the scripts that implement each system, and in each case the exercise is going to be the same: play with the system, see what it can do, modify it as you see fit, and see where your own intuitions take you!\nCode for this system is included in the circle-scape.R script in the materials. Enjoy! Explore!"
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html#shadowed-noise-fields",
    "href": "posts/2024-12-21_art-from-code-4/index.html#shadowed-noise-fields",
    "title": "Art from code IV: Shading tricks",
    "section": "Shadowed noise fields",
    "text": "Shadowed noise fields\nNow that we have a general sense of how to use rayshader to create pretty images, let’s see if we can use it to make something a little more interesting than a shaded map of a few circles laid on top of one another. One place to start is to return to the spatial noise patterns generated by gen_perlin(), gen_simplex() and so on. There’s some potential for interesting art there right?\nBefore we get into that, we’re going to – yet again – need a palette generating function. So once again I’ll define a function to sample palettes using the ggthemes::canva_palettes list. However, this time around I’ll be a little more elaborate. All the palettes in the original object contain exactly four colours. What I’ll with the sample_canva2() function is include an n argument that specifies the number of colours desired, linearly interpolating between colours as necessary.\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nHere’s an example:\n\nsample_canva2(seed = 1)\nsample_canva2(seed = 1, n = 7)\n\n[1] \"#FCC875\" \"#BAA896\" \"#E6CCB5\" \"#E38B75\"\n[1] \"#FCC875\" \"#DBB885\" \"#BAA896\" \"#D0BAA5\" \"#E6CCB5\" \"#E4AB95\" \"#E38B75\"\n\n\nThis functionality is handy in this context to ensure that we have enough different colours to produce nice gradients in our rayshader outputs. When working with ggplot2 the scale_*_gradientn() function took care of that for us, but we’re not using ggplot2 here.\nIn any case, here’s ridge_art(), a function that uses the spatial noise toolkit from the ambient package to produce patterns. The output comes in matrix form rather than as a long grid:\n\nridge_art &lt;- function(seed = NULL, pixels = 2000) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  long_grid(\n    x = seq(from = 0, to = 1, length.out = pixels),\n    y = seq(from = 0, to = 1, length.out = pixels)\n  ) |&gt; \n    mutate(\n      paint = fracture(\n        x = x, \n        y = y,\n        noise = gen_simplex,\n        fractal = ridged,\n        octaves = 8,\n        frequency = 10,\n        seed = seed\n      ),\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\nAll the work in generating images is being done by the gen_simplex() generator, the ridged() fractal function, and the fracture() function that provides ambients API for fractal noise. To give you a sense of what kind of output this system produces natively, here’s an image():\n\nridge_art(1234) |&gt; \n  image(\n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = sample_canva2(seed = 1234, n = 256)\n  ) \n\n\n\n\n\n\n\n\nThat’s quite pretty in its own right, but we can give it a real feeling of depth by using rayshader. The idea is essentially identical to what we did when shading our circles art: compute a height map, a shadow map, and add them together before calling plot_map(). Here’s the code for a shaded_ridge_art() function that does this:\n\nshaded_ridge_art &lt;- function(seed = NULL) {\n  \n  art &lt;- ridge_art(seed) \n  height_shade(\n    heightmap = art,\n    texture = sample_canva2(seed, 256)\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = art, \n        sunaltitude = 30, \n        sunangle = 90,\n        multicore = TRUE, \n        zscale = .05\n      ), \n      max_darken = .1\n    ) |&gt;\n    plot_map()\n}\n\nHere’s our ridged art piece rendered as a shaded version:\n\ntic()\nshaded_ridge_art(1234)\n\n\n\n\n\n\n\ntoc()\n\n18.801 sec elapsed\n\n\nJust because they’re pretty and it’s not that hard to generate new pieces – one of the joys of generative art is that the moment you make one piece you like you can immediately make many more in the same style – here are a few more outputs from the system:\nshaded_ridge_art(100)\nshaded_ridge_art(101)\nshaded_ridge_art(102) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the shaded-ridge-art.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html#fractured-terrain",
    "href": "posts/2024-12-21_art-from-code-4/index.html#fractured-terrain",
    "title": "Art from code IV: Shading tricks",
    "section": "Fractured terrain",
    "text": "Fractured terrain\nBack in the early days of the pandemic I made a series of generative art pieces called Quarantine Moods that was, well, pretty incoherent. Not very surprising: I was trapped indoors and stressed, so there’s no theme or structure to the whole thing. Later on though I found the code for one of the pieces that I really liked and reworked it to create a new system that I called Ice Floes. Pieces from this system have a jagged, fractured geometric look to them. One of the first thoughts I had when exploring the rayshader package was that these images would generate some really interesting shadows, and it would be fun to see what happens when I applied rayshader methods to those outputs. So… that’s what I did!\nThe first step in the process is to recreate the ice floes system, or at least something very similar to it. The trick behind this system is to generate spatial noise defined over a different space to the one I intend to plot at the end. I generate new coordinates by constructing a map from the original coordinates to the corresponding curl space. Or, to put it in less pretentious terms, I use curl_noise() to produce a new set of coordinates that I’m going to feed into other noise processes. Here’s the function I’ll use to that:\n\ntransform_to_curl_space &lt;- function(x, y, frequency = 1, octaves = 10) {\n  curl_noise(\n    generator = fracture,\n    noise = gen_simplex,\n    fractal = fbm,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  )\n}\n\nThe next step is to use Worley noise to construct a set of cells, in this transformed space. To do that I’ll define a helper function that takes a set of coordinates (in whatever space) as input and outputs values associated with the cells:\n\ndefine_worley_cells &lt;- function(x, y, frequency = 3, octaves = 6) {\n  fracture(\n    noise = gen_worley,\n    fractal = billow,\n    octaves = octaves,\n    frequency = frequency,\n    value = \"cell\",\n    x = x,\n    y = y\n  ) |&gt;\n    rank() |&gt; \n    normalise()\n}\n\nNow back in the original space, we’ll use the cell values to (discontinuously) add offsets to the x- and y-coordinates, and then generate simplex noise using those offset coordinates. The net effect of this is that we have the simplex noise varies smoothly within cells (whose borders are quite peculiar because they’re generated in the curl space) but discontinuous between cells. This is going to give us an image that is both smooth and jagged.\nAnyway, this means we need one more helper function:\n\nsimplex_noise &lt;- function(x, y, frequency = .1, octaves = 10) {\n  fracture(\n    noise = gen_simplex,\n    fractal = ridged,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  ) |&gt;\n    normalise()\n}\n\nNow we have all the pieces we need to construct an ice_floe() function that is more or less equivalent to my original system:\n\nice_floe &lt;- function(seed) {\n  \n  set.seed(seed)\n  \n  grid &lt;- long_grid(\n    x = seq(0, 1, length.out = 2000),\n    y = seq(0, 1, length.out = 2000)\n  )\n  \n  coords &lt;- transform_to_curl_space(grid$x, grid$y)\n  \n  grid |&gt;\n    mutate(\n      cells = define_worley_cells(coords$x, coords$y),\n      paint = simplex_noise(x + cells, y + cells),\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\nTo give you a sense of what images from the original system look like when coloured using one of the canva palettes, I’ll again use image() to plot the output of the base system:\n\nice_floe(170) |&gt; \n  image(\n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = sample_canva2(seed = 170, n = 256)\n  )\n\n\n\n\n\n\n\n\nCreating the shaded version of the system proceeds the same way it did when we created the shaded_ridge_art() function. We call ice_floe() to create a matrix of elevations, construct an appropriately shaded elevation map using height_shade(), and then call add_shadow() to add a shadow map generated using ray_shade(). Then we call plot_map() to create the output:\n\nshaded_ice_floe &lt;- function(seed) {\n  \n  art &lt;- ice_floe(seed)\n  \n  height_shade(\n    heightmap = art,\n    texture = sample_canva2(seed, 256)\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = art, \n        sunaltitude = 30, \n        sunangle = 90,\n        multicore = TRUE, \n        zscale = .005\n      ), \n      max_darken = .05\n    ) |&gt;\n    plot_map()\n}\n\nshaded_ice_floe(170)\n\n\n\n\n\n\n\n\nTurns out it’s quite pretty. Here are a few more outputs:\nshaded_ice_floe(100)\nshaded_ice_floe(101)\nshaded_ice_floe(102)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshaded_ice_floe(106)\nshaded_ice_floe(107)\nshaded_ice_floe(108)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the shaded-icescape.R script in the materials."
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html#three-dimensional-art",
    "href": "posts/2024-12-21_art-from-code-4/index.html#three-dimensional-art",
    "title": "Art from code IV: Shading tricks",
    "section": "Three dimensional art",
    "text": "Three dimensional art\nThe examples I’ve shown so far all have a feeling of depth because of the way ray_shade() produces natural looking shadows. They’re not truly 3D renderings though. You can’t rotate them in 3D or display them from different perspectives. Happily, the rayshader package allows you to create 3D plots using the plot_3d() function. Under the hood, this function relies on the rgl package, which in turn provides access to OpenGL. For this function to work, your installation of the rgl package needs to be built with access to OpenGL tools. On windows that should happen automatically, but it can be a little mmore tricky on other operating systems. To get it to work on my Ubuntu machine what I had to do was first install OpenGL. The command I used at the terminal was this:\nsudo apt-get install libgl1-mesa-dev libglu1-mesa-dev\nOnce that was complete, I had to force a reinstall for the rgl package to ensure it had been built with the OpenGL libraries present. At the R console:\ninstall.packages(\"rgl\", force = TRUE)\nHaving done so, everything worked pretty smoothly for me.\nOkay, so what can we do with 3d rendering? To start with, let’s keep things simple and use the “circles” example. I’ve already computed a height map (circle_array) and a shading map (circle_scape) that incorporates the shadows, so I can pass both of the to plot_3d(). It’s a little fiddly, so I had to tinker with the angles and other settings to get a result that worked:\n\nplot_3d(\n  hillshade = circle_scape,\n  heightmap = circle_array,\n  theta = 230,\n  phi = 15,\n  zoom = .8,\n  zscale = .001,\n  baseshape = \"circle\",\n  background = \"#ffffff\",\n  shadow = FALSE,\n  soliddepth = 0,\n  solidcolor = \"#111111\",\n  windowsize = 1200\n)\n\nrender_snapshot(\n  filename = \"circles_3d.png\", \n  clear = TRUE\n)\n\n\nknitr::include_graphics(\"circles_3d.png\")\n\n\n\n\n\n\n\n\nIt kind of looks like a tower. It’s kind of neat in its own right, but the output gets much more fun when you start feeding richer input to plot_3d(). Here’s what happens when I adapt the “ice floes” system to produce truly three dimensional images:\n\nseed &lt;- 170\n\nice_height &lt;- matrix(0, 2500, 2500)\nice_height[251:2250, 251:2250] &lt;- ice_floe(seed)\n\nice_scape &lt;- height_shade(\n  heightmap = ice_height,\n  texture = sample_canva2(seed, 256)\n) |&gt;\n  add_shadow(\n    shadowmap = ray_shade(\n      heightmap = ice_height, \n      sunaltitude = 30, \n      sunangle = 90,\n      multicore = TRUE, \n      zscale = .005\n    ), \n    max_darken = .05\n  )\n\nplot_3d(\n  hillshade = ice_scape,\n  heightmap = ice_height,\n  theta = 45,\n  phi = 30,\n  zoom = .75,\n  zscale = .001,\n  background = \"#ffffff\",\n  shadow = FALSE,\n  soliddepth = .5,\n  solidcolor = \"#222222\",\n  windowsize = c(2500, 1500)\n)\n\nrender_snapshot(\n  filename = \"ice_3d.png\", \n  clear = TRUE\n)\n\n\nknitr::include_graphics(\"ice_3d.png\")\n\n\n\n\n\n\n\n\nEven I have to admit I was impressed with myself this time. That worked way better than I was expecting it to, and I suspect it would look even nicer if I’d taken the time to learn more about hill shading algorithms and used sphere_shade() to create a proper terrain map rather rather than relying on height_shade(). Something to play around with in the future :)\n\n\n\n\n\n\nExercise\n\n\n\nCode for this system is included in the icescape-3d.R script."
  },
  {
    "objectID": "posts/2024-12-21_art-from-code-4/index.html#materials",
    "href": "posts/2024-12-21_art-from-code-4/index.html#materials",
    "title": "Art from code IV: Shading tricks",
    "section": "Materials",
    "text": "Materials\nCode for each of the source files referred to in this section of the workshop is included here. Click on the callout box below to see the code for the file you want to look at. Please keep in mind that (unlike the code in the main text) I haven’t modified these scripts since the original workshop, so you might need to play around with them to get them to work!\n\n\n\n\n\n\ncircle-scape.R\n\n\n\n\n\n\nlibrary(rayshader)\nlibrary(tibble)\nlibrary(ambient)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(tictoc)\nlibrary(dplyr)\n\nis_within_circle &lt;- function(x_coord, y_coord, x_center, y_center, radius) {\n  (x_coord - x_center)^2 + (y_coord - y_center)^2 &lt; radius^2\n}\n\nadditive_circles &lt;- function(n = 5, pixels = 1000, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  # setup canvas\n  art &lt;- long_grid(\n    x = seq(0, 1, length.out = pixels),\n    y = seq(0, 1, length.out = pixels)\n  )\n  art$paint &lt;- 0\n  \n  for(i in 1:n) {\n    \n    # sample a random circle\n    x_center &lt;- runif(1, min = .3, max = .7)\n    y_center &lt;- runif(1, min = .3, max = .7)\n    radius &lt;- runif(1, min = .05, max = .25)\n    \n    # add +1 to all points inside the circle\n    art &lt;- art |&gt;\n      mutate(\n        paint = paint + is_within_circle(\n          x, y, x_center, y_center, radius\n        )\n      )\n  }\n  \n  # normalise paint to [0, 1] range and return\n  art$paint &lt;- normalise(art$paint)\n  return(art)\n}\n\ncircle_art &lt;- additive_circles(seed = 99)\n\ncircle_array &lt;- circle_art |&gt;\n  as.array(value = paint) \n\ncircle_shadow &lt;- ray_shade(\n  heightmap = circle_array,\n  sunaltitude = 15, \n  sunangle = 135,\n  zscale = .01,\n  multicore = TRUE\n)\n\ncircle_scape &lt;- circle_array |&gt; \n  height_shade() |&gt;\n  add_shadow(\n    shadowmap = circle_shadow,\n    max_darken = .1\n  )\n\ntic()\nplot_map(circle_scape, rotate = 270)\ntoc()\n\n\n\n\n\n\n\n\n\n\nicescape-3d.R\n\n\n\n\n\n\nlibrary(rayshader)\nlibrary(tibble)\nlibrary(ambient)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tictoc)\nlibrary(here)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\ntransform_to_curl_space &lt;- function(x, y, frequency = 1, octaves = 10) {\n  curl_noise(\n    generator = fracture,\n    noise = gen_simplex,\n    fractal = fbm,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  )\n}\n\n\ndefine_worley_cells &lt;- function(x, y, frequency = 3, octaves = 6) {\n  fracture(\n    noise = gen_worley,\n    fractal = billow,\n    octaves = octaves,\n    frequency = frequency,\n    value = \"cell\",\n    x = x,\n    y = y\n  ) |&gt;\n    rank() |&gt; \n    normalise()\n}\n\n\nsimplex_noise &lt;- function(x, y, frequency = .1, octaves = 10) {\n  fracture(\n    noise = gen_simplex,\n    fractal = ridged,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  ) |&gt;\n    normalise()\n}\n\n\nice_floe &lt;- function(seed) {\n  \n  set.seed(seed)\n  \n  grid &lt;- long_grid(\n    x = seq(0, 1, length.out = 2000),\n    y = seq(0, 1, length.out = 2000)\n  )\n  \n  coords &lt;- transform_to_curl_space(grid$x, grid$y)\n  \n  grid |&gt;\n    mutate(\n      cells = define_worley_cells(coords$x, coords$y),\n      paint = simplex_noise(x + cells, y + cells),\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\nicescape_3d &lt;- function(seed) {\n  \n  ice_height &lt;- matrix(0, 2500, 2500)\n  ice_height[251:2250, 251:2250] &lt;- ice_floe(seed)\n  \n  ice_scape &lt;- height_shade(\n    heightmap = ice_height,\n    texture = sample_canva2(seed, 256)\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = ice_height, \n        sunaltitude = 30, \n        sunangle = 90,\n        multicore = TRUE, \n        zscale = .005\n      ), \n      max_darken = .05\n    )\n  \n  plot_3d(\n    hillshade = ice_scape,\n    heightmap = ice_height,\n    theta = 45,\n    phi = 30,\n    zoom = .75,\n    zscale = .001,\n    background = \"#222222\",\n    shadow = FALSE,\n    soliddepth = .5,\n    solidcolor = \"#222222\",\n    windowsize = c(2500, 1500)\n  )\n  \n  render_snapshot(\n    filename = here(\"output\", paste0(\"icescape_3d_\", seed, \".png\")), \n    clear = TRUE\n  )\n}\n\ntic()\nicescape_3d(123)\ntoc()\n\n\n\n\n\n\n\n\n\n\nshaded-icescape.R\n\n\n\n\n\n\nlibrary(rayshader)\nlibrary(tibble)\nlibrary(ambient)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(tictoc)\nlibrary(here)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\ntransform_to_curl_space &lt;- function(x, y, frequency = 1, octaves = 10) {\n  curl_noise(\n    generator = fracture,\n    noise = gen_simplex,\n    fractal = fbm,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  )\n}\n\n\ndefine_worley_cells &lt;- function(x, y, frequency = 3, octaves = 6) {\n  fracture(\n    noise = gen_worley,\n    fractal = billow,\n    octaves = octaves,\n    frequency = frequency,\n    value = \"cell\",\n    x = x,\n    y = y\n  ) |&gt;\n    rank() |&gt; \n    normalise()\n}\n\n\nsimplex_noise &lt;- function(x, y, frequency = .1, octaves = 10) {\n  fracture(\n    noise = gen_simplex,\n    fractal = ridged,\n    octaves = octaves,\n    frequency = frequency,\n    x = x,\n    y = y\n  ) |&gt;\n    normalise()\n}\n\n\nice_floe &lt;- function(seed) {\n  \n  set.seed(seed)\n  \n  grid &lt;- long_grid(\n    x = seq(0, 1, length.out = 2000),\n    y = seq(0, 1, length.out = 2000)\n  )\n  \n  coords &lt;- transform_to_curl_space(grid$x, grid$y)\n  \n  grid |&gt;\n    mutate(\n      cells = define_worley_cells(coords$x, coords$y),\n      paint = simplex_noise(x + cells, y + cells),\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\nshaded_ice_floe &lt;- function(seed) {\n  \n  art &lt;- ice_floe(seed)\n  \n  height_shade(\n    heightmap = art,\n    texture = sample_canva2(seed, 256)\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = art, \n        sunaltitude = 30, \n        sunangle = 90,\n        multicore = TRUE, \n        zscale = .005\n      ), \n      max_darken = .05\n    ) |&gt;\n    plot_map()\n}\n\ntic()\nshaded_ice_floe(123)\ntoc()\n\n\n\n\n\n\n\n\n\n\nshaded-ridge-art.R\n\n\n\n\n\n\nlibrary(rayshader)\nlibrary(tibble)\nlibrary(ambient)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(tictoc)\nlibrary(dplyr)\n\nsample_canva2 &lt;- function(seed = NULL, n = 4) {\n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |&gt;\n    (\\(x) colorRampPalette(x)(n))()  \n}\n\nridge_art &lt;- function(seed = NULL, pixels = 2000) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  long_grid(\n    x = seq(from = 0, to = 1, length.out = pixels),\n    y = seq(from = 0, to = 1, length.out = pixels)\n  ) |&gt; \n    mutate(\n      paint = fracture(\n        x = x, \n        y = y,\n        noise = gen_simplex,\n        fractal = ridged,\n        octaves = 8,\n        frequency = 10,\n        seed = seed\n      ),\n      paint = normalise(paint)\n    ) |&gt;\n    as.array(value = paint)\n}\n\nshaded_ridge_art &lt;- function(seed = NULL) {\n  \n  art &lt;- ridge_art(seed) \n  height_shade(\n    heightmap = art,\n    texture = sample_canva2(seed, 256)\n  ) |&gt;\n    add_shadow(\n      shadowmap = ray_shade(\n        heightmap = art, \n        sunaltitude = 30, \n        sunangle = 90,\n        multicore = TRUE, \n        zscale = .05\n      ), \n      max_darken = .1\n    ) |&gt;\n    plot_map()\n}\n\ntic()\nshaded_ridge_art(100)\ntoc()"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html",
    "href": "posts/2024-10-06_fs/index.html",
    "title": "For fs",
    "section": "",
    "text": "– okay i’m doing it. gonna write a post about the fs package  – but why tho?  – because it’s cool and fun  – girl it’s literally a tool for interacting with the file system within a statistical programming language. in no possible universe is this cool, fun, or exciting. you are the absolute worst engagement farmer in the history of engagement farming  – pfft, everyone’s gonna love it babe, you’ll see. this shit is girt af  – hon “girt” died the moment the olympics ended. it is as passé as a raygun meme. we’re all doing moo deng now, so all you’re accomplishing here is giving away just how long this post has been in drafts. but hey you do you. i just wanna see how this ends…\nUm. So.\nNow that my alter ego has brutally shattered all my hopes and dreams for this blog post, I suppose I should introduce it properly? This is a post about the fs R package, and as I tried to explain to that bitch who lives in the back of my head and kills every source of love and joy in my world, it provides a cross-platform, uniform interface to file system operations. It is, in a sense, a drop-in replacement for base R functions like file.path(), list.files() and so on. It supplies a cleaner and more consistent interface to these functions, and provides a few other handy tools that aren’t available in the base R equivalents. It’s a\n– no sorry gonna have to step in here. where are you going with this? you just lost like 99% of your possible readership the moment you plagiarised from the package documentation. wtf girl\nlovely little package. Sheesh. She’s not very nice, is she?"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#why-should-anyone-care-about-this",
    "href": "posts/2024-10-06_fs/index.html#why-should-anyone-care-about-this",
    "title": "For fs",
    "section": "Why should anyone care about this?",
    "text": "Why should anyone care about this?\nA package like fs is hard to write about. It’s not a sparkly shiny thing, it’s not the new hotness, and low-level utility tooling is… well, it’s inherently boring. In everyday life, about 90% of my usage of the fs package is just me using the path() function to specify a file path, and… okay, let’s load the package and see what that looks like, yeah?\n\nlibrary(fs)\npath(\"hello\", \"world.txt\")\n\nhello/world.txt\n\n\nIn this example I’m passing two input strings, \"hello\" and \"world.txt\", and from these the path() function has concatenated them using an appropriate separator character so that the output can be interpreted as a (relative) path to a file. Even in the world of statistical programming it is hard to think of anything you could do with a computer that is less exciting than this. It is undeniably, unequivocally, unbelievably dull.\n– no shit  – do you mind? i’m trying to set the stage here  – fine. please, do continue. this should be good\nInterruptions notwithstanding, the thing about boring tasks is that they’re often neglected. Especially if those boring things look simple. And to be fair, neglecting a thing that looks simple is probably an okay thing to do if it actually is simple, but it can create problems if there are hidden nuances underneath the apparently-simple appearance. Interacting with the file system is one of those things. I cannot count the number of times when I’ve encountered code that looks like this:\n\n# I am setting up my project now...\ndir1  &lt;- \"project_root/a_folder/another_folder\"\nfile &lt;- \"file.ext\" \n\n# ...and 3000 lines of code later I do this\npaste(dir1, file, sep = \"\")\n\n[1] \"project_root/a_folder/another_folderfile.ext\"\n\n\nObviously, this is not the result we wanted. Unlike path(), the paste() function is not specifically designed to construct file paths: it’s a tool for concatenating strings and it makes no distinction between strings that look like file paths and strings that don’t.\nWhen looking at a code extract like the one above, it’s so very easy to think “well I would never be that dumb”, but the “3000 lines later” part is crucial here. The moment any project starts to grow in scope – be it a developer project or an analysis project – you reach a point where the code base is large enough that you can’t remember what you did earlier, and it is horrifyingly easy to be exactly that stupid.\nOn top of that, if you happened to be lucky enough not to make the error above, there’s a pretty good chance you’ll mess up the other way and write code like this:\n\n# this time I terminate my folder path with a slash...\ndir2 &lt;- \"project_root/a_folder/\"\n\n# ...and 3000 lines later I forget I did that\npaste(dir2, file, sep = \"/\")\n\n[1] \"project_root/a_folder//file.ext\"\n\n\nIn a way, this is a worse mistake. In the first example you’ll definitely notice the problem because your code breaks the moment you try to work with a file that doesn’t exist, and R will throw an error. But the second one won’t do that. It’s a valid path to the relevant file, so your code will work just fine… right up to the point that you try to write a regular expression that operates on paths and that extra slash breaks something. Worse yet, if your project has expanded to the point that you’re writing a regex to operate on vectors of paths you can be entirely certain you’ve lost tract of the initial mistake that created the problem, and you’re trapped in debugging mode for an hour and a half trying to work out where you went wrong.\nASK. ME. HOW. I. KNOW.\nIn any case, the point in all this is that human working memory capacity is about 7 plus or minus 2 “chunks” of meaninful information:1 we literally do not have the ability to hold a lot of code in our mind at once. So if you manage your file paths using paste() I guarantee you that you will mess this up eventually. Not because you’re stupid, but because you are human.\nWouldn’t it be nice if we had a function… let’s call it path()… that protects us from this particular kind of human frailty? Of course it would."
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#lorem-ipsum-text",
    "href": "posts/2024-10-06_fs/index.html#lorem-ipsum-text",
    "title": "For fs",
    "section": "Lorem ipsum text",
    "text": "Lorem ipsum text\nNow that we’ve established some motivation for caring about this topic\n– lol. lmao even  – oh hush \nit will be convenient to have a little tool that generates lorem ipsum text that we can write to files that we’ll then manipulate using fs. To that end, I’ll define a lorem_text() function that uses the lorem and withr packages to reproducibly generate paragraphs of lorem ipsum text:\n\nlorem_text &lt;- function(seed, paragraphs, digit_pad = 3) {\n  lorem &lt;- withr::with_seed(seed, lorem::ipsum(paragraphs))\n  names(lorem) &lt;- purrr::imap_chr(lorem, \\(x, y) paste(\n    stringr::str_pad(seed, width = digit_pad, pad = \"0\"),\n    stringr::str_pad(y, width = digit_pad, pad = \"0\"),\n    stringr::str_to_lower(stringr::str_remove_all(x, \" .*$\")),\n    sep = \"_\"\n  ))\n  lorem\n}\n\nTo call this function we pass the seed value for the random number generator to use, and the number of paragraphs of lorem ipsum text to create:2\n\nlorem_text(seed = 999, paragraphs = 3)\n\nAmet arcu suscipit donec cras inceptos rhoncus varius hac? Sociosqu luctus iaculis; ut sociosqu porta risus tristique phasellus? Duis porta in placerat phasellus class. Dapibus nostra ac, aptent nam tempus mattis eleifend metus.\nLorem molestie in elementum nascetur scelerisque cum pulvinar felis massa. Fringilla est tortor auctor nulla tristique ac mi commodo vitae. Torquent sodales eget lacinia quam elementum sodales. Cras porttitor iaculis curae eleifend fringilla pellentesque nascetur. Integer ligula per dignissim, dapibus feugiat nullam urna tristique viverra sociis felis etiam.\nSit posuere suscipit accumsan mus curabitur nullam, etiam scelerisque donec justo libero posuere. Mattis curae et litora.\nThough not obvious from the printed output, the data structure that this function returns is a named list under the hood:\n\nnames(lorem_text(seed = 999, paragraphs = 3))\n\n[1] \"999_001_amet\"  \"999_002_lorem\" \"999_003_sit\"  \n\n\nThe names here follow a regular pattern: they contain the seed number, the paragraph number, and the first word in the paragraph, separated by underscores. In the examples below, these names become file names, and the text in each paragraph become the content written to the various files.\n– thrilling  – so you’re just set on doing this? you’re going to snipe at me the whole way through? –  –  –  –  – yes."
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#building-paths",
    "href": "posts/2024-10-06_fs/index.html#building-paths",
    "title": "For fs",
    "section": "Building paths",
    "text": "Building paths\nSo as I was saying earlier, about 90% of my usage of the fs package is via the path() function used to construct file paths, so it’s the natural place to start. Here’s a very simple example that specifies the path from my blog root to this quarto document:\n\npath(\"posts\", \"2024-09-15_fs\", \"index.qmd\")\n\nposts/2024-09-15_fs/index.qmd\n\n\nI’m building this post on linux, so paths are separated by the \"/\" character. If I were building on windows, I’d get a different result.\nThe path() function is vectorised and doesn’t require that the paths in question actually exist on the machine, so I can do something like this to define a vector of paths that I can work with later on:\n\nlorem &lt;- lorem_text(seed = 1, paragraphs = 20)\nlorem_paths &lt;- path(\"lorem\", names(lorem))\nlorem_paths\n\nlorem/001_001_consectetur lorem/001_002_ipsum       \nlorem/001_003_elit        lorem/001_004_lorem       \nlorem/001_005_consectetur lorem/001_006_sit         \nlorem/001_007_sit         lorem/001_008_adipiscing  \nlorem/001_009_adipiscing  lorem/001_010_lorem       \nlorem/001_011_lorem       lorem/001_012_adipiscing  \nlorem/001_013_amet        lorem/001_014_sit         \nlorem/001_015_elit        lorem/001_016_ipsum       \nlorem/001_017_adipiscing  lorem/001_018_consectetur \nlorem/001_019_amet        lorem/001_020_consectetur \n\n\nThese are relative paths, and since (by default) quarto blog posts are executed with the working directory set to the folder that contains the document, these paths are implicitly taken relative to this folder.\n– wow how exci…  – shut up, nobody wants to hear from you  – uh huh"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#file-system-operations",
    "href": "posts/2024-10-06_fs/index.html#file-system-operations",
    "title": "For fs",
    "section": "File system operations",
    "text": "File system operations\nThe second most common thing I find myself using the fs package for is basic file system operations: creating files and folders, copying, deleting, and moving files, etc. For example, the paths I specified in the previous section all refer to a folder called “lorem”, but that folder doesn’t currently exist. Indeed, I can verify that no such folder exists using the dir_exists() function”:\n\ndir_exists(\"lorem\")\n\nlorem \nFALSE \n\n\nThat’s handy to know, because I actually do want this folder to exist, and fortunately I can create the folder I want from R by using dir_create(), and then verify that it now exists:\n\ndir_create(\"lorem\")\ndir_exists(\"lorem\")\n\nlorem \n TRUE \n\n\nLike all functions in fs, these are vectorised operations. For example, I can test for the existence of multiple folders at once like this:\n\ndir_exists(c(\"lorem\", \"ipsum\"))\n\nlorem ipsum \n TRUE FALSE \n\n\nIn any case, now that the “lorem” directory exists, I can use file_create() to create the files listed in the lorem_paths vector I defined earlier. Again, file_create() is vectorised, so I can pass the vector of file names directly with no need to write a loop:\n\nfile_create(lorem_paths)\n\nThough there is no output printed to the console, all the files I requested have now been created. To see this, I can use dir_ls() to return a vector containing all the file names within a specified folder:\n\ndir_ls(\"lorem\")\n\nlorem/001_001_consectetur lorem/001_002_ipsum       \nlorem/001_003_elit        lorem/001_004_lorem       \nlorem/001_005_consectetur lorem/001_006_sit         \nlorem/001_007_sit         lorem/001_008_adipiscing  \nlorem/001_009_adipiscing  lorem/001_010_lorem       \nlorem/001_011_lorem       lorem/001_012_adipiscing  \nlorem/001_013_amet        lorem/001_014_sit         \nlorem/001_015_elit        lorem/001_016_ipsum       \nlorem/001_017_adipiscing  lorem/001_018_consectetur \nlorem/001_019_amet        lorem/001_020_consectetur \n\n\nOkay, that’s nice, but I don’t actually want a folder full of empty files. So let’s delete the folder and everything in it. That’s easy enough to do with dir_delete()\n\ndir_delete(\"lorem\")\n\nAnd just like that, the files and folder are gone. Alternatively, if I had wanted only to delete some of the files I could have used file_delete() to be a little more selective!"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#file-trees",
    "href": "posts/2024-10-06_fs/index.html#file-trees",
    "title": "For fs",
    "section": "File trees",
    "text": "File trees\nOkay that’s handy. As a slightly fancier example, though, let’s try creating files with a little more structure to them. Rather than write each of the lorem files to the same directory, we can place them in subfolders based on the first word in the lorem text. To do that, I’ll need to create these directories:\n\nlorem_dirs &lt;- unique(stringr::str_remove(names(lorem), \"^.*_\"))\nlorem_dirs\n\n[1] \"consectetur\" \"ipsum\"       \"elit\"        \"lorem\"       \"sit\"        \n[6] \"adipiscing\"  \"amet\"       \n\n\nHowever, I don’t want to create these as top level folders: my file structure could become a mess if I do that. Instead, I’ll create them as subfolders of a “nonsense” folder. I can do this with a single call to dir_create():\n\ndir_create(path(\"nonsense\", lorem_dirs))\n\nThis command creates the “nonsense” folder itself, and populates it with all the subfolders listed in lorem_dirs. To see this displayed as a nice file tree, I’ll use the dir_tree() function:\n\ndir_tree(\"nonsense\")\n\nnonsense\n├── adipiscing\n├── amet\n├── consectetur\n├── elit\n├── ipsum\n├── lorem\n└── sit\n\n\nHaving created a nested directory structure, I can now define the paths to which I want to write files:\n\nlorem_paths &lt;- path(\n  \"nonsense\", \n  stringr::str_remove(names(lorem), \"^.*_\"), \n  names(lorem)\n)\nlorem_paths\n\nnonsense/consectetur/001_001_consectetur\nnonsense/ipsum/001_002_ipsum\nnonsense/elit/001_003_elit\nnonsense/lorem/001_004_lorem\nnonsense/consectetur/001_005_consectetur\nnonsense/sit/001_006_sit\nnonsense/sit/001_007_sit\nnonsense/adipiscing/001_008_adipiscing\nnonsense/adipiscing/001_009_adipiscing\nnonsense/lorem/001_010_lorem\nnonsense/lorem/001_011_lorem\nnonsense/adipiscing/001_012_adipiscing\nnonsense/amet/001_013_amet\nnonsense/sit/001_014_sit\nnonsense/elit/001_015_elit\nnonsense/ipsum/001_016_ipsum\nnonsense/adipiscing/001_017_adipiscing\nnonsense/consectetur/001_018_consectetur\nnonsense/amet/001_019_amet\nnonsense/consectetur/001_020_consectetur\n\n\nFor each path in the lorem_paths vector, and each passage of text in the lorem object, we can write the text to the corresponding file like this:\n\npurrr::walk(\n  seq_along(lorem),\n  \\(x) brio::write_lines(\n    text = lorem[[x]],\n    path = lorem_paths[x]\n  )\n)\n\nThe file tree now looks like this:\n\ndir_tree(\"nonsense\")\n\nnonsense\n├── adipiscing\n│   ├── 001_008_adipiscing\n│   ├── 001_009_adipiscing\n│   ├── 001_012_adipiscing\n│   └── 001_017_adipiscing\n├── amet\n│   ├── 001_013_amet\n│   └── 001_019_amet\n├── consectetur\n│   ├── 001_001_consectetur\n│   ├── 001_005_consectetur\n│   ├── 001_018_consectetur\n│   └── 001_020_consectetur\n├── elit\n│   ├── 001_003_elit\n│   └── 001_015_elit\n├── ipsum\n│   ├── 001_002_ipsum\n│   └── 001_016_ipsum\n├── lorem\n│   ├── 001_004_lorem\n│   ├── 001_010_lorem\n│   └── 001_011_lorem\n└── sit\n    ├── 001_006_sit\n    ├── 001_007_sit\n    └── 001_014_sit"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#file-information",
    "href": "posts/2024-10-06_fs/index.html#file-information",
    "title": "For fs",
    "section": "File information",
    "text": "File information\nSometimes it is useful to retrieve information about a file, analogous to the stat command on linux systems. From the terminal, you’d get output that looks like this:3\n\nsystem(\"stat nonsense/lorem/001_004_lorem\")\n\n\n\n  File: nonsense/lorem/001_004_lorem\n  Size: 474         Blocks: 8          IO Block: 4096   regular file\nDevice: 252,1   Inode: 1586402     Links: 1\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/danielle)   Gid: ( 1000/danielle)\nAccess: 2024-10-06 14:10:44.468497699 +1100\nModify: 2024-10-06 14:10:44.468497699 +1100\nChange: 2024-10-06 14:10:44.468497699 +1100\n Birth: 2024-10-06 14:10:44.468497699 +1100\n\n\nThe file_info() function in the fs package mirrors this behaviour, all nicely vectorised so you can pass a vector of file paths, and with output organised into a tidy little tibble to make it easy to work with programmatically:\n\nfile_info(lorem_paths[1:4])\n\n# A tibble: 4 × 18\n  path                                     type         size permissions modification_time   user     group    device_id hard_links special_device_id   inode block_size blocks flags generation access_time         change_time         birth_time         \n  &lt;fs::path&gt;                               &lt;fct&gt; &lt;fs::bytes&gt; &lt;fs::perms&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;             \n1 nonsense/consectetur/001_001_consectetur file          307 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586371       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n2 nonsense/ipsum/001_002_ipsum             file          322 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586372       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n3 nonsense/elit/001_003_elit               file          397 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586401       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n4 nonsense/lorem/001_004_lorem             file          474 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586402       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n\n\nThere is an analogous function dir_info() that can be applied to a directory, and the output is structured the same way:\n\ndir_info(\"nonsense\", recurse = TRUE)\n\n# A tibble: 27 × 18\n   path                                     type             size permissions modification_time   user     group    device_id hard_links special_device_id   inode block_size blocks flags generation access_time         change_time         birth_time         \n   &lt;fs::path&gt;                               &lt;fct&gt;     &lt;fs::bytes&gt; &lt;fs::perms&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt; &lt;dttm&gt;              &lt;dttm&gt;              &lt;dttm&gt;             \n 1 nonsense/adipiscing                      directory          4K rwxr-xr-x   2024-10-06 14:10:44 danielle danielle     64513          2                 0 1586368       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 2 nonsense/adipiscing/001_008_adipiscing   file              415 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586419       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 3 nonsense/adipiscing/001_009_adipiscing   file              370 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586420       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 4 nonsense/adipiscing/001_012_adipiscing   file              254 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586514       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 5 nonsense/adipiscing/001_017_adipiscing   file              393 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1589481       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 6 nonsense/amet                            directory          4K rwxr-xr-x   2024-10-06 14:10:44 danielle danielle     64513          2                 0 1586369       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 7 nonsense/amet/001_013_amet               file              367 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586515       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 8 nonsense/amet/001_019_amet               file              313 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1589484       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n 9 nonsense/consectetur                     directory          4K rwxr-xr-x   2024-10-06 14:10:44 danielle danielle     64513          2                 0 1586360       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n10 nonsense/consectetur/001_001_consectetur file              307 rw-rw-r--   2024-10-06 14:10:44 danielle danielle     64513          1                 0 1586371       4096      8     0          0 2024-10-06 14:10:44 2024-10-06 14:10:44 2024-10-06 14:10:44\n# ℹ 17 more rows\n\n\nThat being said, it’s pretty uncommon in my experience to need all that information. Often the only thing you’re interested in is the file size column, and the fs package provides a convenience function that extracts that information for you:\n\nfile_size(lorem_paths)\n\n307 322 397 474 196 599 501 415 370 135 228 254 367 360 357 375 393 552 313 523\n\n\n\n– wha… oh, huh, i guess i lost consciousness there. so you’re still talking huh?  – sweetie you’re literally a figment of my imagination you don’t have a consciousness to lose  – bitch  – ¯\\_(ツ)_/¯"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#path-arithmetic",
    "href": "posts/2024-10-06_fs/index.html#path-arithmetic",
    "title": "For fs",
    "section": "Path arithmetic",
    "text": "Path arithmetic\nMy absolute favourite collection of functions within fs are the ones that can be used to perform “path arithmetic”, by which what I really mean is handy string manipulations for common tasks that save me from the horrior of writing a regular expression. Because, like all right-thinking people, I loathe regular expressions with a passion I usually reserve for real estate agents and people who don’t pick up after their dogs.\nTo illustrate the idea, let’s think about some common tasks we might need to perform using the lorem_paths vector:\n\nlorem_paths\n\nnonsense/consectetur/001_001_consectetur\nnonsense/ipsum/001_002_ipsum\nnonsense/elit/001_003_elit\nnonsense/lorem/001_004_lorem\nnonsense/consectetur/001_005_consectetur\nnonsense/sit/001_006_sit\nnonsense/sit/001_007_sit\nnonsense/adipiscing/001_008_adipiscing\nnonsense/adipiscing/001_009_adipiscing\nnonsense/lorem/001_010_lorem\nnonsense/lorem/001_011_lorem\nnonsense/adipiscing/001_012_adipiscing\nnonsense/amet/001_013_amet\nnonsense/sit/001_014_sit\nnonsense/elit/001_015_elit\nnonsense/ipsum/001_016_ipsum\nnonsense/adipiscing/001_017_adipiscing\nnonsense/consectetur/001_018_consectetur\nnonsense/amet/001_019_amet\nnonsense/consectetur/001_020_consectetur\n\n\nThe most common task that I have to do regularly with paths like this is extract the file name. Under other circumstances I’d have to spend time asking myself “are these paths correctly formatted?” and “god, how do I write a basic regex again????” but thankfully the path_file() functions saves me from this terrible fate:\n\npath_file(lorem_paths)\n\n [1] \"001_001_consectetur\" \"001_002_ipsum\"       \"001_003_elit\"       \n [4] \"001_004_lorem\"       \"001_005_consectetur\" \"001_006_sit\"        \n [7] \"001_007_sit\"         \"001_008_adipiscing\"  \"001_009_adipiscing\" \n[10] \"001_010_lorem\"       \"001_011_lorem\"       \"001_012_adipiscing\" \n[13] \"001_013_amet\"        \"001_014_sit\"         \"001_015_elit\"       \n[16] \"001_016_ipsum\"       \"001_017_adipiscing\"  \"001_018_consectetur\"\n[19] \"001_019_amet\"        \"001_020_consectetur\"\n\n\nAnalogously, if I need to extract the directory name and ignore the file name, I could waste precious seconds of my life thinking about this tedious task using first principles, or I could simply use path_dir() to do this:\n\npath_dir(lorem_paths)\n\n [1] \"nonsense/consectetur\" \"nonsense/ipsum\"       \"nonsense/elit\"       \n [4] \"nonsense/lorem\"       \"nonsense/consectetur\" \"nonsense/sit\"        \n [7] \"nonsense/sit\"         \"nonsense/adipiscing\"  \"nonsense/adipiscing\" \n[10] \"nonsense/lorem\"       \"nonsense/lorem\"       \"nonsense/adipiscing\" \n[13] \"nonsense/amet\"        \"nonsense/sit\"         \"nonsense/elit\"       \n[16] \"nonsense/ipsum\"       \"nonsense/adipiscing\"  \"nonsense/consectetur\"\n[19] \"nonsense/amet\"        \"nonsense/consectetur\"\n\n\nMuch easier, and frankly more reliable, than trying to do the job myself.\nThere’s even a path_common() function that returns the part of the path that is shared by all paths in the vector. I’ll admit I don’t use that one as often, but it’s kind of nice that the package supplies this. I appreciate the attention to detail involved in recognising that sometimes you do actually need this:\n\npath_common(lorem_paths)\n\nnonsense\n\n\nSure, I already knew that “nonsense” is the folder containing all these files because I designed this little exercise that way, but still pretty handy, especially when you combine it with path_abs() that converts a relative path to an absolute path to find the actual location on my machine that contains all these files:\n\nlorem_paths |&gt; \n  path_common() |&gt; \n  path_abs()\n\n/home/danielle/GitHub/djnavarro/blog/posts/2024-10-06_fs/nonsense\n\n\nYou can also call path_split() to split paths into a list of character vectors, where each such vector contains one element per level in the file hierarchy. This behaves analogously to base split() or stringr::str_split(), but automatically splits using the relevant file separator character on your operating system:\n\npath_split(lorem_paths[1:2])\n\n[[1]]\n[1] \"nonsense\"            \"consectetur\"         \"001_001_consectetur\"\n\n[[2]]\n[1] \"nonsense\"      \"ipsum\"         \"001_002_ipsum\"\n\n\nIt’s not the prettiest of outputs, but notice that you can use this as the basis for a list column in a data frame that you can then unnest with the assistance of tidyr:\n\nlorem_paths |&gt; \n  path_split() |&gt; \n  tibble::tibble(level = _) |&gt; \n  tidyr::unnest_wider(col = \"level\", names_sep = \"_\")\n\n# A tibble: 20 × 3\n   level_1  level_2     level_3            \n   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;              \n 1 nonsense consectetur 001_001_consectetur\n 2 nonsense ipsum       001_002_ipsum      \n 3 nonsense elit        001_003_elit       \n 4 nonsense lorem       001_004_lorem      \n 5 nonsense consectetur 001_005_consectetur\n 6 nonsense sit         001_006_sit        \n 7 nonsense sit         001_007_sit        \n 8 nonsense adipiscing  001_008_adipiscing \n 9 nonsense adipiscing  001_009_adipiscing \n10 nonsense lorem       001_010_lorem      \n11 nonsense lorem       001_011_lorem      \n12 nonsense adipiscing  001_012_adipiscing \n13 nonsense amet        001_013_amet       \n14 nonsense sit         001_014_sit        \n15 nonsense elit        001_015_elit       \n16 nonsense ipsum       001_016_ipsum      \n17 nonsense adipiscing  001_017_adipiscing \n18 nonsense consectetur 001_018_consectetur\n19 nonsense amet        001_019_amet       \n20 nonsense consectetur 001_020_consectetur\n\n\nNote that this trick also works when the paths are of different lengths. For example, suppose I were to use dir_ls() to return the complete list of all files and folders contained within the “nonsense” folder, some of the paths will be length 2 rather than length 3, because the folder paths are also included in the output. Because unnest_wider() is able to handle ragged list columns, you get this as the output:\n\ndir_ls(\"nonsense\", recurse = TRUE) |&gt; \n  path_split() |&gt; \n  tibble::tibble(level = _) |&gt; \n  tidyr::unnest_wider(col = \"level\", names_sep = \"_\")\n\n# A tibble: 27 × 3\n   level_1  level_2     level_3            \n   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;              \n 1 nonsense adipiscing  &lt;NA&gt;               \n 2 nonsense adipiscing  001_008_adipiscing \n 3 nonsense adipiscing  001_009_adipiscing \n 4 nonsense adipiscing  001_012_adipiscing \n 5 nonsense adipiscing  001_017_adipiscing \n 6 nonsense amet        &lt;NA&gt;               \n 7 nonsense amet        001_013_amet       \n 8 nonsense amet        001_019_amet       \n 9 nonsense consectetur &lt;NA&gt;               \n10 nonsense consectetur 001_001_consectetur\n# ℹ 17 more rows"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#epilogue",
    "href": "posts/2024-10-06_fs/index.html#epilogue",
    "title": "For fs",
    "section": "Epilogue",
    "text": "Epilogue\nSo anyway, that’s about everything I wanted to talk about. It’s not an exhaustive listing of course, and there are a variety of other helper functions in fs, some of which I very occasionally make use of. For instance, you can use file_chmod() to change file permissions, file_touch() to change file access and modification time metadata, file_temp() to create a temporary file, and so on. I find I don’t use these as often, but I’m glad they exist.\n– thats nice hon but seriously, why write any of this? i don’t see the point  – isnt it enough that i wanted to write it? i mean, why else do we even have a blog, if not to write about whatever we feel like writing about? if other people want to read it, good for them, and if they dont… also good for them. ffs, the whole idea of “blogging as thought leadership” needs to die in a fire  – so i guess we really are here to fuck spiders huh?  – always, babe. always"
  },
  {
    "objectID": "posts/2024-10-06_fs/index.html#footnotes",
    "href": "posts/2024-10-06_fs/index.html#footnotes",
    "title": "For fs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNo I absolutely will not be going into details about the subtle differences in working memory capacity as a function of modality and age, or the nuances about what precisely comprises as chunk, or whatever in the well-actually fuck you want to nitpick. Do I look like a cognitive scientist to you?↩︎\nIn the interests of tranparency I should mention that if you tried this code as-is within a quarto or R markdown document, it wouldn’t necessarily be displayed in italics like this. That’s a personal affectation I added in this post to more clearly delineate the end of the R output from the start of the markdown text.↩︎\nSigh. I’m hiding something. If you do this command at the R console, you will indeed get the output shown below. However, if you try to do this from within R markdown or quarto you will not. This is because the output you see here reflects the system stdout, which is different from the R console stdout, and if you want to capture this within the HTML document you have to do something a little fancier, setting intern = TRUE to ensure system() returns the terminal output as a character vector that you can then print to the R console in the usual way with cat(). See this discussion on stackoverflow.↩︎"
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html",
    "href": "posts/2024-07-04_flextable/index.html",
    "title": "Use of flextable",
    "section": "",
    "text": "“Tell me, what is happiness?”  “Happiness? Happiness … is to wake up, on a bright spring morning, after an exhausting first night spent with a beautiful … passionate … multi-murderess.”  “… Shit, is that all?”          – Use of Weapons\nThe year is 1994. A closeted, miserable girl sits on the floor in the library. She is 16, it is her first year at university, and her first time living in a city. She knows very little about the world, or herself for that matter. But the library has a copy of Use of Weapons, and for the moment at least she is somewhere else. Anywhere but here.\nThis is a post about making tables in R using the rather-lovely flextable package, documented terribly well in the flextable book. I use flextable a lot at work, but until recently I hadn’t explored it very thoroughly. Last week I ran into something I didn’t know how to solve1 and found myself diving deeper and inevitably, ended up writing up some notes that by some inscrutable process have transformed themselves into a blog post.2\nIt is also a post about the Culture novels by Iain Banks, one of my all-time favourite science fiction series, and the work of Iain Banks more generally I suppose. I discovered the Culture novels as an undergraduate, and have loved them deeply ever since. There are nine books that make up the Culture novels, starting with the 1988 novel Consider Phlebas and finishing with The Hydrogen Sonata in 2012. Each of the books is a standalone story set in the same universe, and largely revolve around the Culture, a utopian anarchic society of hedonistic humans and manipulative machines.\nI’d imagine that a great many people in my usual audience are familiar with the Culture novels, but for those who are not here’s the wikipedia summary of each of the novels:\nBook TitleCoverDescriptionConsider PhlebasAn episode in a full-scale war between the Culture and the Idirans, told mainly from the point of view of an operative of the Idiran Empire.The Player of GamesA bored member of the Culture is blackmailed into being the Culture's agent in a plan to subvert a brutal, hierarchical empire. His mission is to win an empire-wide tournament by which the ruler of the empire is selected.Use of WeaponsChapters describing the current mission of a Culture special agent born and raised on a non-Culture planet alternate with chapters that describe in reverse chronological order earlier missions and the traumatic events that made him who he is.The State of the ArtA short story collection. Two of the works are explicitly set in the Culture universe (\"The State of the Art\" and \"A Gift from the Culture\"), with a third work (\"Descendant\") possibly set in the Culture universe. In the title novella, the Mind in charge of an expedition to Earth decides not to make contact or intervene in any way, but instead to use Earth as a control group in the Culture's long-term comparison of intervention and non-interference.ExcessionAn alien artifact far advanced beyond the Culture's understanding is used by one group of Minds to lure a civilisation (the behaviour of which they disapprove) into war; another group of Minds works against the conspiracy. A sub-plot covers how two humanoids make up their differences after traumatic events that happened 40 years earlier.InversionsNot explicitly a Culture novel, but recounts what appear to be the activities of a Special Circumstances agent and a Culture emigrant on a planet whose development is roughly equivalent to that of medieval Europe. The interwoven stories are told from the viewpoint of several of the locals.Look to WindwardThe Culture has interfered in the development of a race known as the Chelgrians, with disastrous consequences. Now, in the light of a star that was destroyed 800 years previously during the Idiran War, plans for revenge are being hatched.MatterA Culture special agent who is a princess of an early-industrial society on a huge artificial planet learns that her father and brother have been killed and decides to return to her homeworld. When she returns, she finds a far deeper threat.Surface DetailA young woman seeks revenge on her murderer after being brought back to life by Culture technology. Meanwhile, a war over the digitized souls of the dead is expanding from cyberspace into the real world.The Hydrogen SonataIn the last days of the Gzilt civilisation, which is about to Sublime, a secret from far back in their history threatens to unravel their plans. Aided by a number of Culture vessels and their avatars, one of the Gzilt tries to discover if much of their history was actually a lie.\nThe little blurbs don’t really do justice to the novels themselves, but that’s hardly surprising as I took the text from wikipedia. Perhaps more to the point of the post, it’s worth mentioning that I made this table in R using flextable, and that is maybe a little more surprising. As it happens, it’s pretty easy to make these tables once you wrap your head around how the package works, which is terribly awesome really.\nSo let’s have a look at how it all works, shall we?\nlibrary(flextable)\nlibrary(ftExtra)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)"
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#getting-started",
    "href": "posts/2024-07-04_flextable/index.html#getting-started",
    "title": "Use of flextable",
    "section": "Getting started",
    "text": "Getting started\n\nHe looked out through the open faceplate, and wiped a little sweat from his brow. It was dusk over the plateau. A few metres away, by the light of two moons and a fading sun, he could see the rimrock, frost-whitened. Beyond was the great gash in the desert which provided the setting for the ancient half-empty city where Tsoldrin Beychae now lived.  Clouds drifted, and the dust collected.  “Well,” he sighed, to no-one in particular, and looked up into yet another alien sky. “Here we go again.”          – Use of Weapons\n\nThe pretty table in the opening section is based on the table shown in the wikipedia page on the Culture novels, along with a few other bits and pieces I found on wikipedia. More precisely, it derives from a small csv file that I put together using that page:\n\nnovels &lt;- read_csv(\"culture.csv\", col_types = \"ciccccc\")\nnovels\n\n# A tibble: 10 × 7\n   title         publication_date setting_date isbn  url   description image\n   &lt;chr&gt;                    &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;\n 1 Consider Phl…             1987 1331 CE      1-85… http… \"An episod… cove…\n 2 The Player o…             1988 c. 2083 to … 1-85… http… \"A bored m… cove…\n 3 Use of Weapo…             1990 2092 CE mai… 1-85… http… \"Chapters … cove…\n 4 The State of…             1991 Varies (tit… 0-35… http… \"A short s… cove…\n 5 Excession                 1996 c. 1867 CE … 1-85… http… \"An alien … cove…\n 6 Inversions                1998 Unspecified  1-85… http… \"Not expli… cove…\n 7 Look to Wind…             2000 c. 2167 CE   1-85… http… \"The Cultu… cove…\n 8 Matter                    2008 c. 1887 or … 1-84… http… \"A Culture… cove…\n 9 Surface Deta…             2010 Sometime be… 1-84… http… \"A young w… cove…\n10 The Hydrogen…             2012 c. 2375 CE   978-… http… \"In the la… cove…\n\n\nThe printed data frame is not quite as elegant as the table, but it’s not too hard to see that I constructed the table using the title, url, description, and image columns in this table. As I meander my way through the post I’ll use this data set to make many different tables. Some will be pretty, most will not. Table construction is akin to data visualisation in that respect: it’s very easy to create something that looks “kinda okay”, but takes a lot more effort to make it actually look good.\nTo illustrate this point, let’s have a look at what happens if we use the flextable package to construct a table from the first four columns in the novels data frame, without doing any tinkering at all. The “workhorse” function in the package is called flextable(), and the usual workflow in the package is to (1) do a bit of data wrangling to reformat the data, (2) pipe the reformatted data to flextable(), and then (3) do a lot of finicky work to tidy up the table. Here’s what we get in the simplest case where we only do the first two steps:\n\nnovels |&gt;\n  select(title:isbn) |&gt; \n  flextable()\n\ntitlepublication_datesetting_dateisbnConsider Phlebas1,9871331 CE1-85723-138-4The Player of Games1,988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1,9902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1,991Varies (title story: 1977 CE)0-356-19669-0Excession1,996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1,998Unspecified1-85723-763-3Look to Windward2,000c. 2167 CE1-85723-969-5Matter2,008c. 1887 or 2167 CE1-84149-417-8Surface Detail2,010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2,012c. 2375 CE978-0356501505\n\n\nThe output, in the context of this quarto blog, is an HTML table. The flextable package supports a variety of output formats, not merely HTML, but in the interests of “brevity” – she says, as if she were even capable of writing a short blog post – I won’t talk about other formats in this post.\nVisually speaking, this table is… well, it’s okay. It’s readable, and it’s not hideous, but it can be improved upon in any number of ways."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#column-widths",
    "href": "posts/2024-07-04_flextable/index.html#column-widths",
    "title": "Use of flextable",
    "section": "Column widths",
    "text": "Column widths\n\nHe was tall and very dark-skinned and he had fabulously blond hair and a voice that could raise bumps on your skin at a hundred meters, or, better still, millimeters.3      – Excession\n\nLet us begin the task of making our table a little more attractive. The most obvious shortcoming in the previous table is the column widths: all four columns are the same width, which makes very little sense when the content of the columns varies quite considerably in length. We can adjust the column widths manually by passing a vector of widths (in inches) as the cwidth argument to flextable(), like so:\n\nnovels |&gt;\n  select(title:isbn) |&gt; \n  flextable(cwidth = c(3, 1, 4, 2))\n\ntitlepublication_datesetting_dateisbnConsider Phlebas1,9871331 CE1-85723-138-4The Player of Games1,988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1,9902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1,991Varies (title story: 1977 CE)0-356-19669-0Excession1,996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1,998Unspecified1-85723-763-3Look to Windward2,000c. 2167 CE1-85723-969-5Matter2,008c. 1887 or 2167 CE1-84149-417-8Surface Detail2,010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2,012c. 2375 CE978-0356501505\n\n\nThere are situations where this manual control is helpful, and indeed I’ll rely on the cwidth argument for several of the tables in this post, but it’s often an annoying process of trial and error trying to find widths that you like. Fortunately, you can sometimes bypass this process with the autofit() function which attempts to select nice column widths for you. If we pipe the flextable to autofit() we get this…\n\nnovels |&gt;\n  select(title:isbn) |&gt; \n  flextable() |&gt; \n  autofit()\n\ntitlepublication_datesetting_dateisbnConsider Phlebas1,9871331 CE1-85723-138-4The Player of Games1,988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1,9902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1,991Varies (title story: 1977 CE)0-356-19669-0Excession1,996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1,998Unspecified1-85723-763-3Look to Windward2,000c. 2167 CE1-85723-969-5Matter2,008c. 1887 or 2167 CE1-84149-417-8Surface Detail2,010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2,012c. 2375 CE978-0356501505\n\n\n…and honestly that’s pretty good. Not quite as nice as the widths I chose manually based on my personal sense of aesthetics, but good enough for a technical report. When you’re working under time pressure the autofit() functionality is so helpful."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#column-labels",
    "href": "posts/2024-07-04_flextable/index.html#column-labels",
    "title": "Use of flextable",
    "section": "Column labels",
    "text": "Column labels\n\nHe looked up from it at the stars again, and the view was warped and distorted by something in his eyes, which at first he thought was rain.      –The Player of Games\n\nIt is the bitter nature of all data work that the moment you fix one problem with any analysis your attention is called immediately to the next thing that is wrong. So it goes with table construction. Having tidied up the column spacing, the eye is drawn to the glaring problem with the column headers. By default flextable will use the variable names as column headers, which is almost never a good idea: the properties that make a good variable name are rarely the same as those that make a nice column header, and so inevitable every flextable pipeline ends up calling set_header_labels() to override the default. This function takes name-value pairs as inputs, like this:\n\nnovels |&gt;\n  flextable(\n    col_keys = c(\"title\", \"publication_date\", \"setting_date\", \"isbn\"),\n    cwidth = c(3, 1, 4, 2)\n  ) |&gt; \n  set_header_labels(\n    title = \"Book Title\",\n    publication_date = \"Published\",\n    setting_date = \"Story Date\", \n    isbn = \"ISBN\"\n  )\n\nBook TitlePublishedStory DateISBNConsider Phlebas1,9871331 CE1-85723-138-4The Player of Games1,988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1,9902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1,991Varies (title story: 1977 CE)0-356-19669-0Excession1,996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1,998Unspecified1-85723-763-3Look to Windward2,000c. 2167 CE1-85723-969-5Matter2,008c. 1887 or 2167 CE1-84149-417-8Surface Detail2,010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2,012c. 2375 CE978-0356501505\n\n\nIn this extract, the call to set_header_labels() is fairly self-explanatory: when I pass a name-value pair like title = \"Book Title\" what doing is telling flextable to use \"Book Title\" as the displayed label for the data column called title. However, there’s a little subtlety to call attention to here.\nNotice that this version of the code doesn’t use select() to manipulate the data passed to flextable(). Instead, what I’ve done is specify the col_keys argument to flextable() as an alternative way to indicate which data columns to include. Most of the time you don’t actually bother with this: it’s usually easier to use select() to extract the relevant variables, and I totally could have done that here if I’d wanted to. However there are cases where you’ll find yourself building a table column from multiple data columns, and in such cases it can be useful to be aware that “column keys are the data variable names” is merely a default, and one you can override."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#column-formats",
    "href": "posts/2024-07-04_flextable/index.html#column-formats",
    "title": "Use of flextable",
    "section": "Column formats",
    "text": "Column formats\n\nYOU MAY ENTER.  THERE IS DEATH HERE.  BE WARNED.4      – Consider Phlebas\n\nIn what can only be described as a blatant plot device, Consider Phlebas features an alien species called the Dra’Azon who, well, basically do nothing throughout the entire novel except loom over everyone else as a vaguely menacing presence that prevents them from doing the blindingly obvious thing that they all want to do, thereby setting into motion the entire convoluted sequence of events that comprise the novel. Only one Dra’Azon ever appears in the book, and it only communicates by text on a screen and that text is FORMATTED IN ALL CAPS.\nThat seems as good a reason as any to start talking about column formatting.\nWhen deciding how to format the values displayed in the cells, flextable tries to supply sensible defaults for different data types, but you can override these defaults with the assistance of the colformat_*() functions:\n\ncolformat_char(): Format character cells\ncolformat_date(): Format date cells\ncolformat_datetime(): Format datetime cells\ncolformat_double(): Format numeric cells\ncolformat_image(): Format cells as images\ncolformat_int(): Format integer cells\ncolformat_lgl(): Format logical cells\ncolformat_num(): Format numeric cells\n\nTo give an example, the table we have so far doesn’t display the publication dates very nicely. The default convention for numeric values uses commas as the “big mark” character (e.g., one million would be written “1,000,000” not “1000000”). That isn’t very helpful in this case because the column should actually be interpreted as year, and by convention we normally write “1987” for the year rather than “1,987”. We can fix this by using the colformat_int() function to set big.mark = \"\":\n\nnovels |&gt;\n  select(title:isbn) |&gt; \n  flextable(cwidth = c(3, 1, 4, 2)) |&gt; \n  set_header_labels(\n    title = \"Book Title\",\n    publication_date = \"Published\",\n    setting_date = \"Story Date\", \n    isbn = \"ISBN\"\n  ) |&gt; \n  colformat_int(big.mark = \"\")\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nI’m pretty happy with this table. I might want to tweak the visual style a little, certainly, but in the normal course of events this is probably where I’d stop. But because there are several other flextable concepts that I want to illustrate using this table, I’ll store it as the base table so that I can use it again without needing to repeat the code:\n\nbase &lt;- novels |&gt;\n  select(title:isbn) |&gt; \n  flextable(cwidth = c(3, 1, 4, 2)) |&gt; \n  set_header_labels(\n    title = \"Book Title\",\n    publication_date = \"Published\",\n    setting_date = \"Story Date\", \n    isbn = \"ISBN\"\n  ) |&gt; \n  colformat_int(big.mark = \"\")\n\nOkay, now that this is done, there’s a little more I want to say about column formatting. In my experience you can solve most formatting issues using the colformat_*() functions and a little data wrangling with dplyr, but occasionally you need to do something a little fancier. To that end, I’ll briefly mention that you can specify your own formatter function to be applied to specific columns by using the set_formatter() function.\nSuppose, for example, that I wanted the book titles to be shown in all-caps while leaving all other text columns alone. The str_to_upper() function from the stringr package will do this for me, so all I have to do is tell flextable to use it as the formatter for the title variable:\n\nbase |&gt; \n  set_formatter(title = str_to_upper)\n\nBook TitlePublishedStory DateISBNCONSIDER PHLEBAS19871331 CE1-85723-138-4THE PLAYER OF GAMES1988c. 2083 to 2087/88 CE1-85723-146-5USE OF WEAPONS19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XTHE STATE OF THE ART1991Varies (title story: 1977 CE)0-356-19669-0EXCESSION1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8INVERSIONS1998Unspecified1-85723-763-3LOOK TO WINDWARD2000c. 2167 CE1-85723-969-5MATTER2008c. 1887 or 2167 CE1-84149-417-8SURFACE DETAIL2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9THE HYDROGEN SONATA2012c. 2375 CE978-0356501505\n\n\nMore elaborate formatting functions can be constructed in the call to set_formatter(). For instance, let’s suppose that I have been instructed to modify the setting_date column in order to (a) remove the ambigious entries entirely (b) drop the “c.” for circa, (c) remove the “CE” string for current era, and (d) replace the “BCE” string with the older “BC” convention. This isn’t hard to do with the help of a few text manipulation operations, so I might define a custom formatter as follows:\n\nbase |&gt; \n  set_formatter(\n    title = str_to_upper,\n    setting_date = function(x) x |&gt; \n      str_replace_all(\"BCE\", \"BC\") |&gt; \n      str_remove_all(\"CE\") |&gt; \n      str_remove_all(\"c\\\\.\") |&gt; \n      str_remove_all(\"^(Varies|Unspe).*\") |&gt; \n      str_squish() \n  )\n\nBook TitlePublishedStory DateISBNCONSIDER PHLEBAS198713311-85723-138-4THE PLAYER OF GAMES19882083 to 2087/881-85723-146-5USE OF WEAPONS19902092 main narrative. 1892 start of secondary narrative.1-85723-135-XTHE STATE OF THE ART19910-356-19669-0EXCESSION19961867 main setting. 1827 and 633 BC flashbacks.1-85723-394-8INVERSIONS19981-85723-763-3LOOK TO WINDWARD200021671-85723-969-5MATTER20081887 or 21671-84149-417-8SURFACE DETAIL2010Sometime between 2767 and 29671-84149-893-9THE HYDROGEN SONATA20122375978-0356501505\n\n\nI’m not sure this version is actually any better, but this is of course not the point."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#table-themes",
    "href": "posts/2024-07-04_flextable/index.html#table-themes",
    "title": "Use of flextable",
    "section": "Table themes",
    "text": "Table themes\n\nBy their names you could know them, Horza thought as he showered. The Culture’s General Contact Units, which until now had borne the brunt of the first four years of the war in space, had always chosen jokey, facetious names. Even the new warships they were starting to produce, as their factory craft completed gearing up their war production, favoured either jocular, sombre or downright unpleasant names, as though the Culture could not take entirely seriously the vast conflict in which it had embroiled itself.      – Consider Phlebas\n\nThe peculiar conventions of Culture ship names are a recurring theme in the novels, and every Banks fan probably has their own personal favourite: mine is Anticipation of a New Lover’s Arrival, The. The different ship classes have different proclivities, and you can often guess the ship class just by looking at the name. It’s a whole thing.\nSince we are talking about thematic elements – see how cleverly I worked that segue in? – it is probably a good idea for me to talk about the theming system in flextable. The flextable package supports a system for visual themes, not entirely dissimilar to the way the ggplot2 data visualisation package does. Like ggplot2, the package supplies several themes that you can use right out of the box, simply by piping the flextable object to the relevant theme_*() function. If you’re not keen on writing your own, a descriptor that I suspect applies to most of us, these are the options you have available to you:\n\ntheme_alafoli()\ntheme_apa()\ntheme_booktabs() (default)\ntheme_box()\ntheme_tron()\ntheme_tron_legacy()\ntheme_vader()\ntheme_vanilla()\ntheme_zebra()\n\nThat being said, it’s worth noting that the theming system in flextable is much, much simpler than the theme system in ggplot2. A theme function in flextable is defined solely by convention: it’s just a function that takes a flextable as input and returns a modified flextable as output, so it’s not particularly onerous to write your own if you feel like it.5 In any case, just to give you a flavour of what is on offer, here’s a few of the themes that come bundled with flextable:\n\nbase |&gt; theme_box()\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\n\nbase |&gt; theme_alafoli()\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\n\nbase |&gt; theme_tron()\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nSadly, despite its obvious awesomeness, I have not yet found an excuse to use theme_tron() in my analysis work. Life is so often filled with disappointment.\n\nIn life you hoped to do what you could  but mostly you did what you were told  and that was the end of it.6      – Matter"
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#table-defaults",
    "href": "posts/2024-07-04_flextable/index.html#table-defaults",
    "title": "Use of flextable",
    "section": "Table defaults",
    "text": "Table defaults\nIn most cases that I’ve come across in real life, tables are not constructed in isolation from one another. Typically you have to produce a collection of tables for the same report, and as such all tables need to have the same visual style. At the time a table is constructed by calling flextable(), a collection of default values are applied to the table, values that shape the visual style of the resulting table. You can get a list of these values by calling get_flextable_defaults(). Conveniently, flextable allows you to modify them with set_flextable_defaults(), like so:\n\nset_flextable_defaults(\n  font.color = \"purple\",\n  background.color = \"ghostwhite\",\n  table.layout = \"autofit\"\n)\n\nWhen I set a default like this, it won’t affect any table that I’ve already defined (e.g., if I print the base table now, it won’t use the new style), but these defaults will be applied to any new table that I construct:\n\nnovels |&gt; \n  select(title:isbn) |&gt; \n  flextable()\n\ntitlepublication_datesetting_dateisbnConsider Phlebas1,9871331 CE1-85723-138-4The Player of Games1,988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1,9902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1,991Varies (title story: 1977 CE)0-356-19669-0Excession1,996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1,998Unspecified1-85723-763-3Look to Windward2,000c. 2167 CE1-85723-969-5Matter2,008c. 1887 or 2167 CE1-84149-417-8Surface Detail2,010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2,012c. 2375 CE978-0356501505\n\n\nIt’s not uncommon, then, to call set_flextable_defaults() once at the top of a script. That way you can define a style that applies to all plots that get produced in the R session. As an act of politeness, it’s probably wise to reset the defaults to their factory settings at the end of the script. You can do that as follows:\n\ninit_flextable_defaults()\n\nSeems like an especially good idea in this post, because frankly I do not want all my tables to be rendered in a lilac-and-ghost-white theme. Nice as a novelty, but it would get annoying very fast."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#table-parts",
    "href": "posts/2024-07-04_flextable/index.html#table-parts",
    "title": "Use of flextable",
    "section": "Table parts",
    "text": "Table parts\n\nThe rose-red stones were jumbled and askew. Most of the streets were gone, long ago buried under the soft encroaching sands. Ruined arches, fallen lintels, collapsed walls littered the slopes of sand; at the scalloped edge of shore, brushed by the waves, more fallen blocks broke the incoming waves. A little out to sea, tilted towers and the fragments of an arch rose from the waters, sucked at by the waves like the bones of the long-drowned.7      – The Bridge\n\nIt’s often helpful to define formatting rules that apply only to a subset of the table. For instance, we might want to use boldface text for the column headers but not the body of the table. Or perhaps some rows or columns should appear in a different colour for one reason or another. To do this effectively it is helpful to understand how flextables are structured. All flextables are comprised of three parts: a set of header rows at the top, a grid of cells in the table body, and a set of footer rows at the bottom. Many functions in flextable have a part argument that you can use to select one (or all) of these three parts. For example, the bg() function is used to set the background colour, and the example below takes our base table and gives the header a red background, the body a green background, and the footer a blue background:\n\nbase |&gt; \n  bg(bg = \"#ff000040\", part = \"header\") |&gt; # red header\n  bg(bg = \"#00cc0040\", part = \"body\") |&gt;   # green body\n  bg(bg = \"#0000ff40\", part = \"footer\") |&gt; # blue footer\n  theme_box()\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nAs you can see from this output, the default in flextable is to have a single header row containing the column labels, a body comprised of the data values in the table, and no footer at all. We can add extra content to the header and footer if we want using the add_header_lines() and add_footer_lines() functions:\n\nbase |&gt;\n  add_header_lines(\"The Culture Novels\") |&gt; \n  add_footer_lines(\"by Iain M. Banks\") |&gt; \n  bg(bg = \"#ff000040\", part = \"header\") |&gt; # red header\n  bg(bg = \"#00cc0040\", part = \"body\") |&gt;   # green body\n  bg(bg = \"#0000ff40\", part = \"footer\") |&gt; # blue footer\n  theme_box()\n\nThe Culture NovelsBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505by Iain M. Banks\n\n\nNotice that the add_header_lines() and add_footer_lines() functions create extra rows that span all the cells in the table. Sometimes we might prefer a little more fine-grained control, by creating a new row that is subdivided into cells. We can do this with the add_header_row() and add_footer_row() functions:\n\nbase |&gt;\n  add_header_row(\n    values = c(\"A\", \"Header\", \"Row\"), \n    colwidths = c(1, 2, 1)\n  ) |&gt; \n  add_footer_row(\n    values = c(\"And\", \"A\", \"Footer\", \"Row\"), \n    colwidths = c(1, 1, 1, 1)\n  ) |&gt; \n  bg(bg = \"#ff000040\", part = \"header\") |&gt; # red header\n  bg(bg = \"#00cc0040\", part = \"body\") |&gt;   # green body\n  bg(bg = \"#0000ff40\", part = \"footer\") |&gt; # blue footer\n  theme_box()\n\nAHeaderRowBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505AndAFooterRow\n\n\nNotice that these new rows are subdivided into cells. For the footer, I’ve not merged anything so there are four cells in the footer row; whereas for the new header row there are only three cells because I merged two of them using the colwidths argument.\nIf I were more serious about these particular tables, I’d use the align() function to tidy up the text alignment and other formatting in the new header and footer rows, but I’m not really interested in doing that here so I’ll move on."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#row-and-column-selectors",
    "href": "posts/2024-07-04_flextable/index.html#row-and-column-selectors",
    "title": "Use of flextable",
    "section": "Row and column selectors",
    "text": "Row and column selectors\n\nAll great promises are threats, I suppose, to the way things have been until that point, to some aspect of our lives, and we all suddenly become conservative, even though we want and need what the promise holds, and look forward to the promised change at the same time.      – The Hydrogen Sonata\n\nThe part argument discussed in the previous section allows you to apply formatting rules selectively to the header, body, and footer of the table, but it doesn’t let you apply rules selectively to a subset of rows or columns. If you need to do this, you can use the row selector argument i and the columns selector argument j. These selector arguments can be specified in three ways:\n\nNumerical values select rows or columns using their numerical index\nCharacter values can select one or more columns by name\nFormulas can be used to define a logical selection\n\nIt’s a lot easier to understand when you see it in action. In this example, I use i = 1:2 (selection by indices) to set the background colour for the top two rows to purple, and i = ~ publication_date &gt; 2000 (logical selection) to set the background colour for the bottom three rows to orange:\n\nbase |&gt; \n  bg(i = 1:2, bg = \"#cc22cc40\") |&gt;                    # purple rows\n  bg(i = ~ publication_date &gt; 2000, bg = \"#ffa50040\") # orange rows\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nAlong the same lines the next example uses j = 1:2 to set the background colour for the first two columns to brown, and j = \"isbn\" (selection by name) to set a teal background for the fourth column:\n\nbase |&gt; \n  bg(j = 1:2, bg = \"#80471c40\") |&gt;  # brown columns\n  bg(j = \"isbn\", bg = \"#00808040\")  # teal column\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nNotice that these background colours are applied only to the body of the table. However, the bg() function has a part argument, and we can extend this background shading to the entire table by settingh part = \"all\"“:\n\nbase |&gt; \n  bg(j = 1:2, bg = \"#80471c40\", part = \"all\") |&gt;  # brown columns\n  bg(j = \"isbn\", bg = \"#00808040\", part = \"all\")  # teal column\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nIf you specify a row selection and column selection, the formatting rule is applied to those cells that satisfy both criteria:\n\nbase |&gt; \n  bg(i = 1:2, j = 3:4, bg = \"#cc22cc40\") |&gt;   # purple cells\n  bg(i = 7:8, j = 1:2, bg = \"#00808040\")      # teal cells  \n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nNote that formatting rules are applied sequentially, allowing you to override previously defined formatting. For example, in this example I set the top two rows to purple, then set the rightmost column to teal, and then finally set the bottom three rows to orange. The result looks like this:\n\nbase |&gt; \n  bg(i = 1:2, bg = \"#cc22cc40\") |&gt;                    # purple rows\n  bg(j = \"isbn\", bg = \"#00808040\") |&gt;                 # teal column\n  bg(i = ~ publication_date &gt; 2000, bg = \"#ffa50040\") # orange rows\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nAs you can see from looking at this output, the formatting has been applied sequentially: the cells in the top right are shown in teal rather than purple because the teal-column formatting takes place after the purple-row formatting is applied; and the cells in the bottom-right are orange because the orange-row formatting occurs after the teal-column formatting."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#visual-style",
    "href": "posts/2024-07-04_flextable/index.html#visual-style",
    "title": "Use of flextable",
    "section": "Visual style",
    "text": "Visual style\n\nAnd in her mind saw again the line of desert hills beyond the stone balustrade of the hotel room balcony, and the faint crease of dawn-light above, suddenly swamped by the stuttering pulses of silent fire from beyond the horizon. She had watched – dazed and dazzled and wondering – as that distant eruption of annihilation had lit up the face of her lover.8          – Against a Dark Background\n\nAgainst a Dark Background is one of the most interesting Banks novels to me. Unlike the Culture novels in which the setting balances hope and despair, the world in Against a Dark Background is unrelentingly grim. Like all of his science fiction the writing is poetic, the setting is imaginative, and the story is gripping, but it is a very bleak novel. The main characters spend the entire novel being hunted for no good reason, the civilisation is perpetually on the edge of nihilistic self-destruction, and – let’s be honest – despite having been written in 1993 the book feels better suited to the much darker world of 2024. There is no redemption, everyone dies, and the deaths are entirely pointless.\nDid she have a reason for mentioning this here? No, no she did not.\nAnyway… in that spirit, let us now forsake the substance of a table and consider matters of style. Under the hood the flextable has a system that allows you to apply quite complex styling rules to the contents of a cell, but most of the time you don’t need to use it. Instead, most of the time you will find yourself defining visual style using one or more of the convenience functions that flextable supplies. For instance, to control the appearance of the text you can use these functions:\n\nfont(): Set font\nfontsize(): Set font size\nitalic(): Set italic font\nbold(): Set bold font\ncolor(): Set font colour\nhighlight(): Text highlight colour\nrotate(): Rotate cell text\n\nTo control the alignment of cell contents, you can use these:\n\nalign(): Set horizontal alignment\nalign_text_col(): Set horizontal alignment for text columns\nalign_nottext_col(): Set horizontal alignment for non-text columns\nvalign(): Set vertical alignment\n\nThe appearance of the cell body itself can be controlled with these:\n\nbg(): Set background colour\npadding(): Set paragraph paddings\nline_spacing(): Set text spacing\n\nTo give a small flavour of how these functions can be used, here’s an example where I’ve tweaked background colours, font formatting, and content alignment in various ways:\n\nbase |&gt;\n  bg(part = \"header\", bg = \"grey30\") |&gt; \n  color(part = \"header\", color = \"ghostwhite\") |&gt; \n  italic(j = \"title\") |&gt; \n  align_nottext_col(align = \"left\")\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505"
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#borders",
    "href": "posts/2024-07-04_flextable/index.html#borders",
    "title": "Use of flextable",
    "section": "Borders",
    "text": "Borders\n\nAn Outside Context Problem was the sort of thing most civilizations encountered just once, and which they tended to encounter rather in the same way a sentence encountered a full stop.      – Excession\n\nIn my own personal journey through the Culture novels, Excession suffered a little unfairly because it was the book I read immediately after Use of Weapons. With the benefit of hindsight I think it’s one of the better Culture novels, but at the time I was a little underwhelmed because I was comparing it to Use of Weapons, which remains my favourite work of the series by a long margin. When you view Excession on its own terms, however, it’s a fabulous novel and the source of one of the most iconic sentences from the whole series:\n\nThe usual example given to illustrate an Outside Context Problem was imagining you were a tribe on a largish, fertile island; you’d tamed the land, invented the wheel or writing or whatever, the neighbours were cooperative or enslaved but at any rate peaceful and you were busy raising temples to yourself with all the excess productive capacity you had, you were in a position of near-absolute power and control which your hallowed ancestors could hardly have dreamed of and the whole situation was just running along nicely like a canoe on wet grass… when suddenly this bristling lump of iron appears sailless and trailing steam in the bay and these guys carrying long funny-looking sticks come ashore and announce you’ve just been discovered, you’re all subjects of the Emperor now, he’s keen on presents called tax and these bright-eyed holy men would like a word with your priests.      – Excession\n\nOh how I wish I could write like that.\nIn any case, having mentioned the Outside Context Problem trope in terms of a kind of civilisational border incursion, it seems like a good moment to discuss table borders. The flextable package supplies a collection of functions you can use to control lines and borders that demarcate the cell and tables. You can use these functions to define horizontal and vertical lines:\n\nhline(): Set horizontal borders\nhline_bottom(): Set bottom horizontal border\nhline_top(): Set top horizontal border\nvline(): Set vertical borders\nvline_left(): Set left vertical borders\nvline_right(): Set right vertical borders\n\nBorders are defined using the following:\n\nborder_inner(): Set vertical and horizontal inner borders\nborder_inner_h(): Set horizontal inner borders\nborder_inner_v(): Set vertical inner borders\nborder_outer(): Set outer borders\nborder_remove(): Remove borders\nsurround(): Set borders for a selection of cells\n\nTo give an example, here’s a table where I’ve added a vertical line to the right of the first column using vline(), and set a thick black border around the whole table using border_outer()\n\nbase |&gt;\n  vline(j = \"title\") |&gt; \n  border_outer(\n    border = fp_border_default(\n      color = \"black\", \n      width = 2\n    )\n  )\n\nBook TitlePublishedStory DateISBNConsider Phlebas19871331 CE1-85723-138-4The Player of Games1988c. 2083 to 2087/88 CE1-85723-146-5Use of Weapons19902092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991Varies (title story: 1977 CE)0-356-19669-0Excession1996c. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998Unspecified1-85723-763-3Look to Windward2000c. 2167 CE1-85723-969-5Matter2008c. 1887 or 2167 CE1-84149-417-8Surface Detail2010Sometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012c. 2375 CE978-0356501505\n\n\nNotice that when defining the outer border I called fp_border_default() to define the style that should be applied."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#composing-cell-contents",
    "href": "posts/2024-07-04_flextable/index.html#composing-cell-contents",
    "title": "Use of flextable",
    "section": "Composing cell contents",
    "text": "Composing cell contents\n\n“In all the human societies we have ever reviewed, in every age and every state, there has seldom if ever been a shortage of eager young males prepared to kill and die to preserve the security, comfort and prejudices of their elders, and what you call heroism is just an expression of this fact; there is never a scarcity of idiots.”      – Use of Weapons\n\nThere are so many things I love about Use of Weapons. The structure of the book is amazing, with interwoven chapters alternating in two timelines, one moving forward and the other moving back, only to meet in the middle at the end.9 The interactions among the three main characters – the traumatised psychotic Culture agent Cheradenine Zakalwe, his hedonistic handler Diziet Sma, and the sadistic suitcase-shaped drone Skaffen-Amtiskaw – are amazing, and quotes like the one above are common. It is truly my favourite book in the series, and the theme of the book is brought out explicitly near the end with the following line…\n\nBut such consummate skill, such ability, such adaptability, such numbing ruthlessless, such a use of weapons when anything could become weapon…     – Use of Weapons\n\nIt is with this quote in mind – well, sort of – that I’ll now dive a little more deeply into the way cell contents are constructed in flextable. I try to avoid doing this too often in my own code because there is effort involved, but it is sometimes useful to know that flextable is very adaptable. You can compose() the contents of a cell in quite elaborate ways should you wish to do so.10 The compose() function is a general-purpose tool for constructing the contents of (a subset of) the cells in a table, and as such it has the usual arguments i, j, and part that you can use to define the selection of cells to be composed. In addition, it has a value argument that you can use to specify precisely what should be displayed in the selected cells. I’ll dive into this in more detail in a moment, but to start with let’s do something simple, and use compose() to append a “CE” suffix to the publication dates in our table:\n\nbase |&gt; \n  compose(\n    j = \"publication_date\",\n    value = as_paragraph(as.character(publication_date), \" CE\")\n  )\n\nBook TitlePublishedStory DateISBNConsider Phlebas1987 CE1331 CE1-85723-138-4The Player of Games1988 CEc. 2083 to 2087/88 CE1-85723-146-5Use of Weapons1990 CE2092 CE main narrative. 1892 CE start of secondary narrative.1-85723-135-XThe State of the Art1991 CEVaries (title story: 1977 CE)0-356-19669-0Excession1996 CEc. 1867 CE main setting. c. 1827 CE and c. 633 BCE flashbacks.1-85723-394-8Inversions1998 CEUnspecified1-85723-763-3Look to Windward2000 CEc. 2167 CE1-85723-969-5Matter2008 CEc. 1887 or 2167 CE1-84149-417-8Surface Detail2010 CESometime between 2767 CE and c. 2967 CE1-84149-893-9The Hydrogen Sonata2012 CEc. 2375 CE978-0356501505\n\n\nTo understand what’s happening here, it’s helpful to understand that in the language of flextable the contents of a cell are referred to as a paragraph. That’s not normally how we think about tables, but it makes sense when you recognise that cells can contain long passages of text and even line breaks if you want. The underlying data structure for a paragraph is essentially a data frame:\n\nas_paragraph(\"Use of Weapons\", \"by Iain M. Banks\")\n\n[[1]]\n               txt font.size italic bold underlined color shading.color\n1   Use of Weapons        NA     NA   NA         NA  &lt;NA&gt;          &lt;NA&gt;\n2 by Iain M. Banks        NA     NA   NA         NA  &lt;NA&gt;          &lt;NA&gt;\n  font.family hansi.family eastasia.family cs.family vertical.align width\n1        &lt;NA&gt;         &lt;NA&gt;            &lt;NA&gt;      &lt;NA&gt;           &lt;NA&gt;    NA\n2        &lt;NA&gt;         &lt;NA&gt;            &lt;NA&gt;      &lt;NA&gt;           &lt;NA&gt;    NA\n  height  url eq_data word_field_data img_data .chunk_index\n1     NA &lt;NA&gt;    &lt;NA&gt;            &lt;NA&gt;     NULL            1\n2     NA &lt;NA&gt;    &lt;NA&gt;            &lt;NA&gt;     NULL            2\n\nattr(,\"class\")\n[1] \"paragraph\"\n\n\nEach row in the data frame defines a specific chunk that is associated with its own set of formatting rules. If I want to specify that the first chunk should be displayed in boldface and the second should be displayed in italic, I can use the as_chunk() function to construct each chunk separately, and use fp_text_default() to specify the style for the text chunk in question:\n\nas_paragraph(\n  as_chunk(\"Use of Weapons\", props = fp_text_default(bold = TRUE)),\n  as_chunk(\"by Iain M. Banks\", props = fp_text_default(italic = TRUE))\n)\n\n[[1]]\n               txt font.size italic  bold underlined color shading.color\n1   Use of Weapons        11  FALSE  TRUE      FALSE black   transparent\n2 by Iain M. Banks        11   TRUE FALSE      FALSE black   transparent\n  font.family hansi.family eastasia.family   cs.family vertical.align width\n1 DejaVu Sans  DejaVu Sans     DejaVu Sans DejaVu Sans       baseline    NA\n2 DejaVu Sans  DejaVu Sans     DejaVu Sans DejaVu Sans       baseline    NA\n  height  url eq_data word_field_data img_data .chunk_index\n1     NA &lt;NA&gt;    &lt;NA&gt;            &lt;NA&gt;     NULL            1\n2     NA &lt;NA&gt;    &lt;NA&gt;            &lt;NA&gt;     NULL            2\n\nattr(,\"class\")\n[1] \"paragraph\"\n\n\nInspecting the contents of this data frame suggests that we’ve successfully defined the data structure we want, so now we can put it into practice. Here’s a very simple example in which I construct a single table column from two data columns, applying a different visual style to each of the two source components:\n\ndat &lt;- tibble(\n  title = c(\"Consider Phlebas\", \"The Player of Games\", \"Use of Weapons\"),\n  author = \"Iain M. Banks\"\n)\n\ndat |&gt; \n  flextable(col_keys = \"book\") |&gt; \n  set_header_labels(book = \"Book\") |&gt; \n  compose(\n    j = \"book\",\n    value = as_paragraph(\n      as_chunk(title, props = fp_text_default(bold = TRUE)),\n      \" \", \n      as_chunk(author, props = fp_text_default(italic = TRUE))\n    )\n  ) |&gt;\n  autofit()\n\nBookConsider Phlebas Iain M. BanksThe Player of Games Iain M. BanksUse of Weapons Iain M. Banks\n\n\nI have no desire to dive much deeper than this in the current blog post, but I hope it’s clear from this simple example that you can do a lot of work with a well-crafted call to compose(). It is a powerful tool, and in some circumstances a weapon."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#merging-cells",
    "href": "posts/2024-07-04_flextable/index.html#merging-cells",
    "title": "Use of flextable",
    "section": "Merging cells",
    "text": "Merging cells\n\nOh, they never lie. They dissemble, evade, prevaricate, confound, confuse, distract, obscure, subtly misrepresent and willfully misunderstand with what often appears to be a positively gleeful relish and are generally perfectly capable of contriving to give one an utterly unambiguous impression of their future course of action while in fact intending to do exactly the opposite, but they never lie. Perish the thought.      – Look to Windward\n\nA recurring theme in most of the Culture novels is that the spaceships constructed by the Culture are intelligent – indeed hyper-intelligent – machines that are often major characters in the books. Though mostly benevolent in their intentions, Culture ships are manipulative and intensely political actors, and a great deal of the action in the books stems from their scheming. There are a lot of ship characters in the books, and as such it seems appropriate at this point to pivot over to a data set I put together containing a list of ships that are referred to in the Culture novels. I’ll use that to count the number of ships from each civilisation in each of the novels:\n\nships &lt;- read_csv(\"ship-list.csv\", show_col_types = FALSE)\n\nship_count &lt;- ships |&gt; \n  mutate(Civilisation = case_when(\n    str_detect(Civilisation, \"^Culture \") ~ \"Culture\",\n    str_detect(Civilisation, \"^Non-aligned \") ~ \"Non-aligned\",\n    TRUE ~ Civilisation\n  )) |&gt; \n  group_by(Novel, Civilisation) |&gt;\n  count(name = \"Ships\") |&gt; \n  ungroup() |&gt; \n  arrange(Novel, desc(Ships))\n\nship_count\n\n# A tibble: 26 × 3\n   Novel            Civilisation          Ships\n   &lt;chr&gt;            &lt;chr&gt;                 &lt;int&gt;\n 1 Consider Phlebas Culture                  11\n 2 Consider Phlebas Non-aligned               2\n 3 Consider Phlebas Idiran                    1\n 4 Excession        Culture                  47\n 5 Excession        Affront                   7\n 6 Excession        later Sleeper Service     1\n 7 Look to Windward Culture                  40\n 8 Look to Windward Chelgrian                 3\n 9 Matter           Culture                  15\n10 Matter           Morthanveld               4\n# ℹ 16 more rows\n\n\nLet’s take a look at what happens when we pass this data frame to flextable(), using theme_box() so that I can call attention to the cell boundaries:\n\nship_count |&gt; \n  flextable() |&gt; \n  autofit() |&gt; \n  theme_box()\n\nNovelCivilisationShipsConsider PhlebasCulture11Consider PhlebasNon-aligned2Consider PhlebasIdiran1ExcessionCulture47ExcessionAffront7Excessionlater Sleeper Service1Look to WindwardCulture40Look to WindwardChelgrian3MatterCulture15MatterMorthanveld4MatterNariscene2Surface DetailCulture16Surface DetailGFCF6Surface DetailJhlupian1Surface DetailNauptre Reliquaria1The Hydrogen SonataCulture26The Hydrogen SonataLiseiden5The Hydrogen SonataGzilt3The Hydrogen SonataIwenick2The Hydrogen SonataZihdren-Remnanter2The Hydrogen SonataCulture-Zihdren-Remnanter hybrid1The Hydrogen SonataRonte1The Player of GamesCulture14The Player of GamesAzadian1The State of the ArtCulture38Use of WeaponsCulture7\n\n\nAs a first pass this table is not terrible, but the repetitive text in the leftmost column is annoying. What we would really like to do here is merge those cells together so that each book title appears only once in the table. To that end, flextable supplies a collection of functions you can use to merge cells:\n\nmerge_at(): Merge flextable cells into a single one\nmerge_h(): Merge flextable cells horizontally\nmerge_h_range(): Rowwise merge of a range of columns\nmerge_none(): Delete flextable merging information\nmerge_v(): Merge flextable cells vertically\n\nFor the current example, the one we want is merge_v(), and it’s pretty straightforward:\n\nship_count |&gt; \n  flextable() |&gt; \n  autofit() |&gt; \n  merge_v(j = \"Novel\") |&gt; \n  theme_box()\n\nNovelCivilisationShipsConsider PhlebasCulture11Non-aligned2Idiran1ExcessionCulture47Affront7later Sleeper Service1Look to WindwardCulture40Chelgrian3MatterCulture15Morthanveld4Nariscene2Surface DetailCulture16GFCF6Jhlupian1Nauptre Reliquaria1The Hydrogen SonataCulture26Liseiden5Gzilt3Iwenick2Zihdren-Remnanter2Culture-Zihdren-Remnanter hybrid1Ronte1The Player of GamesCulture14Azadian1The State of the ArtCulture38Use of WeaponsCulture7\n\n\nMuch nicer.\nAs an aside, while merging cells is one way to solve the “repeated entries” issue, there are other solutions. Another approach I’ve occasionally used is to use the as_grouped_data() function in flextable to modify the data structure before it is passed to flextable(). If we take that approach with our ship_count data set, this is what happens to the data frame:\n\nship_count |&gt; \n  as_grouped_data(groups = \"Novel\") |&gt; \n  as_tibble()\n\n# A tibble: 35 × 3\n   Novel            Civilisation          Ships\n   &lt;chr&gt;            &lt;chr&gt;                 &lt;int&gt;\n 1 Consider Phlebas &lt;NA&gt;                     NA\n 2 &lt;NA&gt;             Culture                  11\n 3 &lt;NA&gt;             Non-aligned               2\n 4 &lt;NA&gt;             Idiran                    1\n 5 Excession        &lt;NA&gt;                     NA\n 6 &lt;NA&gt;             Culture                  47\n 7 &lt;NA&gt;             Affront                   7\n 8 &lt;NA&gt;             later Sleeper Service     1\n 9 Look to Windward &lt;NA&gt;                     NA\n10 &lt;NA&gt;             Culture                  40\n# ℹ 25 more rows\n\n\nAs you can see from the output, the repeated values have been merged in the data set itself. As a general rule, when working with flextable you almost always have the choice between (a) doing the work by modifying the data frame itself or (b) modifying the table after it has been constructed. Sometimes it’se easier to modify the data set, other times it’s easier to modify the table. Anyway, here’s what happens when we construct a table from the “grouped data”:\n\nship_count |&gt; \n  as_grouped_data(groups = \"Novel\") |&gt; \n  flextable() |&gt; \n  autofit()\n\nNovelCivilisationShipsConsider PhlebasCulture11Non-aligned2Idiran1ExcessionCulture47Affront7later Sleeper Service1Look to WindwardCulture40Chelgrian3MatterCulture15Morthanveld4Nariscene2Surface DetailCulture16GFCF6Jhlupian1Nauptre Reliquaria1The Hydrogen SonataCulture26Liseiden5Gzilt3Iwenick2Zihdren-Remnanter2Culture-Zihdren-Remnanter hybrid1Ronte1The Player of GamesCulture14Azadian1The State of the ArtCulture38Use of WeaponsCulture7\n\n\nAlso a reasonable solution to the problem."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#markdown-columns",
    "href": "posts/2024-07-04_flextable/index.html#markdown-columns",
    "title": "Use of flextable",
    "section": "Markdown columns",
    "text": "Markdown columns\n\nThe point is, there is no feasible excuse for what are, for what we have made of ourselves. We have chosen to put profits before people, money before morality, dividends before decency, fanaticism before fairness, and our own trivial comforts before the unspeakable agonies of others      – Complicity\n\nI have to confess that I’ve read more of Banks’ science fiction novels than his literary fiction work, but even so I’ve probably read half a dozen of them. They’re very good, but I don’t love them as much as the space operas. One of the ones that has stuck with me through the years is Complicity, a nasty little story that doesn’t shy away from expressing an opinion or two. Banks had a great deal to say about matters political, and I can’t help but share a lot of his anger about the world we live in.\nAaaaaaanyway…\nAt this point in the post I reach the problem I encountered at work that provided the impetus to write this blog post. One irritating limitation to flextable is that it doesn’t support markdown syntax within data columns. I’ve become so accustomed to (or spoiled by) tools that automatically support markdown that I habitually write in markdown syntax, and so it annoys me that I can’t ask flextable to parse a data column as markdown. As it turns out though – as I would have discovered had I been more diligent earlier on and read the book all the way through to the end – this missing functionality is supported via the ftExtra package.\nHere’s a fairly simple example. The novels data frame has a column called url that contains links to the individual wikipedia pages for each of the Culture novels. Suppose I wanted to create a table that contains two columns, one that displays the title of the novel and links to the url of the wikipedia page, and another that contains the description of the novel. One way to do that in flextable would be to construct a new column title_md that specifies the link in markdown format, and then format that column using the colformat_md() function supplied by the ftExtra package:\n\nnovels |&gt; \n  mutate(title_md = paste0(\"[\", title, \"](\", url, \")\")) |&gt; \n  flextable(col_keys = c(\"title_md\", \"description\")) |&gt; \n  set_header_labels(\n    title_md = \"Book Title\",\n    description = \"Description\"\n  ) |&gt; \n  colformat_md(j = \"title_md\") |&gt; \n  autofit()\n\nBook TitleDescriptionConsider PhlebasAn episode in a full-scale war between the Culture and the Idirans, told mainly from the point of view of an operative of the Idiran Empire.The Player of GamesA bored member of the Culture is blackmailed into being the Culture's agent in a plan to subvert a brutal, hierarchical empire. His mission is to win an empire-wide tournament by which the ruler of the empire is selected.Use of WeaponsChapters describing the current mission of a Culture special agent born and raised on a non-Culture planet alternate with chapters that describe in reverse chronological order earlier missions and the traumatic events that made him who he is.The State of the ArtA short story collection. Two of the works are explicitly set in the Culture universe (\"The State of the Art\" and \"A Gift from the Culture\"), with a third work (\"Descendant\") possibly set in the Culture universe. In the title novella, the Mind in charge of an expedition to Earth decides not to make contact or intervene in any way, but instead to use Earth as a control group in the Culture's long-term comparison of intervention and non-interference.ExcessionAn alien artifact far advanced beyond the Culture's understanding is used by one group of Minds to lure a civilisation (the behaviour of which they disapprove) into war; another group of Minds works against the conspiracy. A sub-plot covers how two humanoids make up their differences after traumatic events that happened 40 years earlier.InversionsNot explicitly a Culture novel, but recounts what appear to be the activities of a Special Circumstances agent and a Culture emigrant on a planet whose development is roughly equivalent to that of medieval Europe. The interwoven stories are told from the viewpoint of several of the locals.Look to WindwardThe Culture has interfered in the development of a race known as the Chelgrians, with disastrous consequences. Now, in the light of a star that was destroyed 800 years previously during the Idiran War, plans for revenge are being hatched.MatterA Culture special agent who is a princess of an early-industrial society on a huge artificial planet learns that her father and brother have been killed and decides to return to her homeworld. When she returns, she finds a far deeper threat.Surface DetailA young woman seeks revenge on her murderer after being brought back to life by Culture technology. Meanwhile, a war over the digitized souls of the dead is expanding from cyberspace into the real world.The Hydrogen SonataIn the last days of the Gzilt civilisation, which is about to Sublime, a secret from far back in their history threatens to unravel their plans. Aided by a number of Culture vessels and their avatars, one of the Gzilt tries to discover if much of their history was actually a lie.\n\n\nOh, look at that – we’re pretty close to constructing the table that I showed at the start of the post."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#inserting-equations",
    "href": "posts/2024-07-04_flextable/index.html#inserting-equations",
    "title": "Use of flextable",
    "section": "Inserting equations",
    "text": "Inserting equations\n\nAllegedly there was some extra set of coordinates, or even a single mathematical operation, a transform, which, when applied to any given set of coordinates in the original list, somehow magically derived the exact position of that system’s portal.     – The Algebraist\n\nOkay fine, I lied slightly in the last section. The problem I was trying to solve at work wasn’t strictly “how to use markdown syntax in flextable” it was something more like “how to construct equations in flextable”. Fortunately the two are related, since markdown supports a lot of the formatting rules you need to construct equations. Often you can solve your problem in “pure” markdown just by a judicious use of font formats, subscripts, and superscripts. Here’s a small table with some mathematical quantities constructed in plain markdown:\n\ntbl &lt;- tribble(\n  ~expression,                  ~text,\n  \"x^2^ + y^2^ = 1\",            \"Formula for the unit circle\",\n  \"**x** = (x~1~, ..., x~n~)\" , \"A vector of length n\"\n)\ntbl\n\n# A tibble: 2 × 2\n  expression                text                       \n  &lt;chr&gt;                     &lt;chr&gt;                      \n1 x^2^ + y^2^ = 1           Formula for the unit circle\n2 **x** = (x~1~, ..., x~n~) A vector of length n       \n\n\nWhen the expression column is parsed as a markdown column we end up with a pretty respectable looking table:\n\ntbl |&gt; \n  flextable() |&gt; \n  colformat_md(j = \"expression\") |&gt; \n  autofit()\n\nexpressiontextx2 + y2 = 1Formula for the unit circlex = (x1, …, xn)A vector of length n\n\n\nHappily, however, the colformat_md() function supports the much more flextible LaTeX equation syntax. So if you have some more elaborate equations that need to appear in a table, you can construct them that way. And as an added bonus, you can mix these equations with other markdown features like footnotes. As before we’ll construct a simple table:\n\ntbl &lt;- tribble(\n  ~expression,                       ~text,\n  \"$x^2 + y^2 = 1$\",                 \"Formula for the unit circle\",\n  \"$\\\\bm{x} = (x_1, \\\\ldots, x_n)$\", \"A vector of length $n$\",\n  \"$\\\\mathcal{R}^n$\",                 \"$n$-dimensional real space\",\n  \"$e^{i\\\\pi} + 1 = 0$\",             \"Euler's identity^[A special case of Euler's formula]\"\n)\ntbl\n\n# A tibble: 4 × 2\n  expression                        text                                    \n  &lt;chr&gt;                             &lt;chr&gt;                                   \n1 \"$x^2 + y^2 = 1$\"                 Formula for the unit circle             \n2 \"$\\\\bm{x} = (x_1, \\\\ldots, x_n)$\" A vector of length $n$                  \n3 \"$\\\\mathcal{R}^n$\"                $n$-dimensional real space              \n4 \"$e^{i\\\\pi} + 1 = 0$\"             Euler's identity^[A special case of Eul…\n\n\nAnd now we can pass this table to flextable() and have it parse both columns as markdown:\n\ntbl |&gt; \n  flextable() |&gt; \n  colformat_md() |&gt; \n  autofit()\n\nexpressiontextx2 + y2 = 1Formula for the unit circlex = (x1,…,xn)A vector of length nℛnn-dimensional real spaceeiπ + 1 = 0Euler’s identity11A special case of Euler’s formula\n\n\nSo nice."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#inserting-images",
    "href": "posts/2024-07-04_flextable/index.html#inserting-images",
    "title": "Use of flextable",
    "section": "Inserting images",
    "text": "Inserting images\n\nThat was how divorced from the human scale modern warfare had become. You could smash and destroy from unthinkable distances, obliterate planets from beyond their own system and provoke stars into novae from light-years off … and still have no good idea why you were really fighting.      – Consider Phlebas\n\nHaving discussed markdown columns already, we are now almost at the point where we can construct the actual table that I showed at the start of the post. The only missing component here is the colformat_image() function, which you can use to parse a column like image (in the novels data frame) which specifies the path to image files that I’d like to display in the table. Using the now-familiar flextable syntax, all we have to do is pipe the table to colformat_image(), tell it to interpret the image column as an image, and give it the width and height dimensions (in inches) for the images:\n\nnovels |&gt; \n  mutate(title_md = paste0(\"[\", title, \"](\", url, \")\")) |&gt; \n  flextable(col_keys = c(\"title_md\", \"image\", \"description\")) |&gt; \n  set_header_labels(\n    title_md = \"Book Title\",\n    description = \"Description\",\n    image = \"Cover\"\n  ) |&gt; \n  colformat_md(j = \"title_md\") |&gt; \n  colformat_image(j = \"image\", width = .75, height = 1.2) |&gt; \n  autofit()\n\nBook TitleCoverDescriptionConsider PhlebasAn episode in a full-scale war between the Culture and the Idirans, told mainly from the point of view of an operative of the Idiran Empire.The Player of GamesA bored member of the Culture is blackmailed into being the Culture's agent in a plan to subvert a brutal, hierarchical empire. His mission is to win an empire-wide tournament by which the ruler of the empire is selected.Use of WeaponsChapters describing the current mission of a Culture special agent born and raised on a non-Culture planet alternate with chapters that describe in reverse chronological order earlier missions and the traumatic events that made him who he is.The State of the ArtA short story collection. Two of the works are explicitly set in the Culture universe (\"The State of the Art\" and \"A Gift from the Culture\"), with a third work (\"Descendant\") possibly set in the Culture universe. In the title novella, the Mind in charge of an expedition to Earth decides not to make contact or intervene in any way, but instead to use Earth as a control group in the Culture's long-term comparison of intervention and non-interference.ExcessionAn alien artifact far advanced beyond the Culture's understanding is used by one group of Minds to lure a civilisation (the behaviour of which they disapprove) into war; another group of Minds works against the conspiracy. A sub-plot covers how two humanoids make up their differences after traumatic events that happened 40 years earlier.InversionsNot explicitly a Culture novel, but recounts what appear to be the activities of a Special Circumstances agent and a Culture emigrant on a planet whose development is roughly equivalent to that of medieval Europe. The interwoven stories are told from the viewpoint of several of the locals.Look to WindwardThe Culture has interfered in the development of a race known as the Chelgrians, with disastrous consequences. Now, in the light of a star that was destroyed 800 years previously during the Idiran War, plans for revenge are being hatched.MatterA Culture special agent who is a princess of an early-industrial society on a huge artificial planet learns that her father and brother have been killed and decides to return to her homeworld. When she returns, she finds a far deeper threat.Surface DetailA young woman seeks revenge on her murderer after being brought back to life by Culture technology. Meanwhile, a war over the digitized souls of the dead is expanding from cyberspace into the real world.The Hydrogen SonataIn the last days of the Gzilt civilisation, which is about to Sublime, a secret from far back in their history threatens to unravel their plans. Aided by a number of Culture vessels and their avatars, one of the Gzilt tries to discover if much of their history was actually a lie.\n\n\nEasy as an easy thing, once you know the trick :)"
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#inserting-plots",
    "href": "posts/2024-07-04_flextable/index.html#inserting-plots",
    "title": "Use of flextable",
    "section": "Inserting plots",
    "text": "Inserting plots\n\nPeople were always sorry. Sorry they had done what they had done, sorry they were doing what they were doing, sorry they were going to do what they were going to do; but they still did whatever it is. The sorrow never stopped them; it just made them feel better. And so the sorrow never stopped.      – Against a Dark Background\n\nThere’s one more topic I want to talk about before wrapping up. Sorry.\nOne feature of flextable that I really love but have never yet had call to use in real life is that it permits you to embed plots within table cells. You can do this in several different ways, but the one I want to mention is embedding ggplot2 graphics. You can do this with the help of the gg_chunk() function that allows you to pass ggplot2 objects to flextable.\nTo illustrate this functionality, I’ll define a silly little plotting function called plot_name_lengths(). This function takes the name of one of the Culture novels as its argument, looks up the names of all the ships referred to in that novel, and then plots a tiny graph listing the number of characters in each ship name (which is, of course, entertaining to Banks readers because the names of Culture ships are famously long and absurd). Then I’ll construct a data frame called ship_name_length that contains the names of the novels as one column, and includes a list of ggplot2 objects as the second column:\n\nplot_name_lengths &lt;- function(novel) {\n  ships |&gt; \n    filter(Novel == novel) |&gt; \n    mutate(len = Name |&gt; replace_na(\"\") |&gt; nchar()) |&gt; \n    arrange(desc(len)) |&gt; \n    mutate(pos = row_number()) |&gt; \n    ggplot(aes(pos, len)) + \n    geom_point() + \n    geom_segment(aes(xend = pos, yend = 0)) + \n    lims(x = c(0, 60), y = c(0, 65)) + \n    theme_void()\n}\n\nship_name_length &lt;- tibble(\n  title = novels$title,\n  plot = lapply(title, plot_name_lengths)\n)\n\nship_name_length\n\n# A tibble: 10 × 2\n   title                plot  \n   &lt;chr&gt;                &lt;list&gt;\n 1 Consider Phlebas     &lt;gg&gt;  \n 2 The Player of Games  &lt;gg&gt;  \n 3 Use of Weapons       &lt;gg&gt;  \n 4 The State of the Art &lt;gg&gt;  \n 5 Excession            &lt;gg&gt;  \n 6 Inversions           &lt;gg&gt;  \n 7 Look to Windward     &lt;gg&gt;  \n 8 Matter               &lt;gg&gt;  \n 9 Surface Detail       &lt;gg&gt;  \n10 The Hydrogen Sonata  &lt;gg&gt;  \n\n\nWhen constructing our table, we now use the compose() function to construct the column of plots, using gg_chunk() to control how the plot is rendered within the table. The result looks like this:\n\nship_name_length |&gt; \n  flextable(cwidth = c(2, 4)) |&gt; \n  set_header_labels(\n    title = \"Title\", \n    plot = \"Ship Name Lengths\"\n  ) |&gt; \n  compose(\n    j = \"plot\", \n    value = as_paragraph(\n      gg_chunk(value = plot, width = 6, height = .8)\n    )\n  ) |&gt; \n  align_nottext_col(align = \"center\")\n\nTitleShip Name LengthsConsider PhlebasThe Player of GamesUse of WeaponsThe State of the ArtExcessionInversionsLook to WindwardMatterSurface DetailThe Hydrogen Sonata\n\n\nLooking at these distributions, I feel quite certain that this data set has used the abbreviated name of a certain ship in The Hydrogen Sonata, because – at 206 characters – “Mistake Not My Current State Of Joshing Gentle Peevishness For The Awesome And Terrible Majesty Of The Towering Seas Of Ire That Are Themselves The Mere Milquetoast Shallows Fringing My Vast Oceans Of Wrath” would probably show up as an outlier."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#epilogue",
    "href": "posts/2024-07-04_flextable/index.html#epilogue",
    "title": "Use of flextable",
    "section": "Epilogue",
    "text": "Epilogue\n\nJust as one might do useful work without fully understanding the job one was engaged in, or even what the point of it was, so the behaviour of devotion still mattered to the all-forgiving God, and just as the habitual performance of a task gradually raised one’s skills to something close to perfection, bringing a deeper understanding of the work, so the actions of faith would lead to the state of faith.      – Surface Detail\n\nSo often I reach the end of these posts and wonder why I write them. The girl reading science fiction on the library floor is long gone now, but so much of her confusion about the world remains. Who is the target audience for these posts? What do I hope they get from reading them? What did I get from the process? It’s honestly a mystery to me.\nTake this post, for example. I’ve written about flextable but I’m hardly an expert in it. I’ve only just started the process of developing a deeper understanding of how it works, and why it is the way it is. It feels sometimes like I am indeed performing the rituals of the code in the hope it leads me to a deeper state of faith in the gods of machine instruction.11 I have learned a great deal more about flextable through the act of writing, and of course I do love the act of writing.\nIn truth I write because I like to write, and perhaps that is justification enough."
  },
  {
    "objectID": "posts/2024-07-04_flextable/index.html#footnotes",
    "href": "posts/2024-07-04_flextable/index.html#footnotes",
    "title": "Use of flextable",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt was a markdown column issue, which I now realise is easy to address.↩︎\nI’m mildly amused that the previous post was also about making tables. It is mostly coincidental, though I suppose both were motivated by problems I’ve encountered in a work context.↩︎\nYes, there are a disproportionate number of quotes that she has selected based purely on their thirst potential. She is not admitting to anything but she may be struggling slightly with her recent decision to delete grindr.↩︎\nWith the benefit of hindsight I should probably have provided the reader with this warning a little earlier in the post. Oh well. But let’s be honest, babes, you’re reading my blog. You should be treating everything on here like one of the Planets of the Dead already.↩︎\nShe says this slightly tongue-in-cheek, in full recognition of the fact that she has never bothered to actually do this herself.↩︎\nThe quote is not formatted like this in the original, but Banks’ writing often has a poetic feel to it, and somehow this line works so well when written like this. Besides, if I hadn’t done so the quote would have left a widowed word on the second line and I just hate that so much.↩︎\nThis quote doesn’t really connect to the rest of the post, and The Bridge isn’t a Culture novel, but it’s an amazing work. Easily my favourite of Banks’ literary fiction works.↩︎\nI think about this quote during sex a lot. More often than is healthy, quite frankly, and I am not prepared to look too closely at this habit to figure out what it says about me except that it’s probably a good thing that I’m not on grindr right now.↩︎\nI once tried to write a long personal essay in the same structure, where one thread discussing the gganimate R package and moving forward in time, and the other discussing my transition moving backwards in time. It was a pale imitation of the original.↩︎\nAs an aside I should mention that because flextable::compose() creates a namespace collision with purrr::compose(), flextable also supplies mk_par() as an alias for compose() in case you happen to have both packages loaded.↩︎\nIt’s been a while since I read Surface Detail, but my recollection is that the quote above takes place in hell, which admittedly casts a dark shadow across the thought.↩︎"
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html",
    "href": "posts/2023-08-08_being-assertive/index.html",
    "title": "Four ways to write assertion checks in R",
    "section": "",
    "text": "Once upon a time in a land far, far away, I was a bright young thing who wrote my data analyses with the kind of self-assured confidence that only a bright young thing can have. I trusted myself to write analysis code that does exactly what I wanted it to do. After all, I was a smart lady who knows her data and knows her analysis tools. In those halycon days of yore, before I’d been badly burned by sequentially arriving data that don’t have precisely the same structure every single time the data updates, I had the naivete to believe that if something changed in unexpected ways, I’d notice it.\nSweet summer child.\nWhat I have learned since then, following the well-trodden path of every embittered old data analyst whose heart has shrivelled into a dark ball of data cynicism, is that none of this is true:\nWorst of all: when my assumptions fail, my code can silently do the wrong thing and never throw an error. This happens very, very easily when data structure can change over time, or when code is reused in a new context. Which… happens a lot, actually.\nReal world data are horrible.\nLearning my lessons the hard way has taught me the importance of writing assertion checks. The idea behind an assertion check is very simple: write some code that makes sure that your code fails loudly by throwing an error as soon as an assumption is violated.1 As the saying goes, you want your analysis code to fail fast and fail loudly every time that something is not “as expected”.\nSo. Let’s talk about four different approaches to writing assertions in R.2"
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html#just-stopifnot-scott",
    "href": "posts/2023-08-08_being-assertive/index.html#just-stopifnot-scott",
    "title": "Four ways to write assertion checks in R",
    "section": "Just stopifnot(), Scott",
    "text": "Just stopifnot(), Scott\nHere’s a simplified version of a function that I use a lot in my generative art workflows. The identifier() function constructs a unique identifier for an output generated from a particular system:\n\nidentifier &lt;- function(name, version, seed) {\n  version &lt;- stringr::str_pad(version, width = 2, pad = \"0\")\n  seed &lt;- stringr::str_pad(seed, width = 4, pad = \"0\")\n  paste(name, version, seed, sep = \"_\") \n}\n\nSo let’s say I’m creating a piece from a version 1 system called “rtistry”, and using 203 as my random seed. The unique identifier for this piece would be as follows:\n\nidentifier(name = \"rtistry\", version = 1, seed = 203)\n\n[1] \"rtistry_01_0203\"\n\n\nThe idea here is that:\n\nThe identifier should consist of exactly three parts, separated by underscores\nThe first part should be the name of the generative art system\nThe second part should specify the version of the system as a two-digit number\nThe third part should specify the RNG seed used to generate this piece as a four-digit number\n\nFor most of my systems this will produce a globally unique identifier, since I try to design them so that the only input parameter to the system is the RNG seed.\nNotice, though, that there are some unstated – and unchecked! – assumptions about the kind of input that the function will receive. It’s implicitly assumed that name will be a character string that does not have any underscores, periods, or white spaces, and it’s also assumed that version and seed are both positive valued integers (or at least “integerish”) with upper bounds of 99 and 9999 respectively. Weirdness happens when I break those assumptions with my input:\n\nidentifier(name = \"r tistry\", version = 1.02, seed = 203)\n\n[1] \"r tistry_1.02_0203\"\n\n\nAs a rule, of course, I don’t deliberately pass bad inputs to my functions, but if I want to be defensive about it, I should validate the inputs so that identifier() throws an error if I make a mistake and pass it input that violates the assumptions. The base R function stopifnot() is designed to solve exactly this problem:\n\nidentifier &lt;- function(name, version, seed) {\n  \n  # throw error if any of the following assertions fail\n  stopifnot(\n    length(name) == 1,    # name must be a scalar\n    length(version) == 1, # version must be a scalar\n    length(seed) == 1,    # seed must be a scalar\n    rlang::is_integerish(version),  # version must be a whole number\n    rlang::is_integerish(seed),     # seed must be a whole number\n    !stringr::str_detect(name, \"[[:space:]._]\"), # name can't have spaces, periods, or underscores \n    seed &gt; 0,      # seed must be positive\n    seed &lt; 10000,  # seed must be less than 10000\n    version &gt; 0,   # version must be positive\n    version &lt; 100  # version must be less than 100\n  )\n  \n  # the actual work of the function\n  version &lt;- stringr::str_pad(version, width = 2, pad = \"0\")\n  seed &lt;- stringr::str_pad(seed, width = 4, pad = \"0\")\n  paste(name, version, seed, sep = \"_\") \n}\n\nUsing stopifnot() in this way causes all of the following to error and throw informative error messages:\n\nidentifier(\"r tistry\", 1, 203)\n\nError in identifier(\"r tistry\", 1, 203): !stringr::str_detect(name, \"[[:space:]._]\") is not TRUE\n\nidentifier(\"rtistry\", 1.02, 203)\n\nError in identifier(\"rtistry\", 1.02, 203): rlang::is_integerish(version) is not TRUE\n\nidentifier(\"rtistry\", 1, 20013)\n\nError in identifier(\"rtistry\", 1, 20013): seed &lt; 10000 is not TRUE\n\n\nThe error messages aren’t the prettiest, but they do the job. In each case you can look at the error message and figure out what went wrong when calling the identifier() function. That said, you can sort of see the limitations to stopifnot() by looking at my source code: because stopifnot() throws pretty generic error messages that you can’t customise, my first instinct when writing the function was to group all my assertions into a single stopifnot() call, and then – because there isn’t a lot of structure to my assertion code – I’ve added comments explaining what each assertion does. That’s… fine. But not ideal.\nAs it turns out, there are ways to provide more informative error messages with stopifnot(). You can write a stopifnot() assertion as a name-value pair:\n\nstopifnot(\"`version` must be scalar\" = length(version) == 1)\n\nIf this assertion is violated, the error message thrown by the stopifnot() function corresponds to the name of the assertion, as illustrated below:\n\nversion &lt;- 1:3 \nstopifnot(\"`version` must be scalar\" = length(version) == 1)\n\nError: `version` must be scalar\n\n\nIt’s kind of clunky but it works.\nActually, I have a confession to make. I actually didn’t know this trick until I’d already posted the original version of this post to the internet, so I have Jim Gardner to thank for kindly called my attention to it.\nSummary: stopifnot() is suprisingly effective. It’s very general, and works for any expression that yields TRUE or FALSE. There are no dependencies since it’s a base R function. It does have some downsides: dealing with error messages is a bit clunky, and the code isn’t always the prettiest, but nevertheless it does the job that needs doing."
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html#just-assert_that-kat",
    "href": "posts/2023-08-08_being-assertive/index.html#just-assert_that-kat",
    "title": "Four ways to write assertion checks in R",
    "section": "Just assert_that(), Kat",
    "text": "Just assert_that(), Kat\nThe assertthat package is designed to provide a drop-in replacement for the stopifnot() function, one that allows you to compose your own error messages when an assertion fails. It does have a variety of other convenience functions, but to be honest the main advantage over stopifnot() is the superior control over the error message. In practice, I find that this functionality allows me to write assertion code that is (a) easier to read, and (b) produces better error messages when an assertion fails.\nTo illustrate, here’s the code I end up with when I revisit my generative art identifier() function using assertthat:\n\nlibrary(assertthat)\n\nidentifier &lt;- function(name, version, seed) {\n  \n  assert_that(\n    length(name) == 1,\n    length(version) == 1,\n    length(seed) == 1,\n    msg = \"`name`, `version`, and `seed` must all have length 1\"\n  )\n\n  assert_that(   \n    !stringr::str_detect(name, \"[[:space:]._]\"),\n    msg = \"`name` must not contain white space, periods, or underscores\"\n  )\n\n  assert_that(\n    rlang::is_integerish(version),\n    version &gt; 0,\n    version &lt; 100,\n    msg = \"`version` must be a whole number between 1 and 99\"\n  )\n   \n  assert_that(\n    rlang::is_integerish(seed),\n    seed &gt; 0, \n    seed &lt; 10000,\n    msg = \"`seed` must be a whole number between 1 and 9999\"    \n  )\n  \n  # the actual work of the function\n  version &lt;- stringr::str_pad(version, width = 2, pad = \"0\")\n  seed &lt;- stringr::str_pad(seed, width = 4, pad = \"0\")\n  paste(name, version, seed, sep = \"_\") \n}\n\nLike stopifnot(), the assert_that() function allows you to construct arbitrary assertions, which I find useful. Additionally, the assert_that() function has some nice properties when compared to stopifnot(). Because it takes a msg argument that allows you to specify the error message, it gently encourages you to group together all the assertions that are of the same kind, and then write an informative message tailored to that subset of the assertion checks. This produces readable code because the error message is right there next to the assertions themselves, and the assertions end up being more organised than when I used stopifnot() earlier.\nIn any case, let’s have a look. First, let’s check that this works:\n\nidentifier(\"rtistry\", 1, 203)\n\n[1] \"rtistry_01_0203\"\n\n\nSecond, let’s check that all of these fail and throw readable error messages:\n\nidentifier(\"r tistry\", 1, 203)\n\nError: `name` must not contain white space, periods, or underscores\n\nidentifier(\"rtistry\", 1.02, 203)\n\nError: `version` must be a whole number between 1 and 99\n\nidentifier(\"rtistry\", 1, 20013)\n\nError: `seed` must be a whole number between 1 and 9999\n\n\nI find myself preferring this as a way of generating error messages when input arguments to a function don’t receive appropriate input. Because I know what I want the function to do, I’m able to write concise but informative error messages that are appropriate to the specific set of assertions that I’ve included within any particular assert_that() call.\nSummary: The assertthat package has a pretty specific aim: to provide an assert_that() function works as a drop-in replacement for stopifnot() that allows custom error messages. Given that limited goal, it works nicely."
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html#just-assert_-it-kit",
    "href": "posts/2023-08-08_being-assertive/index.html#just-assert_-it-kit",
    "title": "Four ways to write assertion checks in R",
    "section": "Just assert_*() it, Kit",
    "text": "Just assert_*() it, Kit\nThe assertive package provides a large collection of assert_*() functions that are each tailored to a specific type of assertion, and designed to produce error messages that are tailored to that specific case. Here’s an example where I apply this approach to checking the inputs to the identifier() function:\n\nlibrary(assertive)\n\nidentifier &lt;- function(name, version, seed) {\n\n  assert_is_scalar(version)\n  assert_is_scalar(name)\n  assert_is_scalar(seed)\n  \n  assert_is_integer(version)\n  assert_is_integer(seed)\n  assert_all_are_positive(c(seed, version))\n  assert_all_are_less_than(seed, 10000)\n  assert_all_are_less_than(version, 100)\n  \n  assert_all_are_not_matching_regex(name, \"[[:space:]._]\")\n\n  # the actual work of the function\n  version &lt;- stringr::str_pad(version, width = 2, pad = \"0\")\n  seed &lt;- stringr::str_pad(seed, width = 4, pad = \"0\")\n  paste(name, version, seed, sep = \"_\") \n}\n\nI’d probably argue that this is the most readable version of the code yet. The assert_*() functions have such transparently informative names that there’s no need at all for comments. However, there are some downsides to this approach, which become a little more apparent when we look at the error messages that it throws when I pass bad inputs to the identifier() function:\n\nidentifier(\"r tistry\", 1L, 203L)\n\nError in identifier(\"r tistry\", 1L, 203L): is_not_matching_regex : name does not match \"[[:space:]._]\"\nThere was 1 failure:\n  Position    Value                   Cause\n1        1 r tistry matches '[[:space:]._]'\n\nidentifier(\"rtistry\", 1.02, 203L)\n\nError in identifier(\"rtistry\", 1.02, 203L): is_integer : version is not of class 'integer'; it has class 'numeric'.\n\nidentifier(\"rtistry\", 1L, 20013L)\n\nError in identifier(\"rtistry\", 1L, 20013L): is_less_than : seed are not all less than 10000.\nThere was 1 failure:\n  Position Value                          Cause\n1        1 20013 greater than or equal to 10000\n\n\nBecause I don’t have custom error message code in my assertions, the errors that get returned to the user are a little bit opaque. They’re more informative than the stopifnot() versions, and because each assertion throws its own error message tailored to that function, the results are rather better suited to the context. Even so, they’re still quite long and there’s some cognitive effort required by the user to figure out what happened.\nThere’s a second issue here. Notice that when I wanted to pass a good input for seed or version in this version of the function, I used explicitly integer-classed values (e.g., 203L not 203). There’s a reason I did that. The assert_is_integer() function uses is.integer() test for integer status, which returns TRUE only when passed an actual integer. It returns FALSE when passed an “integerish” double:\n\nis.integer(203L)\n\n[1] TRUE\n\nis.integer(203)\n\n[1] FALSE\n\n\nBecause my assertion is a check for integer status not “integerish” status, this version of the identifier() function is more strict about type checking than I really want it to be, and this fails:\n\nidentifier(\"rtistry\", 1, 203)\n\nError in identifier(\"rtistry\", 1, 203): is_integer : version is not of class 'integer'; it has class 'numeric'.\n\n\nNow, to be fair, there are of course many situations where you really do want to be strict about type checking integers: the integer representation of 203L is a different underlying object to the floating point representation of 203, and while R is usually pretty chill about this, it’s important to keep in mind that doubles and integers are fundamentally different data types. That being said, it’s vanishingly rare for this to actually matter in my generative art process, and I’d prefer to let this one slide.\nThis kind of thing is where you can run into some difficulties using the assert_*() functions. If there isn’t a specific assertion function tailored for your use case (as occurs with “integerish” check in identifier()) you’re left with the dilemma of either choosing an assertion that isn’t quite right, or else falling back on a general-purpose assertion like assert_all_are_true(). For example, this works…\n\nlibrary(assertive)\n\nidentifier &lt;- function(name, version, seed) {\n\n  assert_is_scalar(version)\n  assert_is_scalar(name)\n  assert_is_scalar(seed)\n  \n  assert_all_are_true(rlang::is_integerish(c(seed, version)))\n  assert_all_are_positive(c(seed, version))\n  assert_all_are_less_than(seed, 10000)\n  assert_all_are_less_than(version, 100)\n  \n  assert_all_are_not_matching_regex(name, \"[[:space:]._]\")\n\n  # the actual work of the function\n  version &lt;- stringr::str_pad(version, width = 2, pad = \"0\")\n  seed &lt;- stringr::str_pad(seed, width = 4, pad = \"0\")\n  paste(name, version, seed, sep = \"_\") \n}\n\nidentifier(\"rtistry\", 1, 203)\n\n[1] \"rtistry_01_0203\"\n\n\n…but it’s not quite as elegant as you might hope. Nevertheless, I’m not being critical here. It’s impossible to write a package like assertive in a way that covers every use case, and it’s pretty impressive that it has the breadth that it does.\nSummary: Because it provides a huge number of well-named assertion functions, the assertive package tends to produce very readable code, and because each of those functions produces errors that are tailored to that check, the error messages tend to be useful too. It does get a little awkward when there isn’t an assertion for your use case, but usually there’s a way to work around that."
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html#just-assertr-carr",
    "href": "posts/2023-08-08_being-assertive/index.html#just-assertr-carr",
    "title": "Four ways to write assertion checks in R",
    "section": "Just assertr, Carr",
    "text": "Just assertr, Carr\nThe assertr package solves a different problem to the other three methods discussed here. The other three approaches are general-purpose tools and – with various strengths and weaknesses – they’re designed to be used when checking an arbitrary input. The assertr package is more specialised: it focuses on checking a data input, specifically a tabular data object like a data frame or a tibble. Because it’s focused on that particular – and extremely important – special case, it’s able to provide a more powerful way of validating the content of a data frame.\nIn that sense, assertr is complementary to the other three approaches. For example, you could use assertr to check the data input to a function that takes a data frame as the primary argument, but then use (say) assert_that() to test the others.\nTo get started, I’ll load the packages I’m going to use in this section:\n\nlibrary(dplyr)\nlibrary(readr)\nlibrary(assertr)\n\nThe assertr package provides three primary verbs, verify(), assert(), and insist(). They all take a data set as the first argument and (by default) returns the original data set unaltered if the checks pass, which makes it include them as part of a data pipeline. There’s also two row-wise variants assert_rows() and insist_rows(). For the purposes of this post I’ll limit myself to talking about the simplest cases, verify() and assert().\nLet’s start with verify(). The verify() function expects to receive an expression as the first non-data argument amd yields a logical value, which is then evaluated in the data context. If the expression evaluates to FALSE, an error is thrown.\nHere’s a simple example using verify(). My data set comes from the List of Archibald Prize Winners wikipedia page. The Archibald Prize is a one of the most prestigious art prizes in Australia, awarded for painted portraits, and has been awarded (almost!) annually since 1921. My data set looks like this:\n\narchibald &lt;- read_csv(\"archibald.csv\", show_col_types = FALSE)\narchibald\n\n# A tibble: 166 × 6\n   prize           year  artist            title         subject n_finalists\n   &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;             &lt;chr&gt;         &lt;chr&gt;         &lt;dbl&gt;\n 1 Archibald Prize 1921  William McInnes   Desbrowe Ann… Harold…          45\n 2 Archibald Prize 1922  William McInnes   Professor Ha… Willia…          53\n 3 Archibald Prize 1923  William McInnes   Portrait of … Violet…          50\n 4 Archibald Prize 1924  William McInnes   Miss Collins  Gladys…          40\n 5 Archibald Prize 1925  John Longstaff    Maurice Mosc… Mauric…          74\n 6 Archibald Prize 1926  William McInnes   Silk and Lac… Esther…          58\n 7 Archibald Prize 1927  George W. Lambert Mrs Annie Mu… Annie …          56\n 8 Archibald Prize 1928  John Longstaff    Dr Alexander… Alexan…          66\n 9 Archibald Prize 1929  John Longstaff    The Hon W A … Willia…          75\n10 Archibald Prize 1930  William McInnes   Drum-Major H… Harry …          67\n# ℹ 156 more rows\n\n\nTo be precise, there are actually three different prizes included in the data set. There’s the original Archibald Prize (the famous one), and two more recent additions that are awarded using the same pool of entrants: the People’s Choice Award (which is what you’d think), and the Packing Room Prize (awarded by the staff who install the portraits in the gallery).\nFor my first analysis then, I want to do a simple tabulation: count the number of times any given artist has won a particular prize, and sort the results in descending count order. So the analysis part of my data pipeline would look like this:\n\narchibald |&gt; \n  count(artist, prize) |&gt;\n  arrange(desc(n))\n\nHowever, I might want to verify() a few things first. I’d like to check that prize and artist both exist as columns in the data, and both contain character data. I can use the base R function exists() to check that the variables exist within the data context, and is.character() to check the variable type:\n\narchibald |&gt; \n  verify(exists(\"prize\")) |&gt;\n  verify(exists(\"artist\")) |&gt;\n  verify(is.character(\"prize\")) |&gt;\n  verify(is.character(\"artist\")) |&gt;\n  count(artist, prize) |&gt;\n  arrange(desc(n))\n\n# A tibble: 118 × 3\n   artist            prize                     n\n   &lt;chr&gt;             &lt;chr&gt;                 &lt;int&gt;\n 1 William Dargie    Archibald Prize           8\n 2 William McInnes   Archibald Prize           7\n 3 Ivor Hele         Archibald Prize           5\n 4 John Longstaff    Archibald Prize           5\n 5 Vincent Fantauzzo People's Choice Award     4\n 6 Clifton Pugh      Archibald Prize           3\n 7 Eric Smith        Archibald Prize           3\n 8 Robert Hannaford  People's Choice Award     3\n 9 William Dobell    Archibald Prize           3\n10 William Pidgeon   Archibald Prize           3\n# ℹ 108 more rows\n\n\nIn this case, all the verify() checks pass, so no errors are thrown and the analysis proceeds in the usual way. But suppose that the artist variable was actually supposed to be called painter:\n\narchibald |&gt; \n  verify(exists(\"prize\")) |&gt;\n  verify(exists(\"painter\")) |&gt;\n  verify(is.character(\"prize\")) |&gt;\n  verify(is.character(\"painter\")) |&gt;\n  count(painter, prize) |&gt;\n  arrange(desc(n))\n\nverification [exists(\"painter\")] failed! (1 failure)\n\n    verb redux_fn         predicate column index value\n1 verify       NA exists(\"painter\")     NA     1    NA\n\n\nError: assertr stopped execution\n\n\nThere is no painter variable in the data set, so the assertion checks fail, and an error message is thrown. The form of the error message is rather elaborate though. There is a reason why assertr defaults to this strange-looking format: often there are multiple errors that appear in an assertion check, and by default assertr will group them into a table summarising all the issues.\nThere’s something a little repetitive about the validation code I wrote above. If my analysis pipeline involved many variables, it would be a bit obnoxious to write a separate verify() line to check that they all exist. For the column name checks, assertr provides a convenience function has_all_names() that you can use specifically for this purpose:3\n\narchibald |&gt; \n  verify(has_all_names(\"prize\", \"artist\")) |&gt;\n  verify(is.character(\"prize\")) |&gt;\n  verify(is.character(\"artist\")) |&gt;\n  count(artist, prize) |&gt;\n  arrange(desc(n))\n\n# A tibble: 118 × 3\n   artist            prize                     n\n   &lt;chr&gt;             &lt;chr&gt;                 &lt;int&gt;\n 1 William Dargie    Archibald Prize           8\n 2 William McInnes   Archibald Prize           7\n 3 Ivor Hele         Archibald Prize           5\n 4 John Longstaff    Archibald Prize           5\n 5 Vincent Fantauzzo People's Choice Award     4\n 6 Clifton Pugh      Archibald Prize           3\n 7 Eric Smith        Archibald Prize           3\n 8 Robert Hannaford  People's Choice Award     3\n 9 William Dobell    Archibald Prize           3\n10 William Pidgeon   Archibald Prize           3\n# ℹ 108 more rows\n\n\nFor the type checking, however, there’s no equivalent convenience function and if you want to group multiple verify() checks what you want to do is use the assert() function. The first non-data argument to assert() specifies a predicate function that is applied to a set of columns.4 If the predicate function returns FALSE, the assert() function errors.\nRewriting the verify() code from our “successful” example as assert() checks gives us this:\n\narchibald |&gt; \n  verify(has_all_names(\"prize\", \"artist\")) |&gt;\n  assert(is.character, prize, artist) |&gt;\n  count(artist, prize) |&gt;\n  arrange(desc(n))\n\n# A tibble: 118 × 3\n   artist            prize                     n\n   &lt;chr&gt;             &lt;chr&gt;                 &lt;int&gt;\n 1 William Dargie    Archibald Prize           8\n 2 William McInnes   Archibald Prize           7\n 3 Ivor Hele         Archibald Prize           5\n 4 John Longstaff    Archibald Prize           5\n 5 Vincent Fantauzzo People's Choice Award     4\n 6 Clifton Pugh      Archibald Prize           3\n 7 Eric Smith        Archibald Prize           3\n 8 Robert Hannaford  People's Choice Award     3\n 9 William Dobell    Archibald Prize           3\n10 William Pidgeon   Archibald Prize           3\n# ℹ 108 more rows\n\n\nOne thing I really like about the design of assertr is that pipe-friendly assertion checks make it possible to add your assertion checks at the appropriate point in the analysis pipeline. For instance, let’s suppose I want to look at the number of finalists in the Archibald Prize each year. The raw data only records n_finalists for the Archibald Prize, not the Packing Room Prize or the People’s Choice Award. Rows in the data corresponding to those latter prizes will always have NA values for n_finalists, but that isn’t a problem for my proposed analysis. The only missingness of possible concern to me is for the Archibald Prize proper. So I can write my assertion checks like this:\n\narchibald |&gt; \n  verify(has_all_names(\"prize\", \"n_finalists\")) |&gt;\n  assert(is.character, prize) |&gt;\n  assert(is.numeric, n_finalists) |&gt;\n  filter(prize == \"Archibald Prize\") |&gt;\n  assert(\\(x) !is.na(x), n_finalists) |&gt;\n  summarise(\n    min_finalists = min(n_finalists),\n    median_finalists = median(n_finalists),\n    max_finlists = max(n_finalists)\n  )\n\nColumn 'n_finalists' violates assertion 'function(x) !is.na(x)' 2 times\n    verb redux_fn             predicate      column index value\n1 assert       NA function(x) !is.na(x) n_finalists    13    NA\n2 assert       NA function(x) !is.na(x) n_finalists    69    NA\n\n\nError: assertr stopped execution\n\n\nOkay, so there is in fact a case where missingness is a problem in two rows of the data set, for the explicit subset of the data I care about. As it happens though, I simply don’t care when it’s only those two years, so for the purposes of this example I’ll filter those rows out before they even hit the assertion check, and unsurprisingly this runs without erroring:\n\narchibald |&gt; \n  verify(has_all_names(\"prize\", \"n_finalists\")) |&gt;\n  assert(is.character, prize) |&gt;\n  assert(is.numeric, n_finalists) |&gt;\n  filter(prize == \"Archibald Prize\", !is.na(n_finalists)) |&gt;\n  assert(\\(x) !is.na(x), n_finalists) |&gt;\n  summarise(\n    min_finalists = min(n_finalists),\n    median_finalists = median(n_finalists),\n    max_finlists = max(n_finalists)\n  )\n\n# A tibble: 1 × 3\n  min_finalists median_finalists max_finlists\n          &lt;dbl&gt;            &lt;dbl&gt;        &lt;dbl&gt;\n1            15               52          197\n\n\nIn addition to verify() and assert(), there are three other assertion functions in assertr. I’m not going to dive into those for the purposes of this post – that’s what the package documentation is there for! – but the TL;DR is as follows:\n\ninsist() works like assert() but it takes a “predicate generator” function instead of a “predicate” function, which makes it possible to specify an assertion check for a tidy selection of columns and have the predicate generator handle each column according to its own logic\nassert_rows() is a row-wise version of assert()\ninsist_rows() is a row-wise version of insist()\n\nSummary: My overall feeling is that assertr is probably the most powerful tool for assertion checks applied to tabular data. It lacks the generality of the other tools, true, but the special case that it works for is a really important one for data analysts. Data objects tend to have their own special issues, and pretty much every data analysis takes at least one data frame as an input, so it’s really convenient to have a specialised tool for that scenario."
  },
  {
    "objectID": "posts/2023-08-08_being-assertive/index.html#footnotes",
    "href": "posts/2023-08-08_being-assertive/index.html#footnotes",
    "title": "Four ways to write assertion checks in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe idea is very similar to writing unit checks for software development. The difference is that unit tests are run at build time, whereas assertions apply at run time.↩︎\nIt should be noted that these aren’t the only packages out there to support assertions in R. There are at least three others that I’m aware of but haven’t yet tried, and probably many others that I don’t know about. For what it’s worth, these are the other three I know of: the ensurer, checkmate, and tester packages can all be used for this purpose, and I’m sure I could come up with terrible rhymes for those too, but there’s a limit to how much effort I want to put into this post.↩︎\nIn general, assertr doesn’t supply lots of convenience functions, but has_all_names() is an important special case because it’s used to check for the existence of columns, and that requires a special workflow. For type checking assertions, I can group together multiple verify() checks into a single assert() check that takes a tidy selection of columns. But for that to work the columns actually have to exist, so you can’t use assert() for existence checks! Hence (I presume) the inclusion of the has_all_names() convenience function.↩︎\nColumn names are unquoted and are passed through the dots .... The documentation notes that the dots are passed to dplyr::select(), and accordingly the assert() function supports tidy selection.↩︎"
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html",
    "href": "posts/2023-08-14_mrgsolve/index.html",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "",
    "text": "Continuing my informal series of “Danielle learns pharmacometric modelling” posts, today I’ve decided to sit down and teach myself how to use the mrgsolve package in R.\nAs I’m rapidly coming to realise, the world of pharmacometric modelling is an intersting space where there are a large number of domain-specific languages that have been designed to solve a particular subset of the modelling problems faced by analysts in the field, and R serves as a lingua franca that stitches them all together and makes it possible to write analysis scripts that call on multiple tools.1\nWith that as the structure of the ecosystem, what you tend to find are packages that carve out a specific niche by building on top of some other tool. For this post, the niche we’re talking about is model-based simulation. In this context, it’s assumed that the analyst has a specific pharmacometric model in mind (e.g., one-compartment PK model,2 two-compartment PK model, etc etc). We are not attempting to estimate parameters from data, nor are we runing a model testing exercise. The model is presumed to exist already, usually because the analyst has already done the model fitting exercise using their tool of choice.3\nWithin the specific “model simulation” niche there are a number of R packages that people seem to use frequently. There’s the RxODE package4 and its successor rxode2, for example, and mrgsolve falls within the same general niche. I didn’t have any specific reason for deciding to learn mrgsolve first: I had to start somewhere and this seems as good a place as any.\nlibrary(mrgsolve)"
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#a-simple-example",
    "href": "posts/2023-08-14_mrgsolve/index.html#a-simple-example",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "A simple example",
    "text": "A simple example\nOkay, let’s get started. The mrgsolve package is build on top of an open source ODE solver,5 but the user doesn’t need to call it directly. Instead, a model is constructed using a model specification file (more on that later) that is then compiled to C++. This compiled model is used to run simulations, and it’s this compiled model that calls the ODE solvers. As a example, let’s use this code taken from the get started page, which uses modlib() to use one of the predefined model specifications that come bundled with mrgsolve:\n\nmod &lt;- modlib(\"pk1\")\n\nBuilding pk1 ... done.\n\n\nIn this code, \"pk1\" refers to the name of one of the model that comes bundled with mrgsolve… and there will be exactly zero pharmacometricians in this world that are surprised to discover that this is a one-compartment PK model with first-order absorption into the central compartment, and first-order elimination from the central compartment. If we print out the model object, we get a nice little summary:\n\nmod\n\n\n\n-----------------  source: pk1.cpp  -----------------\n\n  project: /home/danielle/R...gsolve/models\n  shared object: pk1-so-3fbc27212d78b \n\n  time:          start: 0 end: 24 delta: 1\n                 add: &lt;none&gt;\n\n  compartments:  EV CENT [2]\n  parameters:    CL V KA [3]\n  captures:      CP [1]\n  omega:         0x0 \n  sigma:         0x0 \n\n  solver:        atol: 1e-08 rtol: 1e-08 maxsteps: 20k\n------------------------------------------------------\n\n\nA few months ago very little of this would have made sense to me, but I’ve – apparently – become familiar enough with conventions in pharamacometrics that this now looks very easy to read. For this initial example, the bits that matter most are these:\n\nWe have a list of compartments: CENT refers to the central compartment, and EV refers to an extravascular dosing compartment through which the drug is administered. Note that although there are two listed compartments, this is really a one-compartment model: the extravascular dosing compartments are a necessary part of the model formalism, but no more than that.\nWe have a list of parameters: clearance (CL) is a measure representing the volume of blood that can be fully cleared of the drug per unit time, volume of distribution (V) measures the size of the central compartment, and KA is the absorption rate constant governing how quickly the drug is absorbed from the extravascular compartment into the central compartment.\nWhen running a simulation, the drug amounts in the compartments CENT and EV will be returned as part of the output. However, we can also specify other “captured” quantities, which in this case adds CP, the drug concentration in the central compartment.6\n\nThe parameter values (i.e. CL, V, KA) are part of the model specification, and you can see the values assigned to those parameters by calling param():\n\nparam(mod)\n\n\n Model parameters (N=3):\n name value . name value\n CL   1     | V    20   \n KA   1     | .    .    \n\n\nWe see that our model assumes a clearance (CL) of 1, an aborption rate constant (KA) of 1, and a volume of distribution equal to 20. The mrgsolve package doesn’t keep track of units: it’s up to the user to make sure all the units are on the appropriate scale.\nNote that the param() function is both the “getter” and the “setter” for model parameters: param(mod) returns a parameter list object containing the parameters of mod, whereas param(mod, CL = 2, KA = 2) returns a modified model object with updated parameters. Later in the post I’ll use param() to modify model parameters in this way.\nOkay so now we have a model object mod that specifies all our pharmacokinetic assumptions. In order to run a simulation, we also need to provide an event schedule that provides dosing information, and we’ll also need to say something about the time points at which we want to simulate the various pharmacokinetic quantities of interest. You can do this in a few different ways but for the purposes of the initial example I’ll do it the same way that the “get started” vignette does, and use a pipe-friendly workflow:\n\nmod |&gt; \n  ev(amt = 100, ii = 24, addl = 9) |&gt;\n  mrgsim(start = 0, end = 480, delta = 0.1)\n\nModel:  pk1 \nDim:    4802 x 5 \nTime:   0 to 480 \nID:     1 \n    ID time     EV   CENT     CP\n1:   1  0.0   0.00  0.000 0.0000\n2:   1  0.0 100.00  0.000 0.0000\n3:   1  0.1  90.48  9.492 0.4746\n4:   1  0.2  81.87 18.034 0.9017\n5:   1  0.3  74.08 25.715 1.2858\n6:   1  0.4  67.03 32.619 1.6309\n7:   1  0.5  60.65 38.819 1.9409\n8:   1  0.6  54.88 44.383 2.2191\n\n\nHere we take the mod object, pipe it to the ev() function that builds the event schedule, and then pipe the output to mrgsim() which then runs the simulation. In this code, the arguments to ev() are all very standard in the field:\n\namt is the amount of drug\nii is the interdose interval\naddl is number of additional doses\n\nThe arguments to mrgsim() are used to specify the time points:\n\nstart is the initial time point (I actually didn’t need to specify it in ths case because the default value is 0)\nend is the final time point\ndelta is the step size (i.e., the amount of time between successive time points)\n\nThe output here is a tabular data structure – not technically a data frame, but I’ll get to that – with sensible column names:\n\nID is a subject identifier (always 1 for this simple example)\ntime is the time point for the simulated measurement\nEV is the drug amount in the extravascular compartment (e.g., the gut, if we’re talking about oral dosing)\nCENT is the drug amount in the central compartment\nCP is the drug concentration in the central compartment\n\nTo help you get a sense of what the simulation results look like, the mrgsolve package provides a plot method for simulation results, so if I’d wanted to I could add a call to plot() at the end of the pipeline, and get this as the output:\n\nmod |&gt; \n  ev(amt = 100, ii = 24, addl = 9) |&gt;\n  mrgsim(start = 0, end = 480, delta = 0.1) |&gt;\n  plot()\n\n\n\n\n\n\n\n\nVery nice."
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#simulation-workflow",
    "href": "posts/2023-08-14_mrgsolve/index.html#simulation-workflow",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "Simulation workflow",
    "text": "Simulation workflow\nNow things get a little messier. The mrgsolve package is designed to support several different kinds of workflow, which is of course a good thing, but very often the price of analytic flexibility is function complexity. It takes some effort to understand all the moving parts to the package, and the different ways in which mrgsolve functions can be called.7\n\nThe model library\nLet’s start by taking a closer look at the library of pre-specified models that come bundled with mrgsolve. They’re stored in a package folder whose location is accessible by calling modlib() with no arguments:\n\nmodlib()\n\n[1] \"/home/danielle/R/x86_64-pc-linux-gnu-library/4.3/mrgsolve/models\"\n\n\nAs you can see, when called with no inputs modlib() doesn’t return a compiled model, and it simply returns the path to the model library folder. If you want a list of the models that come bundled with mrgsolve, you can call modlib() setting list = TRUE:\n\nmodlib(list = TRUE)\n\nmrgsolve internal library:\n\n\n  effect  tmdd  viral1  viral2  emax  irm1  irm2  irm3  irm4  pk1cmt  pk2cmt  pk3cmt  pk1  pk2  pk2iv  popex  pred1  pbpk  1005  nm-like\n\n\nFinally, if you want to build and use one of these model you can call modlib() and pass the name of the model you want as the model argument:\n\nmod &lt;- modlib(model = \"pk1\")\n\nLoading model from cache.\n\n\nIt’s probably stating the obvious, but while the modlib() function works nicely as a tool to support analysts interactively, you probably wouldn’t call it as a developer. For instance, if you want to access the mrgsolve package folder that contains the models, you’d write code that makes very clear that you’re looking for a path (not trying to build a model). Something like this would work better:\n\nfs::path_package(\"mrgsolve\", \"models\")\n\n/home/danielle/R/x86_64-pc-linux-gnu-library/4.3/mrgsolve/models\n\n\nSimilarly, if you want to find the models in the model library folder, that’s easy enough to do with fs::dir_ls() and a simple regular expression. Easy done.\n\n\nBuilding models from file\nAlong the same lines, building one of the bundled models using modlib() is a perfectly sensible thing to do when you’re just starting out and don’t want to write your own model specification files, but after a while you might want to pivot to a different workflow. To that end, there’s an mread() function – and related functions mread_file() and mread_cache() – that reads a model specification file and returns the model object linked to the compiled code. As an example, here’s how I’d use mread() to build the one-compartment model in the previous section:\n\ndir &lt;- fs::path_package(\"mrgsolve\", \"models\")\nmod &lt;- mread(model = \"pk1\", project = dir)\n\nBuilding pk1 ... (waiting) ...\ndone.\n\n\nOptionally you can provide a file name for the model specification file that sits within the project folder, but in this case we don’t need to: if the file argument is unspecified mread() assumes that the file name is the same as the model name with file extension .cpp.8\n\n\nModel objects\nThe mrgsolve package is built using S4 classes and of the great many object oriented programming systems available in R that’s the one I’m least comfortable with.9 But hey… I’ve just reread the S4 chapter in Advanced R, so let’s see how we go with this, shall we? First, I’ll be polite and explicitly load the methods package:\n\nlibrary(methods)\n\nNext let’s see what kind of object mod is and what methods are defined for it:\n\nis(mod)\n\n[1] \"mrgmod\"\n\nmethods(class = \"mrgmod\")\n\n [1] [              [[             $              all.equal     \n [5] as.environment as.list        blocks         cmtn          \n [9] data_set       ev_rx          ev             evd           \n[13] idata_set      init           initialize     knobs         \n[17] loadso         names          omat           param         \n[21] req            Req            revar          see           \n[25] show           smat           stime          summary       \n[29] update         within         zero_re       \nsee '?methods' for accessing help and source code\n\n\nA lot of those methods are unsurprising. For example, the show() method is just the S4 analog of print(). When we print the mod object at the console we’re just calling its show() method:\n\nshow(mod)\n\n\n\n-----------------  source: pk1.cpp  -----------------\n\n  project: /home/danielle/R...gsolve/models\n  shared object: pk1-so-3fbc21a2b760d \n\n  time:          start: 0 end: 24 delta: 1\n                 add: &lt;none&gt;\n\n  compartments:  EV CENT [2]\n  parameters:    CL V KA [3]\n  captures:      CP [1]\n  omega:         0x0 \n  sigma:         0x0 \n\n  solver:        atol: 1e-08 rtol: 1e-08 maxsteps: 20k\n------------------------------------------------------\n\n\nBut there are other methods that are kind of handy when inspecting a mrgmod object. For example, if we wanted to see the source code for the corresponding model specification file we could call the see() method:\n\nsee(mod)\n\n\nModel file:  pk1.cpp \n$PARAM @annotated\nCL   :  1 : Clearance (volume/time)\nV    : 20 : Central volume (volume)\nKA   :  1 : Absorption rate constant (1/time)\n\n$CMT  @annotated\nEV   : Extravascular compartment\nCENT : Central compartment\n\n$GLOBAL\n#define CP (CENT/V)\n\n$PKMODEL ncmt = 1, depot = TRUE\n\n$CAPTURE @annotated\nCP : Plasma concentration (mass/volume)\n\n\nIf we didn’t want quite that much detail, a summary() would have sufficed:\n\nsummary(mod)\n\nModel: pk1\n- Parameters: [3]\n  CL, V, KA\n- Compartments: [2]\n  EV, CENT\n- Captured: [1]\n  CP\n- Outputs: [3]\n  EV, CENT, CP\n\n\nI don’t intend to do an exhaustive walk through of all the methods defined for mrgmod objects. That would be tiresome, and in any case I don’t even know what all of them do yet. But what I will mention is that many of the methods exist to provide public accessors for these internal slots of a mrgmod object. To illustrate, here’s a list of all the slot names:\n\nslotNames(mod)\n\n [1] \"model\"    \"modfile\"  \"package\"  \"soloc\"    \"project\"  \"start\"   \n [7] \"end\"      \"delta\"    \"add\"      \"tscale\"   \"digits\"   \"quiet\"   \n[13] \"verbose\"  \"debug\"    \"preclean\" \"atol\"     \"rtol\"     \"ss_rtol\" \n[19] \"ss_atol\"  \"maxsteps\" \"hmin\"     \"hmax\"     \"ixpr\"     \"mxhnil\"  \n[25] \"shlib\"    \"funs\"     \"omega\"    \"sigma\"    \"request\"  \"param\"   \n[31] \"init\"     \"capture\"  \"Icap\"     \"capL\"     \"Icmt\"     \"cmtL\"    \n[37] \"args\"     \"fixed\"    \"advan\"    \"trans\"    \"mindt\"    \"code\"    \n[43] \"annot\"    \"envir\"    \"plugin\"   \"ss_cmt\"  \n\n\nOkay so one of the slots is called “param”, and denoted @param to remind us that it’s a slot of an S4 object.10 Calling the param() method is the appropriate way to access the @param slot, for instance.11 The see() method is slightly fancier, but it too is essentially an accessor function for the @modelfile and @code slots. If I were an extremely unwise woman who ignored all the best practices for S4 classes I could use a command like cat(mod@code, sep = \"\\n\") and get roughly the same output. This is of course a terrible idea: the slots of an S4 object are considered internal details and not part of the package API. Accessing them directly is considered a faux pas and you have only yourself to blame if the developer later changes the structure of the slots and your code breaks.\nIndeed, the only reason I’m talking about them here is that I find it helpful for building my own mental model of what mrgsolve does, which will become apparent in the next section when I tackle the puzzlingly magical behaviour of the ev() function.\n\n\nEvent objects\nModel objects represent the underlying ODE system. They don’t store information about “interventions” (external forcers) on the system. In the pharmacokinetic context the main intervention we’re thinking about is dosing. An events object returned by ev() returns a event schedule that would be familiar to any pharmacometrician:\n\nevents &lt;- ev(amt = 100, ii = 24, addl = 9)\nevents\n\nEvents:\n  time amt ii addl cmt evid\n1    0 100 24    9   1    1\n\n\nThe events object looks a lot like a data frame, but is technically an S4 object with class “ev”. However, an ev object has only two slots, one of which is @data and – as you’d expect – it stores the data set as a data frame internally. So… yeah, it’s basically a data frame, and since there are as.data.frame() and as_tibble() methods defined for ev objects, so you can coerce it to whatever your preferred form of tabular data object happens to be.12 I’m a tibble girl myself so…\n\ntibble::as_tibble(events)\n\n# A tibble: 1 × 6\n   time   amt    ii  addl   cmt  evid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0   100    24     9     1     1\n\n\nI’ll talk more about other ways to build fancier event schedules later, and you’ll see that it’s perfectly possible to use a simple data frame to specify an event schedule, but we’re not yet at the point where any of that is needed. Right now, all we’re trying to do is understand what happens in the simple simulation I showed at the start of this post.\n\n\nDanielle briefly loses her f**king mind\nVery soon I will move on to mrgsim(), the function that we use to run the simulation itself. Truly, we will get there soon. But we have one little matter to clear up first, related to the behaviour of ev().\nThe previous section makes it look as if ev() is very simple, and viewed from the analyst perspective it really is quite simple. You use it to construct event schedules. However, ev() is not a simple function. It’s an S4 generic with dispatch on the first argument13 and it returns a qualitatively different kind of object when called in a pipeline.\nTo understand the “Danielle briefly loses her f**king mind” aspect to this, let’s return to the model simulation pipeline that I lifted from the “Get Started” vignette and used at the start of the post:\n\nmod |&gt; \n  ev(amt = 100, ii = 24, addl = 9) |&gt;\n  mrgsim(end = 480, delta = 0.1)\n\nModel:  pk1 \nDim:    4802 x 5 \nTime:   0 to 480 \nID:     1 \n    ID time     EV   CENT     CP\n1:   1  0.0   0.00  0.000 0.0000\n2:   1  0.0 100.00  0.000 0.0000\n3:   1  0.1  90.48  9.492 0.4746\n4:   1  0.2  81.87 18.034 0.9017\n5:   1  0.3  74.08 25.715 1.2858\n6:   1  0.4  67.03 32.619 1.6309\n7:   1  0.5  60.65 38.819 1.9409\n8:   1  0.6  54.88 44.383 2.2191\n\n\nIf you’re expecting ev() to return an “ev” object – as indeed it would if I called ev(amt = 100, ii = 24, addl = 9) outside of a pipeline – this code makes absolutely no sense whatsoever. An “ev” object simply does not have the information required to run the simulations. Running a model-based simulation requires an actual model, and an “ev” object does not contain any slots that could possibly store a model object. So… something magical is happening. This code shouldn’t work, but it does???\nI cried briefly. Then I read the documentation properly. Then I cried some more.\nAfter reading the documentation carefully, I now understand what’s going on here, but an explanation is required because if you don’t look closely it looks like magic.14 When the first argument to ev() is a model object, it doesn’t return an event schedule. Instead, it returns another model object.15\n\nev(mod, amt = 100, ii = 24, addl = 9)\n\n\n\n-----------------  source: pk1.cpp  -----------------\n\n  project: /home/danielle/R...gsolve/models\n  shared object: pk1-so-3fbc21a2b760d \n\n  time:          start: 0 end: 24 delta: 1\n                 add: &lt;none&gt;\n\n  compartments:  EV CENT [2]\n  parameters:    CL V KA [3]\n  captures:      CP [1]\n  omega:         0x0 \n  sigma:         0x0 \n\n  solver:        atol: 1e-08 rtol: 1e-08 maxsteps: 20k\n------------------------------------------------------\n\n\nLooking at the printed output, you might think that the output here is identical to the original model object mod, but in this case looks are deceiving. The new model stores the event schedule internally: it’s tucked away in the @args slot.16 To illustrate, let’s assign the output to a variable:\n\nmod_with_ev &lt;- ev(mod, amt = 100, ii = 24, addl = 9)\n\nNow compare the pair:\n\nmod@args\n\nlist()\n\nmod_with_ev@args\n\n$events\nEvents:\n  time amt ii addl cmt evid\n1    0 100 24    9   1    1\n\n\nSo now things become a little clearer. After seeing this, what you might – correctly! – conclude is that at the other end of the pipeline the mrgsim() function is aware that the event schedule might not be passed explicitly, and knows to check within the model object if that is the case. Knowing all this, we’re now in a position to understand what happens during a pipeline like this:\n\nmodlib(\"pk1\", quiet = TRUE) |&gt; \n  ev(amt = 100, ii = 24, addl = 9) |&gt;\n  mrgsim(end = 480, delta = 0.1)\n\nIt’s a clever trick, and I imagine it’s something that a lot of data analysts find super handy. That said, it’s probably not something I would use myself. I’m a simple girl who likes her coffee black and her data pipelines transparent, so I’d probably avoid this particular workflow and instead write code that looks more like this:\n\n# set up the simulation\ndir    &lt;- fs::path_package(\"mrgsolve\", \"models\")\nmod    &lt;- mread_file(file = \"pk1.cpp\", project = dir, quiet = TRUE)\nevents &lt;- ev(amt = 100, ii = 24, addl = 9)\n\n# run the simulation\nout &lt;- mrgsim_e(mod, events = events, end = 480, delta = 0.1)\n\nNotice that I called mrgsim_e() here rather than mrgsim(). Because mrgsolve recognises that sometimes developers might want to call simulation functions programmatically, it provides several restricted versions of mrgsim() that require input in a specific format. The sneaky “event-schedule-inside-the-model-object” piping trick I showed at the start of the post does not work with mrgsim_e(), which – at least to my mind – makes it a safer choice when running simulations programmatically. Later in this post you’ll see me pivot to using mrgsim_d(), for much the same reason.\n\n\nData frames as event schedules\nIn the last section I kind of wrapped myself up in knots trying to get a handle on what ev() does under the hood, and as you can probably tell I have some mixed feelings about it. Fortunately, you don’t have to use it at all if you don’t want to: the mrgsim_d() function takes regular data frame as the data argument, and which plays the same role as the events argument in mrgsim_e(). You can generate event schedules in data frame format using the ev_expand() function:\n\nev_expand(amt = 100, ii = 24, addl = 9)\n\n  ID time amt ii addl cmt evid\n1  1    0 100 24    9   1    1\n\n\nThe output here looks the same, but this time the output is a regular data frame, and so to be defensive in our code we would call mrgsim_d() to run a simulation that requires a data frame as input:\n\n# set up the simulation\ndir    &lt;- fs::path_package(\"mrgsolve\", \"models\")\nmod    &lt;- mread_file(file = \"pk1.cpp\", project = dir, quiet = TRUE)\nevents &lt;- ev_expand(amt = 100, ii = 24, addl = 9)\n\n# run the simulation\nout &lt;- mrgsim_d(mod, data = events, end = 480, delta = 0.1)\n\nAs an aside, you might be wondering why this function is named ev_expand(). It’s not obvious from the example I showed above, but the ev_expand() function is essentially a convenience function that calls to expand.grid() to combine the levels of all variables input, with some extra syntactic sugar that auto-populates certain columns that are required for event schedule data sets. As an example, you might generate an event schedule defined for multiple subjects using a command like this:\n\nev_expand(amt = 100, ii = 24, addl = 9, ID = 1:6)\n\n  ID time amt ii addl cmt evid\n1  1    0 100 24    9   1    1\n2  2    0 100 24    9   1    1\n3  3    0 100 24    9   1    1\n4  4    0 100 24    9   1    1\n5  5    0 100 24    9   1    1\n6  6    0 100 24    9   1    1\n\n\nLater in this post I’ll use ev_expand() in exactly this way.\n\n\nSimulation times\nWe’re almost done unpacking the simple example, but I want to rewrite the code one last time. Until now, every time I’ve mrgsim() and its variants I’ve passed arguments end and delta as a way to override the default assumptions about what time points we would use when running our simulations. Internally, these arguments are used to construct a “tgrid” object that specifies the time points. We can construct this object explicitly by calling tgrid():\n\ntgrid(start = 0, end = 480, delta = 0.1)\n\nstart:  0  end:    480  delta:  0.1  offset: 0  min:    0   max:    480 \n\n\nUsing this knowledge, we can now write our simulation code like this:\n\n# set up the simulation\ndir    &lt;- fs::path_package(\"mrgsolve\", \"models\")\nmod    &lt;- mread_file(file = \"pk1.cpp\", project = dir, quiet = TRUE)\nevents &lt;- ev_expand(amt = 100, ii = 24, addl = 9)\ntimes  &lt;- tgrid(start = 0, end = 480, delta = 0.1)\n\n# run simulation\nout &lt;- mrgsim_d(x = mod, data = events, tgrid = times)\n\nFor the simple example, calling tgrid() explicitly doesn’t by us much, but if you dive into the documentation a little you discover that there are tools for working with tgrid objects that allow you to define the simulation times in much richer ways than a simple grid. But I digress.\n\n\nSimulation code\nTaking a step back, it’s worth thinking a little about the code I’ve ended up with. The piped code I started with probably works nicely for some people, but it’s not my preferred way to do this. The way I think of these things, a simulation has three main inputs (model object, event schedule, simulation times), and I find the code easier to read when these three inputs are passed as three separate arguments. The syntax used to specify the original simulation pipeline is very compact…\n\nout &lt;- modlib(\"pk1\", quiet = TRUE) |&gt; \n  ev(amt = 100, ii = 24, addl = 9) |&gt;\n  mrgsim(end = 480, delta = 0.1)\n\n…but that compactness comes at the expense of slightly obfuscating the inputs to mrgsim(). By way of contrast, this version of the code is considerably more verbose…\n\n# this would normally be the project folder\ndir &lt;- fs::path_package(\"mrgsolve\", \"models\")\n\n# define model, events, and times\nmod    &lt;- mread_file(file = \"pk1.cpp\", project = dir, quiet = TRUE)\nevents &lt;- ev_expand(amt = 100, ii = 24, addl = 9)\ntimes  &lt;- tgrid(start = 0, end = 480, delta = 0.1)\n\n# run simulation\nout &lt;- mrgsim_d(x = mod, data = events, tgrid = times)\n\n…but personally I find it a little easier to understand the structure of the simulation when its written like this. Other people might have different views though.\n\n\nSimulation output\nAt this point the simulation is complete, so we can turn our attention to the output we’ve created. Here’s an example of the output returned by mrgsim() and its friends:\n\nout\n\nModel:  pk1 \nDim:    4802 x 5 \nTime:   0 to 480 \nID:     1 \n    ID time     EV   CENT     CP\n1:   1  0.0   0.00  0.000 0.0000\n2:   1  0.0 100.00  0.000 0.0000\n3:   1  0.1  90.48  9.492 0.4746\n4:   1  0.2  81.87 18.034 0.9017\n5:   1  0.3  74.08 25.715 1.2858\n6:   1  0.4  67.03 32.619 1.6309\n7:   1  0.5  60.65 38.819 1.9409\n8:   1  0.6  54.88 44.383 2.2191\n\n\nAs you’ve probably come to expect at this point, this is not technically a data frame, it’s an S4 object of class “mrgsims”, and can easily be coerced to a data frame using as.data.frame() or as_tibble().\nThe mrgsolve package supplies a plot method for mrgsims objects that generates nice looking lattice plots, making it very easy to quickly produce helpful data visualisations:\n\nplot(out)\n\n\n\n\n\n\n\n\nThe plot method allows you to customise the plot in a fairly flexible way, but there are some limits to what you can do with this approach. It’s not a big drawback though. If additional customisation is needed it’s pretty easy to convert the output to a tibble and then using ggplot2 to create the specific visualisation you want:\n\nlibrary(ggplot2)\n\nout |&gt;\n  tibble::as_tibble() |&gt;\n  tidyr::pivot_longer(\n    cols = c(EV, CENT, CP), \n    names_to = \"variable\", \n    values_to = \"value\"\n  ) |&gt; \n  dplyr::mutate(\n    variable = dplyr::case_when(\n      variable == \"EV\" ~ \"Gut amount\",\n      variable == \"CENT\" ~ \"Central amount\",\n      variable == \"CP\" ~ \"Central concentration\"\n    )\n  ) |&gt;\n  ggplot(aes(time, value)) + \n  geom_line() +\n  facet_wrap(~ variable, scales = \"free_y\") + \n  theme_bw() +\n  theme(strip.background = element_rect(fill = \"white\")) + \n  labs(\n    title = \"Drug amounts and concentrations over time\",\n    x = \"Time (hours)\",\n    y = NULL\n  )"
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#model-specification",
    "href": "posts/2023-08-14_mrgsolve/index.html#model-specification",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "Model specification",
    "text": "Model specification\nUp till now I’ve been relying entirely on prespecified pharmacokinetic models included in the mrgsolve model library. That was a useful thing to do earlier in this post while discussing the mechanics of mrgsim(), ev(), mread(), tgrid() and so on, but in practice you really need to understand how models are specified. I’m not going to attempt a comprehensive discussion of this topic, but if you want more detail, the chapters in the user guide I found most relevant are:\n\nThe model specification chapter\nThe topics chapter\n\n\nExample 1: Two compartment PK model\nThere are two ways to specify a model in mrgsolve: you can pass a string to mread_code(), or you can read it from a model specification file using mread_file(). I’ll be using the latter method here. By convention, model specification files use a “.cpp” file extension, but it’s important to recognise that despite that, a model specification file is not, strictly speaking, C++ code. A model specification consists of a set of code blocks, and only some of those code blocks contain C++ code. Some blocks use R syntax, and others are plain text.17\nIn any case, the first model specification I wrote is contained in the example1.cpp file bundled with this post, and is a very modest tweak to one of the models distributed with the mrgsolve package. Here’s what the file looks like:\n\n\n\nexample1.cpp\n\n[PROB]\n\nThis is a minor variation of the \"pk2cmt\" model that is distributed as\npart of the mrgsolve internal model library. It has a single extravascular\ndosing compartment (the GUT), a central compartment (CENT), and a\nperipheral compartment (PERIPH). Absorption from GUT is first order,\nwhereas elimination from CENT follows Michaelis-Menten kinetics.\n\n[PARAM] @annotated\n\nVC   :  20  : Central volume (volume)\nQ    :   2  : Inter-compartmental clearance (volume/time)\nVP   :  10  : Peripheral volume of distribution (volume)\nKA   : 0.5  : Absorption rate constant (1/time)\nVMAX :   1  : Maximum velocity of elimination (mass/time)\nKM   :   2  : Michaelis constant for elimination (mass/volume)\n\n[CMT] @annotated\n\nGUT    : Drug amount in gut (mass)\nCENT   : Drug amount in central compartment (mass)\nPERIPH : Drug amount in peripherhal compartment (mass)\n\n[GLOBAL]\n\n#define CP (CENT / VC)          // concentration in central compartment\n#define CT (PERIPH / VP)        // concentration in peripheral compartment\n#define CLNL (VMAX / (KM + CP)) // non-linear clearance, per MM kinetics\n\n[ODE]\n\ndxdt_GUT = -KA * GUT;\ndxdt_CENT = KA * GUT - (CLNL + Q) * CP  + Q * CT;\ndxdt_PERIPH = (Q * CP) - (Q * CT);\n\n[CAPTURE] @annotated\n\nCP : Plasma concentration (mass/time)\n\n\nThere are six code blocks in this file. I’ve specified them using square brackets (e.g., [BLOCKNAME]) because it reminds me of TOML, but it’s also valid to use the dollar sign (e.g., $BLOCKNAME).18 Block names are case insensitive: mrgsolve treats [BLOCKNAME] and [blockname] identically. The order in which you specify blocks doesn’t matter, but the order of statements within a block often does matter because some blocks are interpreted as C++ or R code.\nThe interpretation of these blocks is as follows:\n\nThe [PROB] block is purely used to specify comments or notes on the model. It has no functional effect. You’ll very often see this block written in markdown format.\nThe [PARAM] block is used to pass a list of parameter values to be used in the model. When parsing this code block, mrgsolve interprets the values as R expressions (evaluated at build time), so if you were to define the central compartment volume VC to be sqrt(400), the resulting model would store VC as the numeric value 20 within the internal parameter list. Normally, parameters would be defined as a comma separated list of name-value pairs (e.g., VC = 20, Q = 2, ...), but when you use the @annotated option as I have done here, you can write VC : 20 : Central volume (volume). This version of the syntax allows you to provide comments on how each parameter is interpreted. Many of the code blocks support the @annotated option, and in most cases I find myself strongly preferring the annotated syntax.\nThe [CMT] block is used to specify compartment names, and much like the [PARAM] block it supports the @annotated keyword. By default, all compartments are presumed to be initialised with value 0. If you need to set different initial values for the drug amounts in each compartment, use an [INIT] block instead of a [CMT] block.\nThe [GLOBAL] block is used to specify global variables and preprocessor directives in the C++ code that mrgsolve constructs from the model specification file. This block is, not surprisingly, interpreted as literal C++ code. In this example I’ve used the #define directive to indicate that the plasma concentration CP is simply an alias for (CENT / VP), and similarly the tissue19 concentration CT is an alias for (PERIPH / VP), and so on.\nThe [ODE] block is used to specify the differential equations that define the model. For every compartment in your model there is a corresponding dxdt_ variable: for instance, if CENT denotes the drug amount in the central compartment, then there is automatically a variable dxdt_CENT that denotes its first derivative with respect to time, and you must specify the value for all compartment derivatives even if they are zero. The [ODE] block is interpreted as literal C++ code, and you can declare and initialise new variables within the [ODE] block if you want to. Note that you may sometimes see a [DES] code block instead of an [ODE] block. They’re the same thing: [DES] is an alias for [ODE].\nThe [CAPTURE] block is used to indicate variables that should be “captured” in the simulation and returned to the user in R when mrgsim() is called. The user guide doesn’t say so explicitly, but from what I can tell the compartment amount variables are always captured, and you don’t need to list those here. The only things you need to specify here are the other quantities that you want the simulation to return. It supports the @annotated keyword, and I’ve used that here because honestly my sanity dissolves very quickly when trying to read model specification files that don’t use the annotations.\n\nOkay, now that I’ve written my model specification file let’s pivot back to my R session and use it to run a simulation:\n\n# define model, events, and times\nmod    &lt;- mread_file(file = \"example1.cpp\", quiet = TRUE)\nevents &lt;- ev_expand(amt = 10, ii = 24, addl = 19)\ntimes  &lt;- tgrid(start = 0, end = 960, delta = 0.2)\n\n# run simulation and plot results\nout &lt;- mrgsim_d(mod, data = events, tgrid = times)\nplot(out)\n\n\n\n\n\n\n\n\nYes, that all seems to work nicely. No, I did not have any solid justification for choosing these parameters. It’s just a toy.\n\n\nExample 2: Two compartment population-PK model\nThe simulation in the previous section is relatively simple. It’s a one compartment model, there are no random effects or covariates, and the simulation involves only a single subject. The nonlinear clearance aspect is a little fancy, since Michaelis-Menten kinetics aren’t entirely simple, but apart from that there’s not much going on in this model.\nTime to add some complexity. This time around I’ll build a standard two compartment model with first-order absorption and first-order elimination, but I’ll now allow random effects on all model parameters. Conventionally,20 the variables in a population-PK model follow a uniform convention, and this is very often mirrored in software and analysis code. I’m now familiar enough with PK modelling that I’ve internalised these conventions, but since the audience of my blog is wider, here are the key ones:\n\nPopulation typical values are denoted with thetas (\\(\\theta\\), \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\Theta\\))21\nPopulation scale parameters are denoted with omegas (\\(\\omega\\), \\(\\boldsymbol\\omega\\), \\(\\boldsymbol\\Omega\\))\nRandom effect terms are denoted with etas (\\(\\eta\\), \\(\\boldsymbol\\eta\\))\nVariability of the measurement is denoted with sigmas (\\(\\sigma\\), \\(\\boldsymbol\\sigma\\), \\(\\boldsymbol\\Sigma\\))\n\nWith that little refresher out of the way, let’s have a look at the model specification file:\n\n\n\nexample2.cpp\n\n[PROB]\n\nThis is a population-PK two-compartment model.\n\n[PARAM] @annotated\n\nTVVC   : 20  : Typical value for VC (volume)\nTVVP   : 10  : Typical value for VP (volume)\nTVKA   :  1  : Typical value for KA (1/time)\nTVCL   :  1  : Typical value for CL (volume/time)\nTVQ    :  2  : Typical value for Q (volume/time)\n\n[OMEGA] @annotated\n\nEVC   :   2 : Variance of random effect on VC\nEVP   :   1 : Variance of random effect on VP\nEKA   : 0.1 : Variance of random effect on KA\nECL   : 0.1 : Variance of random effect on CL\nEQ    : 0.1 : Variance of random effect on Q\n\n[MAIN]\n\ndouble VC = TVVC * exp(EVC); // central compartment volume\ndouble VP = TVVP * exp(EVP); // peripheral compartment volume\ndouble KA = TVKA * exp(EKA); // absorption rate constant\ndouble CL = TVCL * exp(ECL); // clearance\ndouble Q  = TVQ  * exp(EQ);  // intercompartmental clearance\n\n[CMT] @annotated\n\nGUT    : Drug amount in gut (mass)\nCENT   : Drug amount in central compartment (mass)\nPERIPH : Drug amount in peripherhal compartment (mass)\n\n[GLOBAL]\n\n#define CP (CENT / VC)   // concentration in central compartment\n#define CT (PERIPH / VP) // concentration in peripheral compartment\n\n[ODE]\n\ndxdt_GUT    = -(KA * GUT);\ndxdt_CENT   =  (KA * GUT) - (CL + Q) * CP + (Q * CT);\ndxdt_PERIPH =  (Q * CP) - (Q * CT);\n\n[CAPTURE] @annotated\n\nCP : Plasma concentration (mass/time)\n\n\nThings to notice here:\n\nThe [PARAM] block is essentially the same as last time. The only difference is that I’ve now given all the variables a “TV” prefix, to indicate that they now refer to the population typical value for the corresponding quantity (e.g., TVCL is the typical value for clearance CL). I would have preferred a different naming scheme, personally, but since this approach is pretty standard in the field I’ll adhere to it. In any case, these “typical value” variables collectively form the \\(\\boldsymbol\\theta\\) vector of fixed effects in the model.\nThe purpose of the [OMEGA] block to specify a variance-covariance matrix \\(\\boldsymbol\\Omega\\), such that the vector of random effects \\(\\boldsymbol\\eta_i\\) for the \\(i\\)-th simulated person is sampled from a multivariate normal distribution, \\(\\boldsymbol\\eta_i \\sim \\mbox{Normal}(\\boldsymbol{0}, \\boldsymbol\\Omega)\\). By default, mrgsolve assumes that \\(\\boldsymbol\\Omega\\) is a diagonal matrix, so all you need to do is specify the vector of variances along the main diagonal.22 The [OMEGA] block supports the @annotated option, which I’ve used here to provide human-readable explanations of each of the terms. Note that the variable names I’ve used here are things like ECL, EVP, and so on: the “E” prefix is short for “ETA” and indicates that they refer to the value of the sampled random effect term (i.e., an \\(\\eta\\) value), not the variance itself (i.e., an \\(\\omega\\) value). In that sense I find it helpful to think of the [OMEGA] block as specifying the sampling scheme for the random effect terms, rather than literally a covariance matrix.\nThe [MAIN] block, as you might expect given the name, is interpreted with C++ syntax.23 It serves multiple purposes, but what I’m using it for here is specifying relationships between variables. For instance, the code on line 26 defines the clearance CL for a specific subject as the product of the population typical value TVCL and the exponentiated random effect exp(ECL).24\nThe [CMT], [GLOBAL], [ODE], and [CAPTURE] blocks are all more or less as they were before. The code is a little different because the model is different, but there’s no new concepts required to read these blocks.\n\nNow that we’ve talked through the code, let’s go back to R and run a simulation using this model. In the extract below I’ve run a small simulation with six individuals. They all have the same dosing schedule, but we end up with different data in each case because the model samples the random effect terms separately for each person:\n\n# define model, events, and times\nmod    &lt;- mread_file(file = \"example2.cpp\", quiet = TRUE)\nevents &lt;- ev_expand(amt = 10, ii = 24, addl = 11, ID = 1:6)\ntimes  &lt;- tgrid(start = 0, end = 480, delta = 0.1)\n\n# run simulation and plot results\nout &lt;- mrgsim_d(mod, data = events, tgrid = times)\nplot(out)\n\n\n\n\n\n\n\n\nAs before, I haven’t tried to choose model parameters in the principled way: I just wanted to make sure the code is functioning properly.\n\n\nExample 3: Other customisations\nI’ll go through one last example, just to illustrate some of the other things you can build into your model specification file. The code below specifies a one-compartment population-PK model, and it incorporates a few features that haven’t appeared in any of the examples so far. First, the code:\n\n\n\nexample3.cpp\n\n[PROB]\n\nThis is an example adapted from the user guide \"topics\" section.\n\n[PARAM] @annotated\n\nTVCL : 1.1   : Typical clearance (L/hr)\nTVV  : 35.6  : Typical volume of distribution (L)\nTVKA : 1.35  : Typical absorption rate constant (1/hr)\nWT   : 70    : Weight (kg)\nSEX  : 0     : Sex coded as male = 0, female = 1\nWTCL : 0.75  : Coefficient for the effect of weight on CL\nSEXV : 0.878 : Coefficient for the effect of sex = 1 on V\n\n[MAIN]\n\ndouble CL = TVCL * pow(WT/70, WTCL) * exp(ECL);\ndouble V  = TVV  * pow(SEXV, SEX) * exp(EV);\ndouble KA = TVKA * exp(EKA);\n\n[OMEGA] @correlation @block @annotated\n\nECL : 1.23          : Random effect on CL\nEV  : 0.67 0.4      : Random effect on V\nEKA : 0.25 0.87 0.2 : Random effect on KA\n\n[SIGMA] @annotated\n\nPROP: 0.005  : Proportional residual error\nADD : 0.0001 : Additive residual error\n\n[CMT] @annotated\n\nGUT  : Dosing compartment (mg)\nCENT : Central compartment (mg)\n\n[PKMODEL]\n\nncmt = 1, depot = TRUE\n\n[TABLE]\n\ndouble CP = CENT / V;\ndouble DV = CP * (1 + PROP) + ADD;\n\n[CAPTURE] @annotated\n\nCP : True plasma concentration (mg/L)\nDV : Observed plasma concentration (mg/L)\n\n\nThere’s a few things to note here.\n\nThere is a binary-valued covariate SEX25 and a continuous covariate WT (weight) specified in the [PARAM] block. Coefficients specifying the effect of sex on distribution volume SEXV and the effect of weight on clearance WTCL are also included in the [PARAM] block. Although from a statistical perspective the value of the covariate that varies across person (WT) and the regression coefficient specifying an effect WTCL are very different kinds of thing, from a simulation perspective they’re both just numbers you can feed into the expressions in the [MAIN] block that define the pharmacokinetic quantities CL, V, and KA.\nThe [OMEGA] block now has correlated random effects. I’ve used the @block option to indicate that the off-diagonal elements are included in the code block, and the @correlation option to indicate that the off-diagonal elements are correlations (rather than covariances).\nWe now have sources of measurement error included in the model specification. The scale of the noise terms is set in the [SIGMA] code block, and for this model we include both proportional error (variability scales proportional to the true value) and additive error (constant variability of error). Note that the [SIGMA] block is used only to declare the variables and set their values. To actually incorporate the measurement error into the model code, we use the [TABLE] block.\nThe [TABLE] block contains C++ code that is executed at the end of each time step, before the simulation steps forward to the next point in time. At this point the drug amounts in each compartment have been computed, as have any time-point dependent stochastic terms (i.e., the noise terms specified in the [SIGMA] block), and we can use them to calculate other quantities. In this case, I’ve used the [TABLE] block to calculate the blood plasma concentration CP, and to calculate a hypothetical dependent variable DV that has noise added.\nThe [ODE] block has been replaced with a [PKMODEL] block. This is used to indicate that mrgsolve should use analytic solutions rather than an ODE solver. As you might expect, this is only possible for a smallish subset of models that (a) have analytic solutions that (b) are known to mrgsolve. In this particular case the model is a standard one-compartment model, for which analytic solutions are available. The ncmt = 1 part of this block indicates that it’s a one-compartment model, and the depot = TRUE part indicates that the dosing compartment (the gut, in this case) should be included in the model even though it’s not a “real” compartment. Additional information on how this code block is parsed is here.\n\nTo see all this at work, let’s run a small simulation. First, we’ll read the model specification file and compile the model:\n\nmod &lt;- mread_file(file = \"example3.cpp\", quiet = TRUE)\n\nNext, let’s take a look at the default parameters in this model. As you can see in the output, by default the subject is presumed to be a male who weighs 70kg:\n\nparam(mod)\n\n\n Model parameters (N=7):\n name value . name value\n SEX  0     | TVV  35.6 \n SEXV 0.878 | WT   70   \n TVCL 1.1   | WTCL 0.75 \n TVKA 1.35  | .    .    \n\n\nAs usual, we’ll specify some dosing events and some time points at which the simulation will be evaluated. I’ll also set it up that we always simulate two subjects at whatever parameter values we feed into the simulation:\n\nevents &lt;- ev_expand(amt = 10, ii = 48, addl = 3, ID = 1:2)\ntimes  &lt;- tgrid(start = 0, end = 240, delta = 0.5)\n\nOkay, now let’s run a simulation at the default parameter values. Here’s some data from two male subjects who both weigh 70kg:\n\nout &lt;- mrgsim_d(mod, data = events, tgrid = times)\nplot(out, ~ CP + DV)\n\n\n\n\n\n\n\n\nThe ~ CP + DV formula in the plot() command is used to control which variables are plotted. I don’t want to show everything in these plots: we’re showing the drug concentration in the central compartment CP, and the hypothetical dependent variable DV obtained by adding measurement error to the CP value.\nBut suppose we didn’t actually want to simulate a 70kg male, and instead we want a 60kg female. To specify this, we have to update the relevant parameters in the model:\n\nmod &lt;- param(mod, SEX = 1, WT = 60)\n\nSo now our model parameters look like this:\n\nparam(mod)\n\n\n Model parameters (N=7):\n name value . name value\n SEX  1     | TVV  35.6 \n SEXV 0.878 | WT   60   \n TVCL 1.1   | WTCL 0.75 \n TVKA 1.35  | .    .    \n\n\nSimulating from the model at these parameter settings gives us this:\n\nout &lt;- mrgsim_d(mod, data = events, tgrid = times)\nplot(out, ~ CP + DV)"
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#resources",
    "href": "posts/2023-08-14_mrgsolve/index.html#resources",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "Resources",
    "text": "Resources\nBefore wrapping up, some quick links to some resources:\n\nThe mrgsolve user guide is probably not the place you want to start, because it very quickly dives in deep and talks about the domain specific language used to specify models, but once you’ve wrapped your head around the basics that’s the place to look for details.\nThe mrgsolve vignettes provide a nice place to start, but one thing you need to keep in mind is that the vignettes posted at mrgsolve.org/vignettes are not identical to the vignettes that appear on the pkgdown website (i.e., mrgsolve.org/docs/articles), so it’s worth being careful to check which one you’re looking at!\nSpeaking of which, the mrgsolve pkgdown site provides all the usual documentation that you’d expect of an R package in the usual format. Function reference guides, some vignettes, links to github, etc., all that is there.\nThe learn mrgsolve page on the mrgsolve website has links to presentations, courses, and other things that can be helpful in getting started.\nFinally, there’s a blog associated with the package that has a variety of tips, tricks, updates and news."
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#epilogue",
    "href": "posts/2023-08-14_mrgsolve/index.html#epilogue",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "Epilogue",
    "text": "Epilogue\nAs with so many of my posts, I find that I’ve gotten to the end and I have no idea what else to say. I don’t write a post like this one to advocate for a package or a workflow, and I definitely don’t write these things to express opinions or whatever. I write because I enjoy writing and because the act of writing deepens my own understanding of a topic. Viewed from that perspective, writing this post has served its purpose so… mission accomplished?"
  },
  {
    "objectID": "posts/2023-08-14_mrgsolve/index.html#footnotes",
    "href": "posts/2023-08-14_mrgsolve/index.html#footnotes",
    "title": "Pharmacometric simulation with mrgsolve",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn this respect R is the unchallenged queen of languages in pharmacometrics. It’s very different to data science in the tech space, where Python is the lingua franca and R is seen as a second-class citizen. I have not yet seen a single example of anyone using Python for data analysis in this world. Judging from papers I’ve read, Julia has a small presence (e.g., you can use Turing.jl for building ODE models in Julia), but that’s the only time I’ve ever seen any statistical language other than R in this space.↩︎\nThe acronym PK is universally used as shorthand for “pharmacokinetics”, and a PK model is one where the primary variable you’re interested in modelling is the plasma concentration over time for some drug. If you’re totally new to this space, the post I wrote on non-compartmental analysis was written from a total-newbie perspective and spells out a lot the basic terminology used in PK modelling.↩︎\nThere are a lot of tools in this space: NONMEM is the oldest and most-widely used, but in addition there’s Stan/Torsten, Monolix, nlmixr, Phoenix NLME, and probably many others I don’t know about yet…↩︎\nI can’t tell if the original package is still under active development or if it’s been deprecated. The RxODE package still appears on the nlmixr website and on GitHub, but the package is (at present?) archived on CRAN.↩︎\nSpecifically, it wraps the public domain Fortran library ODEPACK.↩︎\nIn this example it’s not very interesting because CP is just the ratio of CENT and V, and since V doesn’t change over time, the curves for CP and CENT look identical. That isn’t true for simulations with multiple sujects, however, since V can and does vary across individuals.↩︎\nAs an aside, one thing I’ve noticed in the software development world is that developers who don’t work in statistics have a tendency to get very worked up about R having all these hyper-flexible functions that can behave in wildly different ways in different contexts. While I do understand that annoyance, I also think it’s often misplaced. R is fundamentally a statistical programming language and the primary goal is to support analysts. It’s not trying to be Python, and it’s even less interested in being Rust. You wouldn’t think so from social media discussions – which are massively skewed towards developers – but the typical R user is someone working interactively at the console, constructing a script iteratively as they explore and work with the data set they’ve been tasked to analyse. In that context, the hyper-flexibility of a lot of R functions is designed for the convenience of the analyst, not the convenience as a developer.↩︎\nDevelopers reading this are probably wondering where the model shared object ends up when compilation happens. By default mread() puts the compiled model in the R temp folder, but you can override this in a persistent way by setting the \"mrgsolve.soloc\" option, or overriding it in the call to mread() via the soloc argument. You can also suppress compilation if you want by setting compile = FALSE. But probably any devs reading this would likely discover that within 5 minutes of reading the documentation anyway so there’s no real need for me to mention it.↩︎\nI mean… of the widely-used OOP systems, it’s the one I’m least comfortable with. The wild abandon with which R spawns new object oriented programming systems is… a lot to keep up with.↩︎\nWhy yes, this blog post is also an exercise in “Danielle reminds herself how S4 works and no she bloody well is not going to talk about multiple inheritance and multiple dispatch in S4 here she’s not that much of a masochist”.↩︎\nAs an aside, param(mod) returns an S4 object of class “parameter_list” which in turn has a show() method that provides that prettified looking table pf parameters, but from a practical perspective you might just want to coerce it to a regular list using as.list(param(mod)).↩︎\nThere is also an as.ev() function that allows conversion in the other direction.↩︎\nMore precisely, method dispatch takes place off the x argument which is the first argument to the ev() generic, but since we’re talking about pipelines here, the name isn’t as important as the position.↩︎\nAs a rule I don’t dislike “magic” code. I mean… it would be weird to be an R user and not appreciate its virtues. Lazy evaluation and non-standard evaluation in R are powerful tools, and are the basis of a lot of “magic” in R. They make life a lot easier for the analyst but it comes at the price of making life harder for the developer. Because of that I’ve ended up with a habit of trying to dig into the details every time I find R code that feels magical.↩︎\nTo be precise, ev() is an S4 generic with dispatch on the x argument. If x is missing, the relevant ev() method returns an “ev” object. However, if x has class “mrgmod”, the relevant method returns another “mrgmod” object. I’ll confess this makes me a little uneasy.↩︎\nOh look, that apparently-irrelevant excursion she did talking about the mrgmod object slots turns out to be relevant! It’s almost as if she’s written things before!↩︎\nI’m guessing that the logic here is that, even though some code blocks use R syntax, and others aren’t interpreted as code at all, the model build process is such that eventually it all becomes C++, and a C++ compiler constructs the binary. There’s logic to it, but it does feel a bit disorienting seeing “.cpp” files that aren’t actually C++ source.↩︎\nI suspect that the $BLOCKNAME format is actually canonical because that’s the version you see when model code is printed when calling see(), but I’m going to use [BLOCKNAME] throughout this post because I personally find it easier to read. YMMV.↩︎\nConfession: I’m not 100% certain that the “T” in “CT” stands for tissue, since I copied this line from one of the models in the mrgsolve model library, but I think it’s the right interpretation given that the central compartment amount is usually intended to refer to “amount of drug the blood, sort of” and the peripheral compartment is “amount of drug in body tissues, sort of”. The “sort of” is important though: pharamacokinetic compartments are abstractions, and are they only loosely related to the corresponding physiology.↩︎\nAs far as I can tell, most of these conventions are “for compatibility with NONMEM”, and I am very rapidly starting to read “for compatibility with NONMEM” with exactly the same level of jaundiced cynicism that I apply when I find base R documentation that explains that R does something absolutely unhinged “for compatibility with S”. I deeply admire the commitment to backward compatibility and/or notational consistency, but also oh sweet lord in heaven it is EXHAUSTING.↩︎\nIn the spirit of stating assumptions, I’ll also add this. As is typical in many (but not all) disciplines: italicised lower case denotes a scalar, bold italic lower case denotes a vector, bold upper case denotes a matrix.↩︎\nI’m not going to discuss them here, but note that the @block option allows you to pass a complete variance-covariance matrix, and the @correlation option allows you to specify correlations instead of covariances on the off-diagonals. The user guide discusses these and several other options that are supported in the [OMEGA] block. Of particular note: there’s a section in the user guide on [OMEGA] shows you how to use @block and @annotated together.↩︎\nThe [MAIN] block is analogous to the NONMEM $PK block, and indeed [PK] is allowed as an alias for [MAIN] in mrgsolve model specification files.↩︎\nFor folks used to linear regression models and random effect terms that compose additively with the fixed effect, it’s worth noting that in PK models everything tends to be log-linear. For any particular PK quantity \\(x\\), the relationship takes the form \\(\\log x = \\eta_x + \\log \\theta_x\\).↩︎\nI am absolutely not going to enter into any discussion of the respects in which sex is not strictly a binary variable, not in this post. There is a time and a place for that conversation, and this is neither that time nor that place. I mean, we’re talking about a category of statistical models that simplify the entire human body into “a couple of leaky buckets and a tinfoil ODE system”. They are extremely useful models, per George Box’s famous aphorism, but one does not ever presume they are biologically accurate. That would be absurd.↩︎"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html",
    "title": "Crayola crayon colours",
    "section": "",
    "text": "What am I doing? Why am I here? Oh that’s right. So I was having coffee this morning reading the nerd news on mastodon as a girl likes to do and this this very cool post about crayon colours by Kim Scheinberg caught my attention.\nThe image comes from this blog post by Stephen Von Worley – he has a follow up too. Interesting. I realise I am of course about to waste half a day on this…"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#its-the-prologue-baby",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#its-the-prologue-baby",
    "title": "Crayola crayon colours",
    "section": "It’s the prologue, baby",
    "text": "It’s the prologue, baby\nHaving read the blog posts by Von Worley I worked out that the source of the data is the Wikipedia list of Crayola crayon colours, and as it happens I know how to pull data from Wikipedia tables into R. Thanks to this amazing post by Isabella Velásquez, I’d learned all about using the polite package to make sure that my webscraping is appropriate and respectful, and using the rvest package to do the actual scraping. What I had assumed, is that reading the table into R was going to be a simple matter of writing some code like this…\n\nurl &lt;- \"https://en.wikipedia.org/wiki/List_of_Crayola_crayon_colors\"\nraw &lt;- url |&gt;\n  polite::bow() |&gt;\n  polite::scrape() |&gt;\n  rvest::html_nodes(\"table.wikitable\")\n\nThis scrapes all the tables from the page, but I only want the first one. That’s the big table with the listing of Crayolas standard colours. The table itself looks a little like this:\n\nraw[[1]]\n\n{html_node}\n&lt;table class=\"wikitable sortable\"&gt;\n[1] &lt;caption&gt;\\n&lt;/caption&gt;\n[2] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th width=\"10%\" class=\"sortable\"&gt;Color\\n&lt;/th&gt;\\n&lt;th wid ...\n\n\nI don’t want to parse the html myself, but the hope is that I can use something like the html_table() function to extract the table and return a data frame. Okay, let’s give it a go…\n\ncrayola &lt;- raw[[1]] |&gt; rvest::html_table()\n\nWell it hasn’t thrown an error, but when I look at the crayola data frame…\n\ncrayola\n\n# A tibble: 168 × 9\n   Color Name                    Hexadecimal in their website depicti…¹ Years in production[…² Notes `16-Box` `24-Box` `32-Box` `64-Box`\n   &lt;lgl&gt; &lt;chr&gt;                   &lt;chr&gt;                                  &lt;chr&gt;                  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;   \n 1 NA    Red                     \"#ED0A3F\"                              1903–present           \"\"    \"Yes\"    \"Yes\"    \"Yes\"    \"Yes\"   \n 2 NA    Maroon                  \"#C32148\"                              1949–present           \"Kno… \"No\"     \"No\"     \"No\"     \"No\"    \n 3 NA    Scarlet                 \"#FD0E35\"                              1998–present           \"Kno… \"No\"     \"Yes\"    \"Yes\"    \"Yes\"   \n 4 NA    Brick Red               \"#C62D42\"                              1958–present           \"\"    \"No\"     \"No\"     \"No\"     \"Yes\"   \n 5 NA    English Vermilion       \"\"                                     1903–1935              \"Als… \"\"       \"\"       \"\"       \"\"      \n 6 NA    Madder Lake             \"\"                                     1903–1935              \"\"    \"\"       \"\"       \"\"       \"\"      \n 7 NA    Permanent Geranium Lake \"\"                                     1903–circa 1910        \"\"    \"\"       \"\"       \"\"       \"\"      \n 8 NA    Maximum Red             \"\"                                     1926–1944              \"Par… \"\"       \"\"       \"\"       \"\"      \n 9 NA    Chestnut                \"#B94E48\"                              1903–present           \"Kno… \"No\"     \"No\"     \"Yes\"    \"Yes\"   \n10 NA    Orange-Red              \"#FF5349\"                              1958–1990              \"\"    \"\"       \"\"       \"\"       \"\"      \n# ℹ 158 more rows\n# ℹ abbreviated names: ¹​`Hexadecimal in their website depiction[b]`, ²​`Years in production[2]`\n\n\n… I encounter a rather awkward problem. The color field, which renders on the Wikipedia page as a pretty block of colour showing what the crayon colour looks like, is empty. Sure, I do have text containing hex codes for some of the crayons, but the missing data isn’t missing at random. Old crayon colours are the ones systematically missing an official hex code. Okay, so I really would like to have some data in my color column."
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-i-i-think-it-works-like-the-hanky-code",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-i-i-think-it-works-like-the-hanky-code",
    "title": "Crayola crayon colours",
    "section": "Act I: I think it works like the hanky code",
    "text": "Act I: I think it works like the hanky code\nLook, that’s not a big deal right? Not having any colours for a post about colours? I guess the post will be short. Eh. Let’s set that aside and focus on the important things. Those column names need a little cleaning, so I’ll do the thing I always do and break out janitor and dplyr:\n\ncrayola &lt;- crayola |&gt;\n  janitor::clean_names() |&gt;\n  dplyr::rename(\n    listed = hexadecimal_in_their_website_depiction_b,\n    years = years_in_production_2\n  )\n\nMuch nicer:\n\ncrayola\n\n# A tibble: 168 × 9\n   color name                    listed    years           notes                                     x16_box x24_box x32_box x64_box\n   &lt;lgl&gt; &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;           &lt;chr&gt;                                     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 NA    Red                     \"#ED0A3F\" 1903–present    \"\"                                        \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"  \n 2 NA    Maroon                  \"#C32148\" 1949–present    \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"   \n 3 NA    Scarlet                 \"#FD0E35\" 1998–present    \"Known as \\\"Torch Red\\\", 1998.[2]\"        \"No\"    \"Yes\"   \"Yes\"   \"Yes\"  \n 4 NA    Brick Red               \"#C62D42\" 1958–present    \"\"                                        \"No\"    \"No\"    \"No\"    \"Yes\"  \n 5 NA    English Vermilion       \"\"        1903–1935       \"Also spelled \\\"Vermillion\\\".[2]\"         \"\"      \"\"      \"\"      \"\"     \n 6 NA    Madder Lake             \"\"        1903–1935       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 7 NA    Permanent Geranium Lake \"\"        1903–circa 1910 \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 8 NA    Maximum Red             \"\"        1926–1944       \"Part of the Munsell line.[2]\"            \"\"      \"\"      \"\"      \"\"     \n 9 NA    Chestnut                \"#B94E48\" 1903–present    \"Known as \\\"Indian Red\\\" before 1999.[2]\" \"No\"    \"No\"    \"Yes\"   \"Yes\"  \n10 NA    Orange-Red              \"#FF5349\" 1958–1990       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n# ℹ 158 more rows\n\n\nNow where was I? What’s that maxim about never using background colour to encode substantive data in a table? I seem to have run afoul of that. Let’s take a look at the elements of the html table and see if I can work out where things went wrong…\n\ncells &lt;- raw[[1]] |&gt; rvest::html_elements(\"td\")\ncells\n\n{xml_nodeset (1512)}\n [1] &lt;td style=\"background: #ED0A3F; color: white\"&gt; \\n&lt;/td&gt;\n [2] &lt;td&gt;Red\\n&lt;/td&gt;\n [3] &lt;td align=\"center\" style=\"background:#E9E9E9\"&gt;#ED0A3F\\n&lt;/td&gt;\n [4] &lt;td&gt;1903–present\\n&lt;/td&gt;\n [5] &lt;td&gt;\\n&lt;/td&gt;\n [6] &lt;td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [7] &lt;td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [8] &lt;td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n [9] &lt;td style=\"background:#9EFF9E;vertical-align:middle;text-align:cente ...\n[10] &lt;td style=\"background: #C32148; color: white\"&gt; \\n&lt;/td&gt;\n[11] &lt;td&gt;Maroon\\n&lt;/td&gt;\n[12] &lt;td align=\"center\" style=\"background:#E9E9E9\"&gt;#C32148\\n&lt;/td&gt;\n[13] &lt;td&gt;1949–present\\n&lt;/td&gt;\n[14] &lt;td&gt;Known as \"Dark Red\", 1949–1958.&lt;sup id=\"cite_ref-WelterColorName ...\n[15] &lt;td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[16] &lt;td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[17] &lt;td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[18] &lt;td style=\"background:#FFC7C7;vertical-align:middle;text-align:cente ...\n[19] &lt;td style=\"background: #FD0E35; color: white\"&gt; \\n&lt;/td&gt;\n[20] &lt;td&gt;Scarlet\\n&lt;/td&gt;\n...\n\n\nOkay yeah. You can see the problem looking at the 1st and 10th line of the output here. The piece of information we need is embedded in the css style attribute, and it’s only in the style metadata attribute. There’s no data in the actual “td” element for html_table() to capture. I’m going to have to fix that myself I guess. Sigh.\nThe first part of my process was to find the relevant subset of cells. There’s probably a better way to do it, but my approach was based on noting that (a) it’s really easy to find the cells containing the colour names (“Red”, “Maroon”, etc), and (b) the cell to the left of it is always the one that has the background colour that I’m looking for. So, my first step was to manually pull out the text in each cell. That’s easy to do with rvest thanks to the html_text() function, and just to make my life a little easier I used stringr to remove all the \\n characters at the end of each cell:\n\ncell_text &lt;- cells |&gt;\n  rvest::html_text() |&gt;\n  stringr::str_remove_all(\"\\n$\")\n\nAnd here’s the text in the first 20 cells:\n\ncell_text[1:20]\n\n [1] \" \"                                   \n [2] \"Red\"                                 \n [3] \"#ED0A3F\"                             \n [4] \"1903–present\"                        \n [5] \"\"                                    \n [6] \"Yes\"                                 \n [7] \"Yes\"                                 \n [8] \"Yes\"                                 \n [9] \"Yes\"                                 \n[10] \" \"                                   \n[11] \"Maroon\"                              \n[12] \"#C32148\"                             \n[13] \"1949–present\"                        \n[14] \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"\n[15] \"No\"                                  \n[16] \"No\"                                  \n[17] \"No\"                                  \n[18] \"No\"                                  \n[19] \" \"                                   \n[20] \"Scarlet\"                             \n\n\nSo when I now match this text against the colour names stored in crayola$name, I’ll detect “Red” in cell 2, “Maroon” in cell 11, and so on. If I subtract 1 from each of these values, I now have the indices of the table cells that contain the style information I need.\n\nind &lt;- which(cell_text %in% crayola$name) - 1\n\nHere’s what those cells look like:\n\ncells[ind]\n\n{xml_nodeset (168)}\n [1] &lt;td style=\"background: #ED0A3F; color: white\"&gt; \\n&lt;/td&gt;\n [2] &lt;td style=\"background: #C32148; color: white\"&gt; \\n&lt;/td&gt;\n [3] &lt;td style=\"background: #FD0E35; color: white\"&gt; \\n&lt;/td&gt;\n [4] &lt;td style=\"background: #C62D42; color: white\"&gt; \\n&lt;/td&gt;\n [5] &lt;td style=\"background: #CC474B; color: white\"&gt; \\n&lt;/td&gt;\n [6] &lt;td style=\"background: #CC3336; color: white\"&gt; \\n&lt;/td&gt;\n [7] &lt;td style=\"background: #E12C2C; color: white\"&gt; \\n&lt;/td&gt;\n [8] &lt;td style=\"background: #D92121; color: white\"&gt; \\n&lt;/td&gt;\n [9] &lt;td style=\"background: #B94E48; color: white\"&gt; \\n&lt;/td&gt;\n[10] &lt;td style=\"background: #FF5349; color: white\"&gt; \\n&lt;/td&gt;\n[11] &lt;td style=\"background: #FE4C40; color: white\"&gt; \\n&lt;/td&gt;\n[12] &lt;td style=\"background: #FE6F5E; color: white\"&gt; \\n&lt;/td&gt;\n[13] &lt;td style=\"background: #B33B24; color: white\"&gt; \\n&lt;/td&gt;\n[14] &lt;td style=\"background: #CC553D; color: white\"&gt; \\n&lt;/td&gt;\n[15] &lt;td style=\"background: #E6735C; color: white\"&gt; \\n&lt;/td&gt;\n[16] &lt;td style=\"background: #FF9980; color: white\"&gt; \\n&lt;/td&gt;\n[17] &lt;td style=\"background: #E58E73; color: white\"&gt; \\n&lt;/td&gt;\n[18] &lt;td style=\"background: #FF7034; color: white\"&gt; \\n&lt;/td&gt;\n[19] &lt;td style=\"background: #FF681F; color: white\"&gt; \\n&lt;/td&gt;\n[20] &lt;td style=\"background: #FF8833; color: white\"&gt; \\n&lt;/td&gt;\n...\n\n\nThat’s much nicer. Now I have something with a consistent format that I can process without too much pain. The rvest package has a html_attr() function which I can use to pull out the contents of the style attribute as a string. So, after spending a few minutes trying to remember how regular expressions work, I used str_extract() to pull out the hexadecimal codes like this:\n\nbackground &lt;- cells[ind] |&gt;\n  rvest::html_attr(\"style\") |&gt;\n  stringr::str_extract(\"#[0-9ABCDEF]{6}\")\n\nLet’s take a look:\n\nbackground\n\n  [1] \"#ED0A3F\" \"#C32148\" \"#FD0E35\" \"#C62D42\" \"#CC474B\" \"#CC3336\" \"#E12C2C\" \"#D92121\" \"#B94E48\" \"#FF5349\" \"#FE4C40\" \"#FE6F5E\" \"#B33B24\"\n [14] \"#CC553D\" \"#E6735C\" \"#FF9980\" \"#E58E73\" \"#FF7034\" \"#FF681F\" \"#FF8833\" \"#FFB97B\" \"#ECAC76\" \"#E77200\" \"#FFAE42\" \"#F2BA49\" \"#FBE7B2\"\n [27] \"#F2C649\" \"#F8D568\" \"#FCD667\" \"#FED85D\" \"#FBE870\" \"#F1E788\" \"#FFEB00\" \"#B5B35C\" \"#ECEBBD\" \"#FAFA37\" \"#FFFF99\" \"#FFFF9F\" \"#D9E650\"\n [40] \"#ACBF60\" \"#AFE313\" \"#BEE64B\" \"#C5E17A\" \"#5E8C31\" \"#7BA05B\" \"#9DE093\" \"#63B76C\" \"#4D8C57\" \"#01A638\" \"#6CA67C\" \"#5FA777\" \"#93DFB8\"\n [53] \"#33CC99\" \"#1AB385\" \"#29AB87\" \"#00CC99\" \"#00755E\" \"#8DD9CC\" \"#01796F\" \"#30BFBF\" \"#00CCCC\" \"#008080\" \"#8FD8D8\" \"#95E0E8\" \"#6CDAE7\"\n [66] \"#2D383A\" \"#76D7EA\" \"#7ED4E6\" \"#0095B7\" \"#009DC4\" \"#02A4D3\" \"#47ABCC\" \"#2EB4E6\" \"#339ACC\" \"#93CCEA\" \"#2887C8\" \"#003366\" \"#0066CC\"\n [79] \"#1560BD\" \"#0066FF\" \"#A9B2C3\" \"#C3CDE6\" \"#4570E6\" \"#3C69E7\" \"#7A89B8\" \"#4F69C6\" \"#8D90A1\" \"#8C90C8\" \"#7070CC\" \"#9999CC\" \"#ACACE6\"\n [92] \"#766EC8\" \"#6456B7\" \"#3F26BF\" \"#8B72BE\" \"#652DC1\" \"#6B3FA0\" \"#8359A3\" \"#8F47B3\" \"#C9A0DC\" \"#BF8FCC\" \"#803790\" \"#733380\" \"#D6AEDD\"\n[105] \"#C154C1\" \"#FC74FD\" \"#732E6C\" \"#E667CE\" \"#E29CD2\" \"#8E3179\" \"#D96CBE\" \"#D8BFD8\" \"#C8509B\" \"#BB3385\" \"#D982B5\" \"#A63A79\" \"#A50B5E\"\n[118] \"#614051\" \"#F653A6\" \"#DA3287\" \"#FF3399\" \"#FBAED2\" \"#FFB7D5\" \"#FFA6C9\" \"#F7468A\" \"#E30B5C\" \"#FDD7E4\" \"#E62E6B\" \"#DB5079\" \"#FC80A5\"\n[131] \"#F091A9\" \"#FF91A4\" \"#A55353\" \"#CA3435\" \"#FEBAAD\" \"#F7A38E\" \"#E97451\" \"#AF593E\" \"#9E5B40\" \"#87421F\" \"#926F5B\" \"#DEA681\" \"#D27D46\"\n[144] \"#664228\" \"#FA9D5A\" \"#EDC9AF\" \"#FFCBA4\" \"#805533\" \"#FDD5B1\" \"#EED9C4\" \"#665233\" \"#837050\" \"#E6BC5C\" \"#92926E\" \"#E6BE8A\" \"#C9C0BB\"\n[157] \"#DA8A67\" \"#C88A65\" \"#000000\" \"#736A62\" \"#8B8680\" \"#C8C8CD\" \"#D9D6CF\" \"#FFFFFF\" \"#F1D651\" \"#DDEBEC\" \"#D9DAD2\" \"#C0D5F0\"\n\n\nYay, those look like hex colours. Better yet, because I’ve been careful to ensure that I’ve matched everything to the correct colours and in the correct order, I can insert them into the crayola tibble where they should have been in the first place:\n\ncrayola$color &lt;- background\ncrayola\n\n# A tibble: 168 × 9\n   color   name                    listed    years           notes                                     x16_box x24_box x32_box x64_box\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;           &lt;chr&gt;                                     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  \n 1 #ED0A3F Red                     \"#ED0A3F\" 1903–present    \"\"                                        \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"  \n 2 #C32148 Maroon                  \"#C32148\" 1949–present    \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"   \n 3 #FD0E35 Scarlet                 \"#FD0E35\" 1998–present    \"Known as \\\"Torch Red\\\", 1998.[2]\"        \"No\"    \"Yes\"   \"Yes\"   \"Yes\"  \n 4 #C62D42 Brick Red               \"#C62D42\" 1958–present    \"\"                                        \"No\"    \"No\"    \"No\"    \"Yes\"  \n 5 #CC474B English Vermilion       \"\"        1903–1935       \"Also spelled \\\"Vermillion\\\".[2]\"         \"\"      \"\"      \"\"      \"\"     \n 6 #CC3336 Madder Lake             \"\"        1903–1935       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 7 #E12C2C Permanent Geranium Lake \"\"        1903–circa 1910 \"\"                                        \"\"      \"\"      \"\"      \"\"     \n 8 #D92121 Maximum Red             \"\"        1926–1944       \"Part of the Munsell line.[2]\"            \"\"      \"\"      \"\"      \"\"     \n 9 #B94E48 Chestnut                \"#B94E48\" 1903–present    \"Known as \\\"Indian Red\\\" before 1999.[2]\" \"No\"    \"No\"    \"Yes\"   \"Yes\"  \n10 #FF5349 Orange-Red              \"#FF5349\" 1958–1990       \"\"                                        \"\"      \"\"      \"\"      \"\"     \n# ℹ 158 more rows\n\n\nFinally!"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-ii-one-hundred-years-of-tidyr",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-ii-one-hundred-years-of-tidyr",
    "title": "Crayola crayon colours",
    "section": "Act II: One hundred years of tidyr",
    "text": "Act II: One hundred years of tidyr\nIf life were at all fair my data wrangling woes would now be over, but of course they are not. If I’m going to analyse the Crayola data by year it will be useful to me if the year column has nicely formatted data, and of course it does not:\n\ncrayola$years\n\n  [1] \"1903–present\"               \"1949–present\"               \"1998–present\"               \"1958–present\"              \n  [5] \"1903–1935\"                  \"1903–1935\"                  \"1903–circa 1910\"            \"1926–1944\"                 \n  [9] \"1903–present\"               \"1958–1990\"                  \"1997–present\"               \"1958–present\"              \n [13] \"1903–circa 1910\"            \"1903–1944\"                  \"1903–circa 1910\"            \"1990–present\"              \n [17] \"1926–1944\"                  \"1958–present\"               \"1930–present\"               \"1903–present\"              \n [21] \"1993–present\"               \"1926–1944, 1949–1958\"       \"2003–present\"               \"1930–present\"              \n [25] \"1926–1944\"                  \"1998–present\"               \"1903–1990\"                  \"1958–1990\"                 \n [29] \"1903–present\"               \"1990–2017[2][3][4]\"         \"1903–present\"               \"1958–present\"              \n [33] \"1926–1944\"                  \"1903–present\"               \"1958–present\"               \"1926–1944\"                 \n [37] \"1998–present\"               \"1903–1990\"                  \"1926–1944\"                  \"1926–1944\"                 \n [41] \"2003–present\"               \"1903–1935\"                  \"1930–present\"               \"1926–1944\"                 \n [45] \"1993–present\"               \"1993–present\"               \"1998–present\"               \"1926–1944\"                 \n [49] \"1903–present\"               \"1903–1939\"                  \"1949–present\"               \"1949–present\"              \n [53] \"1993–present\"               \"1998–present\"               \"1990–present\"               \"1997–present\"              \n [57] \"1993–present\"               \"1926–1944\"                  \"1903–1949, 1958–present\"    \"1926–1944\"                 \n [61] \"1993–present\"               \"1990–2003\"                  \"1958\"                       \"1949–present\"              \n [65] \"1935–present\"               \"1998–present\"               \"1958–present\"               \"1926–1944\"                 \n [69] \"1949–present\"               \"1993–present\"               \"1990–present\"               \"1926–1958\"                 \n [73] \"1903–1958\"                  \"1949–1958\"                  \"1958–present\"               \"1958–1990\"                 \n [77] \"1903–present\"               \"1958–present\"               \"1993–present\"               \"1949–present\"              \n [81] \"1958–present\"               \"1958–present\"               \"1935–1958\"                  \"2017–present[5]\"           \n [85] \"2003–present\"               \"1999–present\"               \"1998–present\"               \"1903–1958\"                 \n [89] \"1903–circa 1910\"            \"1998–present\"               \"1926–1944\"                  \"1903–circa 1910, 1930–1990\"\n [93] \"1949–present\"               \"1903–1944\"                  \"1926–1944\"                  \"1998–present\"              \n [97] \"1990–present\"               \"1930–1949, 1958–present\"    \"1949–1958\"                  \"1993–present\"              \n[101] \"1949–1958\"                  \"1997–present\"               \"1926–1944\"                  \"1993–present\"              \n[105] \"1990–present\"               \"1997–present\"               \"1903–1930\"                  \"1949–1958\"                 \n[109] \"1949–present\"               \"1958–present\"               \"1949–1958\"                  \"1949–1999\"                 \n[113] \"1958–2003\"                  \"1930–present\"               \"1926–1944\"                  \"1926–1944\"                 \n[117] \"2003–present\"               \"1998–present\"               \"1903–present\"               \"1993–present\"              \n[121] \"1990–present\"               \"1958–present\"               \"1998–present\"               \"1903–present\"              \n[125] \"1958–present\"               \"1993–present\"               \"1998–present\"               \"1935–1958\"                 \n[129] \"1998–present\"               \"1993–present\"               \"1993–present\"               \"1949–present\"              \n[133] \"1926–1944\"                  \"1949–present\"               \"1958–present\"               \"1998–present\"              \n[137] \"1903–present\"               \"1903–present\"               \"1935–1944, 1958–present\"    \"1998–present\"              \n[141] \"1998–present\"               \"1993–present\"               \"1958–present\"               \"1903–1910\"                 \n[145] \"1958–present\"               \"1998–present\"               \"1903–present\"               \"1903–1944\"                 \n[149] \"1958–present\"               \"1998–present\"               \"1903–1990\"                  \"1998–present\"              \n[153] \"1903–circa 1910\"            \"1903–1944\"                  \"1953–present\"               \"1903–present\"              \n[157] \"1903–1915, 1958–present\"    \"1998–present\"               \"1903–present\"               \"1903–1910\"                 \n[161] \"1926–present\"               \"1958–1990\"                  \"1993–present\"               \"1903–present\"              \n[165] \"2021, 2022\"                 \"2021, 2022\"                 \"2021, 2022\"                 \"2021, 2022\"                \n\n\nThere are several problems I’m going to need to solve to get this into a regular form:\n\nThe “circa” text will have to be removed\nWhite spaces will need to be removed\nFootnote text will need to be removed\nThe word “present” will have to be replaced by “2022”\nThe single year “1958” will need to be replaced by an interval “1958-1958”\nThe comma separated list “2021,2022” will need to be an interval “2021-2022”\n\nHere’s a little bit of stringr code that does that:\n\ncrayola &lt;- crayola |&gt;\n  dplyr::mutate(\n    years = years |&gt;\n      stringr::str_remove_all(\" \") |&gt;\n      stringr::str_remove_all(\"\\\\[.\\\\]\") |&gt;\n      stringr::str_remove_all(\"circa\") |&gt;\n      stringr::str_replace_all(\"present\", \"2022\") |&gt;\n      stringr::str_replace_all(\"^1958$\", \"1958-1958\") |&gt;\n      stringr::str_replace_all(\"2021,2022\", \"2021-2022\"),\n  ) \n\nThe years column now has a regular form. Each entry is either a contiguous interval like \"1903-2022\", or a comma separated list of two such intervals like \"1903–1910,1930–1990\":\n\ncrayola$years\n\n  [1] \"1903–2022\"           \"1949–2022\"           \"1998–2022\"           \"1958–2022\"           \"1903–1935\"          \n  [6] \"1903–1935\"           \"1903–1910\"           \"1926–1944\"           \"1903–2022\"           \"1958–1990\"          \n [11] \"1997–2022\"           \"1958–2022\"           \"1903–1910\"           \"1903–1944\"           \"1903–1910\"          \n [16] \"1990–2022\"           \"1926–1944\"           \"1958–2022\"           \"1930–2022\"           \"1903–2022\"          \n [21] \"1993–2022\"           \"1926–1944,1949–1958\" \"2003–2022\"           \"1930–2022\"           \"1926–1944\"          \n [26] \"1998–2022\"           \"1903–1990\"           \"1958–1990\"           \"1903–2022\"           \"1990–2017\"          \n [31] \"1903–2022\"           \"1958–2022\"           \"1926–1944\"           \"1903–2022\"           \"1958–2022\"          \n [36] \"1926–1944\"           \"1998–2022\"           \"1903–1990\"           \"1926–1944\"           \"1926–1944\"          \n [41] \"2003–2022\"           \"1903–1935\"           \"1930–2022\"           \"1926–1944\"           \"1993–2022\"          \n [46] \"1993–2022\"           \"1998–2022\"           \"1926–1944\"           \"1903–2022\"           \"1903–1939\"          \n [51] \"1949–2022\"           \"1949–2022\"           \"1993–2022\"           \"1998–2022\"           \"1990–2022\"          \n [56] \"1997–2022\"           \"1993–2022\"           \"1926–1944\"           \"1903–1949,1958–2022\" \"1926–1944\"          \n [61] \"1993–2022\"           \"1990–2003\"           \"1958-1958\"           \"1949–2022\"           \"1935–2022\"          \n [66] \"1998–2022\"           \"1958–2022\"           \"1926–1944\"           \"1949–2022\"           \"1993–2022\"          \n [71] \"1990–2022\"           \"1926–1958\"           \"1903–1958\"           \"1949–1958\"           \"1958–2022\"          \n [76] \"1958–1990\"           \"1903–2022\"           \"1958–2022\"           \"1993–2022\"           \"1949–2022\"          \n [81] \"1958–2022\"           \"1958–2022\"           \"1935–1958\"           \"2017–2022\"           \"2003–2022\"          \n [86] \"1999–2022\"           \"1998–2022\"           \"1903–1958\"           \"1903–1910\"           \"1998–2022\"          \n [91] \"1926–1944\"           \"1903–1910,1930–1990\" \"1949–2022\"           \"1903–1944\"           \"1926–1944\"          \n [96] \"1998–2022\"           \"1990–2022\"           \"1930–1949,1958–2022\" \"1949–1958\"           \"1993–2022\"          \n[101] \"1949–1958\"           \"1997–2022\"           \"1926–1944\"           \"1993–2022\"           \"1990–2022\"          \n[106] \"1997–2022\"           \"1903–1930\"           \"1949–1958\"           \"1949–2022\"           \"1958–2022\"          \n[111] \"1949–1958\"           \"1949–1999\"           \"1958–2003\"           \"1930–2022\"           \"1926–1944\"          \n[116] \"1926–1944\"           \"2003–2022\"           \"1998–2022\"           \"1903–2022\"           \"1993–2022\"          \n[121] \"1990–2022\"           \"1958–2022\"           \"1998–2022\"           \"1903–2022\"           \"1958–2022\"          \n[126] \"1993–2022\"           \"1998–2022\"           \"1935–1958\"           \"1998–2022\"           \"1993–2022\"          \n[131] \"1993–2022\"           \"1949–2022\"           \"1926–1944\"           \"1949–2022\"           \"1958–2022\"          \n[136] \"1998–2022\"           \"1903–2022\"           \"1903–2022\"           \"1935–1944,1958–2022\" \"1998–2022\"          \n[141] \"1998–2022\"           \"1993–2022\"           \"1958–2022\"           \"1903–1910\"           \"1958–2022\"          \n[146] \"1998–2022\"           \"1903–2022\"           \"1903–1944\"           \"1958–2022\"           \"1998–2022\"          \n[151] \"1903–1990\"           \"1998–2022\"           \"1903–1910\"           \"1903–1944\"           \"1953–2022\"          \n[156] \"1903–2022\"           \"1903–1915,1958–2022\" \"1998–2022\"           \"1903–2022\"           \"1903–1910\"          \n[161] \"1926–2022\"           \"1958–1990\"           \"1993–2022\"           \"1903–2022\"           \"2021-2022\"          \n[166] \"2021-2022\"           \"2021-2022\"           \"2021-2022\"          \n\n\nThat’s better because the data format is now consistent, but it’s not tidy. In the long run, what I really want is a nice tidy tibble: each row should correspond to a single observation. If “Red” was a colour in 1935, then there should be a row in my table for which name = \"Red\" and year = 1935. That’s not quite what I have here, so I have more data wrangling to do and this time tidyr will be my best friend.\nThe first thing I’m going to do is use tidyr::separate() to split the years variable into two variables, years_1 and years_2. The years_1 variable will contain the first time interval for which a particular crayon colour was in production, and the years_2 variable will contain the second interval in which it was in production. For almost all colours, years_2 will be NA. It’s only those special cases like \"1903–1910,1930–1990\" that will have values in both.\nThen, because I don’t really see a need to have two variables that both represent a period of time, I’ll use tidyr::pivot_longer() to give myself a data set in which there is one row for every continuous time interval:\n\ncrayola &lt;- crayola |&gt;\n  tidyr::separate(\n    col = years,\n    into = c(\"years_1\", \"years_2\"),\n    sep = \",\",\n    fill = \"right\"\n  ) |&gt;\n  tidyr::pivot_longer(\n    cols = starts_with(\"years_\"),\n    names_prefix = \"years_\",\n    names_to = \"interval\",\n    values_to = \"years\"\n  ) |&gt;\n  dplyr::filter(!is.na(years))\n\nTo give you a sense of what the data looks like in this form, I’ll pull out the rows corresponding to two different crayon colours, “Maroon” and “Violet-Blue”:\n\ncrayola |&gt;\n  dplyr::filter(name == \"Maroon\" | name == \"Violet-Blue\")\n\n# A tibble: 3 × 10\n  color   name        listed  notes                                     x16_box x24_box x32_box x64_box interval years    \n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;                                     &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;    \n1 #C32148 Maroon      #C32148 \"Known as \\\"Dark Red\\\", 1949–1958.[2]\"    \"No\"    \"No\"    \"No\"    \"No\"    1        1949–2022\n2 #766EC8 Violet-Blue #766EC8 \"Known as \\\"Blue-Violet\\\", 1930–1958.[2]\" \"\"      \"\"      \"\"      \"\"      1        1903–1910\n3 #766EC8 Violet-Blue #766EC8 \"Known as \\\"Blue-Violet\\\", 1930–1958.[2]\" \"\"      \"\"      \"\"      \"\"      2        1930–1990\n\n\nThe “Maroon” crayon has been in production continuously since 1949, so there is only one row in the table for that one. The “Violet-Blue” crayon was in production from 1903 to 1910, and again from 1930 to 1990. These two production periods are each represented as a row.\nExcellent. Next, I’ll use separate() again to split the years interval into two columns, one for the year_started and another for the year_ended. Having done so, the year information is finally in a numeric format, so I can coerce it from character to integer:\n\ncrayola &lt;- crayola |&gt;\n  tidyr::separate(\n    col = years,\n    into = c(\"year_started\", \"year_ended\")\n  ) |&gt;\n  dplyr::mutate(\n    interval = as.integer(interval),\n    year_started = as.integer(year_started),\n    year_ended = as.integer(year_ended)\n  )\n\n\ncrayola\n\n# A tibble: 174 × 11\n   color   name                    listed    notes                      x16_box x24_box x32_box x64_box interval year_started year_ended\n   &lt;chr&gt;   &lt;chr&gt;                   &lt;chr&gt;     &lt;chr&gt;                      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;        &lt;int&gt;      &lt;int&gt;\n 1 #ED0A3F Red                     \"#ED0A3F\" \"\"                         \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"          1         1903       2022\n 2 #C32148 Maroon                  \"#C32148\" \"Known as \\\"Dark Red\\\", 1… \"No\"    \"No\"    \"No\"    \"No\"           1         1949       2022\n 3 #FD0E35 Scarlet                 \"#FD0E35\" \"Known as \\\"Torch Red\\\", … \"No\"    \"Yes\"   \"Yes\"   \"Yes\"          1         1998       2022\n 4 #C62D42 Brick Red               \"#C62D42\" \"\"                         \"No\"    \"No\"    \"No\"    \"Yes\"          1         1958       2022\n 5 #CC474B English Vermilion       \"\"        \"Also spelled \\\"Vermillio… \"\"      \"\"      \"\"      \"\"             1         1903       1935\n 6 #CC3336 Madder Lake             \"\"        \"\"                         \"\"      \"\"      \"\"      \"\"             1         1903       1935\n 7 #E12C2C Permanent Geranium Lake \"\"        \"\"                         \"\"      \"\"      \"\"      \"\"             1         1903       1910\n 8 #D92121 Maximum Red             \"\"        \"Part of the Munsell line… \"\"      \"\"      \"\"      \"\"             1         1926       1944\n 9 #B94E48 Chestnut                \"#B94E48\" \"Known as \\\"Indian Red\\\" … \"No\"    \"No\"    \"Yes\"   \"Yes\"          1         1903       2022\n10 #FF5349 Orange-Red              \"#FF5349\" \"\"                         \"\"      \"\"      \"\"      \"\"             1         1958       1990\n# ℹ 164 more rows\n\n\nWe’re getting close. At this point the last bit of work I have to do to fix the year data is unpack it. Instead of representing the data for Maroon crayons with one row with a year_started value of 1949 and year_ended value of 2022, I want to have a single column called year, and the data should contain one row for every year in which Maroon was in production. Somewhere in the back of my head there is the thought that there must be an easy way to do this with tidyr, but my ingenuity failed me this time and I fell back on my usual solution… purrr.\nIt’s a two-step process. Step one: write a little function that expects to receive the values stored in one row of the existing data frame, and returns a new data frame in the format I want. In this case, I want a tibble that has one row for each year in range starting year_started and ending year_ended, but otherwise has the same structure as the existing data. The unpack_row() function below does that:\n\nunpack_row &lt;- function(color, name, year_started, year_ended, ...) {\n  tibble::tibble(\n    name = name,\n    color = color,\n    year = year_started:year_ended,\n    ...\n  )\n}\n\nSo now I’ll do the unpacking with purrr::pmap_dfr(), sort the rows into a nice order using dplyr::arrange(), and add an id column to ensure that every row in the table has a unique identifier:\n\ncrayola &lt;- crayola |&gt;\n  purrr::pmap_dfr(unpack_row) |&gt;\n  dplyr::arrange(year, color) |&gt;\n  dplyr::mutate(id = dplyr::row_number())\n\nVoilà!\n\ncrayola\n\n# A tibble: 7,749 × 11\n   name                color    year listed    notes                                      x16_box x24_box x32_box x64_box interval    id\n   &lt;chr&gt;               &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;                                      &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;int&gt;\n 1 Black               #000000  1903 \"#000000\" \"\"                                         \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"          1     1\n 2 Midnight Blue       #003366  1903 \"#003366\" \"Known as \\\"Prussian Blue\\\", 1903–1958.[2… \"No\"    \"No\"    \"No\"    \"No\"           1     2\n 3 Pine Green          #01796F  1903 \"#01796F\" \"Known as \\\"Dark Chrome Green\\\" (\\\"Chrome… \"No\"    \"No\"    \"No\"    \"No\"           1     3\n 4 Green               #01A638  1903 \"#01A638\" \"\"                                         \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"          1     4\n 5 Blue (I)            #2EB4E6  1903 \"\"        \"Known as \\\"Celestial Blue\\\", 1935–1949, … \"\"      \"\"      \"\"      \"\"             1     5\n 6 Ultramarine Blue    #3F26BF  1903 \"\"        \"\"                                         \"\"      \"\"      \"\"      \"\"             1     6\n 7 Van Dyke Brown      #664228  1903 \"\"        \"Same color as \\\"Brown\\\" (1903–1910).[2]\"  \"\"      \"\"      \"\"      \"\"             1     7\n 8 Raw Umber           #665233  1903 \"#665233\" \"\"                                         \"\"      \"\"      \"\"      \"\"             1     8\n 9 Medium Chrome Green #6CA67C  1903 \"\"        \"\\\"Chrome Green, Medium\\\" on labels. Prod… \"\"      \"\"      \"\"      \"\"             1     9\n10 Celestial Blue      #7070CC  1903 \"\"        \"\"                                         \"\"      \"\"      \"\"      \"\"             1    10\n# ℹ 7,739 more rows"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iii-i-said-pet-i-said-luv-i-said-pet",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iii-i-said-pet-i-said-luv-i-said-pet",
    "title": "Crayola crayon colours",
    "section": "Act III: I said pet, I said LUV, I said pet…",
    "text": "Act III: I said pet, I said LUV, I said pet…\nOkay I have one last thing to do before I’m done with the data wrangling. Having hex strings associated with each crayon colour is nice and is exactly what I need for plotting, but you can’t construct a nice ordering of colours since colour space is three dimensional, more or less. I’m trying my very best to forget everything I ever learned about the psychophysics of human colour perception, but even so I’m not silly enough to try to work with raw RGB values. Instead I’ll use the colorspace package to convert extract hue/saturation/value coordinates, as well as my slightly-preferred method, CIELUV color coordinates:\n\nHSV &lt;- colorspace::coords(as(colorspace::hex2RGB(crayola$color), \"HSV\"))\nLUV &lt;- colorspace::coords(as(colorspace::hex2RGB(crayola$color), \"LUV\"))\n\nEach of these commands returns a matrix with three columns and the same number of rows as the crayola data frame. The first few rows of the HSV matrix look like this:\n\nHSV[1:10,]\n\n              H         S         V\n [1,]   0.00000 0.0000000 0.0000000\n [2,] 210.00000 1.0000000 0.4000000\n [3,] 175.00000 0.9917355 0.4745098\n [4,] 140.00000 0.9939759 0.6509804\n [5,] 196.30435 0.8000000 0.9019608\n [6,] 249.80392 0.8010471 0.7490196\n [7,]  25.16129 0.6078431 0.4000000\n [8,]  36.47059 0.5000000 0.4000000\n [9,] 136.55172 0.3493976 0.6509804\n[10,] 240.00000 0.4509804 0.8000000\n\n\nSo now I can store all six coordinates in the crayola dataframe, along with the LUV-space version of “hue” which I compute in the last line here:\n\ncrayola &lt;- crayola |&gt;\n  dplyr::mutate(\n    hue = HSV[, \"H\"],\n    sat = HSV[, \"S\"],\n    val = HSV[, \"V\"],\n    L = LUV[, \"L\"],\n    U = LUV[, \"U\"],\n    V = LUV[, \"V\"],\n    hue2 = atan2(V, U)\n  )\n\nThe Wikipedia page on CIELUV has a little more information on this, but really the thing that matters is that the hue2 column is the variable I’m going to use to arrange the crayon colours when plotting them later. And so the moment arrives that, at long last, I have the data…\n\ncrayola\n\n# A tibble: 7,749 × 18\n   name            color  year listed notes x16_box x24_box x32_box x64_box interval    id   hue   sat   val     L      U       V   hue2\n   &lt;chr&gt;           &lt;chr&gt; &lt;int&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 Black           #000…  1903 \"#000… \"\"    \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"          1     1   0   0     0       0     0      0     -3.14 \n 2 Midnight Blue   #003…  1903 \"#003… \"Kno… \"No\"    \"No\"    \"No\"    \"No\"           1     2 210   1     0.4    21.3 -11.9  -40.1   -1.86 \n 3 Pine Green      #017…  1903 \"#017… \"Kno… \"No\"    \"No\"    \"No\"    \"No\"           1     3 175   0.992 0.475  45.4 -36.1    0.638  3.12 \n 4 Green           #01A…  1903 \"#01A… \"\"    \"Yes\"   \"Yes\"   \"Yes\"   \"Yes\"          1     4 140   0.994 0.651  59.5 -54.9   61.6    2.30 \n 5 Blue (I)        #2EB…  1903 \"\"     \"Kno… \"\"      \"\"      \"\"      \"\"             1     5 196.  0.8   0.902  68.7 -44.6  -53.4   -2.27 \n 6 Ultramarine Bl… #3F2…  1903 \"\"     \"\"    \"\"      \"\"      \"\"      \"\"             1     6 250.  0.801 0.749  29.9  -2.77 -96.9   -1.60 \n 7 Van Dyke Brown  #664…  1903 \"\"     \"Sam… \"\"      \"\"      \"\"      \"\"             1     7  25.2 0.608 0.4    31.5  25.9   19.9    0.655\n 8 Raw Umber       #665…  1903 \"#665… \"\"    \"\"      \"\"      \"\"      \"\"             1     8  36.5 0.5   0.4    36.2  14.7   22.3    0.987\n 9 Medium Chrome … #6CA…  1903 \"\"     \"\\\"C… \"\"      \"\"      \"\"      \"\"             1     9 137.  0.349 0.651  63.3 -28.3   26.3    2.39 \n10 Celestial Blue  #707…  1903 \"\"     \"\"    \"\"      \"\"      \"\"      \"\"             1    10 240   0.451 0.8    51.1  -5.49 -76.1   -1.64 \n# ℹ 7,739 more rows"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#the-intermission-is-late-so-what",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#the-intermission-is-late-so-what",
    "title": "Crayola crayon colours",
    "section": "The intermission is late, so what?",
    "text": "The intermission is late, so what?\nAs a rule I make it a habit to assume that, while the internet never forgets anything at all in the general sense, it doesn’t remember anything specific for any interval longer than I’d trust a man not to have conveniently lost my phone number. Moreover, while I also make it a habit not to care greatly about the archival properties of What Went Down In The Stalls At The Duke On Friday, I’m slightly more invested in, um … crayons, I guess. Or data science practices in R. Whatever.\nMy point here is that the table on wikipedia isn’t an archival source so there’s no guarantee that anything I’ve done up to this point is reproducible unless I do a bit of extra work myself and save a copy of the data…\n\nfolder &lt;- here::here(\"posts\", \"2022-12-18_crayola-crayon-colours\")\nreadr::write_csv(crayola, fs::path(folder, \"crayola.csv\"))\n\nAs a convenient side benefit, you can download a copy of the crayola colours data as a csv file from github should you feel so inclined. There’s even a script containing most of the code for this post too :-)"
  },
  {
    "objectID": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iv-not-even-chekhov-expects-it-to-go-off",
    "href": "posts/2022-12-18_crayola-crayon-colours/index.html#act-iv-not-even-chekhov-expects-it-to-go-off",
    "title": "Crayola crayon colours",
    "section": "Act IV: Not even Chekhov expects it to go off",
    "text": "Act IV: Not even Chekhov expects it to go off\nIn any case, the final act of our little drama has arrived at last. Time to make a plot. The drums roll and the audience holds their breath in antici–\nOh let’s just get on with it and load ggplot2 already. Here’s a stacked bar chart showing the number of distinct crayon colours in the Crayola standard set every year from 1903 to 2022. It has grown over time but the growth looks linear, not exponential:\n\nlibrary(ggplot2)\nbase &lt;- crayola |&gt; \n  dplyr::mutate(\n    color = forcats::fct_reorder(color, hue2)\n  ) |&gt; \n  ggplot(aes(\n    x = year,\n    group = color,\n    fill = color\n  )) +\n  scale_fill_identity() +\n  NULL\n\nbase + geom_bar(show.legend = FALSE) \n\n\n\n\n\n\n\n\nBut let’s be honest, shall we? No-one at all (least of all me) is interested in determining whether the rate of growth of Crayola crayon colours in the standard set is exponential or linear. It’s just fun. The real reason we all love the Crayola post was that the image was so terribly pretty, so let’s start making something pretty, yes?\nWe can start getting something a little closer to the original if we set position = \"fill\", and I’m going to use theme_void() because honestly it’s just prettier without the words and numbers getting in the way…\n\nbase + \n  theme_void() +\n  geom_bar(\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nIt’s pretty, but it’s noticeably different from the original one. In my version, there are 39 separate colours depicted on the left hand side, whereas the version that was going around on mastodon (and appears in the original blog posts) has only 8. Out of paranoia, I decided to check the counts in my data…\n\ncrayola |&gt; \n  dplyr::count(year) |&gt;\n  dplyr::filter(year %in% seq(1903, 2003, by = 10))\n\n# A tibble: 11 × 2\n    year     n\n   &lt;int&gt; &lt;int&gt;\n 1  1903    40\n 2  1913    32\n 3  1923    31\n 4  1933    57\n 5  1943    57\n 6  1953    49\n 7  1963    64\n 8  1973    64\n 9  1983    64\n10  1993    80\n11  2003   108\n\n\n…which does seem consistent with what the history of Crayola crayons wikipedia article has to say on the topic too:\n\nEarly Crayola advertising mentions thirty different colors, although there is no official list; in fact thirty-eight different crayons are known from Crayola boxes of this period. The largest labeled assortment was box No. 51, titled Crayola Young Artists’ Drawing Crayons, which included twenty-eight different crayons. Other colors were found in different boxes, including the “Rubens” No. 500, a twenty-four crayon assortment.\n\nOkay, so if I haven’t made a mistake, what is going on? It turns out that although Steven Von Worley’s blog post still manages to point to the “same” Wikipedia page 12 years later, the contents of the page have changed considerably. The original post was published January 15th 2010. Conveniently the wayback machine has a snapshot of that page from only a few weeks later, on February 9th. I have a very different version of the Crayola colours data than Steven’s friend Velociraptor had.\nThere are 133 colours listed in the 2010 version, and it’s missing all the colours that had gone out of production earlier than 1990. “English Vermillion”, for example, was in production from 1903 to 1935. It appears in the 2022 version of the Wikipedia data (and so it’s represented in my plots above), but it was omitted in the 2010 version of the Wikipedia data and so doesn’t appear in the version of the image that went around on Mastodon yesterday.\nHm. So what happens to my data if I crudely simulate a censoring process a little bit like the one that applied to the 2010 version of the Wikipedia page? Let’s simply ignore all crayon colours that were out of production earlier than 1990, shall we?\n\nbase + \n  theme_void() +\n  geom_bar(\n    data =  crayola |&gt; \n      dplyr::group_by(name) |&gt;\n      dplyr::filter(max(year) &gt; 1990) |&gt;\n      dplyr::ungroup() |&gt;\n      dplyr::mutate(color = forcats::fct_reorder(color, hue2)),\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nStill not exactly equivalent, but it’s closer.\nMore to the point however, I think we all have to admit that this is really the kind of data set that desperately needs to be plotted with the aid of coord_polar(), no?\n\nbase + \n  theme_void() +\n  coord_polar(theta = \"y\") + \n  geom_bar(\n    position = \"fill\",\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nI mean… not that anyone really cares what this represents, but each concentric ring represents a year of data: 1903 is in the middle, and 2022 is on the outside. Within each annual ring there is an equal-angle segment for every crayon colour in the data for that year. Whenever the crayons changed, the rings shift a bit. The colours are arranged around the circle by hue. Well, the hue-like quantity computed from the chromaticity components of the LUV coordinates. Whatever.\nAlternatively, we could turn this into a racetrack plot, where each individual colour fills an angular section of constant size, and so only the year with the most crayon colours (2022) wraps the full way round the circle. That gives us this:\n\nbase + \n  theme_void() +\n  coord_polar(theta = \"y\") + \n  geom_bar(\n    linetype = \"blank\",\n    width = 1,\n    show.legend = FALSE\n  ) \n\n\n\n\n\n\n\n\nI’m not sure it has a lot of value as a data visualisation but it is so very pretty!"
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html",
    "href": "posts/2023-06-10_pop-pk-models/index.html",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "",
    "text": "Note: This is the third post in a series on pharmacometric modelling, and implicitly assumes some knowledge of the concepts outlined in the first two:\nIn the earlier posts I didn’t touch any real data sets, and only considered an idealised case where you have data from only a single person and therefore have no need for population models. I’ll address both of those shortcomings in this post."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#prologue",
    "href": "posts/2023-06-10_pop-pk-models/index.html#prologue",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Prologue",
    "text": "Prologue\nAll the way back in 2006 I published a paper in the Journal of Mathematical Psychology called “Modelling individual differences with Dirichlet processes” with a bunch of no-name coauthors.1 In brutal honesty the paper should probably have been called “Everything Danielle knows about Dirichlet processes, with a fig leaf of pretense of applying it to data”, but it did allow me to use the following utterly unhinged quote from an anonymous reviewer on an earlier paper:\n\nI am surprised that the author has used this data set. In my lab, when we collect data with such large individual differences, we refer to the data as “junk”. We then re-design our stimuli and/or experimental procedures, and run a new experiment. The junk data never appear in publications\n\nThis attitude, which I have encountered from time to time in research, utterly puzzles me. One of the most salient things about human thought and behaviour is that… well, people think and act differently to each other. If you want to understand the human mind, you kinda have to engage with that fact and develop experiments and modelling techniques that can accommodate this variation. I expressed the idea schematically in Figure 2 of the paper:\n\n\n\n\n\nI am still quite fond of this figure. It nicely captures the idea that when developing population models we are usually operating in three spaces at once:\n\nAt the data level we have observable quantities that we can measure in our experiments. Each person in our study can have different data values, and people can differ from each other in all kinds of ways\nAt the individual level we seek to infer the parameters of a model that can summarise the observations from a single person in a theoretically meaningful way, and account for how those observations might vary as a function of covariates.\nAt the population level we seek to infer parameters that describe the typical parameter values associated with individuals, and how those parameter values typically vary across individuals.\n\nShockingly, it turns out that this basic framework is not even slightly specific to psychology. It’s something that shows up in almost every scientific domain, and there are standard statistical tools that we use to capture this idea that are surprisingly similar to one another even when the domain of application differs.\nAnd so it transpires that, as I continue the process of teaching myself pharmacometrics, I discover that the modelling tools used in population pharmacokinetics (pop-PK) are shockingly familiar to me. So let’s dive in, shall we?"
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#some-resources",
    "href": "posts/2023-06-10_pop-pk-models/index.html#some-resources",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Some resources",
    "text": "Some resources\nTo make the transition from modelling toy data sets to dealing with real ones, it helps a lot to have some real data to work with, and a worked example that shows how real data are analysed in practice. To that end, I’ve been relying on resources made publicly available by the Population Approach Group of Australia and New Zealand, who have a series of handy tutorials for folks interested in pharmacometric modelling. The one I’m working through at the moment is a 2019 workshop on pop-PK models (link goes to a zip file), which provides a very nice tutorial on building such models.\nThe only catch, from my perspective, is that the tutorial uses the NONMEM pharmacometrics software, and I don’t have access to NONMEM. So as a fun little exercise, I’m going to translate some of the code from the workshop from NONMEM to Stan (and R)."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#the-warfarin-data-set",
    "href": "posts/2023-06-10_pop-pk-models/index.html#the-warfarin-data-set",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "The warfarin data set",
    "text": "The warfarin data set\nThe data set used in the tutorial is based on some old studies in the 1960s on the pharmacokinetics of warfarin, an anticoagulant medication that is probably well-known to most people (I mean, even I know what warfarin is). The data is provided as a csv file called “warfpk.csv”, so my first step will be to import the data into R so that I can work with it.\n\nParsing the data\nReading the data into R using readr::read_csv() turns out to be mostly straightforward. Data are delimited with commas and there’s very little weirdness to deal with. The only thing that is non-standard for an R user is that the data file uses \".\" to specify missing values (which I’m guessing is standard in pharmacometrics), so I’ll need to state that explicitly when reading the data into R to ensure that numeric variables with missing values are correctly parsed as numeric:\n\nwarfpk &lt;- readr::read_csv(\"warfpk.csv\", na = \".\", show_col_types = FALSE)\nwarfpk\n\n# A tibble: 289 × 10\n   `#ID`  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0       0    66.7    50     1   100    -2     0  NA       1\n 2 0       0.5  66.7    50     1    NA    NA     1   0       0\n 3 0       1    66.7    50     1    NA    NA     1   1.9     0\n 4 0       2    66.7    50     1    NA    NA     1   3.3     0\n 5 0       3    66.7    50     1    NA    NA     1   6.6     0\n 6 0       6    66.7    50     1    NA    NA     1   9.1     0\n 7 0       9    66.7    50     1    NA    NA     1  10.8     0\n 8 0      12    66.7    50     1    NA    NA     1   8.6     0\n 9 0      24    66.7    50     1    NA    NA     1   5.6     0\n10 0      36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n\n\nThese column names are pretty standard in the field, in part because standard software like NONMEM expects these names. They’re perfectly fine for R too, with one minor exception: I’m going to rename the id variable using dplyr::rename():\n\nwarfpk &lt;- warfpk |&gt; dplyr::rename(id = `#ID`)\nwarfpk\n\n# A tibble: 289 × 10\n   id     time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 0       0    66.7    50     1   100    -2     0  NA       1\n 2 0       0.5  66.7    50     1    NA    NA     1   0       0\n 3 0       1    66.7    50     1    NA    NA     1   1.9     0\n 4 0       2    66.7    50     1    NA    NA     1   3.3     0\n 5 0       3    66.7    50     1    NA    NA     1   6.6     0\n 6 0       6    66.7    50     1    NA    NA     1   9.1     0\n 7 0       9    66.7    50     1    NA    NA     1  10.8     0\n 8 0      12    66.7    50     1    NA    NA     1   8.6     0\n 9 0      24    66.7    50     1    NA    NA     1   5.6     0\n10 0      36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n\n\nLooking at this output more carefully, there’s one slightly puzzling thing here: the id column looks like it’s supposed to be a numeric id for the study participants, but R has parsed as a character vector. So there must be one non-numeric value in this column. I’d better find out what’s going on there. A bit of digging reveals there’s something peculiar going on with subject 12. Using dplyr::filter() to extract the data for that person we get this:\n\nwarfpk |&gt; dplyr::filter(id |&gt; stringr::str_detect(\"12\"))\n\n# A tibble: 11 × 10\n   id     time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 12      0    75.3    32     1   113    -2     0  NA       1\n 2 12      1.5  75.3    32     1    NA    NA     1   0.6     0\n 3 #12     3    75.3    32     1    NA    NA     1   2.8     0\n 4 12      6    75.3    32     1    NA    NA     1  13.8     0\n 5 12      9    75.3    32     1    NA    NA     1  15       0\n 6 12     24    75.3    32     1    NA    NA     1  10.5     0\n 7 12     36    75.3    32     1    NA    NA     1   9.1     0\n 8 12     48    75.3    32     1    NA    NA     1   6.6     0\n 9 12     72    75.3    32     1    NA    NA     1   4.9     0\n10 12     96    75.3    32     1    NA    NA     1   2.4     0\n11 12    120    75.3    32     1    NA    NA     1   1.9     0\n\n\nMy first thought upon seeing this was that it must be a typo in the data file. No problem, it’s easy enough to remove the # character and convert the id variable to numeric:\n\nwarfpk &lt;- warfpk |&gt; \n  dplyr::mutate(\n    id = id |&gt; \n      stringr::str_remove_all(\"#\") |&gt; \n      as.numeric()\n  )\nwarfpk\n\n# A tibble: 289 × 10\n      id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     0   0    66.7    50     1   100    -2     0  NA       1\n 2     0   0.5  66.7    50     1    NA    NA     1   0       0\n 3     0   1    66.7    50     1    NA    NA     1   1.9     0\n 4     0   2    66.7    50     1    NA    NA     1   3.3     0\n 5     0   3    66.7    50     1    NA    NA     1   6.6     0\n 6     0   6    66.7    50     1    NA    NA     1   9.1     0\n 7     0   9    66.7    50     1    NA    NA     1  10.8     0\n 8     0  12    66.7    50     1    NA    NA     1   8.6     0\n 9     0  24    66.7    50     1    NA    NA     1   5.6     0\n10     0  36    66.7    50     1    NA    NA     1   4       0\n# ℹ 279 more rows\n\n\nFor the purposes of this post I am going to run with this version of the data, but later on it’s going to turn out that the data from participant 12 is the least well-fit by the model, which is a hint that this might have been deliberate. In fact, as I started doing some more digging into NONMEM and learned how to decipher a NONMEM input file, I discovered that the # character appears to be serving a specific function when used as a prefix in this data file. Per this line of the input (see later), it’s being used as an instruction to tell NONMEM to ignore the observation:\n$DATA ..\\warfpk.csv IGNORE=#\nWhat I’m guessing here is that this observation was dropped from the data set in the tutorial for some reason. It’s not obvious to me what the specific reason was for omitting that observation, and it’s possible I should also be filtering out that case, but I’m not going to worry about that for the purposes of this blog post.\n\n\nInterpreting the data\nNow that I have the data, I need to make sense of it. The csv file itself doesn’t give much information about the variables, but when digging into the NONMEM input files included with the workshop materials I found the citations to the original papers from whence the data came. The data originate in papers by O’Reilly and colleagues, published in 1963 and 1968. Both papers are available online in full text, and after reading through them, we can reverse engineer (most of!) a data dictionary:\n\nid: Numeric value specifying the arbitrary identifier for each person\ntime: Time elapsed since dose was administered (in hours)\nwt: Weight of each person (in kilograms)\nage: Age of each person (in years)\nsex: Biological sex of each person (0 = female, 1 = male)2\namt: Dose administered to this person at this time point (in milligrams)\nrate: Uncertain what this refers to, but it has value -2 when drug is administered and missing otherwise\ndvid: Appears to be a dummy variable indicating whether the dependent variable was measured at this time point (0 = false, 1 = true)\ndv: Measured value of the dependent variable (plasma warfarin concentration, in mg/L)\nmdv: Appears to be a dummy variable that is the reverse of dvid, and is presumably an indicator variable whose meaning is “missing dependent variable” (0 = false, 1 = true)\n\nOne peculiarity of the data structure that appears to be quite standard in pharmacokinetics is that the data frame incorporates both measurement events where the drug concentration is measured, and dosing events where a dose of the drug is administered. It’s a perfectly sensible way to organise the data, but later on it will be convenient to separate them in order to pass the data to Stan in a format that it expects. To get a sense of what the dosing events look like, we can extract the relevant subset of the data frame by filtering the data on dvid:\n\nwarfpk |&gt; dplyr::filter(dvid == 0)\n\n# A tibble: 32 × 10\n      id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     0     0  66.7    50     1   100    -2     0    NA     1\n 2     1     0  66.7    50     1   100    -2     0    NA     1\n 3     2     0  66.7    31     1   100    -2     0    NA     1\n 4     3     0  80      40     1   120    -2     0    NA     1\n 5     4     0  40      46     0    60    -2     0    NA     1\n 6     5     0  75.3    43     1   113    -2     0    NA     1\n 7     6     0  60      36     0    90    -2     0    NA     1\n 8     7     0  90      41     1   135    -2     0    NA     1\n 9     8     0  50      27     0    75    -2     0    NA     1\n10     9     0  70      28     1   105    -2     0    NA     1\n# ℹ 22 more rows\n\n\nNotably, not everyone is given the same dose, and in an utterly unsurprising turn of events it turns out that the dose is calculated based on weight (which is in turn, I’m told, a proxy for the volume of distribution in systemic circulation):\n\nlibrary(ggplot2)\nwarfpk |&gt; \n  dplyr::filter(dvid == 0) |&gt;\n  ggplot(aes(wt, amt, colour = factor(sex))) + \n  geom_point(size = 4, show.legend = FALSE) +\n  theme_bw() + \n  labs(x = \"Weight (kg)\", y = \"Dose (mg)\")\n\n\n\n\n\n\n\n\nOkay, that all makes sense.\nNext, let’s take a look at the data from a single subject. The observations in the warfpk data set appear to aggregate data from multiple studies, with the consequence that different subjects can have different measurement schedules. Here’s participant 1, for instance:\n\nwarfpk |&gt; dplyr::filter(id == 1 & dvid == 1)\n\n# A tibble: 6 × 10\n     id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1    24  66.7    50     1    NA    NA     1   9.2     0\n2     1    36  66.7    50     1    NA    NA     1   8.5     0\n3     1    48  66.7    50     1    NA    NA     1   6.4     0\n4     1    72  66.7    50     1    NA    NA     1   4.8     0\n5     1    96  66.7    50     1    NA    NA     1   3.1     0\n6     1   120  66.7    50     1    NA    NA     1   2.5     0\n\n\nA lot of the people in the data set have measurements taken on this schedule, but not everyone does. For comparison purposes, here’s participant 2:\n\nwarfpk |&gt; dplyr::filter(id == 2 & dvid == 1)\n\n# A tibble: 11 × 10\n      id  time    wt   age   sex   amt  rate  dvid    dv   mdv\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     2   0.5  66.7    31     1    NA    NA     1   0       0\n 2     2   2    66.7    31     1    NA    NA     1   8.4     0\n 3     2   3    66.7    31     1    NA    NA     1   9.7     0\n 4     2   6    66.7    31     1    NA    NA     1   9.8     0\n 5     2  12    66.7    31     1    NA    NA     1  11       0\n 6     2  24    66.7    31     1    NA    NA     1   8.3     0\n 7     2  36    66.7    31     1    NA    NA     1   7.7     0\n 8     2  48    66.7    31     1    NA    NA     1   6.3     0\n 9     2  72    66.7    31     1    NA    NA     1   4.1     0\n10     2  96    66.7    31     1    NA    NA     1   3       0\n11     2 120    66.7    31     1    NA    NA     1   1.4     0\n\n\nThis has implications for our model structure: we can’t assume a single set of measurement times that will be identical for all subjects. From a Stan point of view, this means that if we try to pass the observed plasma concentrations as an array of vectors (one per person), it won’t work. Since the vectors can have different lengths that data structure would be a ragged array, and Stan doesn’t allow those. Instead, we’re going to have to pass all the observations as one long vector and use indexing vectors to specify the “breakpoints” that indicate when the data from each person starts and stops."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#why-we-need-population-models",
    "href": "posts/2023-06-10_pop-pk-models/index.html#why-we-need-population-models",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Why we need population models",
    "text": "Why we need population models\nNow that we have a sense of the structure of the data, we can start drawing some plots designed to tell us what’s going on. I’ll start with a very naive kind of visualisation. Here’s a scatterplot of time versus measured drug concentration that doesn’t give you any indication about which observations belong to which person.\n\nwarfpk |&gt; \n  dplyr::filter(\n    dvid == 1, # only include measured times\n    !is.na(dv) # ignore missing dv cases\n  ) |&gt;\n  ggplot(aes(x = time, y = dv, group = factor(id))) + \n  geom_point() +\n  theme_bw() + \n  labs(\n    x = \"Time since dose (hours)\", \n    y = \"Warfarin plasma concentration (mg/L)\"\n  )\n\n\n\n\n\n\n\n\nLooking at the data like this gives you the false impression that the data set is rather noisy.3 To see why this is misleading, let’s add some lines that connect data from the same person:\n\nwarfpk |&gt; dplyr::filter(dvid == 1, !is.na(dv)) |&gt;\n  ggplot(aes(x = time, y = dv, group = factor(id))) + \n  geom_line(colour = \"grey80\") +\n  geom_point() +\n  theme_bw() + \n  labs(\n    x = \"Time since dose (hours)\", \n    y = \"Warfarin plasma concentration (mg/L)\"\n  )\n\n\n\n\n\n\n\n\nThe pattern of lines hints that a lot of this “noise” is not in fact noise, it’s systematic variation across people. This becomes more obvious when we disaggregate the data further and plot the observations from each person in a separate panel:\n\nwarfpk |&gt; dplyr::filter(dvid == 1, !is.na(dv)) |&gt;\n  ggplot(aes(x = time, y = dv, colour = factor(id))) + \n  geom_line(show.legend = FALSE) +\n  geom_point(size = 4, show.legend = FALSE) +\n  facet_wrap(~ factor(id), ncol = 8) +\n  theme_bw() + \n  labs(\n    x = \"Time since dose (hours)\", \n    y = \"Warfarin plasma concentration (mg/L)\"\n  )\n\n\n\n\n\n\n\n\nWhen plotted this way, it’s really clear that the data from each person is in fact quite precise. There’s very little noise in any of these individual-subject plots. They’re all very smooth looking curves: it just so happens that each person is unique and has their own curve. In other words, the vast majority of the variation is systematic difference across people: it’s not measurement error.\nWhen data have this structure, you really (really really really really) need to adopt a statistical approach that accommodates individual differences. You need a pop-PK model.4"
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#deciphering-nonmem-specifications",
    "href": "posts/2023-06-10_pop-pk-models/index.html#deciphering-nonmem-specifications",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Deciphering NONMEM specifications",
    "text": "Deciphering NONMEM specifications\nOkay. Time for a digression, sort of. My primary goal in this post is to implement a pop-PK model from scratch in Stan, using material from a workshop on NONMEM as my launching point. There’s a twin motivation here. On the one hand, I want to extend the Stan code from my last pharmacometrics post so that it can handle pop-PK models. But on the other hand, I also want to familiarise myself with the nomenclature used in the field, which derives heavily from notation used in NONMEM. To that end, I’m going to start trying to write my model using NONMEM-style variable names, and in the process familiarise myself with how NONMEM input files are structured.\nReading NONMEM code takes a bit of effort if, like me, you’re not used to it. Decoding a NONMEM model specification requires you to understand the convention used to describe models, and to understand the syntax used in the input files. I’ll start with the notation.\n\nNotation from NONMEM\nTo make sense of the statistical notation used in NONMEM specifications, I relied quite heavily on the two NONMEM tutorial papers by Bauer (2019) were helpful for me, and an older paper by Bauer et al (2007) that is a little more explicit about the statistical formulation of the models. I’m not 100% certain I’ve done the decoding correctly, but as far as I can tell the following syntactic conventions are very often used:\n\nItalicised lower case Greek symbols refer to scalar parameters: \\(\\theta\\), \\(\\omega\\), \\(\\sigma\\), etc\nBoldfaced upper case Greek symbols denote parameter vectors: \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\omega\\), \\(\\boldsymbol\\sigma\\), etc\nBoldfaced upper case Greek symbols denote parameter matrices: \\(\\boldsymbol\\Theta\\), \\(\\boldsymbol\\Omega\\), \\(\\boldsymbol\\Sigma\\), etc\n\nThere is also a convention assigning meaning to the different Greek letters:\n\nPopulation mean5 parameters are denoted with thetas (\\(\\theta\\), \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\Theta\\))\nPopulation standard deviation6 parameters are denoted (\\(\\omega\\), \\(\\boldsymbol\\omega\\), \\(\\boldsymbol\\Omega\\))\nIndividual departures from population mean are denoted with etas (\\(\\eta\\), \\(\\boldsymbol\\eta\\))\nStandard deviation of measurement error terms is denoted with sigmas (\\(\\sigma\\), \\(\\boldsymbol\\sigma\\), \\(\\boldsymbol\\Sigma\\))\nDifference between individual subject expected values and observation are denoted with epsilons, \\(\\epsilon\\)\n\nTo help myself keep the notation straight, I’ll consider a simple one-compartment intravenous bolus model with first-order elimination, because unlike other models that one has a nice clean analytical closed form for the pharmacokinetic function. If we let \\(f(t, k, V, D)\\) denote the pharmacokinetic function that describes how drug concentration changes as a function of time \\(t\\), dose \\(D\\), elimination rate constant7 \\(k\\), and volume of distribution \\(V\\), this model asserts that the blood plasma drug concentration decays exponentially with time:\n\\[\nf(t, k, V, D) = \\frac{D}{V} \\exp(-kt)\n\\]\nIn this model, the measurement time \\(t\\) and dose \\(D\\) (assumed to be administered at time \\(t = 0\\)) are both part of the study design. The other two quantities, \\(k\\) and \\(V\\), are model parameters that can be different for every person. Because we’re allowing for the possibility that the parameters can vary from person to person, we’ll need notation to describe this variation. At a population level, we have a parameter vector \\(\\boldsymbol{\\theta} = (\\theta_1, \\theta_2)\\) where – somewhat arbitrarily – I’ll say that \\(\\theta_1\\) denotes the typical value for the elimination rate constant \\(k\\), and \\(\\theta_2\\) is the typical value for the volume of distribution \\(V\\). Since these quantities can vary from person to person, we would also – assuming for the sake of simplicity that there is no population correlation between them8 – have a scale vector \\(\\boldsymbol{\\omega} = (\\omega_1, \\omega_2)\\).\nIn this scenario, then, the parameters for the i-th participant would be some function of the typical values \\(\\boldsymbol\\theta\\) and the random effects \\(\\boldsymbol\\eta_i\\) associated with that person. I’m not certain if there’s a standard notation used to describe these transformation functions, so I’ll just use \\(g_1()\\) and \\(g_2()\\) to denote these:\n\\[\n\\begin{array}{rcl}\nk_i &=& g_1(\\theta_1, \\eta_{i1}) \\\\\nV_i &=& g_2(\\theta_2, \\eta_{i2})\n\\end{array}\n\\]\nWhen specifying the statistical model, it’s conventional to assume that the random effect terms \\(\\eta_{ik}\\) are normally distributed with mean zero and standard deviation \\(\\omega_k\\):\n\\[\n\\eta_{ik} \\sim \\mbox{Normal}(0, \\omega_k)\n\\]\nNext, let’s consider the notation for the pharmacokinetic function \\(f()\\) itself. Earlier I wrote out the specific form of this function for a particular model, but since the precise form is different from model to model, we could refer to it generically as \\(f(t, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i)\\).\nIf measurement errors are assumed to be additive – they don’t have to be, but I’m only considering additive error models in this post – the observed concentration \\(y_{ij}\\) for the i-th person at the j-th time point \\(t_{ij}\\) can be expressed as follows:\n\\[\ny_{ij} = f(t_{ij}, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i) + \\epsilon_{ij}\n\\]\nIn this expression, \\(\\epsilon_{ij}\\) is the error associated with person i and time j, and we presume these errors are normally distributed with mean zero and standard deviation \\(\\sigma\\):\n\\[\n\\epsilon_{ik} \\sim \\mbox{Normal}(0, \\sigma)\n\\]\nWith that as preliminary notational exposition, I think I can now make sense of how the model specification in NONMEM is expressed. So let’s turn to that now…\n\n\nReading a NONMEM control file\nOne thing that I really appreciated when going through the tutorial materials is that they include actual NONMEM model specifications that workshop participants can play with. If you unfold the code below, you can see the complete NONMEM control file (which uses a .ctl file extension) for a NONMEM model of the warfarin data:\n\n\nThe complete NONMEM control file\n;O'REILLY RA, AGGELER PM. STUDIES ON COUMARIN ANTICOAGULANT DRUGS\n;INITIATION OF WARFARIN THERAPY WITHOUT A LOADING DOSE.\n;CIRCULATION 1968;38:169-177\n;\n;O'REILLY RA, AGGELER PM, LEONG LS. STUDIES OF THE COUMARIN ANTICOAGULANT\n;DRUGS: THE PHARMACODYNAMICS OF WARFARIN IN MAN.\n;JOURNAL OF CLINICAL INVESTIGATION 1963;42(10):1542-1551\n;\n\n$PROB WARFARIN PK\n$INPUT ID TIME WT AGE SEX AMT RATX DVID DV MDV\n$DATA ..\\warfpk.csv IGNORE=#\n\n$SUBR ADVAN2 TRANS2\n\n$PK\n\n   ; COVARIATE MODEL\n   TVCL=THETA(1)\n   TVV=THETA(2)\n   TVKA=THETA(3)\n\n   ; MODEL FOR RANDOM BETWEEN SUBJECT VARIABILITY\n   CL=TVCL*EXP(ETA(1))\n   V=TVV*EXP(ETA(2))\n   KA=TVKA*EXP(ETA(3))\n\n   ; SCALE CONCENTRATIONS\n   S2=V\n\n$ERROR\n   Y=F+EPS(1)\n   IPRED=F\n\n$THETA\n   (0.01,0.1)   ; POP_CL\n   (0.01,8)     ; POP_V\n   (0.01,0.362) ; POP_KA\n\n$OMEGA\n   0.1 ; PPV_CL\n   0.1 ; PPV_V\n   0.1 ; PPV_KA\n\n$SIGMA \n   0.1 ; RUV_ADD\n\n$EST MAX=9990 SIG=3 PRINT=1 METHOD=COND INTER\n$COV\n\n$TABLE ID TIME DVID Y \nONEHEADER NOPRINT FILE=warf.fit\n\n$TABLE ID KA CL V WT SEX AGE\nONEHEADER NOAPPEND NOPRINT FILE=warf.fit\n\n\nThe control file requires a bit of effort for me – as someone who doesn’t use NONMEM – to work out the structure of the underlying model, but after a little bit of work it wasn’t too hard. Unlike Stan, which is a general-purpose probabilistic programming model for Bayesian analysis, NONMEM is a specific tool that hardcodes particular models of interest to pharmacometricians. The consequence of this is that the control file doesn’t spell out all the details of each model: the user just refers to them by name. You can work out which model is used by looking at the line in the control file that specifies which subroutine is used:\n$SUBR ADVAN2 TRANS2\nOn my own I would have no way to make sense of this without doing a deep dive into the NONMEM user manuals, but happily the workshop notes helpfully explain this. In NONMEM land, this line is refering to two different modules: ADVAN provides a library of pharmacokinetic models that are bundled with the software, and TRANS specifies parameter transformations. Of particular importance is the fact that ADVAN2 refers specifically to a one-compartment model with first-order absorption and first-order elimination. That’s super handy for me because it’s not too different from models I’ve implemented from scratch in Stan previously.\nThe file continues, specifying the pharmacokinetic model (PK) and the error model (ERROR) using the standard notation where “theta” denotes a population level parameter, “eta” denotes a random effect that varies from person to person, and “epsilon” denotes residuals:\n$PK\n\n  ; COVARIATE MODEL\n  TVCL=THETA(1)\n  TVV=THETA(2)\n  TVKA=THETA(3)\n\n  ; MODEL FOR RANDOM BETWEEN SUBJECT VARIABILITY\n  CL=TVCL*EXP(ETA(1))\n  V=TVV*EXP(ETA(2))\n  KA=TVKA*EXP(ETA(3))\n\n  ; SCALE CONCENTRATIONS\n  S2=V\n\n$ERROR\n  Y=F+EPS(1)\n  IPRED=F\nMy goal is to re-write this model in Stan, but to do that I need to first express that as a statistical model rather as NONMEM syntax. So let’s start at the population level. We have three parameters here:\n\nA population typical value for the clearance rate CL, denoted TVCL\nA population typical value for the distribution volume V, denoted TVV\nA population typical value for the absorption rate constant KA, denoted TVKA\n\nThe mapping here is straightfoward:\n\\[\n\\begin{array}{rcl}\n\\mbox{TVCL} &=& \\theta_1 \\\\\n\\mbox{TVV} &=& \\theta_2 \\\\\n\\mbox{TVKA} &=& \\theta_3\n\\end{array}\n\\]\nNow we consider the individual-subject level. At this level we have three parameters per person. For the i-th person, these parameters are:\n\nThe clearance rate CL\\(_i\\)\nThe distribution volume V\\(_i\\)\nThe absorption rate constant KA\\(_i\\)\n\nAs usual, the random effect terms \\(\\eta\\) are normally distributed with mean zero and standard deviation \\(\\omega\\), and the \\(\\theta\\) values are considered fixed effects. However, the population level and subject level parameters do not combine additively, they combine multiplicatively. Specifically, the \\(g(\\theta, \\eta)\\) functions for this model are as follows:\n\\[\n\\begin{array}{rcl}\n\\mbox{CL}_i &=& \\theta_1 \\exp(\\eta_{1i}) \\\\\n\\mbox{V}_i &=& \\theta_2 \\exp(\\eta_{2i}) \\\\\n\\mbox{KA}_i &=& \\theta_3 \\exp(\\eta_{3i})\n\\end{array}\n\\]\nSo far, so good. This makes sense of most of the model specification, but there are a still some confusing parts that require a bit more digging around to decipher. First off, this strange invocation:\n  ; SCALE CONCENTRATIONS\n  S2=V\nThis doesn’t make sense unless you know something about the way that NONMEM has implemented the underlying model. In the 1-compartment IV bolus model that I used as my motivating example (previous section), the pharmacokinetic function \\(f()\\) has a closed form expression for the drug concentration in the central (only) compartment. However, when you implement a pharmacokinetic model using a system of ordinary differential equations (like I did in an earlier post), the values produced by solving the ODE typically refer to the amount of drug in the relevant compartment. To convert these amounts to concentrations you need to scale them, generally by dividing by the volume of said compartment. And thus we have our explanation of the mysterious S2=V instruction. The S2 parameter is the NONMEM scaling parameter for the central compartment. We set this equal to V, i.e., the estimated volume parameter for each subject.9\nAt last we come to the error model:\n$ERROR\n  Y=F+EPS(1)\n  IPRED=F\nThe relevant part here is the line specifying the relationship between the pharmacokinetic function F, the error terms EPS, and the observed data Y. In this case it’s additive, exactly in keeping with what I assumed in my toy example:\n\\[\ny_{ij} = f(t_{ij}, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i) + \\epsilon_{ij}\n\\]\nIt doesn’t have to be. In fact, the hands-on exercise in Lecture 2 of the tutorial I’m working through prepares three versions of this model, one with additive error, one with multiplicative error, and one with a hybrid error model that incorporates additive and multiplicative components. I’ll get to that later, though probably in a future post.\nYay! At long last I think I know the model I need to implement."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#implementation-in-stan",
    "href": "posts/2023-06-10_pop-pk-models/index.html#implementation-in-stan",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Implementation in Stan",
    "text": "Implementation in Stan\n\nPassing the data to Stan\nThe first issue we’ll need to consider is how data should be passed from R to Stan. There’s some nuances here in how we format the data for Stan. As mentioned earlier, Stan doesn’t permit ragged arrays, so we’ll have to pass the observations as one long c_obs vector that aggregates across subjects. Within the model (see below), we’ll use the n_obs vector that records the number of observations per subject to create break points that we can use to extract data from a single person. Here’s the R code I used:\n\nwarfpk_obs &lt;- warfpk[warfpk$mdv == 0, ]\nwarfpk_amt &lt;- warfpk[!is.na(warfpk$rate), ]\n\nt_fit &lt;- c(\n  seq(.1, .9, .1),\n  seq(1, 2.75, .25),\n  seq(3, 9.5, .5),\n  seq(10, 23, 1),\n  seq(24, 120, 3)\n)\n\ndat &lt;- list(\n  n_ids = nrow(warfpk_amt),\n  n_tot = nrow(warfpk_obs),\n  n_obs = purrr::map_int(\n    warfpk_amt$id,\n    ~ nrow(warfpk_obs[warfpk_obs$id == .x, ])\n  ),\n  t_obs = warfpk_obs$time,\n  c_obs = warfpk_obs$dv,\n  dose = warfpk_amt$amt,\n  t_fit = t_fit,\n  n_fit = length(t_fit)\n)\n\nThe interpretation of this input is as follows:\n\nn_ids is an integer indicating the number of subjects\nn_tot is an integer indicating the total number of measurements (aggregating across subjects)\nn_obs is an integer vector of length n_ids indicating the number of measurements for each person\nt_obs is a numeric vector of length n_tot specifying the time at which a measurement was taken\nc_obs is a numeric vector of length n_tot containing all the measurements. The data from all subjects are concatenated into this vector\ndose is a numeric vector of length n_ids specifying the dose administered to each person\nt_fit is a vector of time points for which we would like to estimate the pharamacokinetic function for each person\nn_fit is an integer specfiying the length of t_fit\n\nHere’s what the input data looks like when formatted for my Stan model:\n\ndat\n\n$n_ids\n[1] 32\n\n$n_tot\n[1] 251\n\n$n_obs\n [1] 11  6 11  9 10  8  9 11 17  6 10 10 10 11 10  6  6  6  6  6  6  6  6  6\n[25]  6  6  6  6  6  6  6  6\n\n$t_obs\n  [1]   0.5   1.0   2.0   3.0   6.0   9.0  12.0  24.0  36.0  48.0  72.0\n [12]  24.0  36.0  48.0  72.0  96.0 120.0   0.5   2.0   3.0   6.0  12.0\n [23]  24.0  36.0  48.0  72.0  96.0 120.0   3.0   6.0   9.0  24.0  36.0\n [34]  48.0  72.0  96.0 120.0   3.0   6.0   9.0  12.0  24.0  36.0  48.0\n [45]  72.0  96.0 120.0   6.0  12.0  24.0  36.0  48.0  72.0  96.0 120.0\n [56]   3.0   6.0   9.0  12.0  24.0  36.0  48.0  72.0  96.0   2.0   3.0\n [67]   6.0   9.0  12.0  24.0  36.0  48.0  72.0  96.0 120.0   0.5   1.0\n [78]   2.0   3.0   3.0   6.0   6.0   9.0   9.0  12.0  12.0  24.0  36.0\n [89]  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0   1.5\n[100]   3.0   6.0  12.0  24.0  36.0  48.0  72.0  96.0 120.0   1.5   3.0\n[111]   6.0   9.0  24.0  36.0  48.0  72.0  96.0 120.0   1.5   3.0   6.0\n[122]   9.0  24.0  36.0  48.0  72.0  96.0 120.0   0.5   1.0   2.0   3.0\n[133]   6.0   9.0  24.0  36.0  48.0  72.0  96.0   1.0   3.0   6.0   9.0\n[144]  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0\n[155] 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0\n[166]  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0\n[177]  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0\n[188]  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0\n[199]  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0\n[210]  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0\n[221] 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0  72.0\n[232]  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0  24.0  36.0  48.0\n[243]  72.0  96.0 120.0  24.0  36.0  48.0  72.0  96.0 120.0\n\n$c_obs\n  [1]  0.0  1.9  3.3  6.6  9.1 10.8  8.6  5.6  4.0  2.7  0.8  9.2  8.5  6.4\n [15]  4.8  3.1  2.5  0.0  8.4  9.7  9.8 11.0  8.3  7.7  6.3  4.1  3.0  1.4\n [29] 12.0 13.2 14.4  9.6  8.2  7.8  5.8  4.3  3.0 11.1 11.9  9.8 11.0  8.5\n [43]  7.6  5.4  4.5  3.3  2.3  8.6  8.6  7.0  5.7  4.7  3.3  2.3  1.7 13.4\n [57] 12.4 12.7  8.8  6.1  3.5  1.8  1.5  1.0 17.6 17.3 15.0 15.0 12.4  7.9\n [71]  7.9  5.1  3.6  2.4  2.0  0.0  1.0  4.6 12.7  8.0 12.7 11.5 12.9 11.4\n [85] 11.4 11.0  9.1  8.2  5.9  3.6  1.7  1.1  8.6  8.0  6.0  4.4  3.6  2.8\n [99] 11.4 15.4 17.5 14.0  9.0  8.9  6.6  4.2  3.6  2.6  0.6  2.8 13.8 15.0\n[113] 10.5  9.1  6.6  4.9  2.4  1.9  3.6 12.9 12.9 10.2  6.4  6.9  4.5  3.2\n[127]  2.4  1.3  0.0  2.7 11.6 11.6 11.3  9.7  6.5  5.2  3.6  2.4  0.9  6.6\n[141] 11.9 11.7 12.2  8.1  7.4  6.8  5.3  3.0  2.0 10.4  8.9  7.0  4.4  3.2\n[155]  2.4  7.6  6.4  6.0  4.0  3.1  2.0  7.6  6.6  5.4  3.4  1.2  0.9  6.6\n[169]  5.3  3.6  2.7  1.4  1.1  9.6  8.0  6.6  5.6  3.5  2.3  7.3  6.1  4.3\n[183]  3.2  2.3  1.9  8.9  8.4  8.0  4.4  3.2  1.7  9.8  8.4  6.6  4.8  3.2\n[197]  2.4  8.2  7.5  6.8  5.5  4.5  3.7 11.0 10.0  8.2  6.0  3.7  2.6 10.0\n[211]  9.0  7.3  5.2  3.7  2.7 11.8  9.2  7.7  4.9  3.4  2.7 10.1  8.0  6.0\n[225]  4.9  3.4  2.0  8.3  7.0  5.6  4.1  3.1  2.2  9.9  7.5  6.5  4.1  2.9\n[239]  2.3  9.5  7.8  6.4  4.5  3.4  2.5  8.9  7.7  6.9  4.4  3.5  2.5\n\n$dose\n [1] 100.0 100.0 100.0 120.0  60.0 113.0  90.0 135.0  75.0 105.0 123.0 113.0\n[13] 113.0  75.0  85.0  87.0 117.0 112.0  95.5  88.5  93.0  87.0 110.0 115.0\n[25] 112.0 120.0 120.0 120.0 153.0 105.0 125.0  93.0\n\n$t_fit\n [1]   0.10   0.20   0.30   0.40   0.50   0.60   0.70   0.80   0.90   1.00\n[11]   1.25   1.50   1.75   2.00   2.25   2.50   2.75   3.00   3.50   4.00\n[21]   4.50   5.00   5.50   6.00   6.50   7.00   7.50   8.00   8.50   9.00\n[31]   9.50  10.00  11.00  12.00  13.00  14.00  15.00  16.00  17.00  18.00\n[41]  19.00  20.00  21.00  22.00  23.00  24.00  27.00  30.00  33.00  36.00\n[51]  39.00  42.00  45.00  48.00  51.00  54.00  57.00  60.00  63.00  66.00\n[61]  69.00  72.00  75.00  78.00  81.00  84.00  87.00  90.00  93.00  96.00\n[71]  99.00 102.00 105.00 108.00 111.00 114.00 117.00 120.00\n\n$n_fit\n[1] 78\n\n\n\n\nComputing the pharmacokinetic function\nNext, we need to think a little about the pharmacokinetics involved. The model I want to implement is a one-compartment model with first-order absorption and first-order elimination. To understand what that means, it helps to recognise that at any given point in time there are two drug amounts that the model needs to track: the amount of drug \\(\\mbox{A}_g\\) in the gut that has not yet been absorbed into systemic circulation, and the amount \\(\\mbox{A}_c\\) currently in circulation.\nFirst order absorption with absorption rate constant KA10 means that at each moment in time some proportion KA of the drug amount in the gut \\(A_g\\) is absorbed. Formally, this gives us the following differential equation for gut amounts:\n\\[\n\\frac{d\\mbox{A}_g}{dt} = -\\mbox{KA} \\times \\mbox{A}_{g}\n\\]\nThe same logic applies to the drug quantity in circulation \\(A_c\\). At each point in time it increases by whatever drug amount arrives from the gut, but it also decreases by whatever amount is eliminated. Formally we could express this in terms of a parameter for the elimination rate constant (which would be denoted KE), which would give us the following:\n\\[\n\\frac{d\\mbox{A}_c}{dt} = (\\mbox{KA} \\times \\mbox{A}_{g}) - (\\mbox{KE} \\times \\mbox{A}_{c})\n\\]\nHowever, when it comes to elimination it is more conventional to express the elimination rate constant as the ratio between the clearance (CL: volume of blood that can be completely cleared of drug per unit time), and the volume of distibution (V) through which the drug circulates.11 That gives is the following:\n\\[\n\\frac{d\\mbox{A}_c}{dt} = (\\mbox{KA} \\times \\mbox{A}_{g}) - \\left(\\frac{\\mbox{CL}}{\\mbox{V}} \\times \\mbox{A}_{c}\\right)\n\\] Taken together, these two expressions form a system of ordinary differential equations(ODEs) that I’ll have Stan solve numerically. This will give us a numerical estimate of the pharmacokinetic function \\(f(t_{ij}, \\boldsymbol\\eta_i, \\boldsymbol\\theta, D_i)\\). Well sort of. The function \\(f()\\) describes drug concentrations, but the output of the ODE solver will give us a measure of drug amount. So, in order to compute the pharamacokinetic function, the output of the ODE solver will need to be divided by the estimate of the volume if distribution V for each person.\n\n\nModelling code\nNow let’s have a look at the Stan code. At some point I’d like to start using Torsten for these things rather than reinventing the wheel and coding a standard compartmental model from scratch. However, I’m a bottom-up kind of person12 and I find it useful to write lower-level model code myself a few times before I start relying on pre-built model code. Here’s the complete Stan code for the model I implemented:\n\n\n\nmodel1.stan\n\nfunctions {\n  vector amount_change(real time,\n                       vector state,\n                       real KA,\n                       real CL,\n                       real V) {\n    vector[2] dadt;\n    dadt[1] = - (KA * state[1]);\n    dadt[2] = (KA * state[1]) - (CL / V) * state[2];\n    return dadt;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_ids;\n  int&lt;lower=1&gt; n_tot;\n  int&lt;lower=1&gt; n_fit;\n  array[n_ids] int n_obs;\n  vector[n_ids] dose;\n  array[n_tot] real t_obs;\n  vector[n_tot] c_obs;\n  array[n_fit] real t_fit;\n}\n\ntransformed data {\n  array[n_ids] int start;\n  array[n_ids] int stop;\n  array[n_ids] vector[2] initial_amount;\n  real initial_time = 0;\n\n  // break points within the data vector\n  start[1] = 1;\n  stop[1] = n_obs[1];\n  for(i in 2:n_ids) {\n    start[i] = start[i - 1] + n_obs[i - 1];\n    stop[i] = stop[i - 1] + n_obs[i];\n  }\n\n  // initial states for each person\n  for(i in 1:n_ids) {\n    initial_amount[i][1] = dose[i];\n    initial_amount[i][2] = 0;\n  }\n}\n\nparameters {\n  real&lt;lower=0&gt; theta_KA;\n  real&lt;lower=0&gt; theta_CL;\n  real&lt;lower=0&gt; theta_V;\n  real&lt;lower=.001&gt; omega_KA;\n  real&lt;lower=.001&gt; omega_CL;\n  real&lt;lower=.001&gt; omega_V;\n  real&lt;lower=.001&gt; sigma;\n  vector[n_ids] eta_KA;\n  vector[n_ids] eta_CL;\n  vector[n_ids] eta_V;\n}\n\ntransformed parameters {\n  vector&lt;lower=0&gt;[n_ids] KA;\n  vector&lt;lower=0&gt;[n_ids] CL;\n  vector&lt;lower=0&gt;[n_ids] V;\n  array[n_tot] vector[2] amount;\n  vector[n_tot] c_pred;\n\n  for(i in 1:n_ids) {\n    // pharmacokinetic parameters\n    KA[i] = theta_KA * exp(eta_KA[i]);\n    CL[i] = theta_CL * exp(eta_CL[i]);\n    V[i] = theta_V * exp(eta_V[i]);\n\n    // predicted drug amounts\n    amount[start[i]:stop[i]] = ode_bdf(\n      amount_change,            // ode function\n      initial_amount[i],        // initial state\n      initial_time,             // initial time\n      t_obs[start[i]:stop[i]],  // observation times\n      KA[i],                    // absorption rate\n      CL[i],                    // clearance\n      V[i]                      // volume\n    );\n\n    // convert to concentrations\n    for(j in 1:n_obs[i]) {\n      c_pred[start[i] + j - 1] = amount[start[i] + j - 1, 2] / V[i];\n    }\n  }\n}\n\nmodel {\n  // foolish priors over population parameters\n  // (parameter bounds ensure these are actually half-normals)\n  theta_KA ~ normal(0, 10);\n  theta_CL ~ normal(0, 10);\n  theta_V ~ normal(0, 10);\n  sigma ~ normal(0, 10);\n  omega_KA ~ normal(0, 10);\n  omega_CL ~ normal(0, 10);\n  omega_V ~ normal(0, 10);\n\n  // random effect terms\n  for(i in 1:n_ids) {\n    eta_KA[i] ~ normal(0, omega_KA);\n    eta_CL[i] ~ normal(0, omega_CL);\n    eta_V[i] ~ normal(0, omega_V);\n  }\n\n  // likelihood of observed concentrations\n  c_obs ~ normal(c_pred, sigma);\n}\n\ngenerated quantities {\n  array[n_ids, n_fit] vector[2] a_fit;\n  array[n_ids, n_fit] real c_fit;\n\n  for(i in 1:n_ids) {\n    // predicted drug amounts\n    a_fit[i] = ode_bdf(\n      amount_change,        // ode function\n      initial_amount[i],    // initial state\n      initial_time,         // initial time\n      t_fit,                // observation times\n      KA[i],                // absorption rate\n      CL[i],                // clearance\n      V[i]                  // volume\n    );\n\n    // convert to concentrations\n    for(j in 1:n_fit) {\n      c_fit[i, j] = a_fit[i, j][2] / V[i];\n    }\n  }\n}\n\n\nIn the earlier post I wrote on pharmacokinetic modelling in Stan I already went through the low-level details of what each section in this code does, so I won’t repeat myself here. However, there are a few comments I’ll make about it:\n\nIn all honesty I don’t think the ODE code is even needed here. With first-order absorption dynamics and first-order elimination dynamics, you can analytically calculate the pharmacokinetic curve using a Bateman curve, which I talked about in a another post. However, one of my goals here was to use the Stan ODE solvers in a population model, so for the purposes of this post I’ve chosen to do it the hard way.\nSpeaking of doing things the hard way, note that I’ve called ode_bdf() here rather than ode_rk45(). This is to avoid any issues of ODE stiffness. While there is always the possibility that ODE can remain stiff in the posterior,13 I suspect the real issue here is that (a) I’ve chosen some absurdly diffuse priors, which means that the ODE can be quite poorly behaved during the warmup period for the sampler, and (b) earlier versions of the model had bugs that made the model do weird things. I strongly suspect that with those issues out of the way I could call ode_rk45() and get a considerable speedup. However, for the purposes of this post I’ll leave it as is.\n\nIn any case, here’s the R code to compile the model, run the sampler, and extract a summary representation:\n\nmod &lt;- cmdstanr::cmdstan_model(\"model1.stan\")\nout &lt;- mod$sample(\n  data = dat,\n  chains = 4,\n  refresh = 1,\n  iter_warmup = 1000,\n  iter_sampling = 1000\n)\n\nThis code took a couple of hours to run and I have no desire to repeat the exercise more often than necessary, so I took the sensible step of saving relevant outputs to csv files:\n\nout_summary &lt;- out$summary()\nout_draws &lt;- out$draws(format = \"draws_df\") |&gt; tibble::as_tibble()\nreadr::write_csv(out_summary, fs::path(dir, \"model1_summary.csv\"))\nreadr::write_csv(out_draws, fs::path(dir, \"model1_draws.csv\"))\n\nIn what follows, I’ll read data from these files when examining the behaviour of the model. However, there is a slight wrinkle here…\n\n\nPinned copies of the output files\nThe awkward thing about storing all the output from the MCMC sampler is that the model1_draws.csv file is almost 300Mb in size. As such is slightly too large to store in the github repository that contains this blog, and indeed if you look in the repo you won’t find a copy of that file. That makes things tricky from a reproducibility point of view. I don’t really imagine that anyone else actually needs a copy of this data (why would you????) but there is a chance that I might need to re-run the R code on this post without re-running the sampler. If that ever happens, I’ll need a copy of this file stored somewhere. To that end, I finally got off my lazy arse, taught myself how to use the pins package, created a publicly accessible pinboard on google cloud storage, and hosted a copy of the data file there:14\n\nboard &lt;- pins::board_url(\"storage.googleapis.com/djnavarro-pins/_pins.yaml\")\nboard |&gt; pins::pin_list()\n\n[1] \"diamonds\"       \"warfpk_data\"    \"warfpk_draws\"   \"warfpk_summary\"\n\n\nSo if you’re following along at home – or, much more likely – you are future me who has lost the local copy of the data, you can read the data as follows:\n\nout_summary &lt;- pins::pin_read(board, \"warfpk_summary\") |&gt; tibble::as_tibble()\nout_draws &lt;- pins::pin_read(board, \"warfpk_draws\") |&gt; tibble::as_tibble()\n\nAnyway, the main thing is that one way or another we can assume that the out_summary and out_draws tibbles are both available, so let’s get back to the main thread yes?"
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#model-behaviour",
    "href": "posts/2023-06-10_pop-pk-models/index.html#model-behaviour",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Model behaviour",
    "text": "Model behaviour\nNow that I’ve fit my Stan model, it’s time to take a look at what it does.\n\nPopulation parameters\nLet’s start by taking a look at a high level summary by extracting some relevant information from out_summary. For the time being I’m only really interested in the population level parameters \\(\\boldsymbol\\theta\\), \\(\\boldsymbol\\omega\\), and \\(\\sigma\\), so I’ll filter the results so that only those parameters (and the log-probability) are shown in the output:\n\nout_summary |&gt; dplyr::filter(\n  variable |&gt; stringr::str_detect(\"(theta|omega|sigma|lp)\")\n)\n\n# A tibble: 8 × 10\n  variable     mean   median      sd     mad       q5     q95  rhat ess_bulk\n  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -104.    -103.    1.13e+1 1.12e+1 -124.    -87.2    1.01     431.\n2 theta_KA    0.755    0.667 3.71e-1 1.91e-1    0.445   1.33   1.01     448.\n3 theta_CL    0.135    0.135 8.11e-3 7.43e-3    0.123   0.149  1.00     841.\n4 theta_V     7.70     7.69  3.65e-1 3.59e-1    7.12    8.31   1.00     944.\n5 omega_KA    0.902    0.844 3.00e-1 2.48e-1    0.539   1.47   1.01     423.\n6 omega_CL    0.303    0.298 4.82e-2 4.62e-2    0.233   0.389  1.00    3060.\n7 omega_V     0.235    0.232 3.94e-2 3.70e-2    0.178   0.307  1.00    2693.\n8 sigma       1.08     1.08  5.77e-2 5.88e-2    0.993   1.18   1.00    2910.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nThe fact that all the R-hat values are very close to 1 suggests that my MCMC chains are behaving themselves nicely, and all four chains are giving the same answers. But just to check – and because an earlier version of this model misbehaved quite badly and these plots turned out to be very helpful for diagnosing the problem – I’ll plot the marginal distributions over all the population level parameters (and the log-probability) separately for each chain. To do that, first I’ll extract the relevant columns from the model1_draws.csv file that contains all the raw samples:\n\ndraws &lt;- out_draws |&gt;\n  dplyr::select(tidyselect::matches(\"(theta|sigma|omega|lp|chain)\")) \n\ndraws\n\n# A tibble: 4,000 × 9\n    lp__ theta_KA theta_CL theta_V omega_KA omega_CL omega_V sigma .chain\n   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 -130.     1.15    0.135    8.56     1.81    0.317   0.235  1.15      1\n 2 -143.     1.36    0.139    6.88     1.95    0.435   0.225  1.13      1\n 3 -138.     1.86    0.134    7.18     1.90    0.312   0.204  1.06      1\n 4 -137.     1.55    0.144    7.95     1.46    0.324   0.306  1.11      1\n 5 -130.     1.60    0.138    8.08     1.84    0.290   0.240  1.08      1\n 6 -132.     1.94    0.146    7.33     1.57    0.290   0.234  1.06      1\n 7 -140.     1.45    0.143    7.71     1.99    0.260   0.165  1.20      1\n 8 -132.     1.52    0.142    7.46     1.24    0.325   0.313  1.13      1\n 9 -134.     1.57    0.144    7.57     1.82    0.261   0.174  1.09      1\n10 -133.     1.57    0.143    7.56     1.81    0.265   0.177  1.09      1\n# ℹ 3,990 more rows\n\n\nAfter a little bit of data wrangling, I can create the appropriate plots:\n\ndraws_long &lt;- draws |&gt; \n  tidyr::pivot_longer(\n    cols = !tidyselect::matches(\"(id|chain)\"),\n    names_to = \"parameter\",\n    values_to = \"value\"\n  ) \n\ndraws_long |&gt; \n  ggplot(aes(x = factor(.chain), value, fill = parameter)) +\n  geom_violin(draw_quantiles = c(.25, .5, .75), show.legend = FALSE) + \n  facet_wrap(~ parameter, scales = \"free\", ncol = 4) +\n  theme_bw() + \n  labs(\n    x = \"MCMC chain\",\n    y = \"Sampled parameter value\"\n  )\n\n\n\n\n\n\n\n\nThat looks nice, so it’s time to move on.\n\n\nComparison to NONMEM\nOne of my fears when implementing this model in Stan was that I might have misunderstood what the NONMEM version of the model was doing, and as a consequence I’d end up estimating quite different things to what the conventional NONMEM model does. To help reassure myself that this isn’t the case, it seems wise to compare the parameter estimates that both versions of the model produce. The files distributed with the workshop include these estimates for the NONMEM model, which are as follows:15\n\n\n# A tibble: 7 × 3\n  parameter estimate std_error\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 omega_CL     0.285   0.0405 \n2 omega_KA     0.621   0.158  \n3 omega_V      0.222   0.0320 \n4 sigma        1.02    0.131  \n5 theta_CL     0.134   0.00724\n6 theta_KA     0.59    0.130  \n7 theta_V      7.72    0.347  \n\n\n\nTo save you the effort of scrolling back and forth between this table and the Stan version above, here’s a truncated version of the Stan table showing an apples-to-apples comparison:\n\n\n# A tibble: 7 × 3\n  parameter posterior_mean posterior_sd\n  &lt;chr&gt;              &lt;dbl&gt;        &lt;dbl&gt;\n1 omega_CL           0.303      0.0482 \n2 omega_KA           0.902      0.300  \n3 omega_V            0.235      0.0394 \n4 sigma              1.08       0.0577 \n5 theta_CL           0.135      0.00811\n6 theta_KA           0.755      0.371  \n7 theta_V            7.70       0.365  \n\n\nAs you can see, the two versions are in pretty close agreement. There are some modest differences in the estimates of the population mean and variability in the absorption rate constant KA – compared to NONMEM, the Stan version suggests the absorption rate constant is slightly higher on average, but also more variable across people – but on the whole that seems like a minor discrepancy, and I’m not too concerned about it.\n\n\nModel fits\nNow reassured that my model is doing the right thing, the time has come to do my favourite part of any modelling exercise: comparing the fitted values \\(\\hat{y}\\) produced by the model to the observed values \\(y\\) in the data set. It’s worth noting here that the summary representations produced by Stan are perfectly sufficient to draw these plots, because it includes summary statistics for the \\(\\hat{y}\\) values (the c_pred variables in the Stan model). However, the first version of the model I implemented had some weird bugs that required me to do a slightly deeper dive, and in order to work out where the bugs were I ended up writing code that extracts the relevant summaries from the raw MCMC samples. So, because I have that code at hand already, let’s do it the long way. First up, let’s plot the observed value \\(y\\) against the fitted value \\(\\hat{y}\\) separately for each person. Here’s the code used to extract the relevant data:\n\nfit &lt;- out_draws |&gt; \n  dplyr::select(tidyselect::matches(\"c_pred\")) |&gt;\n  tidyr::pivot_longer(\n    cols = tidyselect::matches(\"c_pred\"),\n    names_to = \"variable\",\n    values_to = \"y_hat\"\n  ) |&gt; \n  dplyr::mutate(\n    obs_num = variable |&gt; \n      stringr::str_remove_all(\".*\\\\[\") |&gt;\n      stringr::str_remove_all(\"\\\\]\") |&gt; \n      as.numeric()\n  ) |&gt;\n  dplyr::group_by(obs_num) |&gt;\n  dplyr::summarise(\n    yh = mean(y_hat),\n    q5 = quantile(y_hat, .05),\n    q95 = quantile(y_hat, .95)\n  )  |&gt;\n  dplyr::arrange(obs_num) |&gt;\n  dplyr::mutate(\n    y = dat$c_obs,\n    time = warfpk_obs$time,\n    id = warfpk_obs$id\n  )\n\nfit\n\n# A tibble: 251 × 7\n   obs_num    yh    q5   q95     y  time    id\n     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1       1  1.44  1.16  1.77   0     0.5     0\n 2       2  2.68  2.19  3.24   1.9   1       0\n 3       3  4.69  3.95  5.50   3.3   2       0\n 4       4  6.19  5.33  7.07   6.6   3       0\n 5       5  8.61  7.75  9.47   9.1   6       0\n 6       6  9.28  8.41 10.2   10.8   9       0\n 7       7  9.14  8.21 10.1    8.6  12       0\n 8       8  6.67  5.80  7.58   5.6  24       0\n 9       9  4.48  3.52  5.42   4    36       0\n10      10  2.99  2.01  3.99   2.7  48       0\n# ℹ 241 more rows\n\n\nAnd here is the code used to draw the pretty picture:\n\nggplot(fit, aes(yh, y, colour = factor(id))) +\n  geom_abline(intercept = 0, slope = 1, colour = \"grey50\") +\n  geom_point(size = 4, show.legend = FALSE) +\n  coord_equal() + \n  facet_wrap(~ factor(id), nrow = 4) + \n  theme_bw() + \n  labs(\n    x = \"Fitted value for drug concentration, y_hat\",\n    y = \"Observed value for drug concentration, y\"\n  )\n\n\n\n\n\n\n\n\nWell that’s just delightful. In almost every case the observed and fitted values line up rather nicely. You just don’t get that kind of prettiness in psychological data without doing some prior aggregation of raw data.\nLet’s take it a step further, and plot the observed data as a pharmacokinetic function (time vs drug concentration) and superimpose these over the model-estimated pharamacokinetic functions for each person. Again, this starts with a bit of data wrangling to extract what we need from the raw samples:\n\nprd &lt;- out_draws |&gt;\n  dplyr::select(tidyselect::matches(\"c_fit\")) |&gt;\n  tidyr::pivot_longer(\n    cols = tidyselect::matches(\"c_fit\"),\n    names_to = \"variable\",\n    values_to = \"y_hat\"\n  ) |&gt; \n  dplyr::group_by(variable) |&gt;\n  dplyr::summarise(\n    yh = mean(y_hat),\n    q5 = quantile(y_hat, .05),\n    q95 = quantile(y_hat, .95)\n  ) |&gt; \n  dplyr::mutate(\n    obs_id = variable |&gt; \n      stringr::str_remove_all(\".*\\\\[\") |&gt;\n      stringr::str_remove_all(\"\\\\]\") \n  ) |&gt;\n  tidyr::separate(obs_id, into = c(\"id\", \"obs_num\"), sep = \",\") |&gt;\n  dplyr::mutate(\n    obs_num = as.numeric(obs_num),\n    id = warfpk_amt$id[as.numeric(id)]\n  ) |&gt;\n  dplyr::arrange(id, obs_num) |&gt;\n  dplyr::mutate(time = rep(dat$t_fit, dat$n_ids))\n\nprd\n\n# A tibble: 2,496 × 7\n   variable       yh    q5   q95    id obs_num  time\n   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 c_fit[1,1]  0.303 0.241 0.378     0       1   0.1\n 2 c_fit[1,2]  0.599 0.478 0.743     0       2   0.2\n 3 c_fit[1,3]  0.885 0.709 1.10      0       3   0.3\n 4 c_fit[1,4]  1.16  0.936 1.44      0       4   0.4\n 5 c_fit[1,5]  1.44  1.16  1.77      0       5   0.5\n 6 c_fit[1,6]  1.70  1.37  2.08      0       6   0.6\n 7 c_fit[1,7]  1.96  1.59  2.39      0       7   0.7\n 8 c_fit[1,8]  2.20  1.79  2.68      0       8   0.8\n 9 c_fit[1,9]  2.45  2.00  2.96      0       9   0.9\n10 c_fit[1,10] 2.68  2.19  3.24      0      10   1  \n# ℹ 2,486 more rows\n\n\nAnd again, we use a little bit of ggplot2 magic to make it pretty:\n\nggplot() +\n  geom_ribbon(\n    data = prd, \n    mapping = aes(time, yh, ymin = q5, ymax = q95),\n    fill = \"grey80\"\n  ) +\n  geom_line(\n    data = prd,\n    mapping = aes(time, yh)\n  ) + \n  geom_point(\n    data = fit, \n    mapping = aes(time, y, colour = factor(id)), \n    size = 4, \n    show.legend = FALSE\n  ) +\n  geom_label(\n    data = tibble::tibble(\n      time = 110, \n      y = 17, \n      id = warfpk_amt$id\n    ),\n    mapping = aes(time, y, label = id)\n  )+ \n  facet_wrap(~ factor(id), nrow = 4) + \n  theme_bw() + \n  theme(\n    strip.text = element_blank(), \n    strip.background = element_blank()\n  ) +\n  labs(\n    x = \"Time (hours)\",\n    y = \"Warfarin plasma concentration (mg/L)\"\n  )\n\n\n\n\n\n\n\n\nIn each of these plots, the grey shaded region is a 90% confidence band16, the solid curve is the posterior predicted pharmacokinetic curve, and the dots are the raw data. As you’d hope and expect, the uncertainty bands are larger in those cases where the measurements were all taken after the drug concentration reaches its peak: if all you’ve observed is the tail17 then you’ll necessarily have some uncertainty about how high the peak was. Anyway, the main thing here is that the plots are very pretty, and you get a strong sense that the model does a good job of capturing the pattern of differences across people.\n\n\nIndividual parameters\nMoving along in our exploration of the model… here’s the empirical distribution of estimated absorption rate constants (KA), clearance values (CL), and volume of distribution (V) parameters across people:\n\npars &lt;- out_draws |&gt; \n  dplyr::select(tidyr::matches(\"^(KA|CL|V)\")) |&gt;\n  tidyr::pivot_longer(\n    cols = tidyselect::everything(), \n    names_to = \"variable\",\n    values_to = \"value\"\n  ) |&gt; \n  tidyr::separate(variable, into = c(\"parameter\", \"ind\"), sep=\"\\\\[\") |&gt;\n  dplyr::mutate(\n    ind = ind |&gt; stringr::str_remove_all(\"\\\\]\") |&gt; as.numeric()\n  ) |&gt; \n  dplyr::group_by(parameter, ind) |&gt; \n  dplyr::summarise(value = mean(value))\n\nggplot(pars, aes(value, fill = parameter)) + \n  geom_histogram(bins = 10, show.legend = FALSE) + \n  facet_wrap(~ parameter, scales = \"free\") +\n  theme_bw() + \n  labs(x = \"Parameter Value\", y = \"Count\")\n\n\n\n\n\n\n\n\nOkay, seems good to me I guess?\n\n\nResiduals\nWe’re getting very close to the end. The last thing I want to do with this model is take a look at the residuals, since that’s usually the best bet if you want to detect non-obvious model failures. In the plot below I’ve plotted the residual term \\(\\hat{y} - y\\) for every observation in the data set. As before, each panel corresponds to an individual subject, and the residuals are plotted as a function of the observed drug concentration \\(y\\):\n\nfit |&gt; \n  dplyr::mutate(residual = y - yh) |&gt;\n  ggplot(aes(y, residual, colour = factor(id))) +\n  geom_abline(slope = 0, intercept = 0, colour = \"grey80\", linewidth = 2) +\n  geom_point(show.legend = FALSE, size = 4) + \n  geom_smooth(formula = y ~ x, method = lm, colour = \"black\", se = FALSE) + \n  theme_bw() + \n  facet_wrap(~ factor(id), ncol = 8, scales = \"free_x\") + \n  labs(\n    x = \"Observed concentration\",\n    y = \"Residual\"\n  )\n\n\n\n\n\n\n\n\nFor most people in the data set, there’s nothing here to suggest systematic model misfit. The residuals are small, and show no systematic pattern. In a few cases, however, the model struggles slightly. As foreshadowed earlier in the post, participant 12 is the most obvious example: the estimated pharamacokinetic curve is a little too flat (the peak isn’t high enough), leading to a systematic pattern where the model overestimates low concentrations and underestimates high concentrations. The effect is quite easy to spot in the residual plot, but virtually invisible in the individual-subject PK curves I showed in the last section. The model error isn’t very large (even for the worst-fit person the posterior predictive PK curves are still awfully good), but it is there.\nThe second version of a residual plot shows the unsigned residuals \\(|\\hat{y} - y|\\) as a function of \\(y\\). Here, what I’m looking for is evidence that the magnitude of the residuals is systematically larger at higher concentrations, and would be a hint that the additive error assumption I’ve used in this model is inadequate for modelling the data:\n\nfit |&gt; \n  dplyr::mutate(residual = abs(y - yh)) |&gt;\n  ggplot(aes(y, residual, colour = factor(id))) +\n  geom_point(show.legend = FALSE, size = 4) + \n  geom_smooth(formula = y ~ x, method = lm, colour = \"black\", se = FALSE) + \n  theme_bw() + \n  facet_wrap(~ factor(id), ncol = 8, scales = \"free_x\") + \n  labs(\n    x = \"Observed concentration\",\n    y = \"Unsigned Residual\"\n  )\n\n\n\n\n\n\n\n\nOn the whole I don’t think there’s much to worry about here. Of course that doesn’t mean that additive errors are theoretically plausible or correct, it simply means that the statistical model appears to work just fine even without tinkering with this assumption."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#epilogue",
    "href": "posts/2023-06-10_pop-pk-models/index.html#epilogue",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Epilogue",
    "text": "Epilogue\nAs someone learning pharmacometrics coming from a rather different technical discipline (mathematical psychology), the thing I find truly surprising in this is how clean the data set is, despite containing relatively few data points. Folks outside of mathematical psychology might find it surprising to learn that there are some extremely effective formal models in psychology, some of which are horrifyingly technical (reading the infamous 2000 paper by Philip Smith in the Journal of Mathematical Psychology nearly broke me as a graduate student), but it’s usually the case for those models that you’re working with extremely noisy or impoverished data, so you need quite a lot of data to work with them safely. Yet here, we have a data set with a mere 32 people and only a handful of observations per person, and it’s entirely possible to fit a stochastic dynamic hierarchical Bayesian model that produces person-specific pharmacokinetic curves that closely match the observed data, only occasionally show hints of systematic misfit, and allow estimation of population level parameters with only modest uncertainty. Granted, from an empirical perspective it’s far more costly to take a measurement, and – quite rightly – there are stricter regulatory and ethical constraints that go into conducting a study like this than there would be for a typical psychological study, but even so, it still comes as a shock to me that it “just works”.\nI could get used to this."
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#references",
    "href": "posts/2023-06-10_pop-pk-models/index.html#references",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "References",
    "text": "References\n\nBauer, R. J., Guzy, S., & Ng, C. (2007). A survey of population analysis methods and software for complex pharmacokinetic and pharmacodynamic models with examples. The AAPS Journal, 9, E60-E83. doi.org/10.1208/aapsj0901007\nBauer, R. J. (2019). NONMEM tutorial part I: Description of commands and options, with simple examples of population analysis. CPT: Pharmacometrics & Systems Pharmacology, 8(8), 525-537. doi.org/10.1002/psp4.12404\nBauer, R. J. (2019). NONMEM tutorial part II: Estimation methods and advanced examples. CPT: Pharmacometrics & Systems Pharmacology, 8(8), 538-556. doi.org/10.1002/psp4.12422\nFoster, D., Abuhelwa, A. & Hughes, J. (2019). Population Analysis Using NONMEM Beginners Workshop. Retrieved from: www.paganz.org/resources/\nNavarro, D. J., Griffiths, T. L., Steyvers, M. and Lee, M. D. (2006). Modeling individual differences using Dirichlet processes. Journal of Mathematical Psychology, 50, 101-122. dx.doi.org/10.1016/j.jmp.2005.11.006\nO’Reilly, R. A., & Aggeler, P. M. (1968). Studies on coumarin anticoagulant drugs: initiation of warfarin therapy without a loading dose. Circulation, 38(1), 169-177. doi.org/10.1161/01.CIR.38.1.169\nO’Reilly, R. A., Aggeler, P. M., & Leong, L. S. (1963). Studies on the coumarin anticoagulant drugs: the pharmacodynamics of warfarin in man. The Journal of Clinical Investigation, 42(10), 1542-1551. doi.org/10.1172%2FJCI104839\nSmith, P. L. (2000). Stochastic dynamic models of response time and accuracy: A foundational primer. Journal of Mathematical Psychology, 44(3), 408-463. doi.org/10.1006/jmps.1999.1260"
  },
  {
    "objectID": "posts/2023-06-10_pop-pk-models/index.html#footnotes",
    "href": "posts/2023-06-10_pop-pk-models/index.html#footnotes",
    "title": "Building a population pharmacokinetic model with Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes this is indeed a joke. Tom Griffiths, Mark Steyvers, and Michael Lee are all considerably more successful as academics than I ever was, so much so that when Mark fed the text of the paper – which is about 99% my writing – into an author-topic model that allows specific passages to be attributed to specific authors, it did not attribute a single word of the text to me. Oh well.↩︎\nTechnically I’m guessing the code here, but there’s a lot more 1s in the data than 0s, and a lot more of male subjects reported by O’Reilly & Aggeler, so it seems a safe bet!↩︎\nThere’s an irony here: if these were psychological data we’d be delighted to have data this clean. Psychological data are as noisy as a drunken fuck on a Friday (to use a technical term). Nevertheless, the reality of the warfarin data set is that the data are actually a lot cleaner than this plot makes it seem.↩︎\nI mean, from a statistical perspective what we’re saying is that we need a hierarchical model, and those are essentially the same thing regardless of the domain of application, but whatever.↩︎\nOkay, technically these don’t have to be means: really what we’re talking about are location parameters. Again, whatevs.↩︎\nSame as above: technically, we’re talking about scale parameters.↩︎\nThe initial version of this post was a little imprecise in terminology, and tended to refer to the rate constants as rate, which is not quite accurate: \\(k\\) parameterises the pharmacokinetic function and controls the rate at which a drug is absorbed/eliminated, but it is not itself a rate. (My thanks to Tiny van Boekel for gently drawing my attention to this terminological lapse on my part!)↩︎\nIf we wanted to consider this correlation then we’d have a full variance-covariance matrix denoted \\(\\boldsymbol\\Omega\\), but I’m not going to go there in this post↩︎\nHonestly, I wasn’t 100% certain that my interpretation was correct, but eventually I managed to find copies of the NONMEM user manuals online and they explain it there.↩︎\nMy notation is a little different to what I used in the previous post. That’s deliberate: last time around I wrote my model code and expressed the differential equations using notation that made sense to me. This time around I’m trying to bring my notation a little closer to the terminology used in the NONMEM control file I’m working from, in order to more closely match typical practice in the field.↩︎\nWhenever I am being lazy I have the bad habit of assuming that the volume of distribution is basically the same thing as “the amount of blood plasma in the body”, but that’s not actually true. Certainly it makes sense as a crude first-approximation mental model, but bodies are complicated things so the reality is messier, and the apparent volume of distribution can be large than the amount of blood in the body.↩︎\nIf you believe the testimony of my ex-boyfriends, that is.↩︎\n…or so I’m told. Honestly, the last time I had to work so hard to keep a straight face in front of a statistical audience was writing academic papers that required me to talk about posterior Weiner processes.↩︎\nNot gonna lie, the only reason I finally forced myself to set this up is that, courtesy of an extremely unwise git reset --hard command while writing this post, I deleted the local copy of the model1_draws.csv file, and – lacking any remote copy of the bloody thing – had to rerun the entire sampler from the beginning to create a new one, wasting a lot of cpu cycles and exhausting most of my patience. Burned hands are the best teachers I guess…↩︎\nThe NONMEM output expressed the standard error as a percentage of the point estimate, but for the purposes of comparison to the Stan model I’ve converted them back to raw values.↩︎\nMore precisely, it’s a Bayesian 90% equal-tail credible region. Whatevs.↩︎\nSometimes referred to as the elimination phase, but of course this is a dynamic system, so in reality absorbption and elimination are happening in parallel all the time. However, in practice you end up with a situation where early on the absorption process is the primary driver and later on the elimination process is the primary driver.↩︎"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html",
    "href": "posts/2022-10-18_arrow-flight/index.html",
    "title": "Building an Arrow Flight server",
    "section": "",
    "text": "This is a post about Arrow Flight. I will probably tell a whimsical anecdote to open this post. Or not. Who knows. Maybe I’ll leave the introductory paragraph like this. That would be pretty on-brand for me actually."
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#the-what-and-why-of-arrow-flight",
    "href": "posts/2022-10-18_arrow-flight/index.html#the-what-and-why-of-arrow-flight",
    "title": "Building an Arrow Flight server",
    "section": "The what and why of Arrow Flight",
    "text": "The what and why of Arrow Flight\nThe central idea behind flight is deceptively simple: it provides a standard protocol for transferring Arrow data over a network. But to understand why this is a Big Deal, you need to have a good sense of what the Arrow ecosystem is all about. For that, I found it helpful to go all the way back1 to the original announcement of flight by Wes McKinney. Here’s how he explained the motivation:\n\nOur design goal for Flight is to create a new protocol for data services that uses the Arrow columnar format as both the over-the-wire data representation as well as the public API presented to developers. In doing so, we reduce or remove the serialization costs associated with data transport and increase the overall efficiency of distributed data systems. Additionally, two systems that are already using Apache Arrow for other purposes can communicate data to each other with extreme efficiency.\n\nTo put this in context, it helps to have a little recap of how the project has grown: Arrow was originally introduced to provide an efficient and language-agnostic standard for representing tabular data in-memory, but as the project has grown it has necessarily expanded in scope. For example, storing data in-memory is not entirely useful if you can’t manipulate it, so Arrow now supplies a powerful compute engine that underpins both the arrow package in R and the pyarrow library in Python, and several others besides. In other words, the compute engine has been developed to solve a practical data science problem.\nArrow Flight evolved from a similar practical concern. It’s pretty trivial to point out that we live in a networked world now, and as consequence it is hard to avoid situations where the data to be analysed are stored on a different machine than the one that does the analysis. In my earlier posts on reticulate and rpy2 I talked about how to efficiently share an Arrow data set between languages, but I implicitly assumed in those posts that the R process and the Python process were running on the same machine. The moment we have processes running on different machines, those tricks don’t work anymore!\nFlight is designed to solve this problem. It’s not a fancypants protocol with lots of different parts. It exists for one purpose: it makes it super easy to transfer Arrow-formatted data. That’s it. It’s pretty flexible though, and you can build other stuff on top of flight, but the design of flight is deliberately simple. It’s meant to be pretty minimal, so you can “just use it” without having to think too hard or do any of the obnoxious implementation work yourself.\n\n\n\n\n\nA biplane in flight. Image by Gerhard from Pixabay\n\n\n\n\n\nPrerequisites\nThere are a couple of prerequisites for this post. Specifically I’ll assume you have the arrow and reticulate packages installed in your R environment, and similarly that your Python environment has pyarrow installed. If you’re only interested in the Python side, you probably don’t need either of the R packages, but R users will need to have the pyarrow installation because the R flight implementation builds on pyarrow."
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#an-r-example",
    "href": "posts/2022-10-18_arrow-flight/index.html#an-r-example",
    "title": "Building an Arrow Flight server",
    "section": "An R example",
    "text": "An R example\nThe implementation of Arrow Flight varies a little across languages. In this post I’m going to focus on the two languages I use most – R and Python – but there’s nothing stopping you from using other languages. For example, the book In-Memory Analytics with Apache Arrow by Matt Topol has worked examples using C++ and Go, in addition to Python.\nFor the purposes of this post I’m going to start with R because the arrow package in R exposes a “high-level” interface that will allow us to start using a flight server without having to dive deeply into how it all works. However, as we’ll see, there are some limitations to this approach – not least of which is the fact that the R implementation turns out to secretly be a Python implementation under the hood – and as the post progresses I’ll pivot to Python in order to unpack some of the lower-level functionality.\nTo do this I’ll need access to the arrow and reticulate packages, and I’ll need to make certain that the Python environment is one that has pyarrow installed. For my machine, the commands to do this look like this:\n\nlibrary(arrow)\nlibrary(reticulate)\nuse_miniconda(\"base\")\n\nIt may be a little different for you depending on your configuration. For more information on this, take a look at the reticulate post I wrote recently.\n\n\nThe flight server\nOkay, so let’s get started by thinking about the simplest possible scenario for using a flight server. In this set up all we want the server to do is to act as a “cache” for Arrow tables. Clients can upload tables to the server, download tables from the server, and so on. That’s all we’re really trying to accomplish, and happily for us this use case is supported out of the box in R.\nHere’s how it works. As I mentioned earlier, R doesn’t actually implement the flight protocol itself: it’s just a wrapper around the Python tools. What that means is the underlying flight server is actually written in Python, and if we want to start that server running from R we have to call the load_flight_server() function that will allow us access to this server from R. Conveniently, the arrow R package comes bundled with a “demo” server that already provides the server side functionality that we want, and I can import it like this:\n\nserver_class &lt;- load_flight_server(\"demo_flight_server\")\n\nWhen I do this, all I’ve done is obtain access to the relevant Python code. I haven’t created a server yet and I haven’t started it running either. Create an instance of the “demo server”, I call the DemoFlightServer() method attached to the server_class object:\n\nserver &lt;- server_class$DemoFlightServer(port = 8089)\n\nWe have now defined a server that, once started, will run on port 8089. The server object has a serve() method that I can call to start it running:\n\nserver$serve()\n\nI’ve written a short script called start_demo_server.R that bundles all these operations together:\n\n\n\nstart_demo_server.R\n\n# load R packages and specify the Python environment\nlibrary(arrow)\nlibrary(reticulate)\nuse_miniconda(\"base\")\n\n# load server class, create instance, start serving\nserver_class &lt;- load_flight_server(\"demo_flight_server\")\nserver &lt;- server_class$DemoFlightServer(port = 8089)\nserver$serve()\n\n\nThe easiest way to start a server running in its very own R process would be to execute this script – or a suitably modified version that refers to an appropriate Python environment and server port – at the terminal, which I could do like this:\nRscript start_demo_server.R &\nThis would start an R process as a background job that creates a server and start it running. As an alternative, if you’re comfortable with using the callr package, you can use callr::r_bg() to create a child R process from your current one. The child process will run in the background, and we can start start the server within that R session without blocking the current one. This code will do exactly that:\n\nr_process &lt;- callr::r_bg(function() {\n  reticulate::use_miniconda(\"base\")  \n  demo &lt;- arrow::load_flight_server(\"demo_flight_server\")\n  server &lt;- demo$DemoFlightServer(port = 8089)\n  server$serve()\n})\n\nRegardless of what method you’ve chosen, I’ll assume that the demo server is now running quietly in the background on port 8089.\n\n\n\n\n\nA hummingbird in flight. Image by Pexels from Pixabay\n\n\n\n\n\n\nThe flight client\nNow that I have this server running, I’ll define a flight client in my current R session that can interact with it. To do that, I call flight_connect():\n\nclient &lt;- flight_connect(port = 8089)\n\nPerhaps unsurprisingly, the R object client is a wrapper around a Python flight client. It comes with various methods that implement low-level flight operations, but I’m going to hold off talking about those for a moment because we won’t need to use the low-level interface in this initial example.\nLet’s start by using the client to ask a simple question: what is stored on the server? The way that data sources are conceptualised in Arrow Flight is as a set of “flights”. Each individual “flight” is a data stream from which the client can download data. The precise implementation of this idea (e.g., what data structures are stored in a single flight) varies from server to server, but in both examples in this post one flight corresponds to one Arrow table.\nTo find out what flights are currently available on our server, we can call the list_flights() function:\n\nlist_flights(client)\n\nlist()\n\n\nHm, okay, there’s nothing there. That makes sense because I haven’t actually uploaded anything to the server yet! Okay, well, let’s suppose I want to store a copy of the airquality data as an Arrow table on my server. As R users are probably aware, this is a data set that comes bundled with R, but just so we’re all on the same page here’s the first few rows of the data set:\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\nThis object is a regular data frame in R: it’s not an Arrow table. Strictly speaking what we want our client to do is send the Arrow table version of this data set to the server, so it will need to be converted. Happily for us, the flight_put() function supplied by the arrow package takes care of that conversion for us. As a result, we can cache an Arrow table copy of the data on the server with one line of code:\n\nflight_put(client, data = airquality, path = \"pollution_data\")\n\nIn this code, the flight_put() function uses the client object to communicate with the server. The data argument specifies the local copy of the data set, and the path argument provides the name for the data on the server. Having uploaded the data we can once again call list_flights(), and we get this as the result:\n\nlist_flights(client)\n\n[1] \"pollution_data\"\n\n\nYay!\nNow, just to prove to you that I’m not cheating, let’s check to make sure that there is no object called pollution_data stored locally within my R session:2\n\npollution_data\n\nError in eval(expr, envir, enclos): object 'pollution_data' not found\n\n\nUnsurprisingly, there is no object called pollution_data available in my current R session. The pollution_data object is stored on the server, not the client. To access that data from the client I can use the flight_get() function:\n\nflight_get(client, \"pollution_data\")\n\nTable\n153 rows x 6 columns\n$Ozone &lt;int32&gt;\n$Solar.R &lt;int32&gt;\n$Wind &lt;double&gt;\n$Temp &lt;int32&gt;\n$Month &lt;int32&gt;\n$Day &lt;int32&gt;\n\nSee $metadata for additional Schema metadata\n\n\nIt works!\n\n\n\n\n\nA flight of stairs. Image by Francis from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#unpacking-the-data-exchange-process",
    "href": "posts/2022-10-18_arrow-flight/index.html#unpacking-the-data-exchange-process",
    "title": "Building an Arrow Flight server",
    "section": "Unpacking the data exchange process",
    "text": "Unpacking the data exchange process\nThe example in the last section is a nice proof-of-concept. It shows that we can use R to start a flight server and use it to upload and download data. But there’s a lot that hasn’t really been explained properly here. The time has come to start digging a little deeper, so we can really get a sense of what’s going on under the hood and how this simple example can be extended. That’s our goal in this section.\nOne thing that I like about the flight functionality exposed through flight_connect(), flight_put(), flight_get(), etc is that it operates at a high level of abstraction. In my day-to-day data analysis work I really don’t want to spend my time thinking about low-level operations. When I tell R to “put” a data set onto the server I want this to happen with one line of code. This high level API is super useful to me on an everyday basis, but it also masks some of the details about how flight works.\nTo give you a sense of what’s being hidden, we can take a closer look at the client object. Here’s a list of some of the methods that are available through the object itself:\nclient$do_put()\nclient$do_get()\nclient$do_action()\nclient$list_action()\nclient$list_flights()\nclient$get_flight_info()\nEach of these methods describes a low level operation available to the flight client. More precisely, these are the actual methods prescribed by the Arrow Flight protocol. Moreover, although I’m showing you this as an R object, in truth these are all Python methods: the R implementation of Arrow Flight is essentially a wrapper around the Python implementation. We can access these methods from R thanks to the magic of reticulate, but – to foreshadow the pivot coming in the next section – eventually we will need to start working with the underlying Python code.\nWhen we look at the names of flight methods, we can see there’s (unsurprisingly) a relationship between those names and the names of the functions exposed in the high-level R interface. As you might expect, the do_put() method for the client is very closely related to the flight_put() function. However, they aren’t the same. The do_put() method doesn’t stream any data to the server: it merely opens a connection to the server, from which we can then stream data with subsequent commands. If calling the do_put() method directly, you would have to take care of the streaming yourself.3 But from the user perspective it’s tiresome to write that code over and over, so the flight_put() function in the R interface provides a convenient high-level wrapper that abstracts over all that.\nIf you’re the analyst working with the data, this is fabulous. But if you’re looking to implement your very own flight server, you probably need to understand what these low level operations are. So that’s where we’re headed next…\n\n\nUnpacking flight_put()\nLet’s start by taking a look at what happens when we call the R function flight_put(). For now, we won’t write any actual code (don’t worry, that will come later!). All we want to do is think about the sequence of operations that takes place. Our goal is to transmit the data to the server, and there’s an Arrow Flight method called do_put() that can do this for us. However, the structure of the interaction is a little more complicated than simply calling do_put(). It’s a multi-step operation that unfolds as shown below:\n\nThe first step in the process occurs when the client calls do_put(), a flight method that takes two arguments: a flight descriptor object that is used to identify the specific data stream that the client wants to be sent – and later on I’ll talk what the descriptor actually looks like – and the schema for the flight data.4 Setting aside the particulars of the syntax – which might be different in every language – here’s what the do_put() function call looks like on the client side:\ndo_put(descriptor, schema)\nPassing the schema on the client side serves a particular purpose: it allows the client to create stream writer and stream reader objects that are returned to the client-side user, and are also passed along to the server. The writer object is the thing that will take care of streaming data to the server, and the reader object is responsible for reading any metadata response that the server happens to send.5\nNow let’s have a look at the server side, where the do_put() method expects three inputs: the flight descriptor, the writer, and the reader. So here’s the signature on the server side:\ndo_put(descriptor, reader, writer)\nAs long as these methods are written appropriately for both the client and the server, we now have a situation where both machines agree on the description of the data and have objects that can take care of the streaming process.\nWe now move to step two in the communication, in which the client streams the data to the server. Once the data arrive on the server side, the do_put() method for the server stores the data along with an appropriate descriptor, so that it can be found later. Optionally, this is followed by a third stage in which the server sends a response containing metadata to the client. In the example server I’ll build in the next section, I won’t bother with that step!\n\n\n\nUnpacking flight_get()\nNext let’s look at flight_get(). When I called this function earlier, it triggered two separate interactions between the client and server. First, the client calls the get_flight_info() method, and the server responds with some information about the data source that includes – among other things – a ticket. Again, the ticket is a particular data structure that I’ll talk more about later, but for now it’s enough to note that it’s a token that uniquely specifies which flight is requested.\nOnce in possession of this ticket, the client can call do_get() to request that the server send the data that matches the ticket, which the server then streams. So the whole exchange looks like this:\n\nSo, in the previous example when I called flight_get(), the process looked like this. On the client side, we used the \"pollution_data\" path to construct a descriptor object and the client used get_flight_info() to request that information about this “flight” from the server:\nget_flight_info(descriptor)\nOn the server side, once the descriptor is received, a flight info object is constructed. The flight info object is comprised of five parts:\n\nThe schema for the data stored by the flight,\nThe flight descriptor object\nA list of one or more endpoints that specify where the data are available for streaming. Each end point includes a location from which to stream, and the associated ticket for that location\nThe total number of records (i.e. rows) stored\nThe total number of bytes to be streamed (i.e., the size of the data)\n\nThis flight info is then returned to the client.\nIt may seem like this arrangement is overly elaborate: why does the client need this much information if only the ticket is needed to request the data? To be honest, for the simple server-client examples I’ve used in this post, this level of complexity is not really needed. However, it’s extremely useful that it’s structured like this when we want to start adopting a more sophisticated setup. One thing it allows, for example, is an arrangement where both the server and client can be distributed across multiple machines, with different endpoints streaming different subsets of the data. Matt Topol discusses some examples where this architecture is employed in In-Memory Analytics with Apache Arrow.\nOnce this flight information has been received by the client, we can extract the ticket from the relevant endpoint (there will be only one endpoint in the server I build in the next section). The client now calls:\ndo_get(ticket)\nThe server then sends a stream reader object that the client can use to receive the stream of data from the server.\n\n\n\n\n\nBalloons in flight. Image by Pexels from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#a-python-example",
    "href": "posts/2022-10-18_arrow-flight/index.html#a-python-example",
    "title": "Building an Arrow Flight server",
    "section": "A Python example",
    "text": "A Python example\nNow that we have a basic understanding of what is happening at a lower level, we can build a flight server of our very own. To do this I’ll switch over to Python. There’s two reasons for doing this. The first reason is that R doesn’t currently have a low level implementation of flight: at the moment it relies on the Python implementation, so it’s easiest to switch completely to Python for the rest of this post.6 The second reason is that Python doesn’t supply a high level API analogous to flight_put(), flight_get() etc, and instead adheres tightly to the Arrow Flight specification. That’s super helpful if you need to design a custom flight server because you get access to the all Arrow Flight functionality, but it also means you need to write a lot more code. To help make that process a little easier, I’ll walk you through how that works in Python now!\n\n\nA tiny flight server\nOur goal in this section is to write our own flight server in Python that does the same job as the one we saw earlier in the R example: it’s a server that allows you to cache copies of Arrow tables. To do so, we’ll start our Python script the way one usually does, with some imports:\n\n\n\ntiny_flight.py\n\nimport pyarrow as pa\nimport pyarrow.flight as flight\n\n\nWhat I’ll do now is define a Python class called TinyServer. The job of this class is to provide server side flight methods for do_get(), do_put(), and others. We’ll be able to use this class to create specific server instances and set them running, in more or less the exact same fashion that we did previously in the R example.\nI’ll explain the code in more detail in a moment after I’ve shown you both the server and the client, but let’s start just by looking at the code. You can find all the code in the tiny_flight.py script that accompanies this post. Here’s the complete code used to define the TinyServer class:\n\n\n\ntiny_flight.py [server]\n\nclass TinyServer(flight.FlightServerBase):\n  \n    def __init__(self, \n                 host = 'localhost', \n                 port = 5678):\n        self.tables = {}\n        self.location = flight                  \\\n                        .Location               \\\n                        .for_grpc_tcp(host, port)\n        super().__init__(self.location)\n    \n    @staticmethod    \n    def server_message(method, name):\n        msg = '(server) '                       \\\n              + method                          \\\n              + ' '                             \\\n              + name.decode('utf-8')\n        print(msg)\n      \n    def do_put(self, context, descriptor, reader, \n               writer):\n        table_name = descriptor.command\n        self.server_message('do_put', table_name)\n        self.tables[table_name] = reader.read_all()\n\n    def do_get(self, context, ticket):\n        table_name = ticket.ticket\n        self.server_message('do_get', table_name)\n        table = self.tables[table_name]\n        return flight.RecordBatchStream(table)\n  \n    def flight_info(self, descriptor):\n        table_name = descriptor.command\n        table = self.tables[table_name]\n\n        ticket = flight.Ticket(table_name)\n        location = self.location.uri.decode('utf-8')\n        endpoint = flight.FlightEndpoint(ticket,\n                                         [location])\n        \n        return flight.FlightInfo(table.schema, \n                                 descriptor, \n                                 [endpoint], \n                                 table.num_rows,\n                                 table.nbytes)\n    \n    def get_flight_info(self, context, descriptor):\n        table_name = descriptor.command\n        self.server_message('get_flight_info',\n                            table_name)\n        return self.flight_info(descriptor)        \n        \n    def list_flights(self, context, criteria):\n        self.server_message('list_flights', b' ')\n        for table_name in self.tables.keys():\n            descriptor = flight                  \\\n                         .FlightDescriptor       \\\n                         .for_command(table_name)\n            yield self.flight_info(descriptor)\n\n    def do_action(self, context, action):\n        if action.type == 'drop_table':\n            table_name = action.body.to_pybytes()\n            del self.tables[table_name]\n            self.server_message('drop_table',\n                                table_name)\n\n        elif action.type == 'shutdown':\n            self.server_message('shutdown', b' ')\n            self.shutdown()\n\n        else:\n            raise KeyError('Unknown action {!r}'.\n                           format(action.type))\n\n    def list_actions(self, context):\n        return [('drop_table', 'Drop table'),\n                ('shutdown', 'Shut down server')]\n\n\nNow, if you’re at all like me this code won’t immediately make sense. Probably you’ll skim over it, read bits of it, and some of it will make sense… but not all of it. There’s a couple of reasons for that. The first and most obvious reason is that it’s a big chunk of code that I haven’t explained yet! The second reason is that (in my opinion) server-side code never makes sense on its own: it only really makes sense when you can place it next to the client-side code so that you can see how the two parts fit together.7 With that in mind, let’s take a quick peek at the client-side code…\n\n\n\nA tiny flight client\nTo accompany a TinyServer, we’ll need a TinyClient that knows how to talk to it. Happily for us, it’s easier to define the client than to define the server, so the source code that defines the TinyClient class is considerably shorter:\n\n\n\ntiny_flight.py [client]\n\nclass TinyClient:\n\n    def __init__(self, host = 'localhost', port = 5678):\n        self.location = flight                      \\\n                        .Location                   \\\n                        .for_grpc_tcp(host, port)\n        self.connection = flight.connect(self.location)\n        self.connection.wait_for_available()\n\n    def put_table(self, name, table):\n        table_name = name.encode('utf8')\n        descriptor = flight                         \\\n                     .FlightDescriptor              \\\n                     .for_command(table_name)\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n        writer.write(table)\n        writer.close()\n      \n    def get_table(self, name):\n        table_name = name.encode('utf8')\n        ticket = flight.Ticket(table_name)\n        reader = self.connection.do_get(ticket)\n        return reader.read_all()\n    \n    def list_tables(self):\n        names = []\n        for flight in self.connection.list_flights():\n            table_name = flight.descriptor.command\n            names.append(table_name.decode('utf-8'))\n        return names\n    \n    def drop_table(self, name):\n        table_name = name.encode('utf8')\n        drop = flight.Action('drop_table', table_name) \n        self.connection.do_action(drop)\n\n\nThese two classes are designed to work in concert: the do_put() method for TinyServer is aligned with the do_put() method for TinyClient,8 and the put_table() function I wrote on the client side is a convenient high-level wrapper that manages the whole “put a table on the server” interaction without requiring the user to do anything other than write a single line of code. That’s the reason I started by showing you all the source code for both parts before explaining any of the specific methods: in the next few sections I’ll walk you through the code, placing the relevant snippets from the server code and the client code next to each other so you can more clearly see how they relate to each other.\n\n\n\n\n\nA kite in flight. Image by Anja from Pixabay\n\n\n\n\n\n\nInitialisation\nLet’s start by looking at what happens when the server and client are initialised. When a new TinyServer or TinyClient object is created, the __init__ function is called:\n\n\n\ntiny_flight.py [server]\n\nclass TinyServer(flight.FlightServerBase):\n  \n    def __init__(self, \n                 host = 'localhost', \n                 port = 5678):\n        self.tables = {}\n        self.location = flight                  \\\n                        .Location               \\\n                        .for_grpc_tcp(host, port)\n        super().__init__(self.location)\n\n\n\n\n\ntiny_flight.py [client]\n\nclass TinyClient:\n\n    def __init__(self, host = 'localhost', port = 5678):\n        self.location = flight                      \\\n                        .Location                   \\\n                        .for_grpc_tcp(host, port)\n        self.connection = flight.connect(self.location)\n        self.connection.wait_for_available()\n\n\nSome things to notice here. At start up, the server and client both call the flight.Location.for_grpc_tcp() function to generate a Location object used to specify the address of the server:\n\nloc = flight.Location.for_grpc_tcp('localhost', 5678)\nprint(loc)\n\n&lt;Location b'grpc+tcp://localhost:5678'&gt;\n\n\nThe important thing in this output is the server address. The localhost:5678 part indicates that the server is running locally on port 5678, and the grpc+tcp:// part tells us what communication protocols are being used. For this server, those protocols are gRPC and TCP. TCP is probably familiar to most data scientists since it’s one of the core protocols of the internet, but gRPC (wonderful as it is) is a little more specialised. I’m not going to talk about how gRPC works in this post, but there are some references at the end. For now, it’s sufficient to recognise that this location object does store the server address. If I’d really wanted to, I could have written code that constructs this string manually9 but there’s no need to do that when the pyarrow flight module supplies built-in location classes to do this for us!\nThe rest of the code is used for initialisation. On the server side, we initialise the server object as an instance of the parent class (i.e., FlightServerBase). On the client side, the first action is to call flight.connect(): this is also an initialisation action that returns an instance of the FlightClient class. In other words there’s a kind of symmetry here: the TinyServer is built on top of the FlightServerBase class, and the TinyClient is built on top of the FlightClient class.10\nThe other thing to notice here is the data structures set up in these initialisations. On the server side we create an empty dictionary called tables (referred to as self.tables since it belongs to the instance not the class) that the server uses to store any data sets that it is sent. On the client side, the self.connection object is used to represent our connection to the server: this object is an instance of the FlightClient class, and it comes equipped with client side methods for do_put(), do_get() etc. Finally, notice that the last action that the client takes when it is initialised is to wait for the connection to the server to be established.\n\n\n\nPutting a table\nNext, let’s take a look at the code used to place data on the server. On the server side, we have to specify the do_put() method. In this case, all my code does is store a copy of the data in self.tables and prints a little message to the server console using the server_message() function:\n\n\n\ntiny_flight.py [server]\n\n    @staticmethod    \n    def server_message(method, name):\n        msg = '(server) '                       \\\n              + method                          \\\n              + ' '                             \\\n              + name.decode('utf-8')\n        print(msg)\n      \n    def do_put(self, context, descriptor, reader, \n               writer):\n        table_name = descriptor.command\n        self.server_message('do_put', table_name)\n        self.tables[table_name] = reader.read_all()\n\n\nThere’s a few things to comment on here. First, let’s note that the server_message() function isn’t very interesting for our purposes. It exists solely to print out messages,11 thereby allowing the server to announce what it’s doing, but the server would work just fine without these messages. However, it does give me an opportunity to mention some things about the arguments to the various functions defined in this code:\n\nserver_message() is a static method – which is why it doesn’t take a self argument. The arguments listed in the function definition are exactly the same as the arguments that are included in function calls later.\ndo_put() is a class method, and so it takes self as the first argument. As is typical for object oriented programming systems, the self argument in class methods is passed implicitly. It’s included in the function definition, but not in the function calls. Internally, what’s going on is that a call like object.method(argument) is translated to Class.method(object, argument) and therefore the object itself implicitly becomes the first argument.\ndo_put() is an Arrow Flight method (as well as a Python class method), and because of that it also takes a context argument that, much like self is passed implicitly. This post isn’t the place to have that discussion – it’s too long already – but for now it suffices to note that Arrow will handle the context argument for us, in an analogous fashion to how Python handles self for us.\n\nNow that we have that sorted, let’s have a look at the part of the code that actually does the server-side work. Specifically, it’s this line in do_put():\n\n\n\ntiny_flight.py [server]\n\n        self.tables[table_name] = reader.read_all()\n\n\nLet’s unpack this line one step at a time.\nThe reader object has been passed to the server as one of the arguments to do_put(), and it’s a RecordBatchStreamReader. That is, it’s an object capable of receiving a stream of Arrow data. When the read_all() method is called, it reads all record batches sent by the client and returns the final result as an Arrow table. This table is then stored in the self.tables dictionary.\nNext, notice that the key against which the table is stored as the value is specified by descriptor.command. This part of the code also needs to be explained! What is a “descriptor” object? What is the “command” attribute of a descriptor? That’s not at all obvious from inspection. To resolve our confusion, it helps to realise that this descriptor object is one of the arguments to the the server-side do_put() function, and the code that creates this object is over on the the client side. So let’s look at the code I wrote for the client side:\n\n\n\ntiny_flight.py [client]\n\n    def put_table(self, name, table):\n        table_name = name.encode('utf8')\n        descriptor = flight                         \\\n                     .FlightDescriptor              \\\n                     .for_command(table_name)\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n        writer.write(table)\n        writer.close()\n\n\nHere we have a put_table() function written in Python that does roughly the same job that the flight_put() function was doing for us in the R example I presented earlier. It’s a high-level wrapper function that sends a do_put() call to the server, streams the data across, and then stops. This line of code in this function is the one that makes the do_put() call:\n\n\n\ntiny_flight.py [client]\n\n        writer, reader = self                       \\\n                         .connection                \\\n                         .do_put(descriptor,\n                                 table.schema)\n\n\nOkay, so the descriptor on the client side is also the thing that later gets used on the server side to create the key against which the table is stored. If we look at the preceding line of code, we can see that the descriptor object is an instance of the FlightDescriptor class. So let’s actually step into the Python console and run the commands required to create a flight descriptor object:12\n\ntable_name = b'name-of-data'\ndescriptor = flight.FlightDescriptor.for_command(table_name)\nprint(descriptor)\n\n&lt;FlightDescriptor command: b'name-of-data'&gt;\n\n\nPerhaps unsurprisingly, the command attribute is in fact the (byte encoded) string that we used to specify the name. In other words, once we strip back all the layers here it turns out that the server stores the data set using the name that the client gave it!\n\ndescriptor.command\n\nb'name-of-data'\n\n\n\n\n\nGetting a table\nNext, let’s have a look at the code used to get data from the server. Just like last time, I’ll put the relevant sections from the server code and the client side code side by side:\n\n\n\ntiny_flight.py [server]\n\n    def do_get(self, context, ticket):\n        table_name = ticket.ticket\n        self.server_message('do_get', table_name)\n        table = self.tables[table_name]\n        return flight.RecordBatchStream(table)\n\n\n\n\n\ntiny_flight.py [client]\n\n    def get_table(self, name):\n        table_name = name.encode('utf8')\n        ticket = flight.Ticket(table_name)\n        reader = self.connection.do_get(ticket)\n        return reader.read_all()\n\n\nOn the client side, the get_table() helper function that I’ve written does two things. First it creates a Ticket object from the name of the data table to be retrieved. It then calls the do_get() flight method to communicate with the server. Then, using the reader object returned by do_get(), it streams the data from the server. The server side code is the mirror image: when the ticket is received, it uses this ticket to retrieve the specific table from self.tables, and returns a stream.\nLooking at these two code extracts side by side we can see that the ticket object returned client-side by flight.Ticket() gets used server-side to retrieve the requested table. So we should take a look at what happens here. What we hope to see is that this ticket produces the same key used to store the data originally: that is, when the server specifies a storage key with table_name = ticket.ticket in the do_get() method, it should match the key created by do_put() when table_name = descriptor.command was executed.\nLet’s verify that this is true!\nSince I already have a table_name object lying around from earlier, let’s run that line of code shall we?\n\nticket = flight.Ticket(table_name)\nprint(ticket)\n\n&lt;Ticket b'name-of-data'&gt;\n\n\nThat looks promising. If we take a peek at ticket.ticket, we see that – yet again – under the hood the ticket is just an alias for the name of the data set:\n\nticket.ticket\n\nb'name-of-data'\n\n\nWell that’s a relief. In the server-side code, the descriptor.command object and the ticket.ticket object both produce the correct key used to index a table.\n\n\n\n\n\nBoarding a flight. Image by Joshua Woroniecki from Pixabay\n\n\n\n\n\n\nGetting information\nOur journey through the source code continues. On the client side I’ve written a function called list_tables() that returns the names of all tables stored on the server. Here’s what that looks like:\n\n\n\ntiny_flight.py [client]\n\n    def list_tables(self):\n        names = []\n        for flight in self.connection.list_flights():\n            table_name = flight.descriptor.command\n            names.append(table_name.decode('utf-8'))\n        return names\n\n\nThe key part of this function is the call to self.connection.list_flights(). That’s where the client contacts the server and requests information. Everything else in the function is there to extract the one piece of information (the name of the table) that we’re interested in and return it to the user.\nPivoting over to the server code, there are two flight methods that are relevant here. The get_flight_info() function is a flight method that returns information about a single flight – where, in this case, there’s a one-to-one mapping between flights and tables – and the list_flights() method can be used to retrieve information about all flights stored on the server:\n\n\n\ntiny_flight.py [server]\n\n    def get_flight_info(self, context, descriptor):\n        table_name = descriptor.command\n        self.server_message('get_flight_info',\n                            table_name)\n        return self.flight_info(descriptor)        \n        \n    def list_flights(self, context, criteria):\n        self.server_message('list_flights', b' ')\n        for table_name in self.tables.keys():\n            descriptor = flight                  \\\n                         .FlightDescriptor       \\\n                         .for_command(table_name)\n            yield self.flight_info(descriptor)\n\n\nThere’s two things to comment on here. First, note that the list_flight() method iterates13 over all the stored keys in the tables dictionary, uses the key to construct a flight descriptor, and then calls the flight_info() helper function that I’ll explain in a moment. In contrast, the get_flight_info() function receives a flight descriptor directly from the client, so it’s much simpler: it just calls flight_info() directly.\nOkay, so now let’s have a look at the flight_info() helper method. Here’s the code for that one:\n\n\n\ntiny_flight.py [server]\n\n    def flight_info(self, descriptor):\n        table_name = descriptor.command\n        table = self.tables[table_name]\n\n        ticket = flight.Ticket(table_name)\n        location = self.location.uri.decode('utf-8')\n        endpoint = flight.FlightEndpoint(ticket,\n                                         [location])\n        \n        return flight.FlightInfo(table.schema, \n                                 descriptor, \n                                 [endpoint], \n                                 table.num_rows,\n                                 table.nbytes)\n\n\nLet’s start by looking at the return value. It’s a FlightInfo object and as I mentioned earlier there are five things needed to create it:\n\ntable.schema is the Schema for the data stored by the flight\nThe flight descriptor object, which was passed as input\nA list of one or more FlightEndpoint objects – in this cases, [endpoint] is a list containing a single endpoint – that specifies where the data are available for streaming. Each endpoint includes a location from which to stream, and the associated ticket for that location\nThe total number of records can be accessed from table.num_rows\nThe total number of bytes can be accessed from table.nbytes\n\nLooking at the rest of the function, you can see that the list of endpoints requires a little work to construct. We need to call flight.Ticket() to construct a ticket object, we need to extract the server location that we stored when the server was initialised, and then we need to call flight.Endpoint() to put these things together. There’s a little more code involved, but thankfully it’s not conceptually difficult.\n\n\n\nCustom actions\nWe’ve encountered (and implemented) four of the methods defined by the Arrow Flight protocol, and as we’ve seen they server different purposes. The do_put() and do_get() methods are used to stream data to and from the server.14 In contrast, the get_flight_info() and list_flights() methods are used to retrieve metadata about the data stored on the server. In this example, these four methods are sufficient to provide all the core functionality. I could stop here if I absolutely wanted to. But there’s one more method I want to draw your attention to: do_action(). In recognition of the fact that real world applications will always need to perform custom operations that weren’t originally built into the protocol, the do_action() method exists to allow the client to request (and the server to perform) custom actions that you can define however you like.\nTo give you a sense of how this works, we’ll add two custom actions to our server: when the client requests a 'drop_table' action, the corresponding table will be deleted from the server, and when the client requests a 'shutdown' action the server will shut itself down.15 Let’s take a look at the server-side code implementing this:\n\n\n\ntiny_flight.py [server]\n\n    def do_action(self, context, action):\n        if action.type == 'drop_table':\n            table_name = action.body.to_pybytes()\n            del self.tables[table_name]\n            self.server_message('drop_table',\n                                table_name)\n\n        elif action.type == 'shutdown':\n            self.server_message('shutdown', b' ')\n            self.shutdown()\n\n        else:\n            raise KeyError('Unknown action {!r}'.\n                           format(action.type))\n\n\nThe do_action() method expects to receive an Action object as input to the action argument. The action.type attribute is where the name of the action is stored, so the code here uses if-else to decide which action to perform, or raise an error if the action type is not recognised. The code implementing the actions is pretty minimal. On a shutdown action, the server calls the self.shutdown() method: this is inherited from FlightServerBase, I didn’t have to implement it myself. For a drop table action, the server inspects the action.body argument to determine the name of the table to be dropped, and then deletes it from self.tables.16\nHow does the client call this method? To see an example of this, let’s flip over to the client side of the code and look at the drop_table() function:\n\n\n\ntiny_flight.py [client]\n\n    def drop_table(self, name):\n        table_name = name.encode('utf8')\n        drop = flight.Action('drop_table', table_name) \n        self.connection.do_action(drop)\n\n\nHappily, it turns out to be simple: first we call flight.Action() to construct the action object itself, passing the action type and action body as arguments. Then we call the built-in do_action() client-side method, which as usual we can access from the self.connection object.\nVoilà! We are done. The server and client are both ready to go. Let’s take them for a spin, shall we?\n\n\n\n\n\nBee flight. Image by Gary Stearman from Pixabay\n\n\n\n\n\n\nUsing our server\nLet’s start with a few imports. In this demonstration I’m going to read data from csv files, so I’ll import the csv submodule from pyarrow. Obviously, I’ll also need access to the server classes, and since the tiny_flight.py script is bundled with this post I can import that too. Finally, I’m going to start the server running in its own thread, so I’ll import threading too:\n\nimport threading\nimport tiny_flight as tiny\nfrom pyarrow import csv\n\nNext, I’ll initialise a server running on port 9001 and start it running in its own thread. This will turn out to be handy because when the client starts interacting with the server, we’ll see the server messages as well as the client output!\n\nserver = tiny.TinyServer(port = 9001)\nthread = threading.Thread(target = lambda: server.serve(), \n                          daemon = True)\nthread.start()\n\nNow that the server is up and running, let’s instantiate a client and have it connect to the server:\n\nclient = tiny.TinyClient(port = 9001)\n\n(server) list_flights  \n\n\nNotice the message prefixed with (server) here: that part of the output is generated by the server running in the thread. It’s not client-side output. We’re only seeing it here because when the client is initialised by the call to flight.connect(), it calls the list_flights() flight method, and the server prints a message using its internal server_message() function.\nOkay. So far so good. Our next step is to create some Arrow tables client side. To make this simpler I have csv files containing copies of the freely-available 1973 New York city air quality and earthquakes near Fiji since 1964 data sets that are both bundled by the datasets R package. I’ll import them both as pyarrow tables:\n\nfijiquakes = csv.read_csv(\"fijiquakes.csv\")\nairquality = csv.read_csv(\"airquality.csv\")\n\nAt the moment both tables exist on the client, and we’d like to cache them on the server. We can do this by calling the put_table() method:\n\nclient.put_table(\"fijiquakes\", fijiquakes)\nclient.put_table(\"airquality\", airquality)\n\n(server) do_put fijiquakes\n(server) do_put airquality\n\n\nAgain, notice that the server prints messages which make clear that the data have arrived on the server side. Of course, the client doesn’t actually know this because my server-side code for do_put() doesn’t implement a server response for the client. But no matter: the client can check manually by calling list_tables():\n\nclient.list_tables()\n\n(server) list_flights  \n['fijiquakes', 'airquality']\n\n\nHere we see two lines of output: the first one is the server-side log, and the second is the output returned client-side showing that both tables exist on the server.\nWe can take this a step further, of course, by retrieving the data from the server cache. We can do that straightforwardly by calling get_table(), and again see that a server-side message is printed, while the table itself is returned to the client:\n\nclient.get_table(\"fijiquakes\")\n\n(server) do_get fijiquakes\npyarrow.Table\nlat: double\nlong: double\ndepth: int64\nmag: double\nstations: int64\n----\nlat: [[-20.42,-20.62,-26,-17.97,-20.42,...,-25.93,-12.28,-20.13,-17.4,-21.59]]\nlong: [[181.62,181.03,184.1,181.66,181.96,...,179.54,167.06,184.2,187.8,170.56]]\ndepth: [[562,650,42,626,649,...,470,248,244,40,165]]\nmag: [[4.8,4.2,5.4,4.1,4,...,4.4,4.7,4.5,4.5,6]]\nstations: [[41,15,43,19,11,...,22,35,34,14,119]]\n\n\nNow, perhaps we decided that we don’t need a cached copy of the airquality table any longer. We can ask the server to remove it by calling drop_table(), and we can confirm the result by calling list_tables() again:\n\nclient.drop_table(\"airquality\")\nclient.list_tables()\n\n(server) drop_table airquality\n(server) list_flights  \n['fijiquakes']\n\n\nYep, that all looks right!\n\n\n\n\n\nBirds in flight. Image by Gerhard from Pixabay"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#where-to-next",
    "href": "posts/2022-10-18_arrow-flight/index.html#where-to-next",
    "title": "Building an Arrow Flight server",
    "section": "Where to next?",
    "text": "Where to next?\nCompared to the rest of the Apache Arrow project, it’s not so easy to find tutorials and documentation about flight. It’s still a little piecemeal. With that in mind, here’s an annotated reading list that will be helpful if you want to explore flight further:\n\nThe original announcement of flight by Wes McKinney on the Apache Arrow blog gives a very good overview of the motivation for why flight was introduced.\nData transfer at the speed of flight by Tom Drabas, Fernanda Foertter, and David Li. This is a blog post on the Voltron Data blog that provides a concrete example of a working flight server written in Python. The Python code I’ve discussed in this post is an elaboration of the content in that post. It’s a good starting point.\nApache Arrow Flight: A Primer by David Li and Tom Drabas. This is another blog post on the Voltron Data website. This one doesn’t have any working code for you to look at, but it provides a good summary of the technologies that Arrow Flight is built upon. It’s a little intense for novices but is pretty handy for intermediate level users who want to take a peek under the hood.\nThe Python documentation flight vignette is pretty readable and goes into a moderate amount of detail, but be aware it implicitly assumes some familiarity with remote procedure calls.\nThe Python cookbook for Arrow contains the most thorough worked example I’ve seen anywhere. It’s a little dense for novice users, but it’s still the one of the most comprehensive resources I’ve seen, and the only one that talks about issues like authentication (which I have not discussed at all here!)\nThe R documentation flight vignette has a succinct overview of how you can use the high-level interface provided by flight_put(), flight_get(), etc. What it doesn’t do (yet?) is discuss the low-level features. At the moment you won’t find a discussion of say client$do_get() and how it relates to flight_get().\nAlong similar lines there are some examples in the R cookbook, but they are also quite minimal.\nShould you be interested in writing an Arrow Flight service in C++, the documentation pages for the C++ Flight implementation may come in handy!\nIf you’re willing to spend some money I thoroughly recommend the chapter on Arrow flight in Matt Topol’s book In-Memory Analytics with Apache Arrow. I found it really helpful for cementing my own understanding. In addition to the worked examples in Python, C++, and Go, the chapter provides some historical context for understanding the difference between RPC frameworks and REST frameworks, and is also the only resource I’m aware of that goes into detail about how more sophisticated network architectures are supported by flight.\nIf you’re keen to understand what is happening under the hood, at some point you’re going to want to read about gRPC. The flight protocol is built on top of gRPC, and a lot of the advanced content you’ll encounter on flight doesn’t make a lot of sense until you’ve started to wrap your head around it. To that end, I found the introduction to gRPC documentation really helpful. You may also want to take a look at the documentation for protocol buffers because in practice that’s doing a lot of the work for us here!\nIf you want to understand the backdrop against which all this sits, it’s also pretty handy to do a bit of digging around and reading the history around remote procedure call (RPC) approaches to distributed computing and representational state transfer (REST) approaches. Even skimming the two linked Wikipedia articles was helpful for me.\nWhen digging around in source code, I found it handy to take a look at these parts of the code base: source code for the R demo server, a Python example server, and the pyarrow flight implementation.\nFinally, while neither one is ideal as a place to start, once I started getting the hang of what I was doing, I have found it handy to browse through the Python flight API reference pages, and to occasionally dip into the official Arrow flight RPC specification. Regarding the latter, my experience was that the images showing how each of the flight methods operates were handy, and the comments shown in the in the “protocol buffer definitions” are nice because they’re maybe the clearest verbal description of what each of the flight methods expects as input and what objects they will return.\n\nHappy hunting!"
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#acknowledgements",
    "href": "posts/2022-10-18_arrow-flight/index.html#acknowledgements",
    "title": "Building an Arrow Flight server",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nMy sincere thanks to Rok Mihevc, David Li, Kae Suarez, and François Michonneau for reviewing earlier versions of this post."
  },
  {
    "objectID": "posts/2022-10-18_arrow-flight/index.html#footnotes",
    "href": "posts/2022-10-18_arrow-flight/index.html#footnotes",
    "title": "Building an Arrow Flight server",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll the way back to October 2019, which is like ancient history by Arrow standards. Sigh. This project moves too damned fast to keep pace with it all.↩︎\nOne of my favourite things about having a quarto blog is that every post is a notebook. It’s technically possible to “cheat” by including hidden code chunks that execute different code than that shown in the post, but it’s something I do very sparingly and only when there’s some weirdness involved. I’m not doing that here. When this post is rendered, it does start a new instance of the demo server in a different R session: every flight server demonstrated here is in fact running in the background so that the post renders, and server side data are all stored by those other processes. There really is no copy of the pollution_data object in the R session used to render this post. It’s somewhere else, as it bloody well should be.↩︎\nSpecifically, when the client calls the do_put() method, a RecordBatchStreamWriter object is returned. This object in turn has write_batch() and write_table() methods that can be used to stream data. It’s a bit beyond the scope of the post to talk about the details here, but if you ever find yourself needing to use this capability this is where to start!↩︎\nOptionally, you can also pass a third “options” argument.↩︎\nThe examples in this post are simple ones where the server doesn’t actually send a response, so the reader object isn’t used for anything↩︎\nAs an aside, it’s not clear to me that this will remain true in the long run. There are a lot of advantages to having access to all the features of the Arrow Flight RPC specification, and if my dev skills are up to the task I may attempt to update the R bindings. No promises though :-)↩︎\nIf I were so inclined I’d probably point out this is the maybe the clearest illustration that we’re adopting an RPC approach to distributed computing – this is not a REST API. After all, if the server code doesn’t make sense without looking at the client code, there’s a pretty good chance you don’t meet the “uniform interface” REST criterion. This isn’t a criticism of flight, of course: there are good reasons to adopt a non-RESTful RPC approach sometimes. However, that’s beyond the scope of this post. It’s already too long as it is!↩︎\nOkay fine, I’m oversimplifying a little here. Technically, the TinyClient object does not have a do_put() method: that’s actually a property of the connection object attached to every TinyClient object.↩︎\nPeople with more experience in Python (well, more than me) would notice the importance of the b'' notation. One thing I didn’t know about Python until recently is that it is quite explicit in specifying how strings are encoded. The b'' notation is used to indicate that this is a “byte literal” string. To convert it to utf-8 text, it needs to be explicitly decoded. I mention this here because later on in the post I’m going to call .encode() and .decode() string methods to switch back and forth between byte literals and utf-8 strings. I’m assuming this is common knowledge among Python users, but coming from R this was a little surprising!↩︎\nThat said, the symmetry isn’t exact: the TinyServer object explicitly subclasses FlightServerBase and overrides several of its methods in order to provide server functionality. In contrast, TinyClient is not a subclass. Rather, every TinyClient object contains a connection object that is a FlightClient instance. The explicitly-defined methods for the TinyClient class internally call the methods of the connection object whenever the client needs to call a flight method.↩︎\nAs an aside, note that all the “strings” that the client and server are using to represent tickets, locations, etc are represented as byte literals. That means that the name argument that gets passed to server_message() will always be a byte literal, not utf-8 encoded. In order to print a message to the console, we need to decode the bytes into utf-8 format, which is why the code for server_message() uses name.decode(\"utf-8\").↩︎\nIn this example I’ve used the for_command() method to construct the flight descriptor. This isn’t the only way to do it: you can use the for_path() method also. I’m only mentioning this because when you look at the code used in other flight servers, you’ll sometimes see the for_path() method used in a similar fashion to the way I’m using for_command() here.↩︎\nFor fellow Python newbies: if you’re unsure about why list_flights() generates return values with yield rather than return, it’s worth taking a little time to read up on Python iterables and generators. There’s an excellent explanation on this stackoverflow question↩︎\nThere is also a do_exchange() method that allows bidirectional streaming, but I’m not going to talk about it in this post↩︎\nI mean yeah, in real life you might want to have some… um… authentication in place before letting an arbitrary client take these kinds of actions, but I’m not going to talk about that here!↩︎\nIf I were being more rigorous I’d include some input checking here, but let’s be realistic: that’s not even close to the most unsafe thing about my simple example. I cannot stress enough that this code is for explanatory purposes, it’s not production code!↩︎"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html",
    "href": "posts/2024-12-16_regex-backreferences/index.html",
    "title": "Baby got backreferences",
    "section": "",
    "text": "At least once a week I experience a particular kind of despair that is all too familiar to programmers. I am sitting alone with my laptop and my code, minding my own business, working quietly. Out of nothing a yawning pit of black despair opens at my feet, barbed tentacles wrap around my legs, poison injects chill into my veins, the icy claws of anxiety rip through my viscera, and a withered voice of pure evil slithers into my brain. The voice speaks to me in the disturbingly-seductive language of Mordor, and a terrible incantation consumes my thoughts:\nwhy not write a regular expression?\nI try to resist. I’m a good girl, I tell the Dark Lord. I would never. Not on a first date anyway. Well, buy a girl a drink first maybe? And… oh it’s so cold outside and, I mean, Sauron is kinda hot. Have you not watched Rings of Power? Sometimes a girl has needs.\nUm. Anyway. What was I talking about? Oh, right. Regular expressions."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#warning",
    "href": "posts/2024-12-16_regex-backreferences/index.html#warning",
    "title": "Baby got backreferences",
    "section": "Warning",
    "text": "Warning\nLet’s start this love affair with darkness with a disclaimer: I have never wanted to write a primer on working with regular expressions in R. I loathe regular expressions, and I am not good at them. About 99% of my knowledge of regular expressions comes from weeping in abject despair with a dozen R documentation tabs open and desperately clicking on random links at www.regular-expressions.info to work out why my regex doesn’t work.\nI am, very clearly, not an expert.\nSo why am I, a grotesquely unqualified woman, writing this post anyway? Honestly, the answer is because this post is the tutorial that I need. I’m writing this as an act of solidarity with future Danielle, who will find inevitably herself crying at her keyboard trying to make a basic regex work, and will need a little kindness and support. With that in mind, and knowing exactly who will be revisiting this post in six months time in tears… you got this girl. I believe in you.\nLet’s get started. In this post I’ll be working mostly with the stringr package, so I’ll load that at the beginning:\n\nlibrary(stringr)\n\nI’ll use a variety of sources for the text, mostly song lyrics. Each of those is stored in a text file so I’ll read the text now:\n\nfeather &lt;- brio::read_lines(\"feather.txt\")\nfemininomenon &lt;- brio::read_lines(\"femininomenon.txt\")\nmidnight &lt;- brio::read_lines(\"midnight.txt\")\nbabygotback &lt;- brio::read_lines(\"babygotback.txt\")\n\nEach of these is a vector of strings. To give you a sense of what they look like, here’s the first few lines in the opening to babygotback:\n\nhead(babygotback)\n[1] \"[Intro: Girl]\"                                                        \n[2] \"Oh my God, Becky, look at her butt, it is so big, ugh\"                \n[3] \"She looks like one of those rap guys' girlfriends, but, ugh, you know\"\n[4] \"Who understands those rap guys? Ugh, they only talk to her\"           \n[5] \"Because she looks like a total prostitute, okay?\"                     \n[6] \"I mean, her butt, it's just so big\"                                   \n\n\nWe shall return to this classic text later, but the first few examples will all use femininomenon:\n\nhead(femininomenon)\n[1] \"[Verse 1]\"                                        \n[2] \"Same old story, time again\"                       \n[3] \"Got so close but then you lost it\"                \n[4] \"Should've listened to your friends\"               \n[5] \"'Bout his girlfriend back in Boston\"              \n[6] \"You sent him pictures and playlists and phone sex\"\n\n\nHopefully you get the idea. Each of these is a vector, each element is a string, and we’ll do some pattern matching on those strings, most often using str_view() from the stringr package to show which parts of the text each regex matches against. It’s not the best tool for using in a script, but it’s ideal as a way of visually inspecting the results of a regular expression matching exercise."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#simple-matches",
    "href": "posts/2024-12-16_regex-backreferences/index.html#simple-matches",
    "title": "Baby got backreferences",
    "section": "Simple matches",
    "text": "Simple matches\nIt begins with the very, very basics. Regular expressions are a tool for matching (and sometimes modifying) patterns in text strings. The simplest way to write a regular expression is to detect strings that match a specific, fixed subset of text. For instance, let’s say I want to find every line in Femininomenon in which Chappell Roan sings the word \"femininomenon\". Using str_view() this is what we get:\n\nstr_view(\n  string = femininomenon,   # the song lyrics\n  pattern = \"femininomenon\" # the regular expression\n)\n[24] │ (It's a fem) It's a &lt;femininomenon&gt;\n[28] │ (It's a fem) It's a &lt;femininomenon&gt;\n[53] │ (It's a fem) It's a &lt;femininomenon&gt;\n[57] │ (It's a fem) It's a &lt;femininomenon&gt;\n[65] │ Well, what we really need is a &lt;femininomenon&gt;\n[66] │ (A what?) A &lt;femininomenon&gt;\n[72] │ (It's a fem) It's a &lt;femininomenon&gt;\n[82] │ (It's a fem) It's a &lt;femininomenon&gt;\n[86] │ (It's a fem) It's a &lt;femininomenon&gt;\n[90] │ Make a bitch, it's a fem (It's a &lt;femininomenon&gt;)\n\n\nIn this output, the teal-highlighted text1 enclosed in angle brackets displays the sections of the text that match the regular expression. In this case our regular expression is very simple. It’s just a literal string \"femininomenon\", so the output highlights every instance of that word.\nNotice also that not every line in the song is shown by str_view(). Only those lines that match the regular expression are included (you can see that in the numbers to the left of each match). However, we can change this behaviour using the match argument to str_view(). For example, if wanted to see only those lines that don’t include the letter e, we could do this:\n\nstr_view(\n  string = femininomenon,\n  pattern = \"e\", \n  match = FALSE\n)\n[11] │ Why can't any man\n[12] │ \n[19] │ \n[20] │ [Chorus]\n[29] │ \n[40] │ Why can't any man\n[41] │ \n[48] │ \n[49] │ [Chorus]\n[58] │ \n[67] │ \n[77] │ \n[78] │ [Chorus]\n[87] │ \n[88] │ [Outro]\n\n\nAlternatively, we could set match = NA. If we do this, str_view() will return every line in the song, whether it matches or not. Here’s an example. Let’s search for every instance of the word \"you\", and set match = NA:\n\nstr_view(\n  string = femininomenon,\n  pattern = \"you\",\n  match = NA\n)\n [1] │ [Verse 1]\n [2] │ Same old story, time again\n [3] │ Got so close but then &lt;you&gt; lost it\n [4] │ Should've listened to &lt;you&gt;r friends\n [5] │ 'Bout his girlfriend back in Boston\n [6] │ You sent him pictures and playlists and phone sex\n [7] │ He disappeared from the second that &lt;you&gt; said\n [8] │ \"Let's get coffee, let's meet up\"\n [9] │ I'm so sick of online love\n[10] │ And I don't understand\n[11] │ Why can't any man\n[12] │ \n[13] │ [Pre-Chorus]\n[14] │ Hit it like, get it hot\n[15] │ Make a bitch, it's a fem\n[16] │ Hit it like, get it hot\n[17] │ Make a bitch, it's a—\n[18] │ Um, can &lt;you&gt; play a song with a fucking beat?\n[19] │ \n[20] │ [Chorus]\n... and 72 more\n\n\nThis output illustrates two things. First, you can see that there’s a match on lines 3, 4, 7, 18, and probably others too (the remaining 72 lines of the song aren’t shown in the output but they are actually included in the str_view() output). Second, it shows that our regular expression isn’t quite doing the job we want it to: the match on line 4 matches the first three letters of the word “your”, and it doesn’t match to line 6 because that line contains the word “You” with an upper case “Y”. If we want a regular expression to match “you” and “You” but not match against “your”, it needs to be something a little more nuanced than setting pattern = \"you\" like I did above.\n\n\n\nI sent him pictures and playlists and regex"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#quantifiers",
    "href": "posts/2024-12-16_regex-backreferences/index.html#quantifiers",
    "title": "Baby got backreferences",
    "section": "Quantifiers",
    "text": "Quantifiers\nFor the moment, let’s set aside the thorny question of how to handle capitalisation, and focus instead on the “you” vs “your” issue. Moreover, let’s move the goalposts, and pretend that our new goal is actually to detect both \"you\" and \"your\". This post is not a place of honour, and I’m not above cheating when it comes to these matters.\nWe can solve our new problem with the help of quantifiers. In regular expression syntax, certain character (sometimes called “metacharacters”) have special meanings. For example, the ? character is used to indicate that the preceding character is optional. When we write \"your?\" as our regular expression, the r? part indicates that the r character is optional. It will match against both \"you\" and \"your\":\n\nstr_view(femininomenon, \"your?\")\n [3] │ Got so close but then &lt;you&gt; lost it\n [4] │ Should've listened to &lt;your&gt; friends\n [7] │ He disappeared from the second that &lt;you&gt; said\n[18] │ Um, can &lt;you&gt; play a song with a fucking beat?\n[33] │ Lying to &lt;your&gt; friends about\n[35] │ Stuck in the suburbs, &lt;you&gt;'re folding his laundry\n[36] │ Got what &lt;you&gt; wanted, so stop feeling sorry\n[47] │ Dude, can &lt;you&gt; play a song with a fucking beat?\n[60] │ Ladies, &lt;you&gt; know what I mean\n[61] │ And &lt;you&gt; know what &lt;you&gt; need\n[76] │ Did &lt;you&gt; hear me? Play the fucking beat\n\n\nHere you can see that it matches the \"you\" on line 3 and the \"your\" on line 4. However, on line 35 where it encounters the word \"you're\" it detects a match, but only to the \"you\" part of the word.\nThere are several quantifiers supported by most regular expression flavours.2 Quantifiers always refer to the previous item, and specify the number of repetitions of the previous item that can be matched:\n\n? matches the previous item zero or one time\n* matches the previous item zero or more times\n+ matches the previous item one or more times\n{n} matches the previous item exactly n times\n{n,} matches the previous item n or more times\n{n,m} matches the previous item at least n times but not more than m times\n\nThis behaviour is (of course!) described in numerous places in the R documentation, but I find it helps a lot to have a concrete example that shows the differences between each of these. In the table below, the rows correspond to the strings \"a\", \"ab\", \"abb\", and so on. Each column corresponds to a a regular expression that is always \"ab\" with a quantifier applied to the letter \"b\". In each cell of the table, I’ve used str_view() to show how each string matches (or doesn’t match) against each of the regexes:\n\n\nCode\ntibble::tibble(\n  string    = c(\"a\", \"ab\", \"abb\", \"abbb\", \"abbbb\", \"abbbbb\"),\n  `ab?`     = str_view(string, pattern = \"ab?\", match = NA),\n  `ab*`     = str_view(string, pattern = \"ab*\", match = NA),\n  `ab+`     = str_view(string, pattern = \"ab+\", match = NA),\n  `ab{2}`   = str_view(string, pattern = \"ab{2}\", match = NA),\n  `ab{2,}`  = str_view(string, pattern = \"ab{2,}\", match = NA),\n  `ab{2,4}` = str_view(string, pattern = \"ab{2,4}\", match = NA)\n)\n\n# A tibble: 6 × 7\n  string `ab?`      `ab*`      `ab+`      `ab{2}`    `ab{2,}`   `ab{2,4}` \n  &lt;chr&gt;  &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt;\n1 a      &lt;a&gt;        &lt;a&gt;        a          a          a          a         \n2 ab     &lt;ab&gt;       &lt;ab&gt;       &lt;ab&gt;       ab         ab         ab        \n3 abb    &lt;ab&gt;b      &lt;abb&gt;      &lt;abb&gt;      &lt;abb&gt;      &lt;abb&gt;      &lt;abb&gt;     \n4 abbb   &lt;ab&gt;bb     &lt;abbb&gt;     &lt;abbb&gt;     &lt;abb&gt;b     &lt;abbb&gt;     &lt;abbb&gt;    \n5 abbbb  &lt;ab&gt;bbb    &lt;abbbb&gt;    &lt;abbbb&gt;    &lt;abb&gt;bb    &lt;abbbb&gt;    &lt;abbbb&gt;   \n6 abbbbb &lt;ab&gt;bbbb   &lt;abbbbb&gt;   &lt;abbbbb&gt;   &lt;abb&gt;bbb   &lt;abbbbb&gt;   &lt;abbbb&gt;b  \n\n\nNotice that these are all eager (sometimes called greedy), in the sense that they will always match to as many repetitions as they possibly can while satisfying the rules. However, you can reverse this behaviour and make a quantifier lazy by appending ? immediately after the quantifier. A lazy quantifier will match against the fewest number of repetitions as possible while still satisfying the rule. Here’s what that looks like:\n\n\nCode\ntibble::tibble(\n  string    = c(\"a\", \"ab\", \"abb\", \"abbb\", \"abbbb\", \"abbbbb\"),\n  `ab??`     = str_view(string, pattern = \"ab??\", match = NA),\n  `ab*?`     = str_view(string, pattern = \"ab*?\", match = NA),\n  `ab+?`     = str_view(string, pattern = \"ab+?\", match = NA),\n  `ab{2}?`   = str_view(string, pattern = \"ab{2}?\", match = NA),\n  `ab{2,}?`  = str_view(string, pattern = \"ab{2,}?\", match = NA),\n  `ab{2,4}?` = str_view(string, pattern = \"ab{2,4}?\", match = NA)\n)\n\n# A tibble: 6 × 7\n  string `ab??`     `ab*?`     `ab+?`     `ab{2}?`   `ab{2,}?`  `ab{2,4}?`\n  &lt;chr&gt;  &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt; &lt;strngr_v&gt;\n1 a      &lt;a&gt;        &lt;a&gt;        a          a          a          a         \n2 ab     &lt;a&gt;b       &lt;a&gt;b       &lt;ab&gt;       ab         ab         ab        \n3 abb    &lt;a&gt;bb      &lt;a&gt;bb      &lt;ab&gt;b      &lt;abb&gt;      &lt;abb&gt;      &lt;abb&gt;     \n4 abbb   &lt;a&gt;bbb     &lt;a&gt;bbb     &lt;ab&gt;bb     &lt;abb&gt;b     &lt;abb&gt;b     &lt;abb&gt;b    \n5 abbbb  &lt;a&gt;bbbb    &lt;a&gt;bbbb    &lt;ab&gt;bbb    &lt;abb&gt;bb    &lt;abb&gt;bb    &lt;abb&gt;bb   \n6 abbbbb &lt;a&gt;bbbbb   &lt;a&gt;bbbbb   &lt;ab&gt;bbbb   &lt;abb&gt;bbb   &lt;abb&gt;bbb   &lt;abb&gt;bbb  \n\n\nObviously, some of those are silly. There’s no point whatsoever in writing a regular expression like ab{2,4}?, because it produces exactly the same behaviour as ab{2}. Nevertheless, there are some instances where lazy quantifiers turn out to be useful, so I’m mentioning them here just in case I come back in tears next year trying to figure out how the accursed things work.\nIn everyday life I find that 95% of the time the only quantifiers I ever need are ?, *, and +, but every now and then I find a need to use the more flexible {} notation. To give one example of this, I’ll use a recent Midnight Pals thread, which I loaded earlier as the midnight vector. If you’re not a regular reader of Midnight Pals, it’s essentially a literary satire that makes oblique references to current events. Edgar Allen Poe, Stephen King, Clive Barker, and Mary Shelley are recurring characters, for instance. So too is JK Rowling, though she doesn’t associate with the other characters and instead tends to live in her own “mysterious circle of robed figures”. To give you a sense of what the dialog typically looks like, here’s the first few lines:\n\nhead(midnight)\n[1] \"[mysterious circle of robed figures]\"            \n[2] \"JK Rowling: hello children\"                      \n[3] \"Rowling: what newsss, wormtongue?\"               \n[4] \"Rowling: how goes the infiltration of bluesssky?\"\n[5] \"Jesse Singal: mommy mommy\"                       \n[6] \"Singal: it was SO HARD\"                          \n\n\nAs illustrated in this extract, one of the conceits of the JKR character is that she hisses many of her lines3 and so the text often contains words like \"bluesssky\" or \"transssses\". So let’s see if we can write a regular expression to detect the hissed words. The actual number of s-repetitions in the JKR dialog varies of course, so we might want to use a regular expression like s{3,} to find instances of three or more successive s characters:\n\nstr_view(midnight, \"s{3,}\")\n [3] │ Rowling: what new&lt;sss&gt;, wormtongue?\n [4] │ Rowling: how goes the infiltration of blue&lt;sss&gt;ky?\n[17] │ Rowling: door'&lt;sss&gt; open, boy&lt;sss&gt;!\n[20] │ Rowling: i know you did, je&lt;ssss&gt;e\n[22] │ Rowling: yess you did great je&lt;sss&gt;e\n[43] │ Rowling: patience, &lt;ssss&gt;isster, we must bide our time\n[54] │ Rowling: sure, je&lt;sss&gt;e might look like a sniveling worm, a nasty crawling slithering little shit, a spineless craven buffoon\n\n\nIt doesn’t work perfectly (e.g., it misses the \"yess\" on line 22), but it’s not too bad given how simple the regex is. That said, it’s also a bit limited insofar as it only detects the hissing (i.e., the s-repetition). Ideally, we’d like a regular expression that captures the entire hissssssed word, but to do that we’ll need to get a little fancier, and dive a little deeper into regular expression syntax. We’ll return to that later.\n\n\n\n\n\nclive. clive don’t be an instigator"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#backslashes-part-i",
    "href": "posts/2024-12-16_regex-backreferences/index.html#backslashes-part-i",
    "title": "Baby got backreferences",
    "section": "Backslashes (part I)",
    "text": "Backslashes (part I)\nThe discussion of quantifiers leads to the natural question: what if I don’t want to use ? as a quantifier, and instead want to match a literal ? in the string. To do that we can escape the special character by prefixing it with a backslash. That is, within a regular expression we would write \\? to match a literal ?.\nIt gets complicated, though, because the way we represent the regular expression is via an R string,4 and \\ has a special meaning in R strings, because it’s used as an escape character in R too. So if we want to “pass” a literal \\ into the regular expression, the thing we need to type in R is \"\\\\\". This is… confusing. I find this helps:\n\nwrite \"\\\\+\" in R to mean \\+ in the regex, and match a literal +\nwrite \"\\\\?\" in R to mean \\? in the regex, and match a literal ?\nwrite \"\\\\*\" in R to mean \\* in the regex, and match a literal *\nwrite \"\\\\{\" in R to mean \\{ in the regex, and match a literal {\n\nNotice the notation I’ve used here: when I enclose a string in quotation marks I’m referring to the string as it has to be specified in R, but when the quote marks are missing I am referring to the raw contents of the string/regex.\nIn any case, since we are using \\ as the escape character, it follows that it too is considered a special character in regular expression syntax. As such, you have to escape the backslash if you want to treat it as a literal:\n\nwrite \"\\\\\\\\\" in R to mean \\\\ in the regex, and match a literal \\\n\nNot particularly pleasant, and as you can imagine it is very easy to write regular expressions that use the wrong number of backslashes and end up in tears.\nIn any case, here’s an example. If I want to find all the question marks in the femininomenon lyrics, this is what I would have to do:\n\nstr_view(femininomenon, \"\\\\?\")\n[18] │ Um, can you play a song with a fucking beat&lt;?&gt;\n[47] │ Dude, can you play a song with a fucking beat&lt;?&gt;\n[63] │ But does it happen&lt;?&gt; (No)\n[64] │ But does it happen&lt;?&gt; (No)\n[66] │ (A what&lt;?&gt;) A femininomenon\n[76] │ Did you hear me&lt;?&gt; Play the fucking beat\n\n\nIt should also be noted that a similar thing happens with regards to special characters like \\n (line feed) that contain a backslash in their notation. The line feed character has no special meaning in a regular expression, but because you need to pass the textual representation from the R string down to the regular expression, you have to escape the backslash. That is:\n\nwrite \"\\\\n\" in R to mean \\n in the regex, and match a line feed\nwrite \"\\\\t\" in R to mean \\t in the regex, and match a tab\n\nI’ll return to this in a later section, because there are other characters that need to be escaped. But these are the ones we need for now."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#character-sets",
    "href": "posts/2024-12-16_regex-backreferences/index.html#character-sets",
    "title": "Baby got backreferences",
    "section": "Character sets",
    "text": "Character sets\nEarlier in the post we encountered the irritating problem of capital letters. If we want to detect all lines where Chappell Roan sings \"you\" or \"your\" without worrying about capitalisation, we need a way of describing “a single character that can either be y or Y”. We can do this by defining a character set (sometimes referred to as a category or class) by enclosing the set of allowed characters in square brackets. For example, the character set [yY] will match to a single instance of y or a single instance of Y. Our regular expression now becomes \"[Yy]our?\", as shown below:\n\nstr_view(femininomenon, \"[Yy]our?\")\n [3] │ Got so close but then &lt;you&gt; lost it\n [4] │ Should've listened to &lt;your&gt; friends\n [6] │ &lt;You&gt; sent him pictures and playlists and phone sex\n [7] │ He disappeared from the second that &lt;you&gt; said\n[18] │ Um, can &lt;you&gt; play a song with a fucking beat?\n[32] │ &lt;You&gt; pretend to love his mother\n[33] │ Lying to &lt;your&gt; friends about\n[35] │ Stuck in the suburbs, &lt;you&gt;'re folding his laundry\n[36] │ Got what &lt;you&gt; wanted, so stop feeling sorry\n[47] │ Dude, can &lt;you&gt; play a song with a fucking beat?\n[60] │ Ladies, &lt;you&gt; know what I mean\n[61] │ And &lt;you&gt; know what &lt;you&gt; need\n[76] │ Did &lt;you&gt; hear me? Play the fucking beat\n\n\nYou can apply quantifiers to character sets, but you need to be a little careful about this. Let’s say I have the character set [femino], and I want to apply the quantifier {4,} to detect four or more repetitions of the character set. The regular expression [femino]{4,} will match against literal repetitions like \"oooo\" or \"mmmmm\", but it will also match to words like \"omen\" and \"feminine\" because every letter in those words belongs to the character set:\n\nstr_view(femininomenon, \"[femino]{4,}\")\n [8] │ \"Let's get c&lt;offee&gt;, let's meet up\"\n[24] │ (It's a fem) It's a &lt;femininomenon&gt;\n[28] │ (It's a fem) It's a &lt;femininomenon&gt;\n[53] │ (It's a fem) It's a &lt;femininomenon&gt;\n[57] │ (It's a fem) It's a &lt;femininomenon&gt;\n[65] │ Well, what we really need is a &lt;femininomenon&gt;\n[66] │ (A what?) A &lt;femininomenon&gt;\n[72] │ (It's a fem) It's a &lt;femininomenon&gt;\n[82] │ (It's a fem) It's a &lt;femininomenon&gt;\n[86] │ (It's a fem) It's a &lt;femininomenon&gt;\n[90] │ Make a bitch, it's a fem (It's a &lt;femininomenon&gt;)\n\n\n… oh yeah, it also matches the \"offee\" in \"coffee\".\n\nRanges and shorthand notation\nWhen referring to alphabetic characters or digits, you can use a hyphen within a character set to define a range of characters. For example, [0-9] is essentially a shorthand for [0123456789] and [a-e] is short hand for [abcde]. You can define multiple ranges within a single character set, so [a-zA-Z0-9] will match against alphanumeric characters.\nSome character sets are used so often that there is a shorthand notation for them:\n\n\\d denotes digits, and is equivalent to [0-9]\n\\w denotes word characters. At a minimum it supports alphanumeric characters, but it will also match against unicode characters used in words.\n\\s denotes whitespace characters, and will match a space \" \", a tab \"\\t\", a carriage return \"\\r\", a line feed \"\\n\", or a form feed \"\\f\"; as well as unicode separator characters if the regex flavour supports unicode\n\nThere are a variety of other shorthand classes too. If you look in the base R documentation, for instance, you’ll find reference to various “POSIX classes” like :digit: and :punct: that do something similar. Over time I’ve become wary about these because they aren’t implemented consistently across regex flavours (see later), so I try to avoid them.\nTo illustrate how to use these pre-specified character sets, I’ll do a little bit of regular expression matching on the opening lines to One by Harry Nilsson, where I’ve used digits to represent the numbers rather than spelling out the words:\n\none &lt;- c(\n  \"1 is the loneliest number that you'll ever do\",\n  \"2 can be as bad as 1\",\n  \"It's the loneliest number since the number 1\"\n)\n\nTo detect the digits in this text, the regular expression we need is just the character set \\d itself. The only nuance here is that we have to specify the regex as an R string, so we have to type \"\\\\d\" in R in order to get what we want:\n\nstr_view(one, \"\\\\d\")\n[1] │ &lt;1&gt; is the loneliest number that you'll ever do\n[2] │ &lt;2&gt; can be as bad as &lt;1&gt;\n[3] │ It's the loneliest number since the number &lt;1&gt;\n\n\nAlternatively, suppose I want to detect all words in the text. One approach (which doesn’t quite work – we’ll come back to this later) would be to look for consecutive sequences of word characters, which we could specify using \\w+ as the regular expression and, to belabour the point, writing \"\\\\w+\" as the R string that specifies that regular expression:\n\nstr_view(one, \"\\\\w+\")\n[1] │ &lt;1&gt; &lt;is&gt; &lt;the&gt; &lt;loneliest&gt; &lt;number&gt; &lt;that&gt; &lt;you&gt;'&lt;ll&gt; &lt;ever&gt; &lt;do&gt;\n[2] │ &lt;2&gt; &lt;can&gt; &lt;be&gt; &lt;as&gt; &lt;bad&gt; &lt;as&gt; &lt;1&gt;\n[3] │ &lt;It&gt;'&lt;s&gt; &lt;the&gt; &lt;loneliest&gt; &lt;number&gt; &lt;since&gt; &lt;the&gt; &lt;number&gt; &lt;1&gt;\n\n\nYou can see why this doesn’t quite do what we want: the apostrophe is not a word character, and it doesn’t get detected. There’s a better approach to this using word boundaries, but I’ll come to that later when talking about anchors.\n\n\n\nI prefer the Aimee Mann cover\n\n\n\n\nLogical operations with character sets\nTo some extent you can perform logical operations on character sets: set negation, set union, set intersection, add set subtraction. These operations are all supported by regular expressions, but this is another thing where the implementation varies a little across regular expression flavours. So let’s talk about these in turn.\nOf the various operations, set union is the easiest one to describe. We’ve already talked about it. If you want to take the union of two character sets, all you have to do is concatenate them in the description. The lower case letters are represented by the range [a-z], and the upper case letters are represented by [A-Z]. The union of these classes is simply [a-zA-Z]. These are set operations, so it doesn’t matter if you include the same element multiple times: [aab] is the same character set as [ab].\nThe second easiest one to talk about is negation. If you want to represent “any character except the ones listed”, use [^ to begin the set rather than [. For example, the set [^femino ] is comprised of all characters except those seven (six letters plus the space). If I wanted to detect all instances where the Chappell Roan lyrics contain five or more consecutive characters that don’t include these letters, I’d do this:\n\nstr_view(femininomenon, \"[^femino ]{5,}\")\n [4] │ Sho&lt;uld'v&gt;e listened to your friends\n [6] │ You sent him pictures and &lt;playl&gt;ists and phone sex\n[24] │ &lt;(It's&gt; a fem) It's a femininomenon\n[28] │ &lt;(It's&gt; a fem) It's a femininomenon\n[35] │ &lt;Stuck&gt; in the &lt;suburbs,&gt; you're folding his laundry\n[53] │ &lt;(It's&gt; a fem) It's a femininomenon\n[57] │ &lt;(It's&gt; a fem) It's a femininomenon\n[66] │ (A &lt;what?)&gt; A femininomenon\n[72] │ &lt;(It's&gt; a fem) It's a femininomenon\n[82] │ &lt;(It's&gt; a fem) It's a femininomenon\n[86] │ &lt;(It's&gt; a fem) It's a femininomenon\n[88] │ &lt;[Outr&gt;o]\n[90] │ Make a bitch, it's a fem &lt;(It's&gt; a femininomenon)\n\n\nYeah, fair enough.\nBefore moving on to other kinds of set logic, it’s worth being explicit about what the [^ notation implies about including ^ in a character set. The short answer is that ^ only has the special meaning of “negation operator” if it follows immediately after the [. In other words, the sets [ab^] and [a^b] are identical and will match against any literal a, b, or ^ that appears in the text. However, [^ab] is a different set entirely, because in this context ^ is interpreted as the negation operator and this will match against anything that is not an a or a b. As an illustration, let’s match against five or more repetitions of [femino ^], and compare it to what we got last time:\n\nstr_view(femininomenon, \"[femino ^]{5,}\")\n [8] │ \"Let's get c&lt;offee&gt;, let's meet up\"\n [9] │ I'm so sick&lt; of on&gt;line love\n[23] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[24] │ (It's a fem) It's a&lt; femininomenon&gt;\n[27] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[28] │ (It's a fem) It's a&lt; femininomenon&gt;\n[38] │ I'm so sick&lt; of on&gt;line love\n[52] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[53] │ (It's a fem) It's a&lt; femininomenon&gt;\n[56] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[57] │ (It's a fem) It's a&lt; femininomenon&gt;\n[65] │ Well, what we really need is a&lt; femininomenon&gt;\n[66] │ (A what?) A&lt; femininomenon&gt;\n[71] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[72] │ (It's a fem) It's a&lt; femininomenon&gt;\n[75] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[81] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[82] │ (It's a fem) It's a&lt; femininomenon&gt;\n[85] │ (Make a bitch) Make a bitch g&lt;o on &gt;and on\n[86] │ (It's a fem) It's a&lt; femininomenon&gt;\n... and 1 more\n\n\nNot surprisingly, this is a very different result.\nFinally, I want to briefly talk about subtraction and intersection. This is where things get tricky in R because these do work in tidyverse regular expressions but do not work in base R regexes, due to the differences in the regular expression engines underneath. In some regex flavours (including the one used in tidyverse), you can use a minus sign - to denote set difference, so it would be entirely valid to use [[a-z]-[aeiou]] to represent lower case consonants. Similarly, you can use && to represent intersection, so [[a-z]]&&[Papa] would be equivalent to [pa]. This notation works in tidyverse (more detail on this later), as illustrated below:\n\nstr_view(femininomenon, \"[[a-z]-[femino]]{5,}\")\n [6] │ You sent him pictures and &lt;playl&gt;ists and phone sex\n[35] │ Stuck in the &lt;suburbs&gt;, you're folding his laundry\n\nstr_view(femininomenon, \"[[a-z]&&[Papa]]{3,}\")\n [7] │ He dis&lt;app&gt;eared from the second that you said\n[22] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[26] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[51] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[55] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[63] │ But does it h&lt;app&gt;en? (No)\n[64] │ But does it h&lt;app&gt;en? (No)\n[70] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[74] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[80] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n[84] │ (Get it hot) Get it hot like P&lt;apa&gt; John\n\n\nBe warned, however: it does not work in base R."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#anchors-and-boundaries",
    "href": "posts/2024-12-16_regex-backreferences/index.html#anchors-and-boundaries",
    "title": "Baby got backreferences",
    "section": "Anchors and boundaries",
    "text": "Anchors and boundaries\nNext on Danielle’s list of “tedious regular expression concepts” are anchors. There are two special characters that are used to represent “start of string” and “end of string”:\n\nWhen used outside of a character set, ^ denotes “start of string”\nWhen used outside of a character set, $ denotes “end of string”\n\nIn my little notes-to-self I always phrase it like this because when you use these characters inside square brackets they are not interpreted as anchors, and sometimes I forget this nuance.\nAs an example of how to use these anchors, let’s say I want to detect the first word in every line in “Femininomenon”, but only if that word starts with the capital letter L. The regular expression ^L will match against any L that appears as the first character in the string, so a first-pass attempt at solving this problem might be to try something like ^L\\w* (i.e., match a starting L followed by zero or more word characters). It’s not the most robust way to solve the problem, but it works just fine in this case:\n\nstr_view(femininomenon, \"^L\\\\w*\")\n[33] │ &lt;Lying&gt; to your friends about\n[60] │ &lt;Ladies&gt;, you know what I mean\n\n\nThe same logic works for end of string. In the example below we have a regular expression that detects the final word in the line, but only if it starts with a lower-case f:\n\nstr_view(femininomenon, \"f\\\\w*$\")\n [4] │ Should've listened to your &lt;friends&gt;\n[15] │ Make a bitch, it's a &lt;fem&gt;\n[24] │ (It's a fem) It's a &lt;femininomenon&gt;\n[28] │ (It's a fem) It's a &lt;femininomenon&gt;\n[44] │ Make a bitch, it's a &lt;fem&gt;\n[53] │ (It's a fem) It's a &lt;femininomenon&gt;\n[57] │ (It's a fem) It's a &lt;femininomenon&gt;\n[65] │ Well, what we really need is a &lt;femininomenon&gt;\n[66] │ (A what?) A &lt;femininomenon&gt;\n[72] │ (It's a fem) It's a &lt;femininomenon&gt;\n[82] │ (It's a fem) It's a &lt;femininomenon&gt;\n[86] │ (It's a fem) It's a &lt;femininomenon&gt;\n[92] │ Make a bitch, it's a fem, fem, fem, &lt;fem&gt;\n\n\nAbout 99% of my use of anchors is either $ or ^ but there are other anchors. It varies across regex flavours, but generally you can use \\A the same way you use ^ to represent start of string, and \\Z to represent end of string. Personally I don’t like these alternatives, because I keep incorrectly thinking that they have some meaningful relationship to alphabetic characters. The ^ and $ notation is pretty arbitrary, but at least it’s not actively misleading in the way that \\A and \\Z are.\nA closely related concept to anchors is the idea of a boundary. Anchors and boundaries are both examples of “zero-length matches”, in the sense that they don’t match against a specific character, they match against a position in the text. A word boundary \\b (the only kind of boundary I am aware of, though there might be others) matches against a position that is followed by a word character (i.e., anything that is matched by \\w), but is preceded by a non-word character or vice versa. Informally stated, \\b matches a start-of-word boundary and an end-of-word boundary.\nAs a simple example, suppose we want to match only those instances in “Femininomenon” where \"fem\" is used as a word. If we don’t consider word boundaries, this doesn’t quite work because it matches the first three characters in \"femininomenon\":\n\nstr_view(femininomenon, \"fem\")\n[15] │ Make a bitch, it's a &lt;fem&gt;\n[24] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[28] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[44] │ Make a bitch, it's a &lt;fem&gt;\n[53] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[57] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[65] │ Well, what we really need is a &lt;fem&gt;ininomenon\n[66] │ (A what?) A &lt;fem&gt;ininomenon\n[72] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[82] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[86] │ (It's a &lt;fem&gt;) It's a &lt;fem&gt;ininomenon\n[90] │ Make a bitch, it's a &lt;fem&gt; (It's a &lt;fem&gt;ininomenon)\n[92] │ Make a bitch, it's a &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;\n\n\nWe can tighten our regex by specifying that \"fem\" must have word boundaries on either side. The regular expression now becomes \\bfem\\b, so the command we type in R looks like this:\n\nstr_view(femininomenon, \"\\\\bfem\\\\b\")\n[15] │ Make a bitch, it's a &lt;fem&gt;\n[24] │ (It's a &lt;fem&gt;) It's a femininomenon\n[28] │ (It's a &lt;fem&gt;) It's a femininomenon\n[44] │ Make a bitch, it's a &lt;fem&gt;\n[53] │ (It's a &lt;fem&gt;) It's a femininomenon\n[57] │ (It's a &lt;fem&gt;) It's a femininomenon\n[72] │ (It's a &lt;fem&gt;) It's a femininomenon\n[82] │ (It's a &lt;fem&gt;) It's a femininomenon\n[86] │ (It's a &lt;fem&gt;) It's a femininomenon\n[90] │ Make a bitch, it's a &lt;fem&gt; (It's a femininomenon)\n[92] │ Make a bitch, it's a &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;\n\n\nMuch better! Notice also that “end of string” and “start of string” do count as a word boundaries. On lines 15, 44, and 92, the line ends in the word \"fem\" and our regular expression captures these cases.\n\n\n\n\n\nPretend I made a boring regex pun about the femmes being “so strung out” here"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#groups",
    "href": "posts/2024-12-16_regex-backreferences/index.html#groups",
    "title": "Baby got backreferences",
    "section": "Groups",
    "text": "Groups\nThe next regular expression concept to talk about are groups. At it’s most basic, a group is a sequence of characters that is treated as if it were a single character for the purposes of quantification etc.5 We can do this by enclosing the characters in parentheses, e.g., (fem) is a group that corresponds to the specific sequence fem. For example, let’s suppose I want to match against \"fem\" or \"femininomenon\". There are lots of ways I could do this, but one of my favourite approaches to this would be to define (ininomenon) as a group, and then declare that this group is optional using the ? quantifier:\n\nstr_view(femininomenon, \"fem(ininomenon)?\")\n[15] │ Make a bitch, it's a &lt;fem&gt;\n[24] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[28] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[44] │ Make a bitch, it's a &lt;fem&gt;\n[53] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[57] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[65] │ Well, what we really need is a &lt;femininomenon&gt;\n[66] │ (A what?) A &lt;femininomenon&gt;\n[72] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[82] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[86] │ (It's a &lt;fem&gt;) It's a &lt;femininomenon&gt;\n[90] │ Make a bitch, it's a &lt;fem&gt; (It's a &lt;femininomenon&gt;)\n[92] │ Make a bitch, it's a &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;, &lt;fem&gt;\n\n\nThis can be handy if a particular sequence that you want to match might be repeated as a whole. Let’s say I want to match \"in\", \"inin\", and \"ininin\" but not match \"inn\". We could do this cleanly with a regular expression like (in)+, though if we want to prevent it from matching the \"in\" in \"inn\" we should also specify that the pattern should be preceded and followed by word boundaries. That is, this almost works:\n\nstr_view(\"i in inin ininin inn out\", \"(in)+\")\n[1] │ i &lt;in&gt; &lt;inin&gt; &lt;ininin&gt; &lt;in&gt;n out\n\n\nThis actually works:\n\nstr_view(\"i in inin ininin inn out\", \"\\\\b(in)+\\\\b\")\n[1] │ i &lt;in&gt; &lt;inin&gt; &lt;ininin&gt; inn out"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#pipes-as-or-operators",
    "href": "posts/2024-12-16_regex-backreferences/index.html#pipes-as-or-operators",
    "title": "Baby got backreferences",
    "section": "Pipes as “or” operators",
    "text": "Pipes as “or” operators\nOne of the more basic features in regular expressions is that when | is used outside of a category, it is interpreted as an “or” operator that is evaluated left to right. It’s so basic that I feel weird about delaying discussion of | until this late in the post, but in practice I find that I use it most often in conjunction with groups and character sets, so I wanted to wait until I’d talked about those concepts first.\nThis time I’ll use the lyrics to Feather by Sabrina Carpenter as the source text, and I’ll look for all instances where she sings the word “feather” or the phrase “no duh”. No, I don’t know why I chose those two, but let’s run with it. A regular expression that does this, taking potential capitalisation into account, would be ([Ff]eather)|([Nn]o duh):\n\nstr_view(feather, \"([Ff]eather)|([Nn]o duh)\")\n[19] │ I feel so much lighter like a &lt;feather&gt; with you off my mind (Ah)\n[23] │ I feel so much lighter like a &lt;feather&gt; with you out my life\n[29] │ Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;, yeah\n[44] │ I feel so much lighter like a &lt;feather&gt; with you off my mind (Ah)\n[48] │ I feel so much lighter like a &lt;feather&gt; with you out my life\n[55] │ Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;\n[59] │ You miss me? &lt;No duh&gt;\n[62] │ You miss me? &lt;No duh&gt; (&lt;No duh&gt;)\n[67] │ (I feel so much lighter like a &lt;feather&gt; with you off my mind)\n[68] │ You miss me? &lt;No duh&gt;\n[70] │ (Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;)\n[72] │ (I feel so much lighter like a &lt;feather&gt; with you off my mind)\n[73] │ You miss me? &lt;No duh&gt;\n[75] │ (Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;, yeah)\n\n\nIn this example, there’s no possible way in which a string can match against both the left hand side and the right hand side, so in this case the order of the two groups doesn’t matter. But that’s not always the case. Let’s suppose I want to match against the exact word “feather”, or match against a substring inside a word that starts with the letter f and ends in the letter t (we can do this with (f\\w*t) as the regex). Should we match the entire word \"feather\" or only match the \"feat\" part of the word? It depends on the order in which the two groups are specified.\nTo see the difference compare this:\n\nstr_view(feather, \"(feather)|(f\\\\w*t)\")\n[15] │ I got you blocked, a&lt;ft&gt;er this, an a&lt;fterthought&gt;\n[19] │ I feel so much lighter like a &lt;feather&gt; with you off my mind (Ah)\n[22] │ You &lt;fit&gt; every stereotype, \"Send a pic\"\n[23] │ I feel so much lighter like a &lt;feather&gt; with you out my life\n[29] │ Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;, yeah\n[44] │ I feel so much lighter like a &lt;feather&gt; with you off my mind (Ah)\n[47] │ You &lt;fit&gt; every stereotype, \"Send a pic\"\n[48] │ I feel so much lighter like a &lt;feather&gt; with you out my life\n[55] │ Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;\n[67] │ (I feel so much lighter like a &lt;feather&gt; with you off my mind)\n[70] │ (Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;)\n[72] │ (I feel so much lighter like a &lt;feather&gt; with you off my mind)\n[75] │ (Like a &lt;feather&gt;, like a &lt;feather&gt;, like a &lt;feather&gt;, yeah)\n\n\nto this:\n\nstr_view(feather, \"(f\\\\w*t)|(feather)\")\n[15] │ I got you blocked, a&lt;ft&gt;er this, an a&lt;fterthought&gt;\n[19] │ I feel so much lighter like a &lt;feat&gt;her with you off my mind (Ah)\n[22] │ You &lt;fit&gt; every stereotype, \"Send a pic\"\n[23] │ I feel so much lighter like a &lt;feat&gt;her with you out my life\n[29] │ Like a &lt;feat&gt;her, like a &lt;feat&gt;her, like a &lt;feat&gt;her, yeah\n[44] │ I feel so much lighter like a &lt;feat&gt;her with you off my mind (Ah)\n[47] │ You &lt;fit&gt; every stereotype, \"Send a pic\"\n[48] │ I feel so much lighter like a &lt;feat&gt;her with you out my life\n[55] │ Like a &lt;feat&gt;her, like a &lt;feat&gt;her, like a &lt;feat&gt;her\n[67] │ (I feel so much lighter like a &lt;feat&gt;her with you off my mind)\n[70] │ (Like a &lt;feat&gt;her, like a &lt;feat&gt;her, like a &lt;feat&gt;her)\n[72] │ (I feel so much lighter like a &lt;feat&gt;her with you off my mind)\n[75] │ (Like a &lt;feat&gt;her, like a &lt;feat&gt;her, like a &lt;feat&gt;her, yeah)\n\n\nAs this illustrates, if a match is detected on the left hand side of the | the regular expression engine uses that match. It won’t use the right hand side if there’s already a match on the left. Admittedly, I’ve been a bit lazy in this by not taking word boundaries or capitalisation into account. You’d probably want something a little more elaborate in real life, but it’s such a silly example that I’ve erred on the side of simplicity here so that you can see the difference that order makes."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#dots-as-any-character",
    "href": "posts/2024-12-16_regex-backreferences/index.html#dots-as-any-character",
    "title": "Baby got backreferences",
    "section": "Dots as “any” character",
    "text": "Dots as “any” character\nIt may surprise the reader (it surprised the author) that I’ve managed to get this far into the post without ever talking about the . metacharacter. This is partly accidental, partly deliberate. The . is a powerful and easily-misused tool: it matches any character except line breaks. There’s a whole article on the dot that cautious novice regex users to be careful when using it… so I am trying to teach myself to be careful and not use it so often!\nAn example of why it’s sometimes counterintuitive. Suppose I want to match the text in each line of “Feather” up to the point that \"ignore\" appears, and then terminate the match. It’s very tempting to use .*ignore as the regular expression, but that doesn’t quite do what we expect:\n\nstr_view(feather, \".*ignore\")\n[13] │ &lt;I slam the door, I hit ignore&gt;\n[38] │ &lt;I slam the door (Slam the door), I hit ignore (Hit ignore&gt;)\n\n\nBecause the * operator is greedy, it “consumes” every instance of \"ignore\" except the last one, which is then matched by the explicitly stated \"ignore\" in the regex. As a consequence, the regex I’ve written here matches all the text up to and including the last \"ignore\" that appears in the line, so it matches incorrectly on line 38.\nWe can fix this by tightening the quantification. Instead of using a greedy operator * that matches as much text as possible, we can use the lazy version *? that matches as little as possible:\n\nstr_view(feather, \".*?ignore\")\n[13] │ &lt;I slam the door, I hit ignore&gt;\n[38] │ &lt;I slam the door (Slam the door), I hit ignore&gt;&lt; (Hit ignore&gt;)\n\n\nNow we get the right answer.\n\n\n\n\n\n\n\n\n\nWhen I use .* in my regex\n\n\n\n\n\n\n\nWhen I check what .* matches"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#intermission-very-important-applied-regex",
    "href": "posts/2024-12-16_regex-backreferences/index.html#intermission-very-important-applied-regex",
    "title": "Baby got backreferences",
    "section": "Intermission: Very important applied regex",
    "text": "Intermission: Very important applied regex\nThis post is getting long. We need a little break. Well, I need a little break. As it happens, we’ve now introduced enough regular expression concepts that we can detect all the hissed words in the “Midnight Pals” dialog hissed by the JK Rowling character. Here they are:\n\n\nstr_view(midnight, \"\\\\b((\\\\w|')*s{3,}(\\\\w|')*)|(yess)\\\\b\") \n [3] │ Rowling: what &lt;newsss&gt;, wormtongue?\n [4] │ Rowling: how goes the infiltration of &lt;bluesssky&gt;?\n[17] │ Rowling: &lt;door'sss&gt; open, &lt;boysss&gt;!\n[20] │ Rowling: i know you did, &lt;jesssse&gt;\n[22] │ Rowling: &lt;yess&gt; you did great &lt;jessse&gt;\n[43] │ Rowling: patience, &lt;ssssisster&gt;, we must bide our time\n[54] │ Rowling: sure, &lt;jessse&gt; might look like a sniveling worm, a nasty crawling slithering little shit, a spineless craven buffoon\n\n\n\nFor a variety of reasons this makes me happy. But let’s move on, shall we?"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#backreferences",
    "href": "posts/2024-12-16_regex-backreferences/index.html#backreferences",
    "title": "Baby got backreferences",
    "section": "Backreferences",
    "text": "Backreferences\nAt this point we’re starting to get into the weeds. I’ve talked about most of the core features of regular expressions, and to be honest I’d guess that 95% of my regex usage can be accounted for using only the tools described so far. But there are many more fancy tricks you can do with regexes, and the one I use most often is backreferencing.\nHere’s an example. Suppose we are listening to classical music, and we decide we want to detect every instance of a word in Baby Got Back that begins and ends with the letter a. We can do this using the tools we already know. The regular expression a\\w+a will match against strings of characters start and end with a, so we can extend that to require a word starts and end with a by placing a word boundary on either side, giving us \\ba\\w+a\\b as the regular expression:\n\nstr_view(babygotback, \"\\\\ba\\\\w+a\\\\b\")\n[85] │ My &lt;anaconda&gt; don't want none\n\n\nBut now suppose that we want to relax this slightly and find all words that start and end with the same (case-matched) letter, regardless of what that letter actually is. For that to work, we need to have the ability to capture one part of the regular expression, and then refer back to the captured contents elsewhere in the regular expression. The capturing part is straightforward, actually. Earlier when I introduced the () notation for groups, I was a little imprecise in my language. By default, anything enclosed in parentheses is treated as a capture group, and you can refer back to them numerically: \\1 is a backreference to the first capture group, \\2 refers to the second one, and so one.6\nTo give a simple example, imagine a markup language in which footnotes are specified using markers like %a%, %b% and so on, and the marker would need to appear on either side of the footnote text itself. Some example strings that you might see:\n\ncomments &lt;- c(\n  \"some text %a%footnote%a% some text\",\n  \"%a% none of this %b% is a footnote\",\n  \"some more text %b%another footnote%b%\"\n)\n\nNow consider the regular expression %[a-z]%. This will match against %a%, %b% and so on. Let’s have a look at what it detects:\n\nstr_view(comments, \"%[a-z]%\")\n[1] │ some text &lt;%a%&gt;footnote&lt;%a%&gt; some text\n[2] │ &lt;%a%&gt; none of this &lt;%b%&gt; is a footnote\n[3] │ some more text &lt;%b%&gt;another footnote&lt;%b%&gt;\n\n\nNext we need to specify this as a capture group, which we do by wrapping it in parentheses. That gives us (%[a-z]%) as our regular expression. Because the content of this group is captured, I can insert a \\1 to refer to the specific text matched by that group. So we could build a regular expression like (%[a-z]%).*+\\1, which looks for a footnote marker like %a% or %b%, followed by some arbitrary text, and then followed by a matching footnote marker:\n\nstr_view(comments, \"(%[a-z]%).*\\\\1\")\n[1] │ some text &lt;%a%footnote%a%&gt; some text\n[3] │ some more text &lt;%b%another footnote%b%&gt;\n\n\nIt’s a little bit risky to use the .* (as I discussed earlier), but it works for this particular problem, and hopefully you get the basic idea.\nArmed with this newfound knowledge, we can return to the Baby Got Backreferences problem. Earlier on I used the expression \\ba\\w+a\\b to detect words that start and end with the letter a. To generalise this to “words that start and end with the same letter”, I can replace the first a in this expression with a capture group that will match against any word character (i.e., (\\w)), and then replace the second a with a backreference \\1 to that captured group. In other words, we started out with \\ba\\w+a\\b, and we modify it to \\b(\\w)\\w+\\1\\b. When specified as an R string, we need to add the extra backslashes as usual, and that gives us this:\n\nstr_view(babygotback, \"\\\\b(\\\\w)\\\\w+\\\\1\\\\b\")\n [15] │ Wanna pull up tough 'cause you notice &lt;that&gt; butt was stuffed\n [21] │ But &lt;that&gt; butt you got makes (Me-me so horny)\n [24] │ Well, use me, use me 'cause you ain't &lt;that&gt; average groupie\n [28] │ Take the average black man and ask him &lt;that&gt;\n [33] │ Shake &lt;that&gt; healthy butt, baby got back\n [43] │ And when I'm throwin' a &lt;gig&gt;\n [51] │ So find &lt;that&gt; juicy double\n [53] │ Beggin' for a piece of &lt;that&gt; bubble\n [58] │ A word to the thick soul &lt;sisters&gt;, I wanna get with ya\n [63] │ A lot of &lt;simps&gt; won't like this song\n [65] │ And I'd &lt;rather&gt; stay and play\n [78] │ Cosmo ain't got &lt;nothin&gt;' to do with my selection\n [85] │ My &lt;anaconda&gt; don't want none\n [88] │ But please don't lose &lt;that&gt; butt\n [89] │ Some brothers wanna play &lt;that&gt; hard role\n [90] │ And tell you &lt;that&gt; the butt ain't gold\n [93] │ So Cosmo &lt;says&gt; you're fat\n [94] │ Well I ain't down with &lt;that&gt;\n[108] │ And &lt;kick&gt; them nasty thoughts\n\n\nVictory! Those nasty thoughts have indeed been kicked.\n\n\n\nWhen the backreference actually works"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#lookaheadlookbehind",
    "href": "posts/2024-12-16_regex-backreferences/index.html#lookaheadlookbehind",
    "title": "Baby got backreferences",
    "section": "Lookahead/lookbehind",
    "text": "Lookahead/lookbehind\nContinuing with the all-important task of matching patterns in the immortal words of Sir Mix-a-Lot, it’s time to talk about lookahead and lookbehind. Suppose I want to find words that begin and end with the letter \"a\", but I don’t actually want to include the letter a at the end. In other words, I need to “look ahead” to check if the d in anaconda is followed by an a\\b (i.e., a at the end of the word), but not include the a\\b in the match. We can do this by creating a special “lookahead group” that encloses the a\\b. However, instead of writing (a\\b) like we would for a capturing group, we use (?= at the start to indicate that it’s a lookahead group. In other words, a lookahead group that detects (but does not match) the a at the end of anaconda is (?=a\\b), and the full regex is \\ba\\w+(?=a\\b). Here’s what that looks like when we put it into practice:\n\nstr_view(babygotback, \"\\\\ba\\\\w+(?=a\\\\b)\")\n[85] │ My &lt;anacond&gt;a don't want none\n\n\nBy analogy to lookahead groups, regular expressions also support lookbehind groups. They’re exactly the same concept as a lookahead group, but this time we’re looking at the text preceding the match rather than the text following it. The syntax for a lookbehind group is very similar, but we open the group with (?&lt;= to specify that it’s lookbehind not lookahead. So, for instance, if we want to detect but not match the first a in anaconda, we use (?=a\\b) to do so. The full regex to exclude both terminal a characters is therefore (?&lt;=\\ba)\\w+(?=a\\b) and we end up with this:\n\nstr_view(babygotback, \"(?&lt;=\\\\ba)\\\\w+(?=a\\\\b)\")\n[85] │ My a&lt;nacond&gt;a don't want none\n\n\nA slightly more realistic example would be to detect all the text inside a parenthetical, but not including the parentheses themselves. We can do this with a completely unintelligible regex like (?&lt;=\\()[^)]+(?=\\)). It’s a crime against humanity, but it works:\n\nstr_view(babygotback, \"(?&lt;=\\\\()[^)]+(?=\\\\))\")\n[21] │ But that butt you got makes (&lt;Me-me so horny&gt;)\n[30] │ So, fellas (&lt;Yeah&gt;), Fellas (&lt;Yeah&gt;)\n[31] │ Has your girlfriend got the butt? (&lt;Hell yeah&gt;)\n[32] │ Tell 'em to shake it (&lt;Shake it&gt;) Shake it (&lt;Shake it&gt;)\n[36] │ (&lt;L.A. face with a Oakland booty&gt;)\n[38] │ (&lt;L.A. face with a Oakland booty&gt;)\n[39] │ (&lt;L.A. face with a Oakland booty&gt;)\n[60] │ But I gotta be straight when I say I wanna— (&lt;Uh&gt;)\n[68] │ So, ladies (&lt;Yeah&gt;), Ladies (&lt;Yeah&gt;)\n[69] │ If you wanna roll in my Mercedes (&lt;Yeah&gt;)\n\n\nAwesome.\nOh… by the way? This trick works in tidyverse, but it won’t work in base R unless you set perl = TRUE, because the default regular expression engine in base R doesn’t support lookahead and lookbehind groups.7 Which seems like a good time to discuss…"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#three-regex-flavours",
    "href": "posts/2024-12-16_regex-backreferences/index.html#three-regex-flavours",
    "title": "Baby got backreferences",
    "section": "Three regex flavours",
    "text": "Three regex flavours\nA persistent headache I have when writing regular expressions, besides the fact that they suck, is that there are so many slight variations on the same idea. At regular-expressions.info, for instance, you can find quick lookup tables for a wide variety of different regular expression engines. It lists regular expressions in R as one of those systems, but it’s important to remember that this refers to the syntax used by base R tools like grep(), gsub(), gregexpr() and so on. Or, more precisely, it refers to the default POSIX standard for extended regular expressions (ERE). Base R actually supports two different engines, so if you set perl = TRUE when calling base R functions then you would need to look at the rules for PCRE (perl compatible regular expressions). In tidyverse, regular expressions are usually handled with the stringr package that is built on top of stringi, which in turn uses the ICU engine that conforms to Unicode standards and as such provides comprehensive Unicode support. The stringi regular expressions page has a nice discussion.\nLike an idiot, I forget this on a semi-regular basis, and I try to debug something by looking up the wrong regex syntax and yes, this sometimes matters. For example, when I discussed pre-specified character classes like digits (\\d) and word characters (\\w), or character ranges like [0-9] and [a-z], I made a deliberate decision not to discuss the predefined POSIX classes. These include things like [[:alpha:]] that matches alphabetic characters, and [[:digit:]] matches numeric digits. The digit and alphabetic character classes are fairly unproblematic, but there’s also a [[:punct:]] class that is intended to match punctuation characters. I find myself very tempted to use this class because, honestly, who wants to be bothered listing all the punctuation characters by hand? However, it’s a very bad idea to do this because not only do the results depend on system locale (which is to be expected for anything that involves text), there is absolutely no consistency at all in how this class is implemented in different regex engines.\nTo illustrate this I’ll use an example that I have shamelessly stolen directly from the stringi documentation. But first, because I cannot even with the base R regular expression syntax,8 I’ll define a base_extract_all() function that is roughly analogous to stringr::str_extract_all(), but uses the base R functions to do the work:\n\nbase_extract_all &lt;- function(string, pattern, perl = FALSE) {\n  matches &lt;- gregexpr(pattern = pattern, text = string, perl = perl)\n  regmatches(x = string, m = matches)\n}\n\nNext I’ll define a string punct, which contains an assortment of different characters that one might be inclined to call “punctuation”:\n\npunct &lt;- \",./|\\\\&gt;&lt;?;:'\\\"[]{}-=_+()*&^%$€#@!`~×‒„”\"\n\nSo, what happens when we match punct to \"[[:punct:]]\"? Well, it depends heavily on which engine you’re using. If you’re using ERE (i.e., base R with perl = FALSE), you get this as the result:\n\npunct |&gt; \n  base_extract_all(\"[[:punct:]]\") |&gt; \n  unlist() |&gt; \n  cat()\n, . / | \\ &gt; &lt; ? ; : ' \" [ ] { } - = _ + ( ) * & ^ % $ € # @ ! ` ~ × ‒ „ ”\n\n\nWhat about PCRE? Let’s set perl = TRUE and have a look:\n\npunct |&gt; \n  base_extract_all(\"[[:punct:]]\", perl = TRUE) |&gt; \n  unlist() |&gt; \n  cat()\n, . / | \\ &gt; &lt; ? ; : ' \" [ ] { } - = _ + ( ) * & ^ % $ # @ ! ` ~\n\n\nOkay yeah that is not even close to being the same thing. But what about the ICU engine? If you’re working in tidyverse, internally you’re probably relying on this engine and, well…\n\npunct |&gt; \n  str_extract_all(\"[[:punct:]]\") |&gt; \n  unlist() |&gt; \n  cat()\n, . / \\ ? ; : ' \" [ ] { } - _ ( ) * & % # @ ! ‒ „ ”\n\n\nLe sigh. Of course. Totally different again.\nLittle discrepancies like this are the reason why at various different points in the text, I’ve had to pause and mention things that work differently across the three regex engines used in R. While the differences between the engines are not large, it only takes one tiny difference to mess up an otherwise perfectly crafted regex. Something that looks like it works when you use stringr::str_detect() will suddenly break when you use the same regex in grep().\nIt is annoying.\n\n\n\nThree rings for the regex kings, under th.. okay yeah that’s a bit of a reach"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#backslashes-part-ii",
    "href": "posts/2024-12-16_regex-backreferences/index.html#backslashes-part-ii",
    "title": "Baby got backreferences",
    "section": "Backslashes (part II)",
    "text": "Backslashes (part II)\nWe are getting very close to the end of the post (I promise!), but before I wrap up I want to return to the vexed topic of using the backslash to escape special characters. I talked about this at the beginning of the post, yes, but at that stage we hadn’t dived very deep into regular expressions. Since then we’ve covered a lot more territory, and in the process we’ve encountered more special characters and more syntax. With that in mind, I’ll revisit that subject now and provide a more comprehensive discussion. Additionally, because – as I’ve hinted – the rules inside a character set are different to the rules that apply outside a character set, I’ll handle those two cases separately.\n\nSpecial characters outside a character set\nOutside of a character set, the characters +, ., ?, *, and must all be escaped because otherwise they will be interpreted as quantifiers:\n\nwrite \"\\\\+\" in R to mean \\+ in the regex, and match a literal +\nwrite \"\\\\?\" in R to mean \\? in the regex, and match a literal ?\nwrite \"\\\\*\" in R to mean \\* in the regex, and match a literal *\n\nIt obviously follows that since \\ is used as the escape character it too must be escaped if you want to treat it as a literal. Moreover, since we’ve used | to represent the “or” operator and . to represent the “any” character, you will not be at all surprised to learn that these also must be escaped if you want to represent them as literal characters:\n\nwrite \"\\\\\\\\\" in R to mean \\\\ in the regex, and match a literal \\\nwrite \"\\\\|\" in R to mean \\| in the regex, and match a literal |\nwrite \"\\\\.\" in R to mean \\. in the regex, and match a literal .\n\nTo help make this concrete, here’s a minimal example:\n\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\+\")\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\*\")\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\?\")\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\\\\\\")\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\|\")\nstr_view(string = \"+ . ? * \\\\ |\", pattern = \"\\\\.\")\n[1] │ &lt;+&gt; . ? * \\ |\n[1] │ + . ? &lt;*&gt; \\ |\n[1] │ + . &lt;?&gt; * \\ |\n[1] │ + . ? * &lt;\\&gt; |\n[1] │ + . ? * \\ &lt;|&gt;\n[1] │ + &lt;.&gt; ? * \\ |\n\n\nThe same logic applies to special characters like \\n (line feed) and \\t (tab), as well as regular expressions like \\b (boundary), \\w (word character), and \\s (whitespace). This also applies to \\u, if we wish to specify a unicode character in the regex. For these characters we need to escape the \\ in R, in order to match the corresponding literal in the regex:\n\nwrite \"\\\\n\" in R to mean \\n in the regex, and match a line feed\nwrite \"\\\\t\" in R to mean \\t in the regex, and match a tab\nwrite \"\\\\u2300\" in R to mean \\u2300 in the regex, and match a literal ⌀\nwrite \"\\\\b\" in R to mean \\b in the regex, and match a word boundary\nwrite \"\\\\w\" in R to mean \\w in the regex, and match a word character\nwrite \"\\\\s\" in R to mean \\s in the regex, and match a whitespace character\n\nIt also applies for the anchor characters ^ and $:\n\nwrite \"\\\\^\" in R to mean \\^ in the regex, and match a literal ^\nwrite \"\\\\$\" in R to mean \\$ in the regex, and match a literal $\n\nOn top of this, we have several other characters that need to be escaped within the regex, because brackets, braces, and parentheses have syntactic meaning. However, the rules here are subtle. For the opening brace/bracket/parenthesis, you always have to escape:\n\nwrite \"\\\\[\" in R to mean \\[ in the regex and match a literal [\nwrite \"\\\\(\" in R to mean \\( in the regex and match a literal (\nwrite \"\\\\{\" in R to mean \\{ in the regex and match a literal {\n\nBut for closing, the story is different. Specifically, you don’t have to escape the closing bracket ]:\n\nwrite \"]\" in R to mean ] in the regex and match a literal ]\nwrite \"\\\\)\" in R to mean \\) in the regex and match a literal )\nwrite \"\\\\}\" in R to mean \\} in the regex and match a literal }\n\nTo make this a little clearer, this is how you do it for the opening bracket/parenthesis/brace:\n\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"\\\\[\")\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"\\\\(\")\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"\\\\{\")\n[1] │ &lt;[&gt;blah] (blah) {blah}\n[1] │ [blah] &lt;(&gt;blah) {blah}\n[1] │ [blah] (blah) &lt;{&gt;blah}\n\n\nThis is how you do it for the closing ones:\n\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"]\")\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"\\\\)\")\nstr_view(string = \"[blah] (blah) {blah}\", pattern = \"\\\\}\")\n[1] │ [blah&lt;]&gt; (blah) {blah}\n[1] │ [blah] (blah&lt;)&gt; {blah}\n[1] │ [blah] (blah) {blah&lt;}&gt;\n\n\n\n\nSpecial characters inside a character set\nInside a character set (i.e., within a square bracketed expression), the rules are different and inconsistent across regex flavours. For tidyverse (ICU engine) and base R with perl = TRUE (PCRE engine), there are four special characters you need to worry about: ], ^, \\, and -. Everything else can be added to a character set without needing any form of escaping. Here’s an example where the character set is comprised of various characters that would normally need to be escaped, but don’t need to be escaped inside a character set:\n\nchars &lt;- c(\n  \"+\", \".\", \"?\", \"*\", \"\\\\\", \"|\", \n  \"(\", \")\", \"[\", \"]\", \"{\", \"}\",\n  \"^\", \"$\", \"-\"\n)\nstr_view(chars, \"[a+.?*$|(){}]\")\n [1] │ &lt;+&gt;\n [2] │ &lt;.&gt;\n [3] │ &lt;?&gt;\n [4] │ &lt;*&gt;\n [6] │ &lt;|&gt;\n [7] │ &lt;(&gt;\n [8] │ &lt;)&gt;\n[11] │ &lt;{&gt;\n[12] │ &lt;}&gt;\n[14] │ &lt;$&gt;\n\n\nHere’s an example where each of the other four special characters are escaped. For the sake of my sanity I’ll include spaces in the character set. This allows me to write it like this, [ \\] \\^ \\\\ \\- ], and when specified as an R string it looks like this:\n\nstr_view(chars, \"[ \\\\] \\\\\\\\ \\\\^ \\\\- ]\")\n [5] │ &lt;\\&gt;\n[10] │ &lt;]&gt;\n[13] │ &lt;^&gt;\n[15] │ &lt;-&gt;\n\n\nIn addition to this, there are a few other handy tricks you can use to make the character set simpler:\n\nif an unescaped ] appears as the first element of the character set, it is a literal\nif an unescaped ^ appears in anything other than the first element, it is also a literal\nif an unescaped - appears as the first or last element, it is a literal:\n\nSo this actually works for detecting right brackets, carets, and hyphens:\n\nstr_view(chars, \"[]^-]\")\n[10] │ &lt;]&gt;\n[13] │ &lt;^&gt;\n[15] │ &lt;-&gt;\n\n\nIn all these examples I’m using str_view(), so we’re using the “tidyverse style” regular expressions here based on the ICU engine. If we switch over to base R and set perl = TRUE (and hence use the PCRE engine), the results would be the same. However, the rules for the POSIX ERE engine are slightly different. If you’re using base R with the default perl = FALSE setting, the only character that you can escape within a character set is the backslash itself, so if you want to match ], ^, or - then you have to rely on the positional tricks shown above. Here’s how you’d do it using the ERE engine:\n\ncat(grep(\"[]^\\\\\\\\-]\", chars, value = TRUE))\n\\ ] ^ -\n\n\nGod this is exhausting.\n\n\n\n\n\nVery much like the show in question, this post is running a little longer than anyone could reasonably have expected"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#substitution",
    "href": "posts/2024-12-16_regex-backreferences/index.html#substitution",
    "title": "Baby got backreferences",
    "section": "Substitution",
    "text": "Substitution\nThere’s one last topic to talk about: substitution. Throughout the post I’ve focused almost entirely on detecting patterns in text. In base R you might use grep() to accomplish this task, whereas in stringr you’d probably use str_detect().9 The other main thing I use regular expressions for is making substitutions, in which the matched text is replaced by something new. In base R you’d probably use gsub() for this, whereas in stringr you’d most likely use str_replace() or str_replace_all().\nIn most cases where I’ve had to use it substitution doesn’t introduce any new concepts: the “replacement” text is usually a fixed string that is inserted wherever a match is found. For example, if I want to replace the word \"cat\" with the emoji \"🐈\", I would do something like this:\n\ncat(str_replace_all(\n  string = \"if you concatenate two cats you get a catcat\",\n  pattern = \"\\\\bcats?\\\\b\",\n  replacement = \"🐈\"\n))\nif you concatenate two 🐈 you get a catcat\n\n\nOkay yes, in this example I had to do a little bit of work in the pattern argument to come up with the regex that I wanted to use to define matches, but the replacement argument is very simple. Nothing new here really.\nHowever, there are some fancy things that you can do with replacement text. In particular, all three regular expression engines used in R support backreferences within the replacement text. Here it is in all three engines:\n\na_vile_lie &lt;- \"dogs are better than cats\"\n\n# tidyverse + ICU\ncat(str_replace_all(\n  string = a_vile_lie,\n  pattern = \"(dogs)(.*)(cats)\",\n  replacement = \"\\\\3\\\\2\\\\1\"\n), sep = \"\\n\")\n\n# base R + ERE\ncat(gsub(\n  pattern = \"(dogs)(.*)(cats)\",\n  replacement = \"\\\\3\\\\2\\\\1\",\n  x = a_vile_lie\n), sep = \"\\n\")\n\n# base R + PCRE\ncat(gsub(\n  pattern = \"(dogs)(.*)(cats)\",\n  replacement = \"\\\\3\\\\2\\\\1\",\n  x = a_vile_lie,\n  perl = TRUE\n), sep = \"\\n\")\ncats are better than dogs\ncats are better than dogs\ncats are better than dogs\n\n\nAs an aside, this last example is very closely related to the problem I was trying to solve when I finally broke down and decided to write the post. The thing I actually wanted to do was manipulate text inside parentheses using backreferences in the replacement string, without matching the parentheses themselves. To solve that you need to use backreferences, but you also need to use lookahead and lookbehind functionality:\n\n# tidyverse + ICU\ncat(str_replace_all(\n  string = \"ab(cd)efg[hi]j\",\n  pattern = \"(?&lt;=[\\\\[(])(\\\\w+)(?=[)\\\\]])\",\n  replacement = \"\\\\1,\\\\1\"\n), sep = \"\\n\")\n\n# base R + PCRE\ncat(gsub(\n  pattern = \"(?&lt;=[\\\\[(])(\\\\w+)(?=[)\\\\]])\",\n  replacement = \"\\\\1,\\\\1\",\n  x = \"ab(cd)efg[hi]j\",\n  perl = TRUE\n), sep = \"\\n\")\nab(cd,cd)efg[hi,hi]j\nab(cd,cd)efg[hi,hi]j\n\n\nIt would be trickier to do this with the ERE engine because it doesn’t support lookahead/lookbehind behaviour, but somehow I was convinced it could be done. I ended up reading far too much and… well, this is where we ended up."
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#im-so-sorry-for-my-loss",
    "href": "posts/2024-12-16_regex-backreferences/index.html#im-so-sorry-for-my-loss",
    "title": "Baby got backreferences",
    "section": "I’m so sorry for my loss",
    "text": "I’m so sorry for my loss\n\nrules &lt;- tibble::tribble(\n                 ~pattern, ~replacement,\n       \"([Dd]o)(-[Dd]o)+\",         \"🎶\", # do-do, do-do-do -&gt; musical note\n                \"feather\",         \"🪶\", # feather -&gt; emoji feather\n                   \"wine\",         \"🍷\", # wine -&gt; emoji wine\n   \"(?&lt;=\\\\()[Aa]h(?=\\\\))\",         \"😌\", # (ah) -&gt; emoji relief\n     \"\\\\[([^\\\\[\\\\]]+)\\\\]\",         \"🎙️\"  # replace square bracketed text\n)\n\nstr_rewrite &lt;- function(string, rules) {\n  purrr::reduce2(\n    .x = rules$pattern, \n    .y = rules$replacement, \n    .f = str_replace_all, \n    .init = string\n  )\n}\n\nfeather |&gt; \n  str_rewrite(rules) |&gt; \n  cat(sep = \"\\n\")\n🎙️\n(🎶, 🎶, 🎶, 🎶)\n(🎶, 🎶, 🎶, 🎶)\nOh, not another take\n\n🎙️\nOh, it's like that, I'm your dream come true\nWhen it's on a platter for you\nThen you pull back when I try to make plans\nMore than two hours in advance, mm\n\n🎙️\nI slam the door, I hit ignore\nI'm saying, \"No, no, no, no more\"\nI got you blocked, after this, an afterthought\nI finally cut you off\n\n🎙️\nI feel so much lighter like a 🪶 with you off my mind (😌)\nFloatin' through the memories like whatever, you're a waste of time (😌)\nYour signals are mixed, you act like a bitch\nYou fit every stereotype, \"Send a pic\"\nI feel so much lighter like a 🪶 with you out my life\nWith you out my life\n\n🎙️\n(🎶, 🎶, 🎶, 🎶)\n(🎶, 🎶, 🎶, 🎶)\nLike a 🪶, like a 🪶, like a 🪶, yeah\n\n🎙️\nIt feels so good\nNot carin' where you are tonight\nAnd it feels so good\nNot pretendin' to like the 🍷 you like\n\n🎙️\nI slam the door (Slam the door), I hit ignore (Hit ignore)\nI'm saying, \"No, no, no, no more\"\nI got you blocked, excited to never talk, I\nI'm so sorry for your loss\n\n🎙️\nI feel so much lighter like a 🪶 with you off my mind (😌)\nFloatin' through the memories like whatever, you're a waste of time (😌)\nYour signals are mixed, you act like a bitch (A bitch)\nYou fit every stereotype, \"Send a pic\"\nI feel so much lighter like a 🪶 with you out my life\nWith you out my life\n\n🎙️\n(🎶, 🎶, 🎶, 🎶)\nAh, mm\n(🎶, 🎶, 🎶, 🎶)\nLike a 🪶, like a 🪶, like a 🪶\n\n🎙️\nYou want me? I'm done\nYou miss me? No duh\nWhere I'm at, I'm up where I'm at\nYou want me? I'm done (I'm done)\nYou miss me? No duh (No duh)\nWhere I'm at, I'm up (I'm up) where I'm at\n\n🎙️\nYou want me? I'm done\n(I feel so much lighter like a 🪶 with you off my mind)\nYou miss me? No duh\nWhere I'm at, I'm up where I'm at\n(Like a 🪶, like a 🪶, like a 🪶)\nYou want me? I'm done\n(I feel so much lighter like a 🪶 with you off my mind)\nYou miss me? No duh\nWhere I'm at, I'm up where I'm at\n(Like a 🪶, like a 🪶, like a 🪶, yeah)"
  },
  {
    "objectID": "posts/2024-12-16_regex-backreferences/index.html#footnotes",
    "href": "posts/2024-12-16_regex-backreferences/index.html#footnotes",
    "title": "Baby got backreferences",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAt the R console, this highlighting appears automatically. Normally you wouldn’t see this highlighting in a quarto document like this one, because quarto strips out the ANSI control characters that the console uses to add colour to the output. However, for the purposes of this post I cheated a little, by writing a knitr hook that crudely mimics the same behaviour in this document.↩︎\nThere are a ghastly number of different regular expression flavours out there. They are very similar to each other, but have subtle differences. Somewhat frustratingly, there are three different regular expression flavours that are widely used in R (two in base R and one in tidyverse), and every now and then I find myself running into cases where they don’t produce identical results. More on this later in the post.↩︎\nI usually imagine the character has a yuan-ti thing going on↩︎\nI’m not talking about raw strings in this post. Sorry.↩︎\nYes I know it’s more nuanced than that, hush, I’ll get to that momentarily.↩︎\nThere’s also a mechanism for naming groups and name-based referencing, but I never use it.↩︎\nThat’s another one of those fun things where there are in fact three separate flavours of regex in widespread usage in R. ICU (used by stringi and stringr) and PCRE (base R with perl = TRUE) both support lookahead and lookbehind assertions, as well as negative lookahead (?!blah) and negative lookbehind (?&lt;!blah), but the ERE engine (base R with perl = FALSE) does not.↩︎\nI mean, wtf. I am genuinely sympathetic to R core, and deeply appreciate their willingness to maintain backward compatibility even for the bits of R that are just bizarre. But oh my god.↩︎\nI know I know. I’ve mostly used str_view() rather than str_detect() because it’s convenient for visually illustrating the regex match. But you wouldn’t work with this in a script↩︎"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html",
    "title": "Arrays and tables in Arrow",
    "section": "",
    "text": "My catholic taste in the devil\nAll gilded and golden, yes, I’m your girl\nHell, if it glitters, I’m going\n  – Heaven is Here, Florence + The Machine\nIf you’ve made the life choice to become a developer advocate with a focus on Apache Arrow, you’re probably not unfamiliar with masochism.\nDon’t believe me? Let’s consider my past choices:\nI don’t regret any of these choices, particularly the fact that they have helped keep me gainfully employed, but there’s no denying the fact that a lot of blood and tears have been spilled in the endeavour.\nIn any case, what I am trying to convey to you, dear reader, is that – setting aside the superficial trappings of whips and chains and the various other devices that propelled E. L. James to great fortune – I am intimately acquainted with pain. It is important to me that you understand this, and that when I mention the pain I encountered when trying to learn how the arrow R package works, I am not using the term lightly.\nSo let talk about my latest pain point, shall we?"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#data-objects",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#data-objects",
    "title": "Arrays and tables in Arrow",
    "section": "Data objects",
    "text": "Data objects\nHere’s the thing that has been giving me grief. Suppose you are an R user who is new to this whole Apache Arrow business. You’ve installed the arrow package, and you’re now reading the Get Started page in the hopes that you too will be able to, well, get started. When you visit this page, one of the very first things you encounter is a table listing a variety of data structures used by Arrow. Specifically, the table tells you that Arrow has classes for zero-dimensional data (scalars), one-dimensional data (arrays and other vector-like data), and two-dimensional data (tabular or data frame-like data). It shows you that…\n…actually, you know what? Instead of describing it, let’s take a look at the actual table. Here’s what it tells you about the hierarchy of data structures in arrow:\n\n\n\n\n\n\n\n\n\n\nDim\nClass\nDescription\nHow to create an instance\n\n\n\n\n0\nScalar\nsingle value and its DataType\nScalar$create(value, type)\n\n\n1\nArray\nvector of values and its DataType\nArray$create(vector, type)\n\n\n1\nChunkedArray\nvectors of values and their DataType\nChunkedArray$create(..., type) or alias chunked_array(..., type)\n\n\n2\nRecordBatch\nlist of Arrays with a Schema\nRecordBatch$create(...) or alias record_batch(...)\n\n\n2\nTable\nlist of ChunkedArray with a Schema\nTable$create(...), alias arrow_table(...), or arrow::read_*(file, as_data_frame = FALSE)\n\n\n2\nDataset\nlist of Tables with the same Schema\nDataset$create(sources, schema) or alias open_dataset(sources, schema)\n\n\n\n\nNow, perhaps there are some devilishly clever R users who can look at this table and immediately decode all its mysteries. But I will be honest with you, and confess that I am not one of these people. When I first started learning Arrow, I had no idea what any of this meant. This whole table was completely intimidating. I looked at it and thoughts roughly along the following lines went through my head:\n\nOh… f**k me. I’m completely out of my depth, I am too stupid to understand any of this. I should quit now and find a new job before everyone realises I’m a total fraud. They made a terrible mistake hiring me and… blah blah blah\n\nThe self-pity went on for a while and the names I called myself became quite inappropriate for a family restaurant, but I’ll be kind and spare you the tiresome details.\nEventually I remembered that this is my impostor syndrome talking and that I am in fact quite good at learning technical concepts. The problem I’m encountering here is that this table isn’t self-explanatory, and isn’t accompanied by the explanatory scaffolding that helps new users orient themselves. That’s a documentation issue, not a user problem. At a later point someone1 might need to add a few explanatory paragraphs and probably a vignette to ensure that new Arrow users don’t get confused at this point, but for now let’s see if we can’t unpack it here?\nLooking at this table, a new user might have some very reasonable questions. What exactly is a ChunkedArray and how is it different from an Array? Why are these necessary as distinct concepts? While we are at it, what is a RecordBatch, a Table and a Dataset, and what makes them different from one another? Unless someone takes the time to explain it all to you, it does look like Arrow is unnecessarily complicated, doesn’t it? These are core concepts in Arrow, but new users don’t know what they are yet!\nIn short, the time has come to tell the story behind this table. With that in mind, I’ll go through this table row by row and talk about what each line actually means.2\nAdventure!\nRomance!\nDrama!\nJoy!\nI am absolutely not going to deliver any of those things, but hopefully this will be useful.\n\nlibrary(arrow)\nlibrary(spotifyr)\nlibrary(dplyr)\nlibrary(tibble)\noptions(scipen = 20)"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#scalars",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#scalars",
    "title": "Arrays and tables in Arrow",
    "section": "Scalars",
    "text": "Scalars\nLet’s start with scalars. A scalar object is simply a single value, that can be of any type. It might be an integer, a string, a timestamp, or any of the different data types that Arrow supports. I won’t talk about the different types in this post because I already wrote an extremely long post on that topic. For the current purposes, what matters is that a scalar is one value. It is “zero dimensional”. All higher order data structures are built on top of scalars, so they are in some sense fundamental, but there is not much I need to say about them for this post. For the record though, you can create a scalar using Scalar$create():\n\nScalar$create(\"hi\")\n\nScalar\nhi\n\n\nOh the excitement. I can barely contain myself."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#arrays",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#arrays",
    "title": "Arrays and tables in Arrow",
    "section": "Arrays",
    "text": "Arrays\n\nAll the gods have been domesticated\nAnd heaven is now overrated\n  – Cassandra, Florence + The Machine\n\nOkay, so scalars aren’t very interesting for the purposes of this post. Let’s turn our attention to arrays next. An array is roughly analogous to a vector in R, and the arrow package is written in a way that allows you to interact with Arrow arrays in ways that feel familiar to R users.\nI can create an array using Array$create():\n\narr &lt;- Array$create(c(\"hello\", \"cruel\", \"world\"))\narr\n\nArray\n&lt;string&gt;\n[\n  \"hello\",\n  \"cruel\",\n  \"world\"\n]\n\n\nI can create a subset of an array using square brackets:\n\narr[2:3]\n\nArray\n&lt;string&gt;\n[\n  \"cruel\",\n  \"world\"\n]\n\n\nNotice that I used 2:3 here to extract the 2nd and 3rd elements of the array. Unlike R, Arrow uses zero-based indexing, so if I were writing this using “Arrow native” code, the relevant subset would be 1:2. However, as a general principle, arrow tries to make Arrow data structures behave like R native objects. The design principle here is that you should be able to use your usual R code to manipulate Arrow data objects, without needing to think too much about the Arrow implementation.\nI’m stressing the principle now because later in this post I am going to violate it!\nBecause one of the other things arrow does is expose a low-level interface to Arrow. You don’t need to use this (and in general you don’t really need to), but it’s there if you want it, and because this post explores the R/Arrow interface I am going to use it sometimes. When that happens, you’ll start to see zero-based indexing appear! I promise I will signpost this every time it happens so you aren’t caught unawares.\n\n\nStructure of arrays\nAnyway, as I was saying, an array in Arrow is analogous to a vector in R: it is a sequence of values with known length, all of which have the same type. When you’re using the arrow package on a day-to-day basis, you really don’t need to know much more than that. But if you want to understand data objects in Arrow properly, it helps to do a slightly deeper dive. All the low level details are described on the Arrow specification page, but the full specification is a little overwhelming when you’re first starting out. I’ll start by introducing two key concepts:\n\nThe data in an array are stored in one or more buffers. A buffer is a sequential virtual address space (i.e., block of memory) with a given length. As long as you have a pointer specifting the memory address for the buffer (i.e., where it starts), you can reach any byte in the buffer using an “offset” value that tells you the location of that byte relative to the start of the buffer.\nThe physical layout of an array is a term used to describe how data in an array is laid out in memory, without taking into account of how that information is interpreted. For example, a 32-bit signed integer and 32-bit floating point number have the same layout: they are both 32 bits, represented as 4 contiguous bytes in memory. The meaning is different, but the layout is the same. However, unlike simple scalars, an array can have a relatively complex layout, storing data and metadata in a structured arrangement.\n\n\n\nLayouts and buffers\nLet’s unpack some of these ideas using a simple array of integer values:\n\narr &lt;- Array$create(c(1L, NA, 2L, 4L, 8L))\narr\n\nArray\n&lt;int32&gt;\n[\n  1,\n  null,\n  2,\n  4,\n  8\n]\n\n\nWhat precisely is this thing? Well that’s a mess of different questions. In one sense, the answer is straightforward. It’s an Arrow array, and the values contained within the array are all stored as signed 32 bit integers:\n\narr$type\n\nInt32\nint32\n\n\nBut that’s not a very satisfying answer at some level. What does this thing look like in memory? How is the information structured? In other words, what is the physical layout of this object?\nThe Arrow documentation page helps us answer that. Our array contains two pieces of metadata, namely the length of the array (i.e. 5) and a count of the number of null values (i.e., 1), both of which are stored as 64-bit integers. The arrow package makes it easy to extract these values, because the Array object has fields and methods that will return them:\n\narr$length()\n\n[1] 5\n\narr$null_count\n\n[1] 1\n\n\nOkay, that seems reasonable. What about the data itself? Where is that stored? In Arrow, these are stored within buffers, a contiguous block of memory assigned to the array. The number of buffers associated with an array depends on the exact type of data being stored. For an integer array such as arr, there are two buffers, a validity bitmap buffer and a data value buffer. So we have a data structure that could be depicted like this:\n\n\n\n\n\n\n\n\n\nIn this figure I’ve shown the array as a grey rectangle subdivided into two parts, one for the metadata and the other for the buffers. Underneath I’ve unpacked this it a little, showing the contents of the two buffers in the area enclosed in a dotted line. At the lowest level of the figure, you can see the contents of specific bytes. Notice that the numbering of the bytes starts at zero: I’m referring to Arrow data structures here, and Arrow is zero-indexed. Later in the post I’ll talk about how you can access the raw content of these buffers, but for now let’s talk about what each of these buffers contains.\n\n\n\nThe validity bitmap buffer\nThe validity bitmap is binary-valued, and contains a 1 whenever the corresponding slot in the array contains a valid, non-null value. Setting aside some very tiresome technicalities we can imagine that the validity bitmap is a buffer that contains the following five bits:\n10111\nExcept… this isn’t really true, for three reasons. First, memory is allocated in byte-size units, so we have to pad it out to the full 8 bits. That gives us the bitmap 10111000. Second, that’s still a little inaccurate because – assuming you read left to right – you’re looking it with the “most significant bit” first (i.e., big endian format), and the bits are actually organised with the least significant bit first (i.e., little endian format) so the bits in this byte should be shown in the reverse order, 00011101. Third, this is still misleading because I’ve not padded it enough. For reasons that make a lot of sense if you start diving into the Arrow specifications at a low level, you have to imagine another 503 trailing zeros.3 So that the nice and neat 10111 I’ve shown above actually looks like this in memory:\n\n\n\n\n\nByte 0 (validity bitmap)\nBytes 1-63\n\n\n\n\n00011101\n0 (padding)\n\n\n\n\n\nI probably wouldn’t have gone into quite this much detail, except for the fact that you can find this exact example when reading about physical layouts in the Arrow documentation, and I think it’s helpful to have a clear point of contact between this post and the documentation.\nAnyway, I realise I’m being boring. So let’s move on.\n\n\n\nThe data value buffer\nOkay, now let’s have a look at the value buffer. It’s essentially the same logic. Again notice that its padded out to a length of 64 bytes to preserve natural alignment, but for our purposes those details don’t matter too much. Here’s the diagram showing the physical layout, again lifted straight from the Arrow specification page:\n\n\n\n\n\n\n\n\n\n\n\n\n\nBytes 0-3\nBytes 4-7\nBytes 8-11\nBytes 12-15\nBytes 16-19\nBytes 20-63\n\n\n\n\n1\nunspecified\n2\n4\n8\nunspecified\n\n\n\n\n\nEach integer occupies 4 bytes, as required by the int32 data type. (If you want to know more about how Arrow represents integers, it’s discussed in the data types post).\n\n\n\n\n\nPeeking inside arrays\nI mentioned earlier that arrow exposes some “low level” tools that allow you to interact with Arrow data objects in more of a bare bones fashion than a data analyst normally would. For example, you wouldn’t normally have a need to extract the raw bytes that comprise the buffers in an array. There’s no “high level” interface that lets you do this. But if you really want to see what’s going on under the hood you absolutely can, and arrow lets you do this. To show you how it works, I’ll uses a small data set containing the track listing for the new Florence + The Machine album, Dance Fever:\n\ndance_fever &lt;- read_csv_arrow(\"dance_fever_tracks.csv\")\ndance_fever\n\n# A tibble: 14 × 3\n   track_number title             duration\n          &lt;int&gt; &lt;chr&gt;                &lt;int&gt;\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n\n\nI’ll start by taking the duration variable and creating an Arrow array from it:\n\nduration &lt;- Array$create(dance_fever$duration)\nduration\n\nArray\n&lt;int32&gt;\n[\n  280,\n  234,\n  213,\n  236,\n  280,\n  227,\n  73,\n  258,\n  111,\n  214,\n  231,\n  48,\n  165,\n  262\n]\n\n\nAs a reminder, here’s a crude schematic diagram showing how that object is laid out. It has some metadata that you’ve already learned how to extract (e.g., using duration$null_count), and it has two data buffers that I talked about at tedious length but haven’t actually shown you yet:\n\n\n\n\n\n\n\n\n\nTo take a more detailed look at the data stored in the duration object, we can call its data() method to return an ArrayData object. Admittedly, the results are not immediately very exciting:\n\nduration$data()\n\nArrayData\n\n\nThis output is a little underwhelming because at the moment the print method for an ArrayData object doesn’t do anything except print the class name. Boring! However, because an ArrayData object is stored as an R6 object, all the information is tucked away in an environment. We can find out the names of objects contained in that environment easily enough:\n\nnames(duration$data())\n\n [1] \".__enclos_env__\" \"buffers\"         \"offset\"          \"null_count\"     \n [5] \"length\"          \"type\"            \".:xp:.\"          \"clone\"          \n [9] \".unsafe_delete\"  \"print\"           \"class_title\"     \"set_pointer\"    \n[13] \"pointer\"         \"initialize\"     \n\n\nHm. There’s a buffers variable in there. I wonder what that is…\n\nduration$data()$buffers\n\n[[1]]\nNULL\n\n[[2]]\nBuffer\n\n\nOh look, there are two buffers here! What’s the chance that the first one is the validity bitmap and the second one is the data buffer? (Answer: 100% chance). It turns out that this is another situation where a Buffer object belongs to an R6 class with a boring print method. I could bore you by going through the same process I did last time, but I’d rather not waste your time. It turns out that Buffer objects have a data() method of their own. When we call the data() method returns the bytes stored in the relevant buffer as a raw vector. At long last, we can pull out the raw bytes:\n\ndata_buffer &lt;- duration$data()$buffers[[2]]$data()\ndata_buffer\n\n [1] 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 18 01 00 00 e3 00 00 00 49\n[26] 00 00 00 02 01 00 00 6f 00 00 00 d6 00 00 00 e7 00 00 00 30 00 00 00 a5 00\n[51] 00 00 06 01 00 00\n\n\nShockingly, I personally cannot read binary, but as it turns out the readBin() function from base R is perfectly well equipped to do that. Let’s see what happens when we interpret these 56 bytes as a sequence of 14 integers:4\n\nreadBin(data_buffer, what = \"integer\", n = 14)\n\n [1] 280 234 213 236 280 227  73 258 111 214 231  48 165 262\n\n\nThose are the values stored in the duration array. Yay!\n\n\n\nPrettier ArrayData\nA little digression before we move on to talking about chunked arrays. Later in this post I’ll occasionally want to show you the internal structure of an array, just so you can see that the buffers and metadata have the values you’d expect them to. The information I need for this is stored in the ArrayData object returned by a command like duration$data() but as we saw in the last section there’s no convenient way to display these objects. To make this a little simpler, I wrote my own array_layout() function that shows you the metadata and buffer contents associated with an Arrow array – the source code is here) — that doesn’t work for all array types, but can handle the ones I’m using in this post. When applied to the duration array it produces this output:\n\nduration |&gt;\n  array_layout()\n\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• data : 280 234 213 236 280 227 73 258 111 214 231 48 165 262\n\n\nThe output here is divided into two sections, structured to mirror how the Arrow columnar specification is described on the website (and also to mirrot the diagrams in the post). There is one section showing the metadata variables stored: array length, and a count of the number of null values. Underneath that we have a section listing all the buffers associated with an array. For an integer array like duration there are two buffers, the validity bitmap buffer and the data values buffer.\nThe array_layout() function also works for string arrays and produces similar output. However, character data in Arrow are stored using three buffers rather than two. As before the first buffer stores the validity bitmap. The second buffer is a vector of offsets specifying the locations for each of the substrings. The third buffer contains the character data itself. Here’s an example of that:\n\ndance_fever$title |&gt;\n  Array$create() |&gt;\n  array_layout()\n\n\n── Metadata \n• length : 14\n• null count : 0\n\n── Buffers \n• validity : null\n• offset : 0 4 8 19 31 48 63 77 86 100 108 115 124 132 145\n• data : KingFreeChoreomaniaBack in TownGirls Against GodDream Girl EvilPrayer\nFactoryCassandraHeaven Is HereDaffodilMy LoveRestraintThe BombMorning Elvis\n\n\nIf you want more information about how character data are stored in Arrow and how the offset buffer and data buffer are used to define the array as a whole, I wrote about it in tiresome detail in my data types post. For the purposes of this post, it’s enough to understand that string arrays are organised using these three buffers."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#chunked-arrays",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#chunked-arrays",
    "title": "Arrays and tables in Arrow",
    "section": "Chunked arrays",
    "text": "Chunked arrays\n\nI need my golden crown of sorrow, my bloody sword to swing\nI need my empty halls to echo with grand self-mythology\n’Cause I am no mother, I am no bride\nI am king\n  – King, Florence + The Machine\n\nThe next entry in the table of data objects refers to “chunked arrays”. In most respects a chunked array behaves just like a regular array. It is a one-dimensional data structure. It requires every value stored to be of the same type: all integers, or all strings, or whatever. From the perspective of a data scientist who just wants to analyse the data, an array and a chunked array are essentially identical. Under the hood, however, they are quite different – and the reason for this is fundamentally a data engineering issue. In this section I’ll unpack this.\nRecall at the beginning I emphasised that an array is an immutable object. Once an array has been created by Arrow, the values it stores cannot be altered. The decision to make arrays immutable reduces the need to create copies: many other objects can all safely refer to the array (via pointers) without making copies of the data, safe in the knowledge that it is impossible5 for anyone else to change the data values. For large data sets that’s a huge advantage: you really don’t want to be making copies of data if you can avoid doing so. Immutable arrays are good.\nWell, mostly good.\nThere are some limitations to immutable arrays, and one of the big ones is prompted by the very simple question: what happens when a new batch of data arrives? An array is immutable, so you can’t add the new information to an existing array. The only thing you can do if you don’t want to disturb or copy your existing array is create a new array that contains the new data. Doing that preserves the immutability of arrays and doesn’t lead to any unnecessary copying – which keeps us happy(ish) from a data engineering perspective – but now we have a new problem: the data are now split across two arrays. Each array contains only one “chunk” of the data. We need some way of “pretending” that these two arrays are in fact a single array-like object.\nThis is the problem that chunked arrays solve. A chunked array is a wrapper around a list of arrays, and allows you to index their contents “as if” they were a single array. Physically, the data are still stored in separate places – each array is one chunk, and these chunks don’t have to be adjacent to each other in memory – but the chunked array provides us will a layer of abstraction that allows us to pretend that they are all one thing.\n\n\nList-like aspects\nHere’s an example. I’ll take some lyrics to King by Florence + The Machine, and use the chunked_array() function from arrow to store them as a chunked array that is comprised of three smaller arrays:\n\nking &lt;- chunked_array(\n  c(\"I\", \"am\", \"no\", \"mother\"), # chunk 0\n  c(\"I\", \"am\", NA, \"bride\"),    # chunk 1\n  c(\"I\", \"am\", \"king\")          # chunk 2\n)\n\nAn alternative way to do the same thing would be to use the create() method of the R6 object ChunkedArray. In fact, the chunked_array() function is just a slightly nicer wrapper around the same functionality that ChunkedArray$create() provides. But that’s a bit of a digression. Let’s take a look at the object I just created:\n\nking\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"I\",\n    \"am\",\n    \"no\",\n    \"mother\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    null,\n    \"bride\"\n  ],\n  [\n    \"I\",\n    \"am\",\n    \"king\"\n  ]\n]\n\n\nThe double bracketing in this output is intended to highlight the “list-like” nature of chunked arrays. There are three separate arrays that I have created here, wrapped in a handly little container object that is secretly a list of arrays, but allows that list to behave just like a regular one-dimensional data structure. Schematically, this is what I’ve just created:\n\n\n\n\n\n\n\n\n\nAs this figure illustrates, there really are three arrays here. I can pull them out individually by referring to their position in the array list by using the chunk() method that all chunked array objects possess. This is another one of those situations where I’m using a low-level feature, and the zero-based indexing in Arrow reappears. To extract the second chunk, here’s what I do:\n\nking$chunk(1)\n\nArray\n&lt;string&gt;\n[\n  \"I\",\n  \"am\",\n  null,\n  \"bride\"\n]\n\n\nNotice from the output that this chunk is a vanilla Array object, and I can take a peek at the underlying metadata and buffers associated with that object by using the array_layout() function I wrote earlier. Here’s what that chunk looks like:\n\nking$chunk(1) |&gt; \n  array_layout()\n\n\n── Metadata \n• length : 4\n• null count : 1\n\n── Buffers \n• validity : 1 1 0 1\n• offset : 0 1 3 3 8\n• data : Iambride\n\n\nHopefully by now this all looks quite familiar to you! The Array object here has length 4, contains 1 missing value (referred to as null values in Arrow), and because it is a string array, it contains three buffers: a validity bitmap, an offset buffer, and a data buffer.\n\n\n\nVector-like aspects\nIn the previous section I highlighted the fact that internally a chunked array is “just” a list of arrays and showed you how you can interact with a chunked array in a “list-like” way. Most of the time though, when you’re working with a chunked array as a data analyst you aren’t really interested in its list-like properties, what you actually care about is the abstraction layer that provides it with vector-like properties. Specifically, what you actually care about is the fact that a chunked array is a one-dimensional object with a single indexing scheme. Let’s go back to the king data to illustrate this. Suppose I want to extract a subset of the elements. Specifically I want to grab the 3rd through 6th elements. These slots actually belong to different arrays, and it would be a pain to extract the 3rd and 4th slots from the first array, and the 1st and 2nd slots from the second array. No data analyst wants that headache. Fortunately, I don’t have to:\n\nking[3:6]\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"no\",\n    \"mother\"\n  ],\n  [\n    \"I\",\n    \"am\"\n  ]\n]\n\n\nAs an R user you are probably breathing a sigh of relief to see the return of one-based indexing! Again I should stress that this is the norm: as a general rule, the arrow package tries to mimic R conventions whenever you are “just trying to do normal R things”. If you’re trying to manipulate and analyse data, the intention is that your regular dplyr functions should work the same way they always did, and the same goes for subsetting data. In R, the first element of a vector is element 1, not element 0, and that convention is preserved here. The only time you’re going to see arrow adopt zero-based indexing is when you are interacting with Arrow at a low level.\nAnother thing to highlight about chunked arrays is that the “chunking” is not considered semantically meaningful. It is an internal implementation detail only: you should never treat the chunk as a meaningful unit! Writing the data to disk, for example, often results in the data being organised into different chunks. Two arrays that have the same values in different chunking arrangements are deemed equivalent. For example, here’s the same four values as king[3:6] all grouped into a single chunk:\n\nno_mother &lt;- chunked_array(c(\"no\", \"mother\", \"I\", \"am\"))\nno_mother\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"no\",\n    \"mother\",\n    \"I\",\n    \"am\"\n  ]\n]\n\n\nWhen I test for equality using ==, you can see that the results are shown element-wise. All four elements are the same, so the result is a (chunked) array of four true values:\n\nno_mother == king[3:6]\n\nChunkedArray\n&lt;bool&gt;\n[\n  [\n    true,\n    true,\n    true,\n    true\n  ]\n]\n\n\nThe intention, ultimately, is that users should be able to interact with chunked arrays as if they were ordinary one-dimensional data structures without ever having to think much about their list-like nature. Chunked arrays exist as an abstraction to help bridge the gap between the needs of the data engineer and the needs of the data scientist. So except in those special cases where you have to think carefully about the engineering aspect, a data analyst should be able to treat them just like regular vectors!"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#record-batches",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#record-batches",
    "title": "Arrays and tables in Arrow",
    "section": "Record batches",
    "text": "Record batches\nNow that we have a good understanding of zero dimensional data objects (scalars), and one dimensional data objects (arrays and chunked arrays), the time has come to make the jump into the second dimension and talk about tabular data structures in arrow. In the data objects table I presented at the start of the post there are three of these listed: record batches, tables, and datasets. Record batches are the simplest of the three so I’m going to start there, but in everyday data analysis you’re not likely to be using them very much: in practice tables and datasets are the things you’re likely to care about most. Even so, from a learning standpoint it really helps to have a good understanding of record batches because the other concepts are built on top of them.\nA record batch is table-like data structure comprised of a sequence of arrays. The arrays can be of different types but they must all be the same length. Each array is referred to as one of the “fields” or “columns” of the record batch. This probably sounds terribly familiar to you as an R user, because – apart from a few differences in terminology – what I’ve just described to you is essentially the same kind of thing as a data frame. The parallels between record batches and data frames run deeper too:\n\nIn R, the columns in a data frame must be named.6 Record batches have the same property: each field must have a (UTF8-encoded) name, and these names form part of the metadata for the record batch.\nA data frame in R is secretly just a list of vectors, and like any other list it does not really “contain” those vectors: rather it consists of a set of pointers that link to those vector objects. There’s a good discussion of list references in chapter 2 of Advanced R. Record batches in Arrow are much the same. When stored in memory, the record batch does not include physical storage for the values stored in each field, it simply contains pointers to the relevant array objects. It does, however, contain its own validity bitmap.\n\nTo illustrate, let’s return to our dance_fever data set. Here it is as a data frame (well, tibble technically, but whatever):\n\ndance_fever\n\n# A tibble: 14 × 3\n   track_number title             duration\n          &lt;int&gt; &lt;chr&gt;                &lt;int&gt;\n 1            1 King                   280\n 2            2 Free                   234\n 3            3 Choreomania            213\n 4            4 Back in Town           236\n 5            5 Girls Against God      280\n 6            6 Dream Girl Evil        227\n 7            7 Prayer Factory          73\n 8            8 Cassandra              258\n 9            9 Heaven Is Here         111\n10           10 Daffodil               214\n11           11 My Love                231\n12           12 Restraint               48\n13           13 The Bomb               165\n14           14 Morning Elvis          262\n\n\nThe arrow package provides two different ways to create a record batch. I can either use RecordBatch$create() or I can use the record_batch() function. The latter is simpler, so I’ll do that. The record_batch() function is pretty flexible, and can accept inputs in several formats. I can pass it a data frame, one or more named vectors, an input stream, or even a raw vector containing appropriate binary data. But I don’t need all that fancy complexity here so I’ll just give it a data frame:\n\ndf &lt;- record_batch(dance_fever)\ndf\n\nRecordBatch\n14 rows x 3 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n$duration &lt;int32&gt;\n\n\nThe output is amusingly terse. It doesn’t give a preview of the data, but it kindly confirms that this is a record batch containing 14 rows and 3 columns. It also tells me the column names and the type of data stored in each column. The arrow package supplies a $ method for record batch objects, and it behaves the same way you’d expect for a data frame. If I want to look at a particular column in my record batch, I can refer to it by name like so:\n\ndf$title\n\nArray\n&lt;string&gt;\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\",\n  \"Girls Against God\",\n  \"Dream Girl Evil\",\n  \"Prayer Factory\",\n  \"Cassandra\",\n  \"Heaven Is Here\",\n  \"Daffodil\",\n  \"My Love\",\n  \"Restraint\",\n  \"The Bomb\",\n  \"Morning Elvis\"\n]\n\n\nAt an abstract level the df object behaves like a two dimensional structure with rows and columns, but in terms of how it is represented in memory it is fundamentally a list of arrays, and so schematically I’ve drawn it like this:\n\n\n\n\n\n\n\n\n\nIn some respects it’s structurally similar to a chunked array, insofar record batches and chunked arrays are both lists of arrays, but in other ways they are quite different. The arrays indexed in a record batch can be different types, but they must be the same length: this is required to ensure that at a high level we can treat a record batch like a two dimensional table. In contrast, the arrays indexed by a chunked array can be different lengths, but must all be the same type: this is required to ensure that at a high level we can treat a chunked array like a one dimensional vector.\nReturning to the practical details, it’s worth noting that in addition to the $ operator that refers to columns by name, you can use double brackets [[ to refer to columns by position. Just like we saw with chunked array, these positions follow the R convention of using 1 to refer to the first element. The df$title array is the 2nd column in our record batch so I can extract it with this:\n\ndf[[2]]\n\nArray\n&lt;string&gt;\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\",\n  \"Girls Against God\",\n  \"Dream Girl Evil\",\n  \"Prayer Factory\",\n  \"Cassandra\",\n  \"Heaven Is Here\",\n  \"Daffodil\",\n  \"My Love\",\n  \"Restraint\",\n  \"The Bomb\",\n  \"Morning Elvis\"\n]\n\n\nFinally there is a [ method that allows you to extract subsets of a record batch in the same way you would for a data frame. The command df[1:10, 1:2] extracts the first 10 rows and the first 2 columns:\n\ndf[1:10, 1:2]\n\nRecordBatch\n10 rows x 2 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n\n\nIf you are wondering what df[1:2] returns, try it out for yourself. Hopefully you will not be surprised!\n\n\nSerialisation\nBefore I move on to talk about Arrow tables, I want to make a small digression. At the beginning of this post I mentioned that my very first attempt to write about Arrow ended up becoming a post about data serialisation in R that had nothing to do with Arrow. That didn’t happen entirely by accident, and I’ll try to explain some of that now.\nWhen we talk about data serialisation, what we’re talking about is taking a data structure stored in memory and organising it into a format that is suitable for writing to disk (serialising to file format) or transmitting over some other communication channel (serialising to a stream). From the beginning, data structures in Arrow were designed together with file formats and streaming formats, with the intention that – to the extent that it is practical and doesn’t mess with other important design considerations – the thing you send across the communication channel (or write to disk) has the same structure as the thing you need to store in memory. That way, when the data arrive at the other end, you don’t have to do a lot of work reorganising the data.\nThat… makes sense, right?\nIf I want to tell you to meet me under the bridge, the message I should send you should be “meet me under the bridge”. It makes no sense whatsoever for me to say “meet me [the title of that really annoying Red Hot Chilli Peppers song]” and expect you to decode it. There is no point in me expending effort deliberately obscuring what I’m trying to say, and then forcing you to expend effort trying to interpret my message.\nYet, surprisingly, that’s what happens a lot of the time when we send data across communication channels. For example, suppose you and I are both R users. We both work with data frames. Because a data frame is fundamentally a list of variables (each of which is a vector), we use data that are organised column-wise: the first thing in a data frame is column 1, then column 2, then column 3, and so on. Okay, cool. So now let’s say you want to send me a data set, and what you do is send me a CSV file. A CSV file is written row by row: the first thing in a CSV file is row 1, then row 2, then row 3. It is a row-wise data structure. In order for you to send data to me, what has to happen is you take your column-wise data frame, invert it so that it is now a row-wise structure, write that to a CSV and then send it to me. At the other end, I have to invert the whole process, transforming the row-wise structure into a column-wise organisation that I can now load into memory as a data frame.7\nUm… that doesn’t make sense.\nThis particular problem arises quite a lot, largely because serialisation formats and in-memory data structures aren’t always designed in tandem. To get around this, Arrow specifies the Interprocess Communication (IPC) serialisation format that is designed specifically to ensure that Arrow data objects can be transmitted (and saved) efficiently. Because data sets are typically organised into tabular structures, the primitive unit for communication is the record batch. I’m not going to dive into the very low level details of how IPC messages are structured, but the key thing for our purposes is that IPC is designed to ensure that the structure of the serialised record batch is essentially identical to the physical layout of an in-memory record batch.\nI’ll give a very simple example. Let’s take the first few rows of the dance_fever data set and convert them into a small record batch:\n\ndf_batch_0 &lt;- record_batch(dance_fever[1:4,])\ndf_batch_0\n\nRecordBatch\n4 rows x 3 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n$duration &lt;int32&gt;\n\n\nSuppose I want to share this. Currently this is an object in memory that consists of three arrays (which are contiguous in memory) but as we’ve seen earlier, these arrays are themselves comprised of multiple buffers. What the IPC format does is collect the relevant metadata into a “data header”,8 and then lay out the buffers one after the other. Glossing a few minor details9, this is essentially what the IPC message would look like for this record batch:\n\n\n\n\n\n\n\n\n\nAt this point you’re probably thinking okay that’s nice Danielle, but how do I do this? There are three functions you can use:\n\nTo send the data directly to an output stream use write_ipc_stream().\nTo write data in IPC format to a static file on disk it is referred to as a “feather” formatted file,10 and you use write_feather().\nTo construct the same sequence of bytes but return them in R as raw vectors, you can use write_to_raw().\n\nIn order to show you what the byte stream actually looks like, I’ll use the write_to_raw() function:\n\ndf_ipc_0 &lt;- write_to_raw(df_batch_0)\ndf_ipc_0\n\n  [1] ff ff ff ff f0 00 00 00 10 00 00 00 00 00 0a 00 0c 00 06 00 05 00 08 00 0a\n [26] 00 00 00 00 01 04 00 0c 00 00 00 08 00 08 00 00 00 04 00 08 00 00 00 04 00\n [51] 00 00 03 00 00 00 7c 00 00 00 3c 00 00 00 04 00 00 00 a0 ff ff ff 00 00 01\n [76] 02 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 08 00 00 00 64 75 72 61\n[101] 74 69 6f 6e 00 00 00 00 8c ff ff ff 00 00 00 01 20 00 00 00 d4 ff ff ff 00\n[126] 00 01 05 10 00 00 00 1c 00 00 00 04 00 00 00 00 00 00 00 05 00 00 00 74 69\n[151] 74 6c 65 00 00 00 04 00 04 00 04 00 00 00 10 00 14 00 08 00 06 00 07 00 0c\n[176] 00 00 00 10 00 10 00 00 00 00 00 01 02 10 00 00 00 28 00 00 00 04 00 00 00\n[201] 00 00 00 00 0c 00 00 00 74 72 61 63 6b 5f 6e 75 6d 62 65 72 00 00 00 00 08\n[226] 00 0c 00 08 00 07 00 08 00 00 00 00 00 00 01 20 00 00 00 00 00 00 00 ff ff\n[251] ff ff f8 00 00 00 14 00 00 00 00 00 00 00 0c 00 16 00 06 00 05 00 08 00 0c\n[276] 00 0c 00 00 00 00 03 04 00 18 00 00 00 58 00 00 00 00 00 00 00 00 00 0a 00\n[301] 18 00 0c 00 04 00 08 00 0a 00 00 00 8c 00 00 00 10 00 00 00 04 00 00 00 00\n[326] 00 00 00 00 00 00 00 07 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00\n[351] 00 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 10 00 00 00 00 00 00\n[376] 00 00 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 14 00 00 00 00 00 00 00\n[401] 28 00 00 00 00 00 00 00 1f 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 00\n[426] 00 00 00 00 00 00 00 48 00 00 00 00 00 00 00 10 00 00 00 00 00 00 00 00 00\n[451] 00 00 03 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00\n[476] 00 00 00 00 00 00 00 00 00 00 00 00 00 04 00 00 00 00 00 00 00 00 00 00 00\n[501] 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 00 00 00 00 04\n[526] 00 00 00 08 00 00 00 13 00 00 00 1f 00 00 00 00 00 00 00 4b 69 6e 67 46 72\n[551] 65 65 43 68 6f 72 65 6f 6d 61 6e 69 61 42 61 63 6b 20 69 6e 20 54 6f 77 6e\n[576] 00 18 01 00 00 ea 00 00 00 d5 00 00 00 ec 00 00 00 ff ff ff ff 00 00 00 00\n\n\nTo reassure you that this byte stream does indeed contain the relevant information, I’ll use the read_ipc_stream() function to decode it. By default this function returns data to R natively as a tibble, which is fine for my purposes:\n\nread_ipc_stream(df_ipc_0)\n\n# A tibble: 4 × 3\n  track_number title        duration\n         &lt;int&gt; &lt;chr&gt;           &lt;int&gt;\n1            1 King              280\n2            2 Free              234\n3            3 Choreomania       213\n4            4 Back in Town      236\n\n\nWe can use the same logic to write data to disk. As mentioned above, when writing data in IPC format to file, the result is called a “feather” file. So okay, let’s take the rest of the dance_fever data, and write it to a feather file:\n\ndance_fever[5:14,] |&gt;\n  record_batch() |&gt;\n  write_feather(\"df_ipc_1.feather\")\n\nNow we can read this feather file from disk:\n\nread_feather(\"df_ipc_1.feather\")\n\n# A tibble: 10 × 3\n   track_number title             duration\n          &lt;int&gt; &lt;chr&gt;                &lt;int&gt;\n 1            5 Girls Against God      280\n 2            6 Dream Girl Evil        227\n 3            7 Prayer Factory          73\n 4            8 Cassandra              258\n 5            9 Heaven Is Here         111\n 6           10 Daffodil               214\n 7           11 My Love                231\n 8           12 Restraint               48\n 9           13 The Bomb               165\n10           14 Morning Elvis          262\n\n\nYay! It’s always nice when things do what you expect them to do.\nBefore moving on, there’s one last thing I should mention. The feather file format is a handy thing to know about, and can be very convenient in some instances, but it’s not really optimised to be the best “big data file format”. It’s intended to be the file format analog of IPC messages, and those in turn are designed for optimal streaming of Arrow data. Because of this, in practice you will probably not find yourself using the feather format all that much. Instead, you’re more likely to use something like Apache Parquet, which is explicitly designed for this purpose. Arrow and parquet play nicely with one another, and arrow supports reading and parquet files using the read_parquet() and write_parquet() functions. However, parquet is a topic for a future post, so that’s all I’ll say about this today!"
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#tables",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#tables",
    "title": "Arrays and tables in Arrow",
    "section": "Tables",
    "text": "Tables\n\nTell me where to put my love\nDo I wait for time to do what it does?\nI don’t know where to put my love\n  – My Love, Florence + The Machine\n\nEarlier when I introduced the concept of chunked arrays, I explained that Arrow needs these structures because arrays are immutable objects, and Arrow is designed to avoid copying data whenever possible: when a new block of data arrive, it is stored as its own array without disturbing the existing ones, and we use the chunked array as a wrapper that lets us pretend that all these chunks are laid out end to end in a single vector. The previous section shows you exactly how that can happen. If I have a data set that arrives sequentially as a sequence of record batches, I have this problem for every column in the data set! Quite by accident11 that’s what happened in the last section – the dance_fever data set has been serialised in two parts. In that example it happened because I wanted to show you what an IPC stream looked like (creating one record batch for that) as well as what a feather file looks like (creating another record batch), but in real life it’s more likely to happen every time you receive an update on an ongoing data collection process (e.g., today’s data arrive).\nTo deal with this situation, we need a tabular data structure that is similar to a record batch with one exception: instead of storing each column as an array, we now want to store it as a chunked array. This is what the Table class in arrow does. Schematically, here’s what the data structure for a table looks like:\n\n\n\n\n\n\n\n\n\nTables have a huge advantage over record batches: they can be concatenated. You can’t append one record batch to another because arrays are immutable: you can’t append one array to the end of another array. You would have to create a new array with all new data – and do this for every column in the data – which is a thing we really don’t want to do. But because tables are built from chunked arrays, concatenation is easy: all you have to do is update the chunked arrays so that they include pointers to the newly-arrived arrays as well as the previously-existing arrays.\nBecause tables are so much more flexible than record batches, functions in arrow tend to return tables rather than record batches. Unless you do what I did in the previous section and deliberately call record_batch() you’re not likely to encounter them as the output of normal data analysis code.12 For instance, in the previous section I serialised two record batches, one to a file and one to a raw vector. Let’s look at what happens when I try to deserialise (a.k.a. “read”) them. First the IPC stream:\n\ndf_table_0 &lt;- read_ipc_stream(\n  file = df_ipc_0, \n  as_data_frame = FALSE\n)\ndf_table_0\n\nTable\n4 rows x 3 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n$duration &lt;int32&gt;\n\n\nThat’s the same data as before, but it’s a table not a record batch. Each column is a chunked array, not an array. The same happens when I read from the feather file:\n\ndf_table_1 &lt;- read_feather(\n  file = \"df_ipc_1.feather\", \n  as_data_frame = FALSE\n)\ndf_table_1\n\nTable\n10 rows x 3 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n$duration &lt;int32&gt;\n\n\nAgain, this is a table. In general, you won’t get a record batch in arrow unless you explicitly ask for one. Tables are the default tabular data structure, which is usually what you want anyway.\nOkay, so now I have the two fragments of my data set represented as tables. The difference between the table version and the record batch version is that the columns are all represented as chunked arrays. Each array from the original record batch is now one chunk in the corresponding chunked array in the table:\n\ndf_batch_0$title\n\nArray\n&lt;string&gt;\n[\n  \"King\",\n  \"Free\",\n  \"Choreomania\",\n  \"Back in Town\"\n]\n\ndf_table_0$title\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ]\n]\n\n\nIt’s the same underlying data (and indeed the same immutable array is referenced by both), just enclosed by a new, flexible chunked array wrapper. However, it is this wrapper that allows us to concatenate tables:\n\ndf &lt;- concat_tables(\n  df_table_0,\n  df_table_1\n)\ndf\n\nTable\n14 rows x 3 columns\n$track_number &lt;int32&gt;\n$title &lt;string&gt;\n$duration &lt;int32&gt;\n\n\nThis is successful (yay!) and the result will behave exactly like a two dimensional object with $, [[, and [ operators that behave as you expect them to13 (yay!), but if you look closely you can still see the “seams” showing where the tables were appended:\n\ndf$title\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"King\",\n    \"Free\",\n    \"Choreomania\",\n    \"Back in Town\"\n  ],\n  [\n    \"Girls Against God\",\n    \"Dream Girl Evil\",\n    \"Prayer Factory\",\n    \"Cassandra\",\n    \"Heaven Is Here\",\n    \"Daffodil\",\n    \"My Love\",\n    \"Restraint\",\n    \"The Bomb\",\n    \"Morning Elvis\"\n  ]\n]\n\n\nWhen tables are concatenated the chunking is preserved. That’s because those are the original arrays, still located at the same spot in memory. That’s efficient from a memory use perspective, but again, don’t forget that the chunking is not semantically meaningful, and there is no guaranteed that a write-to-file operation (e.g., to parquet format) will preserve those chunks."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#datasets",
    "title": "Arrays and tables in Arrow",
    "section": "Datasets",
    "text": "Datasets\n\nWhat kind of man loves like this?\nTo let me dangle at a cruel angle\nOh, my feet don’t touch the floor\nSometimes you’re half in and then you’re half out\nBuy you never close the door\n  – What Kind Of Man, Florence + The Machine\n\nSo what about datasets? They’re the last item on that table, and you might be wondering where they fall in all this. I’m not going to dive into the details on datasets in this post, because they’re a whole separate topic and they deserve their own blog post. However, it’s a little unsatisfying to write all this and not say anything about them, so I’ll give a very quick overview here.\nUp to this point I’ve talked about tabular data sets that are contained entirely in memory. When such data are written to disk, they are typically written to a single file. For larger-than-memory data sets, a different strategy is needed. Only a subset of the data can be stored in memory at any point in time, and as a consequence it becomes convenient to write the data to disk by partitioning it into many smaller files. This functionality is supported in Arrow via Datasets.\nI’ll give a simple example here, using a small data set. Let’s suppose I’ve downloaded the entire Florence + The Machine discography using the spotifyr package:\n\nflorence &lt;- get_discography(\"florence + the machine\")\nflorence\n\n\n\n# A tibble: 414 × 41\n# Groups:   album_name [18]\n   artist_name     artist_id album_id album_type album_images album_release_date\n   &lt;chr&gt;           &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;      &lt;list&gt;       &lt;chr&gt;             \n 1 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 2 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 3 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 4 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 5 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 6 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 7 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 8 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n 9 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n10 Florence + The… 1moxjboG… 0uGwPmq… album      &lt;df [3 × 3]&gt; 2022-05-18        \n# ℹ 404 more rows\n# ℹ 35 more variables: album_release_year &lt;dbl&gt;,\n#   album_release_date_precision &lt;chr&gt;, danceability &lt;dbl&gt;, energy &lt;dbl&gt;,\n#   key &lt;int&gt;, loudness &lt;dbl&gt;, mode &lt;int&gt;, speechiness &lt;dbl&gt;,\n#   acousticness &lt;dbl&gt;, instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, track_id &lt;chr&gt;, analysis_url &lt;chr&gt;, time_signature &lt;int&gt;,\n#   artists &lt;list&gt;, available_markets &lt;list&gt;, disc_number &lt;int&gt;, …\n\n\nThe florence data frame is of course quite small, and I have no real need to use Arrow Datasets: it’s small enough that I can store it natively in R as a tibble! But it will suffice to illustrate concepts that come in handy when working with large datasets.\nLet’s suppose I want to partition this in into many data files, using the album release year as the basis for the partitioning. To do this I’ll use the write_dataset() function, specifying partitioning = \"album_release_year\" to ensure that files are created after splitting the data set by release year. By default, the write_dataset() function writes individual data files in the parquet format, which is in general a very good default choice for large tabular data sets. However, because I have not talked about Apache Parquet in this post, I’ll make a different choice and write the data files in the feather format that we’ve seen earlier in this post. I can do that by setting format = \"feather\". Finally, I’ll set path = \"spotify_florence\" to ensure that all the files are stored in a folder by that name. That gives this command:\n\nflorence |&gt; \n  select(where(~!is.list(.))) |&gt;  # drop list columns\n  as_arrow_table() |&gt;             # convert to an arrow table\n  write_dataset(                  # write to multi-file storage\n    path = \"spotify_florence\",\n    format = \"feather\",\n    partitioning = \"album_release_year\"\n  )\n\nThe result is that the following files are written to disk:\n\nlist.files(\"spotify_florence\", recursive = TRUE)\n\n [1] \"album_release_year=2009/part-0.arrow\"  \n [2] \"album_release_year=2009/part-0.feather\"\n [3] \"album_release_year=2010/part-0.arrow\"  \n [4] \"album_release_year=2010/part-0.feather\"\n [5] \"album_release_year=2011/part-0.arrow\"  \n [6] \"album_release_year=2011/part-0.feather\"\n [7] \"album_release_year=2012/part-0.arrow\"  \n [8] \"album_release_year=2012/part-0.feather\"\n [9] \"album_release_year=2015/part-0.arrow\"  \n[10] \"album_release_year=2015/part-0.feather\"\n[11] \"album_release_year=2018/part-0.arrow\"  \n[12] \"album_release_year=2018/part-0.feather\"\n[13] \"album_release_year=2022/part-0.arrow\"  \n[14] \"album_release_year=2022/part-0.feather\"\n\n\nThese file names are written in “Hive partitioning” format. It looks a little weird the first time you encounter it, because = is a character most coders instinctively avoid including in file names because it has such a strong meaning in programming contexts. However, when files are named in Hive partitioning format, the intended interpretation is exactly the one you implicitly expect as a coder: it’s a field_name=value statement, so you will often encounter files with names like\n/year=2019/month=2/data.parquet\nFor more information see the help documentation for the hive_partitioning() function in the arrow package.\nIn any case, the key thing is that I’ve now written the data to disk in a fashion that splits it across multiple files. For the Florence + The Machine discography data this is is really not needed because the entire spotify_florence folder occupies a mere 320kB on my hard drive. However, elsewhere on my laptop I have a copy of the infamous New York City Taxi data set, and that one occupies a rather more awkward 69GB of storage. For that one, it really does matter that I have it written to disk in a sensible format!\nHaving a data set stored in a distributed multi-file format is nice, but it’s only useful if I can open it and work with it as if it were the same as a regular tabular data set. The open_dataset() function allows me to do exactly this. Here’s what happens when I open the file:\n\nflorence_dataset &lt;- open_dataset(\"spotify_florence\", format = \"feather\")\nflorence_dataset\n\nFileSystemDataset with 14 Feather files\nartist_name: string\nartist_id: string\nalbum_id: string\nalbum_type: string\nalbum_release_date: string\nalbum_release_date_precision: string\ndanceability: double\nenergy: double\nkey: int32\nloudness: double\nmode: int32\nspeechiness: double\nacousticness: double\ninstrumentalness: double\nliveness: double\nvalence: double\ntempo: double\ntrack_id: string\nanalysis_url: string\ntime_signature: int32\ndisc_number: int32\nduration_ms: int32\nexplicit: bool\ntrack_href: string\nis_local: bool\ntrack_name: string\ntrack_preview_url: bool\ntrack_number: int32\ntype: string\ntrack_uri: string\nexternal_urls.spotify: string\nalbum_name: string\nkey_name: string\nmode_name: string\nkey_mode: string\ntrack_n: double\nalbum_release_year: int32\n\nSee $metadata for additional Schema metadata\n\n\nOkay yes, the output makes clear that I have loaded something and it has registered the existence of the 7 constituent files that comprise the dataset as a whole. But can I work with it? One of the big selling points to the arrow package is that it supplies a dplyr backend that lets me work with Tables as if they were R data frames, using familiar syntax. Can I do the same thing with Datasets?\n\ndanceability &lt;- florence_dataset |&gt; \n  select(album_name, track_name, danceability) |&gt;\n  distinct() |&gt;\n  arrange(desc(danceability)) |&gt; \n  head(n = 10) |&gt; \n  compute()\n\ndanceability\n\nTable\n10 rows x 3 columns\n$album_name &lt;string&gt;\n$track_name &lt;string&gt;\n$danceability &lt;double&gt;\n\nSee $metadata for additional Schema metadata\n\n\nYes. Yes I can. Because I called compute() at the end of this pipeline rather than collect(), the results have been returned to me as a Table rather than a data frame. I did that so that I can show that the danceability output is no different to the Table objects we’ve seen earlier, constructed from ChunkedArray objects:\n\ndanceability$track_name\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"Heaven Is Here\",\n    \"King\",\n    \"King\",\n    \"Hunger\",\n    \"My Love - Acoustic\",\n    \"Ghosts - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"What The Water Gave Me - Demo\",\n    \"South London Forever\",\n    \"What The Water Gave Me - Demo\"\n  ]\n]\n\n\nIf I want to I can convert this to a tibble, and discover that “Dance Fever” does indeed contain the most danceable Florence + The Machine tracks, at least according to Spotify:\n\nas.data.frame(danceability)\n\n# A tibble: 10 × 3\n   album_name                   track_name                    danceability\n   &lt;chr&gt;                        &lt;chr&gt;                                &lt;dbl&gt;\n 1 Dance Fever                  Heaven Is Here                       0.852\n 2 Dance Fever                  King                                 0.731\n 3 Dance Fever (Deluxe)         King                                 0.73 \n 4 High As Hope                 Hunger                               0.729\n 5 Dance Fever (Deluxe)         My Love - Acoustic                   0.719\n 6 Lungs (Deluxe Version)       Ghosts - Demo                        0.681\n 7 Ceremonials (Deluxe Edition) What The Water Gave Me - Demo        0.68 \n 8 Ceremonials                  What The Water Gave Me - Demo        0.68 \n 9 High As Hope                 South London Forever                 0.679\n10 Ceremonials (Deluxe Edition) What The Water Gave Me - Demo        0.678\n\n\n\nI am certain you are as reassured by this as I am."
  },
  {
    "objectID": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#footnotes",
    "href": "posts/2022-05-25_arrays-and-tables-in-arrow/index.html#footnotes",
    "title": "Arrays and tables in Arrow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMe.↩︎\nIn the near future, I hope that the documentation itself is going to tell this story but sometimes it’s easier to do the same job in an informal blog post where I have the luxury of going overboard with “authorial voice” and “narrative”, and all those other fancy things that writers love.↩︎\nQuick explanation: As a general rule, if you want things to be efficient you want the beginnings and endings of your data structures to be naturally aligned, in the sense that the memory address is a multiple of the data block sizes. So on a 64-bit machine, you want the memory address for every data structure to start on a multiple of 64 bits. Apparently that makes lookup easier or something. Unfortunately, I’ve only specified 8 bits (i.e. 1 byte) so if I wanted to ensure that the validity bitmap is naturally aligned I’m going to need to add another 7 bytes worth of padding in order to make it to the full 64 bits. This method of aligning data structures in memory is referred to as “8 byte alignment”. However, what Arrow does in this situation is 64 byte alignment, so each data structure has to be 64 bytes long at a minimum. This design feature exists to allow efficient use of modern hardware, and if you want to know more, it’s discussed in the Arrow documentation.↩︎\nMore precisely, signed 32 bit integers.↩︎\nI know, I know. Nothing is impossible. But you know what I mean.↩︎\nIf you don’t specify names when creating a data frame, R will create them for you. For example, when you create an ostensibly unnamed data frame with a command like x &lt;- data.frame(1:2, 3:4), you’ll find that x still has a names attribute, and names(x) returns X1.2, X3.4↩︎\nOn the off chance that you’re one of those extremely sensible people who chose not to read my data serialisation post, I’ll mention that the native RDS format that R uses avoids this trap. Much like Arrow serialisation formats, RDS preserves the column wise organisation to data frames.↩︎\nThe data header for contains the length and null count for each flattened field. It also contains the memory offset (i.e., the start point) and length of every buffer that is stored in the message body.↩︎\nFor instance, there are padding requirements involved that I am ignoring here.↩︎\nFull disclosure: there are a few minor differences between the IPC streaming format and the feather file format. I’ve glossed over those in this post but you can find the details on the Arrow specification page.↩︎\nOkay fine, I’m lying. I did it on purpose.↩︎\nIf you ever do need to convert a record batch to a table you can use as_arrow_table().↩︎\nBecause tables are built from chunked arrays, and chunked arrays are an abstraction layer designed to ensure that the distinct arrays can be treated as if they were one contiguous vector, Arrow tables inherit all those features. You can subset tables with $, [[, and [ the same way you can for record batches.↩︎"
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html",
    "href": "posts/2023-05-16_stan-ode/index.html",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "",
    "text": "For someone who has spent most of her professional life as a Bayesian statistician, it’s strange to admit that I’m only moderately experienced with Stan. My early work in Bayesian modelling involved some Laplace approximations to Bayes factors, MCMC samplers for posterior sampling, and in a few cases I would even resort to using BIC. I hand-coded everything myself, which was super helpful for understanding the mechanics underpinning the statistical inference, but terribly inefficient. When I did start using a domain-specific language for my probabilistic inference I mostly used JAGS.1\nEventually I started hearing the whispers…\n“Have you heard the good news about Hamiltonian Monte Carlo?” the Stan believers would ask me.\nWith some regret I would have to reply that I was saddled with numerous latent discrete parameters for theoretical reasons,2 and because the Stan stans are in fact lovely humans they would all express their sympathies, offer their thoughts and prayers, and think to themselves “there but for the grace of God go I”.\nLong story short, it’s taken me a long time to be in a position to make the most of Stan. I’ve used it and enjoyed it but not really had the opportunity to dive in properly. In recent weeks, however, I’ve been talking with some lovely folks who work in pharmacometrics, and they have problems that are very well suited to this kind of modelling. Excellent… a pretext!"
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#setting-up",
    "href": "posts/2023-05-16_stan-ode/index.html#setting-up",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "Setting up",
    "text": "Setting up\nThe first thing I have to do is go through the installation process. Yes, I have used Stan before, but that was a few laptops ago and things have changed a little since then. One happy little development from my perspective is that R users now have multiple options for interacting with Stan. The last time I used Stan the accepted method was to use the RStan package,3 and… there’s absolutely nothing wrong with RStan. It’s a great package. Really. It’s just… there’s a lot going on there, you know? Lots of bells and whistles. It’s powerful. It makes my head hurt.\nAlso, I can’t get the bloody thing to install on my Ubuntu box. I have no idea why.\nFortunately, nowadays there is also CmdStanR, a lightweight interface to Stan. It suits my style of thinking nicely because it provides R6 classes with methods that interact more or less directly with Stan. It does mean that you have to work harder to finesse the outputs, but I honestly don’t mind doing that kind of thing. As is usually the case with the Stan folks, the documentation is really nice so I won’t bother talking about the installation process. Suffice to say I’ve got the package working, so now I’ll load it:\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.5.3\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/danielle/.cmdstan/cmdstan-2.32.1\n\n\n- CmdStan version: 2.32.1\n\n\n\nA newer version of CmdStan is available. See ?install_cmdstan() to install it.\nTo disable this check set option or environment variable CMDSTANR_NO_VER_CHECK=TRUE.\n\n\nThe only finicky thing to talk about here lies in the fact that I’m doing all this in the context of a quarto blog post, and specifically a post that is using the knitr engine to execute the code chunks. By default, if knitr encounters a code chunk tagged as the stan language it will look for the RStan package to do the work.4 That’s not going to work for me since I don’t actually have RStan installed on my machine. Thankfully the cmdstanr package makes this easy for me to fix:\n\nregister_knitr_engine()\n\nNow that this is done, quarto/knitr will use cmdstanr to handle all the stan code included in this post.\n\n\n\n\nI’m amazed at how luminescent the white appears in this piece."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#the-context-pharmacokinetic-modelling",
    "href": "posts/2023-05-16_stan-ode/index.html#the-context-pharmacokinetic-modelling",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "The context: pharmacokinetic modelling",
    "text": "The context: pharmacokinetic modelling\nNext, I need a toy problem to work with. In my last post I’d started teaching myself some pharmacokinetic modelling – statistical analysis of drug concentrations over time – and I’ll continue that line of thinking here. In that post I wrote about noncompartmental analysis (NCA), a method for analysing pharmacokinetic data without making strong assumptions about the dynamics underpinning the biological processes of drug absorption and elimination, or the statistical properties of the measurement. NCA has its uses, but often it helps to have a model with a little structure to it.\nIn compartmental modelling, the analyst adopts a simplified model of (the relevant aspects of) the body as comprised of a number of distinct “compartments” that the drug can flow between. A two-compartment model might suppose that in addition to a “central” compartment that comprises systemic circulation, there is also a “peripheral” compartment where drug concentrations accrue in other bodily tissues. The model would thus include some assumptions about the dynamics that describe how the drug is absorbed (from whatever delivery mechanism is used) into (probably) the central compartment, and how it is eliminated from (probably) the central compartment. It would also need dynamics to describe how the drug moves from the central to peripheral compartment, and vice versa. These assumptions form the structural component of the compartmental model.\nIn addition to all this, a compartmental model needs to make statistical assumptions. The structure of the model describes how drug concentrations change over time, but in addition to that we might need a model that describes measurement error, variation among individuals, and covariates that affect the processes.\nIn other words, it’s really cool.\n\n\n\n\nThis totally works as a special effect in some epic fantasy graphic novel or something."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#a-very-simple-one-compartment-model",
    "href": "posts/2023-05-16_stan-ode/index.html#a-very-simple-one-compartment-model",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "A very simple one-compartment model",
    "text": "A very simple one-compartment model\nOkay let’s start super simple. We’ll have a one-compartment model, and we’ll assume bolus intravenous administration.5 That’s convenient because we don’t have to have a model for the absorption process: at time zero the entire dose goes straight into systemic circulation. Assuming we know both the dose \\(D\\) in milligrams and the volume of distribution6 \\(V_d\\), then the drug concentration in the first (and only) compartment \\(C(t)\\) at time \\(t=0\\) is given by \\(C(0) = D/V_d\\). That’s the only thing we need to consider on the absorption (or “influx”) side.\nOn the elimination (or “efflux”) side, there are a number of possible dynamical models we could consider. One of the simplest models assumes that the body is able to “clear” a fixed volume of blood of the drug per unit time. If this clearance rate is constant, some constant proportion \\(k\\) of the current drug concentration will be eliminated during each such time interval. Expressed as a differential equation this gives us:\n\\[\n\\frac{dC(t)}{dt} = - k\\ C(t)\n\\] Unlike many differential equations, this one is easy to solve7 and yields an exponential concentration-time curve:\n\\[\nC(t) = C(0) \\ e^{-kt}\n\\]\nThat completes the structural side to our model. Now the statistical side. Again, we’ll keep it simple: I’m going to assume independent and identically normally distributed errors, no matter how unlikely that is in real life. Reflecting the fact that from a statistics point of view we’re now talking about a discrete set of time points and discrete set of measured drug concentrations, I’ll refer to the \\(n\\) time points as \\(t_1, t_2, \\ldots, t_n\\) and the corresponding observed concentrations as \\(c_1, c_2, \\ldots, c_n\\). In this notation our statistical model is expressed:\n\\[\nc_i = c_0 \\ e^{-kt_i} + \\epsilon_i\n\\]\nwhere\n\\[\n\\epsilon_i \\sim \\mbox{Normal}(0, \\sigma)\n\\]\nOur model therefore has two unknowns, the scale parameter \\(\\sigma\\) and the elimination rate constant parameter \\(k\\). Since we are being Bayesians for the purposes of this post I’ll place some priors over these parameters. However, since we are also being lazy Bayesians for the purposes of this post I’m not even going to pretend I’ve thought much about these priors. I’ve made them up because my actual goal here is to familiarise myself with the mechanics of pharmacokinetic modelling in Stan. The real world practicalities – critically important though they are – can wait!\nAnyway, here’s the priors I used:\n\\[\n\\begin{array}{rcl}\n\\sigma & \\sim & \\mbox{Half-Normal}_+(0, 1) \\\\\nk & \\sim & \\mbox{Half-Normal}_+(0, 5)\n\\end{array}\n\\]\nAgain, I cannot stress this enough: I literally did not think at all about these choices. Never ever adopt such an appalling practice in real life, boys and girls and enby kids!\nIt’s almost time to move onto the implementation but first… I kind of lied. There’s a third unknown. The initial concentration \\(c_0\\) is technically an unknown as well. True, we usually know the dosage to a high degree of precision (if I administer 50μg of a drug, there’s not much error there…), but the volume of the volume of systemic distribution is likely an estimate based on the assumption that blood volume is about 7-8% of body mass. I might guess that this value is about 5.5l but it might be a little more or a little less than that. Allowing the model to have a prior over the true value of \\(v_d\\) makes some intuitive sense and also has the nice consequence of allowing the model to infer the value of \\(c_0\\) from the data. In practice we’re not likely to be far wrong in guessing this quantity, so I’ve used the following prior:\n\\[\nv_d \\sim \\mbox{Normal}(5.5, 1)\n\\]\nwhere 5.5l is my prior estimate.\n\nImplementation in Stan\nMoving along, let’s have a look at how this model would be implemented in Stan. The code for the model is shown below, and – in case you’re not familiar with Stan code – I’ll quickly outline the structure. Stan is a declarative language, not an imperative one: you specify the model, it takes care of the inference. Your code is an abstract description of the model, not a sequence of instructions. In my code below, you can see it’s organised into three blocks:\n\nThe data block defines quantities that the user needs to supply. Some of those correspond to empirical data like concentrations, others are design variables for the study like measurement times.\nThe parameters block defines quantities over which inference must be performed. In this case that’s \\(k\\) and \\(\\sigma\\).\nThe model block specifies the model itself, which in this case includes both the structural and statistical components to our model, and does not distinguish between “likelihood” and “prior”. From the Stan point of view Bayesian inference it’s not really about “priors and likelihood” it’s more of a Doctor Who style “modelly-wobbelly joint probability distribution” kind of thing.\n\nAnyway here it is:\n\n\n\nbolus\n\ndata {\n  int&lt;lower=1&gt; n_obs;\n  real&lt;lower=0&gt; dose;\n  real&lt;lower=0&gt; vol_d;\n  vector[n_obs] t_obs;\n  vector&lt;lower=0&gt;[n_obs] c_obs;\n}\n\nparameters {\n  real&lt;lower=0.01&gt; sigma;\n  real&lt;lower=0.01&gt; k;\n  real&lt;lower=1, upper=12&gt; vol_d_true;\n}\n\nmodel {\n  k ~ normal(0, 5) T[0.01, ];\n  sigma ~ normal(0, 1) T[0.01, ];\n  vol_d_true ~ normal(vol_d, 1);\n  c_obs ~ normal(dose / vol_d_true * exp(-k * t_obs), sigma);\n}\n\n\nSome additional things to note:\n\nStan is strongly typed with (for example) int and real scalar types, and vector types containing multiple reals.\nVariable declarations allow you to specify lower and upper allowable values for variables. It is always good to include those if you know them.\nThere’s some fanciness going on under the hood in how Stan “thinks about” probability distributions in terms of log-probability functions, but I’m glossing over that because Now Is Not The Time.\nI’ve set my lower bounds on the parameters to “something very close to zero” rather than actually zero because (a) those values are wildly implausible anyway, but (b) if the sampler tries to get too close to zero the log probability goes batshit and numerical chaos ensues.\n\nPerhaps more important from the perspective of this post, here’s the important bit of quarto syntax I used when defining the code chunk above. When I defined the code chunk I specified the output.var option by including the following line in the yaml header to the chunk:\n#| output.var: \"bolus\"\nBy specifying output.var: \"bolus\" I’ve ensured that when the quarto document is rendered there is a model object called bolus available in the R session. It’s essentially equivalent to having the code above saved to a file called bolus.stan and then calling the cmdstanr function cmdstan_model() to compile it to C++ with the assistance of Stan:\nbolus &lt;- cmdstan_model(\"bolus.stan\")\nFor future Stan models I’ll just print the name of the output variable at the top of the code chunk so that you can tell which R variable corresponds to which Stan model.\nIn any case let’s take a look at our bolus object. Printing the object yields sensible, if not exciting, output: it shows you the source code for the underlying model:\n\nbolus\n\ndata {\n  int&lt;lower=1&gt; n_obs;\n  real&lt;lower=0&gt; dose;\n  real&lt;lower=0&gt; vol_d;\n  vector[n_obs] t_obs;\n  vector&lt;lower=0&gt;[n_obs] c_obs;\n}\n\nparameters {\n  real&lt;lower=0.01&gt; sigma;\n  real&lt;lower=0.01&gt; k;\n  real&lt;lower=1, upper=12&gt; vol_d_true;\n}\n\nmodel {\n  k ~ normal(0, 5) T[0.01, ];\n  sigma ~ normal(0, 1) T[0.01, ];\n  vol_d_true ~ normal(vol_d, 1);\n  c_obs ~ normal(dose / vol_d_true * exp(-k * t_obs), sigma);\n}\n\n\nPerhaps more helpfully for our purposes, it’s useful to know that this is an object of class CmdStanModel, and if you take a look at the documentation on the linked page, you’ll find a description of the methods available for such objects. There are quite a few possibilities, but a few of particular interest from a statistical perspective are:8\n\n$sample() calls the posterior sampling method implemented by Stan on the model\n$variational() calls the variational Bayes algorithms implemented by Stan on the model\n$optimize() estimates the posterior mode\n\nFor the purposes of this post I’ll use the $sample() method, and in order to call it on my bolus object I’ll need to specify some data to pass from R to the compiled Stan model. These are passed as a list:\n\nbolus_data &lt;- list(\n  dose = 50,\n  vol_d = 5.5,\n  n_obs = 6,\n  t_obs = c(0.1, 0.5, 1.0, 1.5, 2.0, 3.0),\n  c_obs = c(8.4, 3.1, 1.9, 0.6, 0.2, 0.02)\n)\n\nTo quickly visualise these “observed data”, I’ll organise the relevant variables into a data frame and draw a pretty little scatterplot with ggplot2:\n\nlibrary(ggplot2)\ndf &lt;- data.frame(\n  time = bolus_data$t_obs,\n  conc = bolus_data$c_obs\n)\nggplot(df, aes(time, conc)) + \n  geom_point(size = 2) + \n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nDelightful, truly. So neat. So clean. So obviously, obviously fictitious.\nAs a Bayesian9 that has observed the data, what I want to compute is the joint posterior distribution over my parameters \\(k\\) and \\(\\sigma\\). Or, since this has been an unrealistic expectation ever since the death of the cult of conjugacy,10 what I’ll settle for are samples from that joint posterior that I can use to numerically estimate whatever it is that I’m interested in. To do this for our bolus model with the help of cmdstanr, we call bolus$sample():\n\nbolus_fitted &lt;- bolus$sample(\n  data = bolus_data, \n  seed = 451, \n  chains = 4,\n  refresh = 1000\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n\n\nThis is an object of class CmdStanMCMC and again you can look at the linked page to see what methods are defined for it. I’ll keep things simple for now and call the $summary() method, which returns a tibble containing summary statistics associated with the MCMC chains:\n\nbolus_fitted$summary()\n\n# A tibble: 4 × 10\n  variable    mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;      &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1 lp__       3.25   3.61  1.49  1.30  0.346  4.94  1.00    1244.    1507.\n2 sigma      0.556  0.499 0.240 0.185 0.289  1.02  1.00    1453.    1870.\n3 k          2.09   2.04  0.371 0.277 1.60   2.71  1.00    1709.    1712.\n4 vol_d_true 5.04   5.01  0.414 0.369 4.42   5.76  1.00    1795.    1994.\n\n\nEvidently the estimated posterior mean for the elimination rate constant \\(k\\) is 2.09, with a 90% credible interval11 12 of [1.6, 2.71]. Similarly, the standard deviation of the measurement error \\(\\sigma\\) is estimated to have mean 0.56 and 90% interval [0.29, 1.02].\nIf we wanted to we could take this a little further by pulling out the posterior samples themselves using the $draws() method. Internally this method relies on the posterior package, and supports any of the output formats allowed by that package. In this case I’ll have it return a tibble because I like tibbles:\n\nbolus_samples &lt;- bolus_fitted$draws(format = \"draws_df\")\nbolus_samples\n\n# A draws_df: 1000 iterations, 4 chains, and 4 variables\n    lp__ sigma   k vol_d_true\n1  1.893  0.74 2.8        4.3\n2  2.083  0.81 2.7        4.5\n3  2.517  0.67 2.7        4.5\n4  2.543  0.69 2.7        4.4\n5  3.122  0.62 2.5        4.5\n6  2.434  0.52 2.8        4.6\n7  0.073  1.27 2.8        4.5\n8  1.257  0.36 1.6        5.8\n9  1.392  0.35 1.7        5.8\n10 3.846  0.37 1.8        5.4\n# ... with 3990 more draws\n# ... hidden reserved variables {'.chain', '.iteration', '.draw'}\n\n\nYou could then go on to do whatever you like with these samples but I have other fish to fry so I’m going to move on.\n\n\nGenerated quantities\nThe model I built in the previous section is perfectly fine, as far as it goes, but it’s missing something that matters a lot to me. There’s nothing in the code that allows me to use the inferred model parameters to make predictions about the shape of the concentration-time curve across the full range of times. To do that I’ll introduce a “generated quantities” block to my code, as shown below:\n\n\n\nbolus2\n\ndata {\n  int&lt;lower=1&gt; n_obs;\n  int&lt;lower=1&gt; n_fit;\n  real&lt;lower=0&gt; dose;\n  real&lt;lower=0&gt; vol_d;\n  vector[n_obs] t_obs;\n  vector&lt;lower=0&gt;[n_obs] c_obs;\n  vector[n_fit] t_fit;\n}\n\nparameters {\n  real&lt;lower=0.01&gt; sigma;\n  real&lt;lower=0.01&gt; k;\n  real&lt;lower=1, upper=12&gt; vol_d_true;\n}\n\nmodel {\n  k ~ normal(0, 5) T[0.01, ]; \n  sigma ~ normal(0, 1) T[0.01, ];\n  vol_d_true ~ normal(vol_d, 1);\n  c_obs ~ normal(dose / vol_d_true * exp(-k * t_obs), sigma);\n}\n\ngenerated quantities {\n  vector&lt;lower=0&gt;[n_fit] c_fit = dose / vol_d_true * exp(-k * t_fit);\n}\n\n\nWhen I do this, the data that I pass to this version of the model needs to be modified too. It needs to specify the times for which we want to generate fitted curves:\n\nt_fit &lt;- seq(0, 3, 0.05)\nbolus2_data &lt;- list(\n  n_obs = 6,\n  n_fit = length(t_fit),\n  dose = 50,\n  vol_d = 5.5,\n  t_obs = c(0.1, 0.5, 1.0, 1.5, 2.0, 3.0),\n  c_obs = c(8.4, 3.1, 1.9, 0.6, 0.2, 0.02),\n  t_fit = t_fit\n)\n\nNow that we have an augmented model and augmented data set, we can re-run our sampling procedure using the new code:\n\nbolus2_fitted &lt;- bolus2$sample(\n  data = bolus2_data, \n  seed = 123, \n  chains = 4,\n  refresh = 1000\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\n\nWarning: 3 of 4000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nThe bolus2_fitted object contains both the parameter values sampled during the MCMC routine, and the generated quantities that emerged in the process. If I wanted to I could use those generated quantities as is. However, because there’s some value in separating the “posterior prediction” process from the “posterior sampling” process, it’s generally considered best practice to create a new set of generated quantities sampled using the posterior parameter distribution. We can do that by calling the $generate_quantities() method of our fitted model object:\n\nbolus2_generated &lt;- bolus2$generate_quantities(\n  fitted_params = bolus2_fitted,\n  data = bolus2_data,\n  seed = 666\n)\n\nRunning standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.5 seconds.\n\n\nThis is a CmdStanGQ object, and again it has $draws() and $summary() methods. For our purposes the $summary() method will suffice as it returns a tibble containing the thing I want to plot:\n\nbolus2_summary &lt;- bolus2_generated$summary()\nbolus2_summary\n\n# A tibble: 61 × 7\n   variable   mean median    sd   mad    q5   q95\n   &lt;chr&gt;     &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1 c_fit[1]   9.97   9.99 0.804 0.684  8.64 11.3 \n 2 c_fit[2]   8.98   9.01 0.648 0.560  7.88  9.97\n 3 c_fit[3]   8.09   8.13 0.551 0.473  7.14  8.89\n 4 c_fit[4]   7.29   7.34 0.500 0.404  6.46  7.99\n 5 c_fit[5]   6.57   6.62 0.480 0.363  5.79  7.22\n 6 c_fit[6]   5.93   5.98 0.478 0.345  5.14  6.55\n 7 c_fit[7]   5.35   5.40 0.482 0.351  4.56  5.98\n 8 c_fit[8]   4.83   4.88 0.488 0.361  4.01  5.48\n 9 c_fit[9]   4.36   4.41 0.493 0.370  3.52  5.02\n10 c_fit[10]  3.94   3.98 0.494 0.378  3.10  4.61\n# ℹ 51 more rows\n\n\nThe variable column here is only mildly helpful, so I’ll add a column specifying the actual times associated with each row in this tibble:\n\nbolus2_summary$time &lt;- bolus2_data$t_fit\nbolus2_summary\n\n# A tibble: 61 × 8\n   variable   mean median    sd   mad    q5   q95  time\n   &lt;chr&gt;     &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1 c_fit[1]   9.97   9.99 0.804 0.684  8.64 11.3   0   \n 2 c_fit[2]   8.98   9.01 0.648 0.560  7.88  9.97  0.05\n 3 c_fit[3]   8.09   8.13 0.551 0.473  7.14  8.89  0.1 \n 4 c_fit[4]   7.29   7.34 0.500 0.404  6.46  7.99  0.15\n 5 c_fit[5]   6.57   6.62 0.480 0.363  5.79  7.22  0.2 \n 6 c_fit[6]   5.93   5.98 0.478 0.345  5.14  6.55  0.25\n 7 c_fit[7]   5.35   5.40 0.482 0.351  4.56  5.98  0.3 \n 8 c_fit[8]   4.83   4.88 0.488 0.361  4.01  5.48  0.35\n 9 c_fit[9]   4.36   4.41 0.493 0.370  3.52  5.02  0.4 \n10 c_fit[10]  3.94   3.98 0.494 0.378  3.10  4.61  0.45\n# ℹ 51 more rows\n\n\nHaving done so I can now draw the plot I really want:\n\ndf &lt;- data.frame(\n  t_obs = bolus2_data$t_obs, \n  c_obs = bolus2_data$c_obs\n)\nggplot(bolus2_summary) + \n  geom_ribbon(aes(time, ymin = q5, ymax = q95), fill = \"grey70\") +\n  geom_line(aes(time, mean)) + \n  geom_point(aes(t_obs, c_obs), data = df, size = 2) + \n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nThe solid line gives our best point estimate of the true concentration-time curve, and the shaded region shows a 90% credible interval that expresses our uncertainty about what part of the space the true curve might actually occupy.13\n\n\n\n\nI think this is my very favourite piece from this system. The texturing worked out so well."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#when-biology-isnt-analytically-tractable",
    "href": "posts/2023-05-16_stan-ode/index.html#when-biology-isnt-analytically-tractable",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "When biology isn’t analytically tractable",
    "text": "When biology isn’t analytically tractable\nEarlier when I justified the use of an exponential concentration-time function \\(C(t)\\), I did so by assuming that the body is able to “clear” a constant volume of blood per unit time. That assumption works reasonably well in some situations. If we suppose that the kidneys14 work a bit like a filter, you can imagine that the body is filtering the blood at a fixed rate, which produces the exponential curve I used earlier. But the real world is a little more complicated than that sometimes.\nSuppose, instead, that the elimination process involves an enzyme-catalysed reaction. That is, the body eliminates the drug (the substrate) by binding it to enzyme, and from this transition state it is catalysed to something else (the product). The dynamics of such a process are a little different: the enzyme concentration is often very low relative to the substrate concentration and the reaction rate saturates: once you’ve hit that point adding more of the drug into the blood won’t speed up the rate of elimination because there’s no free enzyme to catalyse its conversion. Put slightly differently, if elimination involves a saturable process it won’t necessarily have a constant clearance rate, and you won’t see an exponential concentration-time function.\nWell, that’s awkward.\n\nMichaelis-Menten kinetics\nThe paper I’ve relied on for an introduction to compartmental models in pharmacometrics is Holz and Fahr (2001) and they use Michaelis-Menten kinetics as an example of a saturable elimination process. Michaelis-Menten kinetics is characterised by the following differential equation:\n\\[\n\\frac{dC(t)}{dt} = - \\frac{v_{m}}{k_m + C(t)} C(t)\n\\]\nIn this expression, \\(v_{m}\\) is a constant that denoting the maximum velocity of elimination: due to the limitations imposed by the enzyme concentration, the drug concentration cannot decrease faster than this rate. The term \\(k_m\\) is the Michaelis constant: when the drug concentration \\(C(t)\\) equals \\(k_m\\), the elimination rate \\(\\frac{dC(t)}{dt}\\) is exactly half its maximum rate, \\(\\frac{v_m}{2}\\). Both of these properties are easy enough to demonstrate if you’re willing to spend a few minutes playing around with the equation above, but it’s not very interesting, so we’ll move on. A more useful approach is to think about how we should expect Michaelis-Menten kinetics to behave at high and low drug concentrations:\n\nIf the drug concentration \\(C(t)\\) is very large, \\(\\frac{dC(t)}{dt}\\) will be roughly constant and very close to the satuation rate \\(v_m\\). In other words: At high concentrations, the concentration decreases linearly over time.\nIf the drug concentration \\(C(t)\\) is very small, then \\(\\frac{v_m}{k_m + C(t)}\\) will be roughly constant, and \\(\\frac{dC(t)}{dt}\\) will be roughly proportional to the concentration \\(C(t)\\). In other words: At low concentrations the concentration decreases exponentially over time.\n\nNow that we have a sensible intuition, we should try to draw the actual curves. One teeny-tiny problem… this differential equation doesn’t really have an analytic solution.15 We’re going to have to do this numerically.\n\n\nWhat does it look like?\nIn a moment I’m going to implement Michaelis-Menten kinetics using Stan, so as to eventually give me the ability to do Bayesian statistics in a pharmacokinetic model that involves this kind of dynamics. However, I’m a big fan of doing things in small steps. For example, if I weren’t planning to build all this into a probabilistic model, I wouldn’t really need to use Stan at all. R has many different tools for solving differential equations numerically, and I could just use one of those.\nFor instance, if I chose to use the deSolve package, I’d begin by defining an R function mmk() that returns the value of the derivative at a given point in time and given specified parameters, pass it to the ode() solver supplied by deSolve, and then draw myself a pretty little picture using ggplot2:\n\nlibrary(deSolve)\nlibrary(ggplot2)\n\n# differential equation for MM kinetics: it returns a list because\n# that's what ode() expects to receive when it calls this function\nmmk &lt;- function(t, y, parms) {\n  dydt &lt;- - y * parms[\"vm\"] / (parms[\"km\"] + y) \n  return(list(dydt))\n}\n\n# parameters\ndose &lt;- 120 # (in milligrams)\ncirculation_vol &lt;- 5.5  # (in litres)\nmax_elimination &lt;- 8 \nhalving_point &lt;- 4\ntimes &lt;- seq(0, 8, .05) # (in hours)\n\n# use the desolve package\nout &lt;- ode(\n  y = c(\"conc\" = dose/circulation_vol),\n  times = times,\n  func = mmk,\n  parms = c(\n    \"vm\" = max_elimination, \n    \"km\" = halving_point\n  )\n)\n\n# convert matrix to tibble and plot\nout &lt;- as.data.frame(out)\nggplot(out, aes(time, conc)) + \n  geom_line() + \n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nConsistent with the intuitions we developed earlier you can see that on the left hand side this curve looks pretty linear, but on the right hand side it looks pretty much like an exponential decay.\n\n\nThe exact same thing, but in Stan this time\nHaving solved the relevant differential equation in R, it’s helpful – if slightly strange – to imagine solving the exact same problem using Stan. Although Stan is primarily a language for probabilistic inference, it does provide a toolkit for solving differential equations. In normal usage we’d use the Stan ODE solver in the context of a larger statistical model, but – in order to wrap my head around how it works – I found it helpful to try invoking the solver for a dynamical system without incorporating it into any statistical model.\nSo let’s do that for Michaelis-Menten kinetics. If we squint and ignore the details, the basic idea is the same in Stan as it was in the earlier example in R:\n\nWrite a user-defined function called mmk() that takes arguments for the time, current state of the system (in our case concentration), and any other parameters (the maximum elimination rate and the Michaelis constant), and returns the derivatives.\nCall the ODE solver function, in this case ode_rk45(), passing it the user-defined function and other required quantities. This being Stan, we’ll need to ensure those quantities are defined in the data block.\n\nThe actual code is shown below. The function is defined within the functions code block (shocking, right?), input data defined within the data block, and the variable we’re trying to compute (conc) is defined as a generated quantity:\n\n\n\nmmk\n\nfunctions {\n  vector mmk(real time,\n             vector state,\n             real vm,\n             real km) {\n    vector[1] derivative;\n    derivative[1] = - state[1] * vm / (km + state[1]);\n    return derivative;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; nt;\n  vector[1] c0;\n  real t0;\n  array[nt] real ts;\n  real vm;\n  real km;\n}\n\nmodel {\n}\n\ngenerated quantities {\n  array[nt] vector[1] conc = ode_rk45(mmk, c0, t0, ts, vm, km);\n}\n\n\nIt may seem strange that the mmk() function defines the state variable to be a vector of length 1. At first pass it might seem a lot simpler to specify a scalar real: if we did that, then we wouldn’t have the weirdness of defining conc as a one-dimensional length nt array, in which each cell is a vector of length 1, each of which contains a single real number. The layers of nesting seem unnecessary.\nHowever, they are entirely necessary.\nTo see this, it’s important to recognise that mmk() isn’t an arbitrary user defined function, it is the ODE system function and is designed to be passed to one of the solvers, in this case ode_rk45(). In this particular case our state is one-dimensional: we have a one-compartment model, and the only variable that defines the state is the drug concentration in that compartment. But dynamical systems can be – and usually are – multivariate: in a two-compartment model the state would probably be defined in terms of two drug concentrations, one for each compartment. In that case we would require a vector.\nTo accommodate this cleanly, the design choice made in Stan is that the second argument to the system function must be a vector, even if the state happens to be one-dimensional. More generally, the point I’m making here is that to call the ODE solvers you need to ensure that your system function has the appropriate signature.\nIn any case, let’s take our code for a spin. First we’ll create some data that we can pass to Stan:\n\ntimes &lt;- seq(.05, 8, .05)\nmmk_data &lt;- list(\n  c0 = 21.8, \n  t0 = 0, \n  ts = times, \n  vm = 8,\n  km = 4,\n  nt = length(times)\n)\n\nNext, we “sample” from the “model”. This step is, admittedly, super weird. We don’t actually have a model, and there are no parameters to sample and there are no probabilistic aspects to the system at all. This is going to be the world’s shortest MCMC run. The Stan model above is available in R via the mmk object, so I’ll call its $sample() method, specifying fixed_param = TRUE so that Stan doesn’t try to resample parameters that don’t exist. I’m only going to run one “chain” for a single iteration, because I only need to solve the system once:\n\nmmk_fitted &lt;- mmk$sample(\n  data = mmk_data,\n  fixed_param = TRUE, \n  chains = 1, \n  iter_sampling = 1\n)\n\nRunning MCMC with 1 chain...\n\nChain 1 Iteration: 1 / 1 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\n\n\nBecause Stan has dutifully generated the generated quantities, I now have the values for the solved concentrations. I’ll extract those by calling the $draws() method, do a tiny bit of cleanup to wrangle the data into a nice format…\n\nmmk_draws &lt;- mmk_fitted$draws(format = \"draws_df\")\nmmk_draws &lt;- mmk_draws |&gt;\n  tidyr::pivot_longer(\n    cols = tidyr::starts_with(\"conc\"), \n    names_to = \"variable\", \n    values_to = \"conc\"\n  ) |&gt;\n  dplyr::mutate(time = mmk_data$ts)\n\nWarning: Dropping 'draws_df' class as required metadata was removed.\n\nmmk_draws\n\n# A tibble: 160 × 6\n   .chain .iteration .draw variable    conc  time\n    &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n 1      1          1     1 conc[1,1]   21.5  0.05\n 2      1          1     1 conc[2,1]   21.1  0.1 \n 3      1          1     1 conc[3,1]   20.8  0.15\n 4      1          1     1 conc[4,1]   20.5  0.2 \n 5      1          1     1 conc[5,1]   20.1  0.25\n 6      1          1     1 conc[6,1]   19.8  0.3 \n 7      1          1     1 conc[7,1]   19.5  0.35\n 8      1          1     1 conc[8,1]   19.1  0.4 \n 9      1          1     1 conc[9,1]   18.8  0.45\n10      1          1     1 conc[10,1]  18.5  0.5 \n# ℹ 150 more rows\n\n\n…and draw a pretty picture:\n\nggplot(mmk_draws, aes(time, conc)) + \n  geom_line() +\n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nYup, same as before.\n\n\n\n\nHonestly, I just included this one because it’s weird and I’m a sucker for this palette."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#one-compartment-bolus-model-with-mmk-elimination",
    "href": "posts/2023-05-16_stan-ode/index.html#one-compartment-bolus-model-with-mmk-elimination",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "One compartment bolus model with MMK elimination",
    "text": "One compartment bolus model with MMK elimination\nSome preliminaries. For the purposes of building priors it’s useful to think about what kinds of properties you can have sensible intuitions about. It’s a little tricky to set a joint prior over the maximum elimination rate \\(v_m\\) and the Michaelis constant \\(k_m\\) without using some outside knowledge. If the data don’t span a range that lets you unambiguously discover “a linear bit” and “an exponential bit” it’s very easy to trade off one parameter against the other. Increasing \\(v_m\\) and \\(k_m\\) at the same time will slightly change the “bendiness” of the curve, but that’s easily absorbed into the error terms. In other words, if you don’t have sensible constraints you’re going to end up with a serious identifiability problem. With that in mind, the two thoughts I had are:\nI can see why it would be important to understand \\(v_m\\) and \\(k_m\\) theoretically but from a data analysis perspective, I found it a little easier to set priors by thinking about the elimination rate \\(v_0\\) at time \\(t = 0\\). After a little high school algebra we obtain the following expressions to transform from \\(v_0\\) to \\(v_m\\) and vice versa:\n\\[\n\\begin{array}{rcl}\nv_0 &=& v_m \\ \\displaystyle \\frac{c_0}{c_0 + k_m} \\\\ \\\\\nv_m &=& v_0 \\ \\displaystyle \\frac{c_0 + k_m}{c_0}\n\\end{array}\n\\]\nExtending this logic, instead of parameterising the prior in terms of the Michaelis constant \\(k_m\\) (the concentration at which the reaction rate falls to half its maximum value), we can use a design-specific analog, \\(k_0\\), denoting the concentration at which the reaction rate falls to half of \\(v_0\\). Again, some algebraic shenanigans gives us the transformations:\n\\[\n\\begin{array}{rcl}\nk_0 &=& \\displaystyle \\frac{c_0 \\ k_m}{c_0 + 2 k_m} \\\\ \\\\\nk_m &=& \\displaystyle \\frac{c_0 \\ k_0}{c_0 - 2 k_0}\n\\end{array}\n\\]\nIn a sign that I still haven’t really nailed this, the way I’m going to set my priors for the model is kind of unprincipled. I’ll set independent priors over the initial rate \\(v_0\\), and the Michaelis constant \\(k_m\\). I suppose I could make a pretense of a justification for this, by saying that the prior over \\(v_0\\) is used to capture my intutions about the experimental design and the prior over \\(k_m\\) is used to capture my intuitions about the fundamental biological processes as they exist outside the experiment but… yeah, that’s 100% a post hoc rationalisation. The actual reason I’m doing this is that I tried several different ways of setting a prior, and this version seemed to be the least vulnerable to numerical problems. Other versions tended to move into regions of the space where the ODE solver becomes very unhappy, or regions where the target probability density is not well-behaved. Practicality rules my world.\nThe specific prior:\n\\[\n\\begin{array}{rcl}\nv_0 & \\sim & \\mbox{Half-Normal}_+(0, 10) \\\\\nk_m & \\sim & \\mbox{Half-Normal}_+(0, 10)\n\\end{array}\n\\]\nLater, when I plot model posteriors, I’ll be a little more sensible though and use the two interpretable versions: I’ll plot \\(v_m\\) against \\(k_m\\), and \\(v_0\\) against \\(k_0\\).\n\nThe model\nHere’s the Stan code for the model I settled on:16\n\n\n\nmmk_bolus\n\nfunctions {\n  vector mmk(real time,\n             vector state,\n             real vm,\n             real km) {\n    vector[1] derivative;\n    derivative[1] = - state[1] * vm / (km + state[1]);\n    return derivative;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_obs;\n  int&lt;lower=1&gt; n_fit;\n  vector&lt;lower=0&gt;[n_obs] c_obs;\n  array[n_obs] real t_obs;\n  array[n_fit] real t_fit;\n  real&lt;lower=0&gt; vol_d;\n  real&lt;lower=0&gt; dose;\n  real t0;\n}\n\nparameters {\n  real&lt;lower=0.01&gt; sigma;\n  real&lt;lower=0.01, upper=30&gt; v0;\n  real&lt;lower=0.01, upper=50&gt; km;\n  real&lt;lower=1, upper=12&gt; vol_d_true;\n}\n\ntransformed parameters {\n  vector[1] c0;\n  c0[1] = dose / vol_d_true;\n  real&lt;lower=0.01&gt; vm = v0 * (km + c0[1]) / c0[1];\n}\n\nmodel {\n  array[n_obs] vector[1] mu_arr;\n  vector[n_obs] mu_vec;\n\n  v0 ~ normal(0, 10) T[0.01, ];\n  km ~ normal(0, 10) T[0.01, ];\n  sigma ~ normal(0, 1) T[0.01, ];\n  vol_d_true ~ normal(vol_d, 1);\n  \n  mu_arr = ode_rk45(mmk, c0, t0, t_obs, vm, km);\n  for (i in 1:n_obs) {\n    mu_vec[i] = mu_arr[i, 1];\n  }\n  c_obs ~ normal(mu_vec, sigma);\n}\n\ngenerated quantities {\n  array[n_fit] vector[1] c_fit = ode_rk45(mmk, c0, t0, t_fit, vm, km);\n  real&lt;lower=0.01&gt; k0 = (c0[1] * km) / (c0[1] + 2*km);\n}\n\n\nIt’s truly thrilling, is it not? You’ll notice that I’ve introduced a “transformed parameters” block in which I’ve define the c0 and vm variables. These are included as transformed parameters because they are quantities that follow deterministically from the “stochastic” parameters (v0, km, etc) and the data, but – unlike the “generated quantities” – they actually do form part of the model because they are used later. The ODE solver needs access to both c0 and vm in order to calculate the model-predicted concentrations at each time point, so they aren’t purely auxiliary. In contrast, the c_fit and k0 variables that I’ve included in the “generated quantities” block aren’t used for anything: the human user (me) wants to know what these values are, but the model doesn’t strictly need them.\nAnyway, let’s define some data that we can model:\n\nt_obs &lt;- 1:8\nc_obs &lt;- c(14.8, 11.0, 4.5, 1.6, 0.2, 0.01, 0, 0)\nt_fit &lt;- seq(.05, 8, .05)\nmmk_bolus_data &lt;- list(\n  dose = 120,\n  vol_d = 5.5,\n  t0 = 0, \n  c_obs = c_obs,\n  t_obs = t_obs,\n  t_fit = t_fit,\n  n_obs = length(t_obs),\n  n_fit = length(t_fit)\n)\n\nAnd, because human beings typically prefer pretty pictures to tiresome lists of numbers, here’s a plot that shows you what the to-be-modelled data look like:\n\ndf &lt;- data.frame(\n  time = mmk_bolus_data$t_obs,\n  conc = mmk_bolus_data$c_obs\n)\nggplot(df, aes(time, conc)) + \n  geom_point(size = 2) + \n  lims(x = c(0, 8), y = c(0, 25)) +\n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nI kind of like this as a toy data set because it’s (a) very clean, in the sense that it doesn’t feel noisy at all and we expect that the measurement error \\(\\sigma\\) should be pretty small, and (b) there’s just a tiny bit of ambiguity about where precisely the MMK dynamics kick in… looking carefully at these points there’s this feeling that they aren’t really an exponential curve or a linear function, but something in between. So we should be able to model them using a MMK system, but it’s not quite clear where the linear part ends and where the exponential part begins. So it should be instructive…\nOkay, so now let’s run the sampler:\n\nmmk_bolus_fitted &lt;- mmk_bolus$sample(\n  data = mmk_bolus_data, \n  seed = 100, \n  chains = 4,\n  parallel_chains = 2,  \n  refresh = 1000\n)\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 1.8 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 finished in 2.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 1.7 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 1.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.8 seconds.\nTotal execution time: 3.8 seconds.\n\n\nWarning: 2 of 4000 (0.0%) transitions ended with a divergence.\nSee https://mc-stan.org/misc/warnings for details.\n\n\nThat looks nice. A couple of minor complaints from Stan about divergences, but nothing too serious.17 And take a summary:\n\nmmk_bolus_fitted$summary()\n\n# A tibble: 168 × 10\n   variable     mean median    sd   mad     q5   q95  rhat ess_bulk ess_tail\n   &lt;chr&gt;       &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n 1 lp__        3.82   4.20  1.73  1.49   0.506  5.86  1.00    1091.    1561.\n 2 sigma       0.717  0.673 0.247 0.216  0.416  1.18  1.00    1109.    1491.\n 3 v0          6.10   5.89  1.03  0.864  4.80   8.06  1.00     814.    1252.\n 4 km          3.80   2.79  3.34  2.19   0.586 10.6   1.00     906.    1442.\n 5 vol_d_true  5.67   5.69  0.350 0.329  5.08   6.22  1.00     924.    1398.\n 6 c0[1]      21.2   21.1   1.34  1.21  19.3   23.6   1.00     924.    1398.\n 7 vm          7.27   6.69  2.16  1.47   5.00  11.5   1.00     817.    1207.\n 8 c_fit[1,1] 20.9   20.8   1.30  1.17  19.0   23.2   1.00     934.    1413.\n 9 c_fit[2,1] 20.6   20.5   1.25  1.13  18.8   22.8   1.00     946.    1418.\n10 c_fit[3,1] 20.3   20.2   1.21  1.10  18.5   22.5   1.00     958.    1423.\n# ℹ 158 more rows\n\n\nNeato.\n\n\nPosterior densities\nOkay, let’s take a look at the posterior distributions. As I hinted earlier on – and indeed has been pointed out previously in the literature on Michaelis-Menten kinetics – there is a serious identifiability issue when parameters are expressed in the \\((k_m, v_m)\\) space:\n\nmmk_bolus_draws &lt;- mmk_bolus_fitted$draws(format = \"draws_df\")\nggplot(mmk_bolus_draws, aes(km, vm)) + \n  geom_point(alpha = .5) + \n  labs(\n    x = \"Michaelis constant, km\", \n    y = \"Maximum elimination rate, vm\"\n  )\n\n\n\n\n\n\n\n\nThe very high posterior correlation (the value is 0.96) between \\(k_m\\) and \\(v_m\\) makes it nigh-impossible to uniquely identify either one, and I am pretty sure this is also the reason why this particular model is very temperamental. Sure, it looks pretty well behaved in the output above (there are no warning messages and only two divergences reported), but that’s what it looks like after I reparameterised the model into a format that sorta kinda works. The version I implemented originally made Stan cry. A lot.\nIn case you’re interested, here’s what the joint posterior looks like in the \\((k_0, v_0)\\) space. There’s still some correlation there (the value is 0.85), but it has attenuated quite a bit:\n\nggplot(mmk_bolus_draws, aes(k0, v0)) + \n  geom_point(alpha = .5) + \n  labs(\n    x = \"Rate-halving concentration, k0\", \n    y = \"Initial elimination rate, v0\"\n  )\n\n\n\n\n\n\n\n\nThis is the real reason I introduced these transformations! Finally, let’s look at the posterior in the \\((k_m, v_0)\\) space in which I actually specified my priors:\n\nggplot(mmk_bolus_draws, aes(km, v0)) + \n  geom_point(alpha = .5) + \n  labs(\n    x = \"Michaelis constant, km\", \n    y = \"Initial elimination rate, v0\"\n  )\n\n\n\n\n\n\n\n\nThe correlation here is 0.82, pretty similar to the previous one.18\n\n\nCredible intervals for the pharamacokinetic function\nOkay, let’s turn to the curve itself. While there are some difficulties associated with recovering parameters of the Michaelis-Menten process, the concentration-time curves themselves are pretty recoverable. First, we use our posterior parameters to generate posterior predicted values for the concentration over time:\n\nmmk_bolus_generated &lt;- mmk_bolus$generate_quantities(\n  fitted_params = mmk_bolus_fitted,\n  data = mmk_bolus_data,\n  seed = 999\n)\n\nRunning standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 1.0 seconds.\n\nmmk_bolus_generated_summary &lt;- mmk_bolus_generated$summary() |&gt;\n  dplyr::filter(variable |&gt; stringr::str_detect(\"^c_fit\"))\nmmk_bolus_generated_summary$time &lt;- mmk_bolus_data$t_fit\n\nmmk_bolus_generated_summary\n\n# A tibble: 160 × 8\n   variable     mean median    sd   mad    q5   q95  time\n   &lt;chr&gt;       &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1 c_fit[1,1]   20.9   20.8 1.30  1.17   19.0  23.2  0.05\n 2 c_fit[2,1]   20.6   20.5 1.25  1.13   18.8  22.8  0.1 \n 3 c_fit[3,1]   20.3   20.2 1.21  1.10   18.5  22.5  0.15\n 4 c_fit[4,1]   20.0   19.9 1.17  1.07   18.3  22.1  0.2 \n 5 c_fit[5,1]   19.7   19.6 1.13  1.04   18.0  21.7  0.25\n 6 c_fit[6,1]   19.4   19.3 1.08  0.997  17.8  21.3  0.3 \n 7 c_fit[7,1]   19.1   19.0 1.04  0.961  17.5  20.9  0.35\n 8 c_fit[8,1]   18.8   18.7 1.01  0.928  17.3  20.6  0.4 \n 9 c_fit[9,1]   18.5   18.5 0.968 0.898  17.1  20.2  0.45\n10 c_fit[10,1]  18.2   18.2 0.932 0.869  16.8  19.8  0.5 \n# ℹ 150 more rows\n\n\nThen we plot the data. The plot below shows the posterior mean estimate of the curve, with the shaded region corresponding to the 90% equal-tail credible intervals for the concentration at each time point. For comparison purposes the observed data are overplotted:\n\ndf &lt;- data.frame(\n  t_obs = mmk_bolus_data$t_obs, \n  c_obs = mmk_bolus_data$c_obs\n)\n\nggplot(mmk_bolus_generated_summary) + \n  geom_ribbon(aes(time, ymin = q5, ymax = q95), fill = \"grey70\") +\n  geom_line(aes(time, mean)) + \n  geom_point(aes(t_obs, c_obs), data = df, size = 2) + \n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nGood enough! Naturally, there’s more I could do with this model, but that’s enough for this post so I’ll move on to something new.\n\n\n\n\nAnother favourite of mine. I love the illusion of lighting that you get from the bright bits."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#multi-compartment-models",
    "href": "posts/2023-05-16_stan-ode/index.html#multi-compartment-models",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "Multi compartment models",
    "text": "Multi compartment models\nImagine a more complex situation where a drug is administered orally: we now have an influx process where the drug is absorbed from the gut into systemic circulation (the central compartment), from which it is later eliminated. However, if the drug is able to pass between the central compartment and other bodily tissues (the peripheral compartment), we’ll need to model the drug flow between them, since only the drug concentration present in the central compartment is available for potential elimination at any point in time. For the sake of my sanity I’ll assume that it’s just as easy for the drug to pass from central to peripheral as vice versa, so there’s only a single “intercompartmental clearance” parameter associated with this flow. I’m also going to assume that all three processes (absorption, elimination, intercompartmental transfer) involve first order dynamics only.\nIn this scenario the state at time \\(t\\) is defined by three concentrations:19\n\n\\(c_{gt} = C_g(t)\\) is the drug concentration still remaining in the gut\n\\(c_{ct} = C_c(t)\\) is the drug concentration currently in the central compartment\n\\(c_{pt} = C_p(t)\\) is the drug concentration currently in the peripheral compartment\n\nIn practice, it can be more convenient to express this in terms of the absolute amount of drug in each compartment (including the gut) at each time point:\n\n\\(a_{gt}\\) is the drug amount still remaining in the gut\n\\(a_{ct}\\) is the drug amount currently in the central compartment\n\\(a_{pt}\\) is the drug amount currently in the peripheral compartment\n\nwhere, for each compartment, the concentration is simply the amount divided by the relevant volume, \\(c = a /v\\). With this in mind we’ll define the state vector at time \\(t\\) as the three drug amounts, \\(\\mathbf{a}_t = (a_{gt}, a_{ct}, a_{pt})\\).\nStrictly speaking, we have four rate parameters in our model:\n\n\\(k_a\\) is the absorption rate constant (from gut to central compartment)\n\\(k_e\\) is the elimination rate constant from the central compartment\n\\(k_{cp}\\) is the transfer rate from the central to peripheral compartment\n\\(k_{pc}\\) is the transfer rate from the peripheral to central compartment\n\nThe biological assumptions of the model are as follows: the gut can only gain drug amount from the external dose which it loses drug to the central compartment, while the peripheral compartment can only exchange drug with the central compartment, and the central compartment has influx from gut, efflux to urine (or whatever), and interchange with the periphery. To help capture this, we’ll define two volumetric parameters:\n\n\\(v_c\\) is the volume of the central compartment\n\\(v_p\\) is the volume of the peripheral compartment\n\nSecond, we can define two clearance parameters:20\n\n\\(q_e\\) is the clearance rate for elimination (traditionally denoted CL)\n\\(q_i\\) is the intercompartmental clearance (traditionally denoted Q)\n\nUsing this notation we can write the system of differential equations as follows:\n\\[\n\\begin{array}{rcl}\n\\displaystyle \\frac{da_{gt}}{dt} &=& -k_a \\ a_{gt}+ d_t \\\\ \\\\\n\\displaystyle \\frac{da_{ct}}{dt} &=& \\displaystyle k_a \\ a_{gt} + \\frac{q_i}{v_p} \\ a_{pt} - \\frac{q_i}{v_c} \\ a_{ct} - \\frac{q_e}{v_c} \\ a_{ct}  \\\\ \\\\\n\\displaystyle \\frac{da_{pt}}{dt} &=& \\displaystyle - \\frac{q_i}{v_p} \\ a_{pt} + \\frac{q_i}{v_c} \\ a_{ct}\n\\end{array}\n\\]\nRighty-ho. Now that we’re clear on what our unknowns are, let’s make some fictitious data:\n\nobs &lt;- tibble::tibble(\n  t_obs = 1:20,\n  c_obs = c(\n    13.48, 23.66, 28.17, 29.34, 28.02, 25.56, 23.25, \n    21.52, 21.21, 19.30, 14.54, 14.62, 11.71, 12.21, \n    10.21, 10.82, 8.43, 8.79, 7.60, 6.03\n  )\n)\nobs\n\n# A tibble: 20 × 2\n   t_obs c_obs\n   &lt;int&gt; &lt;dbl&gt;\n 1     1 13.5 \n 2     2 23.7 \n 3     3 28.2 \n 4     4 29.3 \n 5     5 28.0 \n 6     6 25.6 \n 7     7 23.2 \n 8     8 21.5 \n 9     9 21.2 \n10    10 19.3 \n11    11 14.5 \n12    12 14.6 \n13    13 11.7 \n14    14 12.2 \n15    15 10.2 \n16    16 10.8 \n17    17  8.43\n18    18  8.79\n19    19  7.6 \n20    20  6.03\n\n\nHere’s a picture of said data:\n\nggplot(obs, aes(t_obs, c_obs)) + \n  geom_point(size = 2) +\n  lims(x = c(0, 20), y = c(0, 30)) +\n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nOkay fine yes that looks like data. It’s quite fictitious of course, but whatever. This is a learning exercise for me, not a serious attempt at modelling. Let’s move on, shall we?\n\nTwo compartment model\nAt this point I want to be completely honest. When I started writing this post I really didn’t think I’d reach the point of writing a multi-compartment model. Nevertheless, here we are. On the one hand, I’m pretty pleased with my progress: I set myself a goal and I went well beyond my goal in this post. On the other hand, I am well aware that when I write, I have an audience. So it’s important to recognise that this part of the post is really pushing the boundaries of my current knowledge. Nevertheless, it sorta kinda works. So let’s go with it yeah? This post is long, and we need to finish it somehow. So. Here’s the code:\n\n\n\ntwo_cpt\n\nfunctions {\n  vector two_cpt(real time,\n                 vector state,\n                 real qe,\n                 real qi,\n                 real vc,\n                 real vp,\n                 real ka) {\n\n    // convenience...\n    real ag = state[1]; // gut amount\n    real ac = state[2]; // central amount\n    real ap = state[3]; // peripheral amount\n\n    // derivative of state vector with respect to time\n    vector[3] dadt;\n\n    // compute derivatives\n    dadt[1] = - (ka * ag);\n    dadt[2] = (ka * ag) + (qi / vp) * ap - (qi / vc) * ac - (qe / vc) * ac;\n    dadt[3] = - (qi / vp) * ap + (qi / vc) * ac;\n\n    return dadt;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; n_obs;\n  int&lt;lower=1&gt; n_fit;\n  vector[n_obs] c_obs;\n  array[n_obs] real t_obs;\n  array[n_fit] real t_fit;\n  vector[3] a0;\n  real t0;\n}\n\nparameters {\n  real&lt;lower=.001&gt; qe;\n  real&lt;lower=.001&gt; qi;\n  real&lt;lower=.001&gt; vc;\n  real&lt;lower=.001&gt; vp;\n  real&lt;lower=.001&gt; ka;\n  real&lt;lower=.001&gt; sigma;\n}\n\ntransformed parameters {\n  // use ode solver to find all amounts at all event times\n  array[n_obs] vector[3] amount = ode_rk45(two_cpt,\n                                           a0,\n                                           t0,\n                                           t_obs,\n                                           qe,\n                                           qi,\n                                           vc,\n                                           vp,\n                                           ka);\n\n  // vector of central concentrations\n  vector[n_obs] mu;\n  for (j in 1:n_obs) {\n    mu[j] = amount[j, 2] / vc;\n  }\n}\n\nmodel {\n  // priors (adapted from Margossian et al 2022)\n  qe ~ lognormal(log(10), 0.25); // elimination clearance, CL\n  qi ~ lognormal(log(15), 0.5);  // intercompartmental clearance, Q\n  vc ~ lognormal(log(35), 0.25); // central compartment volume\n  vp ~ lognormal(log(105), 0.5); // peripheral compartment volume\n  ka ~ lognormal(log(2.5), 1);   // absorption rate constant\n  sigma ~ normal(0, 1);          // measurement error\n\n  // likelihood of observed central concentrations\n  c_obs ~ normal(mu, sigma);\n}\n\ngenerated quantities {\n  array[n_fit] vector[3] amt_fit = ode_rk45(two_cpt,\n                                            a0,\n                                            t0,\n                                            t_fit,\n                                            qe,\n                                            qi,\n                                            vc,\n                                            vp,\n                                            ka);\n  \n  vector[n_fit] c_fit;\n  for (j in 1:n_fit) {\n    c_fit[j] = amt_fit[j, 2] / vc;\n  }\n}\n\n\nNext, we put together a Stan-friendly version of the data, which includes the initial amount of drug a0 in each compartment (gut, central, peripheral):\n\nt_fit &lt;- seq(0, 20, .2)\ntwo_cpt_data &lt;- list(\n  n_obs = nrow(obs),\n  c_obs = obs$c_obs,\n  t_obs = obs$t_obs,\n  a0 = c(1000, 0, 0),\n  t0 = -.01,\n  t_fit = t_fit,\n  n_fit = length(t_fit)\n)\n\ntwo_cpt_data\n\n$n_obs\n[1] 20\n\n$c_obs\n [1] 13.48 23.66 28.17 29.34 28.02 25.56 23.25 21.52 21.21 19.30 14.54 14.62\n[13] 11.71 12.21 10.21 10.82  8.43  8.79  7.60  6.03\n\n$t_obs\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n$a0\n[1] 1000    0    0\n\n$t0\n[1] -0.01\n\n$t_fit\n  [1]  0.0  0.2  0.4  0.6  0.8  1.0  1.2  1.4  1.6  1.8  2.0  2.2  2.4  2.6\n [15]  2.8  3.0  3.2  3.4  3.6  3.8  4.0  4.2  4.4  4.6  4.8  5.0  5.2  5.4\n [29]  5.6  5.8  6.0  6.2  6.4  6.6  6.8  7.0  7.2  7.4  7.6  7.8  8.0  8.2\n [43]  8.4  8.6  8.8  9.0  9.2  9.4  9.6  9.8 10.0 10.2 10.4 10.6 10.8 11.0\n [57] 11.2 11.4 11.6 11.8 12.0 12.2 12.4 12.6 12.8 13.0 13.2 13.4 13.6 13.8\n [71] 14.0 14.2 14.4 14.6 14.8 15.0 15.2 15.4 15.6 15.8 16.0 16.2 16.4 16.6\n [85] 16.8 17.0 17.2 17.4 17.6 17.8 18.0 18.2 18.4 18.6 18.8 19.0 19.2 19.4\n [99] 19.6 19.8 20.0\n\n$n_fit\n[1] 101\n\n\nThrilling, I know!\n\n\nFitting the model\nLet’s see if this works when we run it, shall we? Step one, sample from the model…\n\ntwo_cpt_fitted &lt;- two_cpt$sample(\n  data = two_cpt_data, \n  seed = 100, \n  chains = 4,\n  parallel_chains = 2,  \n  refresh = 1000,\n  show_messages = FALSE\n)\n\nRunning MCMC with 4 chains, at most 2 in parallel...\n\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 14.6 seconds.\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 15.3 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 12.0 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 13.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 13.8 seconds.\nTotal execution time: 47.1 seconds.\n\n\nThat’s a little time consuming and makes me suspect that I’ve implemented this poorly.21 Oh well. Perhaps I’ll revisit later. The important thing is that it sorta works. So let’s move on and take a summary:\n\ntwo_cpt_fitted$summary()\n\n# A tibble: 491 × 10\n   variable      mean  median      sd     mad      q5     q95  rhat ess_bulk\n   &lt;chr&gt;        &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;num&gt;\n 1 lp__       -59.1   -59.1    2.43    2.55   -63.2   -55.5    1.34     9.69\n 2 qe           2.19    2.31   0.385   0.468    1.61    2.64   1.73     6.13\n 3 qi          15.2     3.58  16.9     4.54     0.775  46.1    1.74     6.16\n 4 vc          17.7    17.4    2.88    3.39    13.3    22.3    1.42     8.27\n 5 vp          32.7    11.7   40.3     9.91     5.46  108.     1.73     6.17\n 6 ka           0.431   0.436  0.0616  0.0594   0.324   0.529  1.25    12.1 \n 7 sigma        1.20    1.17   0.238   0.225    0.878   1.64   1.10    25.6 \n 8 amount[1,… 648.    644.    40.4    38.4    586.    721.     1.25    12.1 \n 9 amount[2,… 423.    417.    52.8    49.1    346.    522.     1.25    12.1 \n10 amount[3,… 278.    269.    52.2    47.7    204.    377.     1.25    12.1 \n# ℹ 481 more rows\n# ℹ 1 more variable: ess_tail &lt;num&gt;\n\n\nAs usual, it’s useful to take a look at some results visually. To that end we’ll again generate posterior predicted time-concentration curves:\n\ntwo_cpt_generated &lt;- two_cpt$generate_quantities(\n  fitted_params = two_cpt_fitted,\n  data = two_cpt_data,\n  seed = 999\n)\n\nRunning standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 2.3 seconds.\n\ntwo_cpt_generated_summary &lt;- two_cpt_generated$summary() |&gt;\n  dplyr::filter(variable |&gt; stringr::str_detect(\"^c_fit\"))\n\ntwo_cpt_generated_summary$time &lt;- two_cpt_data$t_fit\ntwo_cpt_generated_summary\n\n# A tibble: 101 × 8\n   variable    mean median     sd    mad     q5    q95  time\n   &lt;chr&gt;      &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt;\n 1 c_fit[1]   0.249  0.229 0.0549 0.0563  0.188  0.342   0  \n 2 c_fit[2]   4.54   4.35  0.657  0.721   3.74   5.66    0.2\n 3 c_fit[3]   8.01   7.81  0.835  0.905   6.94   9.48    0.4\n 4 c_fit[4]  11.0   10.8   0.873  0.918   9.81  12.6     0.6\n 5 c_fit[5]  13.6   13.5   0.859  0.873  12.4   15.1     0.8\n 6 c_fit[6]  15.9   15.8   0.827  0.817  14.7   17.3     1  \n 7 c_fit[7]  17.8   17.8   0.790  0.769  16.7   19.2     1.2\n 8 c_fit[8]  19.6   19.5   0.754  0.721  18.4   20.9     1.4\n 9 c_fit[9]  21.1   21.1   0.721  0.681  20.0   22.3     1.6\n10 c_fit[10] 22.4   22.4   0.692  0.657  21.3   23.6     1.8\n# ℹ 91 more rows\n\n\n\ndf &lt;- data.frame(\n  t_obs = two_cpt_data$t_obs, \n  c_obs = two_cpt_data$c_obs\n)\n\nggplot(two_cpt_generated_summary) + \n  geom_ribbon(aes(time, ymin = q5, ymax = q95), fill = \"grey70\") +\n  geom_line(aes(time, mean)) + \n  geom_point(aes(t_obs, c_obs), data = df, size = 2) + \n  labs(x = \"time\", y = \"concentration\")\n\n\n\n\n\n\n\n\nThat’s good enough for today.\nIt’s probably obvious from the context that this is a work in progress. I haven’t really thought this stuff through yet, and I’m willing to put a lot of money on the proposition this code could be improved considerably. However, that’s quite beside the point for this post. Right now, my goal was to get something to work, and this kinda works. Yay! I am happy.\n\n\n\n\nThe original system these were based on was called “dreamlike”: the feeling of dissolution in this piece is very characteristic of that ancestor system."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#things-i-have-not-discussed",
    "href": "posts/2023-05-16_stan-ode/index.html#things-i-have-not-discussed",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "Things I have not discussed",
    "text": "Things I have not discussed\nThere are many topics that are adjacent to and highly relevant to this post that I have not discussed. The real reason for this is simply that I have limits, and this is just a blog post. But for the sake of listing some of them, here are a few issues around statistical inference that I omitted:\n\nMCMC diagnostics: At various points in the process of writing this post Stan was kind enough to yell very loudly at me about the possibility that things might be going wrong with the MCMC sampler. Having “earned my bones” by writing some truly terrible MCMC samplers of my own, I really appreciate the “fail loudly” aesthetic that Stan has going on. Indeed, the only reason that the models I’ve used here have ended up being… look, let’s call them “okay”, shall we?… is that every time something went awry Stan screamed at me.\nDifferent kinds of ODE solvers: Throughout the post I’ve used ode_rk45() as my ODE solver in Stan, as if that were the only possible choice. It isn’t the only choice, and indeed it’s not even the only solver I used in the process of writing the bloody thing. There were periods in the post development where I couldn’t get the models to behave, and as a consequence of that the sampler kept proposing parameters that pushed the differential equations into a part of the space where they… aren’t pleasant. When that happened the system became “stiff”22 and I had to switch to using ode_bdf(). There is probably a whole blog post to be written about stiffness but… yeah nah. Suffice it to say this is one of those things where the mapmaker feels an urge to scribble “here be dragons” and move along.\nModel checking, and Bayesian workflow more generally: Stating the obvious really, but at no point have I really dived into model checking, model testing, etc. There’s a huge amount that could be said on that. Not gonna say any of it here.\n\nThere are other things missing too. On the pharmacometric side, I have not discussed:\n\nThe logic for choosing one model structure over another. When do you need multiple compartments? When do you need to assume saturating elimination processes? Etc. There’s a very good reason I haven’t spoken about those things: I am not even slightly qualified to do so. That’s something that requires substantive domain knowledge and I’m not there yet.\nIndividual differences (i.e., population pharmacokinetics). In real life you really need to consider variability across people. That’s something we can address with hierarchical models, by assuming the population defines distributions over parameters. I omitted that for simplicity: I’ll get to that later.\nCovariates. Another super important thing in real life is modelling systematic variation across individuals as well as random variation. Predicting the way in which parameters of the process vary across people as a function of other measured characteristics (age, sex, etc) matters hugely. Again, that’s a future topic.\nModelling the effect of the drug (pharmacodynamics): there’s more to these models than simply tracking the drug concentration. Often you need to consider downstream effects… what does the drug do physiologically, or psychologically, and how do those effects persist over time. Future topic babes.\n\nFinally, there are things I’ve missed on the software side. I’ve not talked about Torsten and WinNonlin, and at some point I probably should. But… not today. This post is long enough already. I think we can all agree on that!"
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#resources",
    "href": "posts/2023-05-16_stan-ode/index.html#resources",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "Resources",
    "text": "Resources\nIn my post-academic era I’ve gotten a little lax in my citation habits, but I still care about acknowledging my sources. In addition to the various Wikipedia pages and Stan documentation pages I’ve linked to throughout, this blog post draws heavily on ideas presented in these three papers:\n\nHolz & Fahr (2001). Compartment modelling. Advanced Drug Delivery Reviews. doi.org/10.1016/S0169-409X(01)00118-1\nChoi, Rempala & King (2017). Beyond the Michaelis-Menten equation: Accurate and efficient estimation of enzyme kinetic parameters. Scientific Reports. doi.org/10.1038/s41598-017-17072-z\nMargossian, Zhang & Gillespie (2022). Flexible and efficient Bayesian pharmacometrics modeling using Stan and Torsten, Part I. CPT: Pharmacometrics & Systems Pharmacology. doi.org/10.1002/psp4.12812\n\nNaturally, all the stuff ups and errors are mine and mine alone."
  },
  {
    "objectID": "posts/2023-05-16_stan-ode/index.html#footnotes",
    "href": "posts/2023-05-16_stan-ode/index.html#footnotes",
    "title": "Pharmacokinetic ODE models in Stan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI did try WinBUGS briefly but it was a bit awkward since I wasn’t a Windows user even then.↩︎\nYes, I know that sometimes there are tricks to work around them. It was more effort than it was worth.↩︎\nThere’s also many packages for specific modelling frameworks like brms and rstanarm but for the purposes of this post I’m only interested in packages that supply a general-purpose interface to Stan from R.↩︎\nI mean, technically the real work of compiling the Stan code into an executable and then running the sampler is all being done by Stan itself and has sweet fuck all to do with any R package, but knitr has no way of talking to Stan, so it has to rely on an R package to do that… and by default it asks RStan.↩︎\nOr, to those of us who don’t speak the language, the drug is injected into the bloodstream in a single dose.↩︎\nNot quite the same as the total amount of blood plasma, as I understand it, but approximately the same idea.↩︎\nSo easy even I could do it.↩︎\nI’m adopting the $method() convention here sometimes used when discussing R6 classes to be clear that we’re talking about encapsulated OOP in which methods belong to objects (as is typical in many programming languages) rather than functional OOP in which methods belong to generic functions (as appears in the S3 and S4 systems in R, for instance).↩︎\nNote for young people: This was the pre-2023 equivalent of saying “As an AI language model…”↩︎\nI can still remember quite viscerally the moment that conjugacy died for me as a potentially useful statistical guideline: it was the day I learned that a Dirichlet Process prior is conjugate to i.i.d. sampling from an arbitrary(ish) unknown distribution. DP priors are… well, I don’t want to say “worthless” because I try to see the good in things, but having worked with them a lot over the years I am yet to discover a situation where the thing I actually want is a Dirichlet Process. It’s one of those interesting inductive cases about the projectibility of different properties. What “should” have happened is that the DP-conjugacy property made me more willing to use the DP. Instead, what “actually” happened is that learning about DP-conjugacy made me less willing to trust conjugacy. I knew in my bones that the DP was useless, so I revised my belief about conjugacy. Someone really should come up with a formal language to describe this kind of belief revision… I imagine it would be quite handy.↩︎\nThis is “of course” (see next footnote) an equal-tailed interval rather than a highest density interval.↩︎\nIn technical writing, the term “of course” is an expression that used to denote “something that the author is painfully aware of and some subset of the readership is equally exhausted with, and none of those people really want to talk or hear about for the rest of their living days, but is entirely unknown and a source of total confusion to another subset of the readership who have no idea why this should be obvious because in truth it absolutely is not obvious”. As such it should, of course, be used with caution and with a healthy dose of self-deprecation and rolling-of-the-eyes. It is in this latter spirit that my use of the term is intended: I simply do not wish to devote any more of my life to thinking about the different kinds of credible intervals.↩︎\nWe could supplement it further and maybe add dotted lines even further out that show 90% credible regions for future data (i.e., acknowledging the role of measurement error in the future) but today I don’t feel like doing that!↩︎\nAnd other organs, I guess. I’ve heard rumours the human body contains other organs that actually do things…↩︎\nApparently this is only half true. A deep dive on the relevant wikipedia page suggests that it is possible to solve it analytically, but insofar as the solution involves the Lambert W function I personally would prefer to take a numerical approach.↩︎\nFolks familiar with Stan will probably notice that my code style could be improved a little. It’s probably not ideal that I’m enforcing the truncation range on some of my variables twice, once at variable declaration using the &lt;lower, upper&gt; syntax, and then sometimes later on by truncating a distribution using the T[L, U] syntax. At the moment I find this redundancy helpful to me because it stops me from getting confused (e.g., it means I never look at my own code and wonder why my half-normal is declared to have a normal() distribution), but in the long run I think it’s a bad idea for code maintenance. From that point of view I think it makes sense to have a consistent code style that declares variable bounds once and only once, in a place where future maintainers would expect to find it. That would make it a lot easier to edit later. But that’s the kind of detail I’m not going to worry about right this second.↩︎\nOkay look, if this were linear regression I’d be very worried at seeing any divergences. But for this model? Babe, this is really fucking good. It took a lot of effort to find a parameterisation of the MMK elimination model that doesn’t end in nuclear fire, metaphorically speaking. One or two divergences in the MCMC chains, yeah, I can live with that.↩︎\nLooking at that plot I had wondered if the correlation was being boosted by a few outliers, but that doesn’t seem to be the case: the rank-order correlation for the third plot is 0.82, which compares to 0.83 for the previous plot, and 0.93 for the first one.↩︎\nIt should be noted that my notation is slightly nonstandard. My understanding is that the convention in the literature is to use notation consistent with the WinNonlin software package, as that is generally considered the industry standard and often used for benchmarking purposes. I haven’t done this for a very boring reason: I’ve not yet had a chance to play around with WinNonlin.↩︎\nAgain, I know my notation is nonstandard. But I’m still wrapping my head around all this and, for the time being at least, I like having notation that makes clear that \\(q_e\\) and \\(q_{cp}\\) are both parameters of the same structural kind. I don’t find CL and Q anywhere near as easy to understand.↩︎\nOkay, I should be honest. The real reason I suspect there’s room for improvement here is that I’ve had use show_messages = FALSE to suppress some warnings messages in order to keep the output visually tidy. It’s not terrible: Stan throws some warnings from the ODE solver, especially during the very early stages of warmup, in which it quite rightly screams about being fed insane parameters. It’s not ideal, but it’s not terrible either. I hid the messages to avoid ugly output, not for more nefarious reasons. I mean, don’t get me wrong, this model needs a lot more love than I have given it up to this point. But the hidden warning messages are somewhat boring in this instance.↩︎\nToo easy.↩︎"
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html",
    "href": "posts/2024-11-11_emax-parameters/index.html",
    "title": "Bayesian estimation for Emax regression",
    "section": "",
    "text": "And so we are writing again. Sort of.\nThis is a post about the Emax model, a commonly-used tool in pharmacometrics. I’ve written about it before. At the start of this year I wrote up some notes on the Emax model approaching it from a purely pharmacological perspective, discussing the mechanistic motivation for Emax when working with pharmacodynamic data. What I didn’t talk about in that post is the behaviour of Emax as a statistical model. It is time to redress that limitation, and talk a little about some issues that can arise when estimating the parameters of an Emax model.\nIt is also a post about the rstanemax package, a handy tool by Kenta Yoshida that I’ve only recently discovered, which provides for Bayesian estimation in R for the Emax model.1 The rstanemax package supplies a pre-defined implementation of the Emax model in Stan that you can call from R via rstan, and while I won’t be doing a deep dive into the package here, it’s a very convenient way for me to talk about some statistical issues that can arise in Emax modelling.\nIt’s also a rather long post, and I’ll confess it doesn’t have the same level of “authorial voice” that I usually bring to my blog. I have been feeling pretty miserable ever since the US election, and my energy levels are pretty low right now.2 Sorry.\nlibrary(rstanemax)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(purrr)"
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#the-emax-model",
    "href": "posts/2024-11-11_emax-parameters/index.html#the-emax-model",
    "title": "Bayesian estimation for Emax regression",
    "section": "The Emax model",
    "text": "The Emax model\nLetting \\(x_i\\) denote the observed exposure for the \\(i\\)-th subject, and letting \\(y_i\\) denote the observed reponse, the form of the Emax model for a continuous-valued response3 is typically written as the following nonlinear regression model:\n\\[\ny_i = E_0 + E_{max} \\frac{x_i^\\gamma}{EC_{50}^\\gamma + x_i^\\gamma} + \\epsilon_i\n\\]\nwhere we typically assume iid normal residual error, \\(\\epsilon_i \\sim \\mbox{Normal}(0, \\sigma^2)\\). This model has five parameters that need to be estimated:\n\n\\(E_0\\) is an intercept term and represents the baseline response when drug exposure is zero\n\\(E_{max}\\) is an asymptote term and defines the maximum change from baseline as the drug exposure becomes arbitrarily large\n\\(EC_{50}\\) is a location parameter, and defines the exposure level at which the change from baseline is 50% of the maximum possible change\n\\(\\gamma\\) is the “Hill coefficient” that describes the steepness of the response curve. It is not uncommon to fix \\(\\gamma = 1\\) in Emax modelling, and for the purposes of this post that’s what I’ll be doing here\n\\(\\sigma^2\\) is the residual variance used to describe the level of measurement error in the data\n\nThis situation is about as simple as you can possibly get within the Emax context: we’re abstracting over questions about how exposure is defined4, there are no covariates in the model, and we are assuming that the response variable \\(y\\) is continuous valued with normally distributed measurement error. It does not get any simpler than this in Emax-land.\nTo give a sense of what the Emax model equation (i.e., ignoring the \\(\\epsilon_i\\) terms) looks like, we can implement in R like this:5\n\nemax_fn &lt;- function(exposure, emax, ec50, e0, gamma = 1, ...) {\n  e0 + emax * (exposure ^ gamma) / (ec50 ^ gamma + exposure ^ gamma)\n}\n\nHere’s what the function looks like when visualised. On the left hand side the plot shows the exposure-response curve on a linear scale, whereas on the right hand side the exposure is plotted on a logarithmic scale.\ndat &lt;- tibble(\n  exposure = 1:10000,\n  response = emax_fn(exposure, emax = 10, ec50 = 200, e0 = 10)\n)\npic &lt;- ggplot(dat, aes(exposure, response)) + \n  geom_line() + \n  theme_bw()\npic\npic + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nIn the linear version of the plot we see a steep initial rise in the response, followed by saturation as exposure increases. The logarithmic scale plot allows us to see what is happening with a little more precision: the Emax model implies a logistic relationship between response and log-exposure.6\nTo flesh out the intuition a little better, let’s systematically vary one parameter at a time and show what effect each parameter has on the Emax function. To that end I’ll first write a convenience function emax_effect()…\n\nemax_effect &lt;- function(exposure = 1:10000, \n                        emax = 10, \n                        ec50 = 200, \n                        e0 = 10, \n                        gamma = 1, \n                        ...) {\n  expand_grid(\n    exposure = exposure,\n    emax = emax, \n    ec50 = ec50, \n    e0 = e0,\n    gamma = gamma\n  ) |&gt; \n    mutate(\n      response = emax_fn(exposure, emax, ec50, e0, gamma),\n      emax = factor(emax),\n      ec50 = factor(ec50),\n      e0 = factor(e0),\n      gamma = factor(gamma)\n    ) |&gt; \n    ggplot(aes(exposure, response, ...)) + \n    geom_line() +\n    theme_bw()\n}\n\nThe simplest parameter to understand is \\(E_0\\). It’s an intercept parameter pure and simple. The whole curve shifts up and down as you vary \\(E_0\\), exactly like it would in linear regression:\nemax_effect(e0 = seq(2, 10, 2), color = e0)\nemax_effect(e0 = seq(2, 10, 2), color = e0) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nThe \\(E_{max}\\) parameter is a scaling coefficient applied to the exposure. It is roughly analogous to a slope parameter in linear regression, in the sense that increasing \\(E_{max}\\) “stretches” the curve vertically. More precisely though, increasing \\(E_{max}\\) shifts the asymptotic value for the response: at large exposures the response saturates at \\(E_0 + E_{max}\\), and the curve is scaled to accomodate this.\nemax_effect(emax = seq(2, 10, 2), color = emax)\nemax_effect(emax = seq(2, 10, 2), color = emax) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nIf you compare these plots to the previous ones, you can see why its important to look at the Emax function on the logarithmic scale as well as the linear scale. The difference between how \\(E_0\\) and \\(E_{max}\\) affect the curve are difficult to see on the linear scale, but are very pronounced on the logarithmic scale. This will turn out to be relevant later: if you don’t have a lot of data at the lower end of the log-exposure distribution, it is very hard to estimate \\(E_0\\) and \\(E_{max}\\) separately. But I am getting ahead of myself.\nLet’s now look at the effect of \\(EC_{50}\\). This parameter doesn’t have an exact analog in the linear regression context, but the plots below illustrate the effect this parameter has rather nicely: it’s a shift parameter that moves the whole curve (on the log-scale) rightwards as it increases.\nemax_effect(ec50 = seq(50, 400, 50), color = ec50)\nemax_effect(ec50 = seq(50, 400, 50), color = ec50) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nBecause \\(EC_{50}\\) is on the same scale as the exposure, the size of the rightward shift scales logarithmically rather than linearly as the parameter is varied.\nFinally, let’s quickly look at what happens when the Hill coefficient \\(\\gamma\\) is varied. To be honest, I’m not going consider \\(\\gamma\\) in this post, but it’s still kind of useful to visualise what it does. As usual it’s easier to see the effect on the log-exposure scale. As you can see from the plot on the right, at higher \\(\\gamma\\) values the logistic curve is steeper.\nemax_effect(gamma = 1:5, color = gamma)\nemax_effect(gamma = 1:5, color = gamma) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nNow that we have a general sense of how the Emax function behaves, we can start thinking about it as a statistical model and look at what we expect data to look like when the Emax model applies."
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#simulated-exposures",
    "href": "posts/2024-11-11_emax-parameters/index.html#simulated-exposures",
    "title": "Bayesian estimation for Emax regression",
    "section": "Simulated exposures",
    "text": "Simulated exposures\nGenerating simulated data to fit with an Emax model is slightly tricky, even in the very simple scenario I’m considering. Generating fictitious response data conditional on having a set of observed exposures is easy: the Emax function itself tells you how to generate \\(y_i\\) (response) values given \\(x_i\\) (exposures). Yes, there’s some nuance involved because the additive normal error model for \\(\\epsilon_i\\) (residuals) is a bit silly if you take it literally, but that’s not the hard part.\nThe hard part is generating a set of exposures \\(x_i\\) that are presumed to arise from your experimental design.\nThe reason this part is difficult is that the Emax model is silent on where those exposures come from. As I talked about in the notes on the Emax model post, the scientific justification for the Emax function is based on some (not unreasonable) assumptions about the mechanism of action for the drug. It has a clear interpretation for exposure measures (e.g., \\(C_{max,ss}\\), \\(C_{min,ss}\\)) that correspond to the concentration at a specific moment in time, and while you do have to hand-wave a little more when considering time-averaged exposures or area under the curve (\\(AUC_{ss}\\)) measures, it does make a certain amount of biological sense. Even so, the Emax model is a pharmacodynamic model and is not in any meaningful sense a pharmacokinetic model: you can’t extract exposure measures PK profiles from the Emax model in the same way you can for a compartmental model (e.g. here, here, and here). It’s not designed for that purpose, and I have absolutely no intention of trying to generate fictitious exposures based on a compartmental model simply to play around with Emax. With that in mind I’ve adopted a semi-plausible compromise. Conditional on dose, I will assume for the current purposes that our exposure measure (whatever exposure measure we’re using) is approximately lognormally distributed,7 and I’ll assume that exposure scales linearly with dose. Neither of those two assumptions holds true in general, but it’s good enough for a blog post. You can do this with a simple function like generate_exposure() below, and that will be good enough for now:\n\ngenerate_exposure &lt;- function(dose, n, meanlog = 4, sdlog = 0.5) {\n  dose * qlnorm(\n    p = runif(n, min = .01, max = .99), \n    meanlog = meanlog,\n    sdlog = sdlog\n  )\n}\n\nNext we need to make some assumptions about the study design.\nLet’s suppose for simplicity we have a three-arm design in a phase 2 study that consists of 20 subjects in a placebo arm, alongside 100 subjects each in a 100 mg dosing arm and a 50 mg dosing arm.8 However, it’s not uncommon to encounter exposure-response data sets where these phase 2 data are intermixed with a smaller data set from a phase 1 dose escalation trial (e.g., from a 3+3 design), and these samples can span a substantially wider range of exposures. Rather than try to simulate this in full, I’ll approximate this by including “phase 1” arms that have 3 subjects each at 25, 50, 100, 200, 300, 400, 500, and 600 mg doses.9\nGiven this, for each subject we use the generate_exposure() function to assign them an exposure level:10 11\n\nmake_data &lt;- function(dose, n, condition) {\n  tibble(\n    dose = dose, \n    exposure = generate_exposure(max(dose, .01), n = n), \n    condition = condition\n  ) \n}\nfull_design &lt;- bind_rows(\n  make_data(dose = 100, n = 100, condition = \"100 mg\"),\n  make_data(dose = 50, n = 100, condition = \"50 mg\"),\n  make_data(dose = 0, n = 20, condition = \"placebo\"),\n  make_data(dose = 25, n = 3, condition = \"phase 1\"),\n  make_data(dose = 50, n = 3, condition = \"phase 1\"),\n  make_data(dose = 100, n = 3, condition = \"phase 1\"),\n  make_data(dose = 200, n = 3, condition = \"phase 1\"),\n  make_data(dose = 300, n = 3, condition = \"phase 1\"),\n  make_data(dose = 400, n = 3, condition = \"phase 1\"),\n  make_data(dose = 500, n = 3, condition = \"phase 1\"),\n  make_data(dose = 600, n = 3, condition = \"phase 1\")\n) \nfull_design\n\n# A tibble: 244 × 3\n    dose exposure condition\n   &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    \n 1   100    4271. 100 mg   \n 2   100    3973. 100 mg   \n 3   100    5823. 100 mg   \n 4   100    2563. 100 mg   \n 5   100    5253. 100 mg   \n 6   100    5352. 100 mg   \n 7   100    8409. 100 mg   \n 8   100    4643. 100 mg   \n 9   100    5782. 100 mg   \n10   100    3434. 100 mg   \n# ℹ 234 more rows\n\n\nThis is all a bit hand-wavy, as any pharmacometrician will immediately note. The dosing and sampling regimen aren’t specified in any detail: I am simply assuming that whatever happens on the PK side is such that we can pretend that exposure is approximately lognormal and scales with dose.\nBut let’s set all that to one side and take a look at the distribution of exposures we end up with in an experimental design like this:\npic &lt;- ggplot(full_design, aes(exposure, fill = condition)) +\n  geom_histogram(bins = 20) + \n  theme_bw() +\n  theme(legend.position = \"bottom\")\n\npic\npic + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nThe main thing to note, looking at the histograms that come from this highly-stylised simulated design, is the data that would end up being “visible” as the predictor in an Emax model are a set of exposures \\(x_i\\) that have a positively skewed distribution and are rather unevenly distributed across the exposure range.12 A somewhat more conventional way of looking at this would be to show exposure boxplots stratified by condition, on both a linear scale and a logarithmic scale:\npic &lt;- ggplot(full_design, aes(condition, exposure, fill = condition)) +\n  geom_boxplot(show.legend = FALSE) + \n  theme_bw()\n\npic\npic + scale_y_log10()\n\n\n\n\n\n\n\n\n\n\nLater in this post I’ll come back to this “full” data set that contains placebo samples and phase 1 dose-escalation samples. But I also want to consider the behaviour of the Emax model when you don’t have either of those two groups in the data set. The reason I want to do this is because an Emax model carries a strong assumption that lower and upper bounds for the response can be identified from the data, and it’s not obvious to me that it will behave well if the range of exposures is somewhat restricted.\nIn any case, when we restrict ourselves to the data that emerge from the 50 mg and 100 mg arms of our hypothetical study data, we end up with a vector of exposure values \\(x_i\\) that looks a little bit like a log-normal distribution:\n\nsmall_design &lt;- full_design |&gt; \n  filter(condition %in% c(\"50 mg\", \"100 mg\")) \n\nggplot(small_design, aes(exposure)) +\n  geom_histogram(bins = 20) + \n  theme_bw()\n\n\n\n\n\n\n\n\nHopefully this gives you a sense of what both designs look like.13 Let’s move on, shall we?"
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#simulated-data",
    "href": "posts/2024-11-11_emax-parameters/index.html#simulated-data",
    "title": "Bayesian estimation for Emax regression",
    "section": "Simulated data",
    "text": "Simulated data\nHaving made some choices about the exposure vector that will be supplied to our Emax model, the rest of the simulation process is “just” a matter of writing some convenience functions that we can use later for exploring the behaviour of the model. First, I’ll write an emax_parameters() function that supplies default values, so that if I call emax_parameters(emax = 20), I end up with a list that stores this emax value alongside all the defaults:\n\nemax_parameters &lt;- function(emax  = 10, \n                            ec50  = 4000, \n                            e0    = 10,\n                            gamma = 1,\n                            sigma = .6) {\n  list(\n    emax = emax,\n    ec50 = ec50,\n    e0 = e0,\n    gamma = gamma,\n    sigma = sigma \n  )\n}\n\npar1 &lt;- emax_parameters() # default parameters\npar1\n\n$emax\n[1] 10\n\n$ec50\n[1] 4000\n\n$e0\n[1] 10\n\n$gamma\n[1] 1\n\n$sigma\n[1] 0.6\n\n\nTruly exciting work, I know, but my experience with statistical programming has always been that it pays to take care of these little niceties. Convenience functions like this one don’t play any meaningful scientific role, nor are they important for understanding the statistical properties of the Emax model, but they do make the code easier to read and write.\nAlong the same lines, I’ll also write a generate_emax_data() function that takes an exposure vector and a parameter list (par) as its arguments, and returns a nicely formatted data frame:\n\ngenerate_emax_data &lt;- function(exposure, par = list()) {\n  par &lt;- do.call(emax_parameters, args = par)\n  n &lt;- length(exposure)\n  tibble(\n    exposure = exposure,\n    emax_val = emax_fn(\n      exposure, \n      emax = par$emax, \n      ec50 = par$ec50, \n      e0 = par$e0, \n      gamma = par$gamma\n    ),\n    response = emax_val + rnorm(n, 0, par$sigma)\n  )\n}\n\nAnd just like that we can generate fictitious data sets to which an Emax model can be applied, containing exposure and response columns that are broadly analogous to those we encounter in real data (albeit without any covariate information, etc), along with an emax_val column that removes measurement error from the response, and of course wouldn’t be available in real life:\n\ndat1 &lt;- generate_emax_data(\n  exposure = small_design$exposure, \n  par = par1\n)\ndat1\n\n# A tibble: 200 × 3\n   exposure emax_val response\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    4271.     15.2     14.8\n 2    3973.     15.0     15.2\n 3    5823.     15.9     15.0\n 4    2563.     13.9     13.6\n 5    5253.     15.7     15.5\n 6    5352.     15.7     16.3\n 7    8409.     16.8     16.5\n 8    4643.     15.4     15.6\n 9    5782.     15.9     15.7\n10    3434.     14.6     14.1\n# ℹ 190 more rows\n\n\nTo get a better sense of what these data look like I wrote a convenient plot_emax_data() function that draws a scatterplot of observed response values against observed exposures (black dots), with the true emax function plotted as an underlay (black line), plus a few guide lines that I’ll explain in a moment. Here’s the code for the plotting function:\n\nplot_emax_data &lt;- function(data, par) {\n  min_x &lt;- min(data$exposure)\n  max_x &lt;- max(data$exposure)\n  pred &lt;- tibble(\n    exp = seq(min_x, max_x, length.out = 1000),\n    rsp = do.call(emax_fn, args = c(list(exposure = exp), par))\n  )\n  data |&gt; \n    ggplot(aes(exposure, response)) + \n    geom_vline(xintercept = par$ec50, color = \"red\") +\n    geom_hline(yintercept = par$e0 + par$emax, color = \"red\") +\n    geom_hline(yintercept = par$e0 + par$emax/2, color = \"red\") +\n    geom_hline(yintercept = par$e0, color = \"red\") +\n    geom_line(\n      data = pred,\n      mapping = aes(exp, rsp)\n    ) +\n    geom_point() + \n    theme_bw()    \n}\n\nThe data set dat1 I generated previously is the one I’m going to use later when considering ways in which the Emax regression model can behave poorly, but for reasons that will become clear, I’m actually going to illustrate the plot_emax_data() function by generating a data set that all samples in the full design. Here’s what we get:\n\npic &lt;- full_design$exposure |&gt; \n  generate_emax_data(par = par1) |&gt; \n  plot_emax_data(par = par1)\n\npic \n\n\n\n\n\n\n\n\nLooking at this we can see that the black dots are our observed14 data, and the black line is the true Emax function that descrives the exposure-response relationship. Moving upwards from bottom to top, the three red horizontal lines correspond to the baseline response (i.e., \\(E_0\\)), the 50% response (i.e., \\(E_0 + E_{max}/2\\)), and the maximum response (i.e., \\(E_0 + E_{max}\\)). The vertical red line is the concentration at which the 50% response is attained (i.e., \\(EC_{50}\\)). In other words, the red lines exist to highlight the parameter values for the Emax function.\nNext, let’s redraw this with a logarithmic x-axis scale:\n\npic + scale_x_log10()\n\n\n\n\n\n\n\n\nWhen plotted like this, you get a much stronger sense of how much influence a placebo sample15 can have on the model. In the first plot you could be forgiven for thinking it doesn’t matter much, but when you see it on a logarithmic scale you realise how big a role a placebo group can play in ensuring you can correctly identify the baseline response!\nYou can also understand why, all of a sudden, I am wondering if the Emax model might start to misbehave if we feed it data that don’t have any placebo samples.\nWith that as our motivation, let’s go back to our dat1 data set (constructed using the small design), since that one deliberately doesn’t contain the placebo group. Here’s what the plots look like for this data set:\nplot_emax_data(dat1, par1) \nplot_emax_data(dat1, par1) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nThis is a pretty interesting test case for our Emax model. Will it work out okay, or not? There’s two different factors to consider:\nOn the one hand, even a moments inspection of the red lines in the plot (which, I cannot stress enough, you never have access to in real life) tells us that the observed exposures and responses are somewhat truncated compared to the full range: the true response should vary between 10 and 20 units, but the vast majority of our observed responses fall between 12.5 and 17.5 units. Crudely put, we’re missing about half of the response range. It’s a difficult test case in that sense, but it’s also not that unrealistic: I’ve seen a lot of data sets where the exposure-response relationship looks awfully linear on the log-exposure scale.\nOn the other hand, it’s not the worst possible scenario to think about. If you look at the distribution of the black dots you can they’re nicely distributed on either size of the vertical red line: about half of our observed exposures are below the true \\(EC_{50}\\) value, and about half are above. Consequently, the model can “see” both sides of the log-logistic function when estimating parameters. So that’s a plus."
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#specifying-the-priors",
    "href": "posts/2024-11-11_emax-parameters/index.html#specifying-the-priors",
    "title": "Bayesian estimation for Emax regression",
    "section": "Specifying the priors",
    "text": "Specifying the priors\nTo find out what actually happens, we can use stan_emax() from the rstanemax package to estimate parameters. However, because we will be fitting a Bayesian model, we need to do a little more work to complete the model specification. Specifically, we need to set priors.16\n\nSetting priors in rstanemax\nThe rstanemax package assumes that the priors for the four Emax parameters are independent and normal, and as such we simply describe the prior mean and prior standard deviation for each parameter.17 The priors argument to stan_emax() takes a named list as input, where each element is a vector specifying the mean and standard deviation of the corresponding normal distribution. So we could construct an object like my_prior below,\n\nmy_prior &lt;- list(\n  e0 = c(10, 5),        # e0 prior has mean = 10, sd = 5\n  ec50 = c(4000, 1000), # ec50 prior has mean = 4000, sd = 1000\n  emax = c(5, 5),       # emax prior has mean = 5, sd = 5\n  sigma = c(.6, 1)      # sigma prior has mean = .6, sd = 1\n)\n\nand then pass prior = my_prior when calling stan_emax(). In the simulations that follow, I’m going to “cheat” a little by specifying priors that always – somewhat miraculously – place the prior mode on the true parameter values that generate the data to be fit, and always have a rather large variance. This is not a very reasonable way of working with priors in real life: I am cutting corners here.\nHaving said all that, this is the rather silly emax_priors() function I’m going to use:\n\nemax_priors &lt;- function(par) {\n  list(\n    e0 = c(par$e0, 100),\n    ec50 = c(par$ec50, 5000),\n    emax = c(par$emax, 100),\n    sigma = c(par$sigma, 1)\n  )\n}\n\nIt’s very boring.\nThe thing you actually want to pay attention to here are the standard deviations: they’re all very big. While I’m “centering” my priors (sort of: see next section) so that the prior mode will always be the “true” parameter set (passed via par) that was used to generate the data, the prior variability is set very high… the models I’ll be fitting are extremely agnostic about what parameter values and what data sets they’re expecting to see.\n\n\nTruncated normals are weird\nAt this point in the post, any Bayesian who isn’t new to this game is sighing wearily and muttering darkly about the dangers of setting “agnostic” priors. They would be right to do so. There’s this very naive idea in Bayesian analysis that the best thing to do when fitting models is to set an “uninformative” prior, in order to “let the data speak for themselves”. While that intuition has a certain appeal, it often creates new problems when it runs head first into the data.\nWith that in mind, I’ll highlight one example that is particularly relevant if you’re fitting Emax models using the rstanemax pacakge: truncation matters. Although the documentation to stan_emax() indicates that the priors are normal, this is not entirely true due to the logical constraints imposed on the model parameters: ec50 and sigma cannot be negative valued, and when you peek inside at the stan code you can see that this constraint is enforced. That is, while the model{} block contains these two lines which might suggest to you that the priors are normal,\nec50  ~ normal(prior_ec50_mu,  prior_ec50_sig);\nsigma ~ normal(prior_sigma_mu, prior_sigma_sig);\nif you look at the parameters{} block you discover that rstanemax imposes the non-negativity constraint on both of these parameters:\nvector&lt;lower = 0&gt;[n_covlev_ec50] ec50;\nreal&lt;lower = 0&gt; sigma;\nThis constraint is entirely appropriate: negative variances don’t make any sense (so sigma must be positive), and neither do negative exposures (so ec50 must be positive). In practice, therefore, your priors for ec50 and sigma are in fact truncated normal distributions.18\nNoting this, I’ll cobble together a little R function that allows me to sample from a truncated normal distribution:19\n\nrtnorm &lt;- function(n, mean = 0, sd = 1, lower = -Inf, upper = Inf) {\n  min_q &lt;- pnorm(lower, mean, sd)\n  max_q &lt;- pnorm(upper, mean, sd)\n  u &lt;- runif(n, min = min_q, max = max_q)\n  qnorm(u, mean, sd)\n}\n\nArmed with this handy little function, and the emax_fn() function that I wrote earlier, it’s not too difficult to construct a data frame prior_draws that contains a bunch of parameter sets sampled from the “vague” priors I’m about to use when applying the stan_emax() function:\n\n# total number of draws\nn_draws &lt;- 5000\n\n# table of parameters\nprior_draws &lt;- tibble(\n  id = 1:n_draws,\n  emax  = rnorm(n_draws, mean = 10, sd = 100),\n  ec50  = rtnorm(n_draws, mean = 2000, sd = 5000, lower = 0),\n  e0    = rnorm(n_draws, mean = 10, sd = 100),\n  sigma = rtnorm(n_draws, mean  = .6, sd = 1, lower = 0)\n)\n\nHere’s what the prior distribution of parameters looks like…\n\nprior_draws |&gt; \n  pivot_longer(\n    cols = emax:sigma, \n    names_to = \"parameter\", \n    values_to = \"value\"\n  ) |&gt; \n  ggplot(aes(parameter, value)) + \n  geom_violin(draw_quantiles = .5) +\n  facet_wrap(~parameter, scales = \"free\", nrow = 1) + \n  theme_bw()\n\n\n\n\n\n\n\n\nThe key thing to note here is that using a truncated normal prior for ec50 and sigma leads to a prior that can be highly skewed. That’s not inherently a bad thing, in the sense that there is absolutely no reason at all to think that a prior over ec50 (which lies on the same scale as the exposures) should be symmetric. However, it does mean that the prior mode, prior mean, and prior median will all be different from one another. In this case, for example, the mean/median/mode for my untruncated prior over ec50 was set to 2000, and this is indeed the prior mode, but the prior median is actually closer to 5000. So one needs to be rather careful when setting priors here. If you don’t spend a bit of time thinking about what they actually entail, they might not mean what you think they mean.\nIn my little explorations in this blog post, I’m happy to run with these priors, but in real life I think I’d want to pin this down a little better than I have done here. On the statistical side, if I was truly expecting an ec50 value somewhere around 2000, I don’t think I’d like to use a prior like this one where the prior median sits closer to 5000. Similarly, if the clinical pharmacology folks were telling me that I should expect a positive exposure-response relationship, my prior should strongly prefer positive emax values rather than the “anything goes” prior I’ve set here.\n\n\nImplied prior over Emax functions\nIn fact, I’m going to push this point a little further. An alternative (and sometimes better) way to think about the priors that you’ve supplied to a Bayesian model is to explore what the prior corresponds to in terms of possible data sets that the model expects to encounter. In this case, I’ll simplify this slightly and restrict myself to thinking about the possible Emax functions that are implied by my priors. To that end, I’ll construct anemax_draws data frame that maps each parameter set (well, the first 250 of them) to an Emax function so that we can draw all these functions in a single plot:\n\nemax_draws &lt;- prior_draws |&gt; \n  filter(id &lt;= 250) |&gt; \n  pmap(\\(id, emax, ec50, e0, sigma) {\n    tibble(\n      id = id,\n      exposure = 1000:10000,\n      response = emax_fn(\n        exposure = exposure, \n        emax = emax, \n        ec50 = ec50, \n        e0 = e0\n      )\n    )\n  }) |&gt; \n  bind_rows()\n\nHere’s what those 250 Emax functions look like, shown on a linear exposure scale (left) and logarithmic exposure scale (right):\npic &lt;- emax_draws |&gt; \n  ggplot(aes(exposure, response, group = id)) +\n  geom_line(alpha = .5) + \n  theme_bw()\npic \npic + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nThe take home message here, really, is that the priors I’ve specified here are very agnostic indeed. There are a lot of possible Emax functions that are encompassed by this vague prior, and while I’m willing to consider a prior like this in this blog post – after all, the whole point of the post is for me to play around with the model – in a serious modelling exercise you’d want to ask yourself ahead of time whether this is really the variety of possible Emax functions you’re expecting to see in your study data.\nBut enough about priors. Let’s actually fit the models and see what happens next."
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#fitting-the-model",
    "href": "posts/2024-11-11_emax-parameters/index.html#fitting-the-model",
    "title": "Bayesian estimation for Emax regression",
    "section": "Fitting the model",
    "text": "Fitting the model\nWe finally arrive at the point in the post where I can start applying the model to simulated data. I’ll start by using the full data set I simulated at the beginning (i.e., including the placebo and the phase 1 conditions), and using the “default” emax parameters that I’ve chosen secure in the knowledge that they are well suited to the design. Here’s what the data set looks like in this case:\npar0 &lt;- emax_parameters()\ndat0 &lt;- generate_emax_data(full_design$exposure)\nplot_emax_data(dat0, par0) \nplot_emax_data(dat0, par0) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nDespite all the exhausting legwork I’ve done in setting the stage, the process of estimating the model parameters with stan_emax() is extremely straightforward. There are three arguments we need to specify:\n\nThe formula argument provides the specification of the Emax model in standard regression notation, so in this case that would be formula = response ~ exposure\nThe data argument is used to pass the data set, so in this case we se data = dat0\nThe priors argument is used to specify the (truncated) normal prior for the Emax regression model. Having written the convenience function emax_priors(), all I have to do here is priors = emax_priors(par0)\n\nHere we go:\n\nmod0 &lt;- stan_emax(\n  formula = response ~ exposure, \n  data = dat0,\n  priors = emax_priors(par0)\n)\n\nNormally, when you run this command you would see all the messaging that accompanies any other stan model fitting procedure. However, I’ve suppressed the messages here to keep the blog post clean(ish). If we print the mod0 object that stan_emax() returns, we can see that it gives us a convenient little summary of the posterior distribution, some point estimates of parameters, and some helpful hints about what other functions we can call to extract more information from this model object:20\n\nmod0\n\n---- Emax model fit with rstanemax ----\n\n         mean se_mean     sd    2.5%     25%     50%     75%   97.5%\nemax     9.95    0.00   0.20    9.56    9.82    9.94   10.08   10.33\ne0      10.08    0.00   0.12    9.83   10.00   10.08   10.16   10.33\nec50  4116.54    5.94 251.66 3647.91 3944.70 4108.53 4279.38 4646.47\ngamma    1.00     NaN   0.00    1.00    1.00    1.00    1.00    1.00\nsigma    0.57    0.00   0.03    0.52    0.55    0.57    0.58    0.62\n        n_eff Rhat\nemax  2498.59    1\ne0    1743.98    1\nec50  1797.71    1\ngamma     NaN  NaN\nsigma 3151.35    1\n\n* Use `extract_stanfit()` function to extract raw stanfit object\n* Use `extract_param()` function to extract posterior draws of key parameters\n* Use `plot()` function to visualize model fit\n* Use `posterior_predict()` or `posterior_predict_quantile()` function to get\n  raw predictions or make predictions on new data\n* Use `extract_obs_mod_frame()` function to extract raw data \n  in a processed format (useful for plotting)\n\n\nTo get a sense of what we can do with some of these helper functions, I’ll just call the plot method and see what it creates:\n\nplot(mod0)\n\n\n\n\n\n\n\n\nOkay yes that’s a sensible default: the plot() method returns a ggplot2 object that can be styled however you like. The plot itself shows a scatterplot of the data, the estimated regression line, and a 90% credible interval around that line. The plot method contains additional arguments that you can use to customise the output. Of particular interest are these four:\n\nshow.ci is a logical value indicating whether a credible interval should be displayed (default is TRUE)\nshow.pi is a logical value indicating whether a prediction interval should be displayed (default is FALSE)\nci and pi refer to the range of the intervals (default is 0.9 in both cases)\n\nSo, for instance, if I wanted to display a 95% prediction interval instead of a 90% credible interval, and wanted to use the black and white theme I’ve been using for other plots in this blog post, I could do this:\n\npic &lt;- plot(mod0, show.ci = FALSE, show.pi = TRUE, pi = 0.95) + theme_bw()\npic\n\n\n\n\n\n\n\n\nThe fact that this is a ggplot object makes it easy to replot this on a log-exposure scale:\n\npic + scale_x_log10()\n\n\n\n\n\n\n\n\nAs much as I like the plot that gets returned by the plot() method, it’s not quite what I want to look at in this post. I’m more curious about the posterior distribution itself, so instead I’ll use the extract_param() function to recover the parameter sets sampled by stan:\n\nsmp0 &lt;- extract_param(mod0)\nsmp0\n\n# A tibble: 4,000 × 6\n   mcmcid  emax    e0  ec50     gamma     sigma\n    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl[1d]&gt; &lt;dbl[1d]&gt;\n 1      1 10.2  10.0  4141.         1     0.552\n 2      2 10.1   9.99 4103.         1     0.550\n 3      3  9.63  9.96 3652.         1     0.550\n 4      4  9.97 10.0  4026.         1     0.555\n 5      5 10.2   9.99 4248.         1     0.593\n 6      6  9.92 10.2  4325.         1     0.580\n 7      7 10.0   9.90 3793.         1     0.595\n 8      8  9.73  9.92 3639.         1     0.560\n 9      9  9.65  9.97 3759.         1     0.545\n10     10 10.1  10.2  4528.         1     0.544\n# ℹ 3,990 more rows\n\n\nThen I’ll write a handy little helper function that returns a list of pairwise scatterplots so that I can better visualise the joint distribution over the parameters:\n\nplot_emax_pars &lt;- function(data, par = list()) {\n  \n  plot_pair &lt;- function(data, x, y, true_x, true_y) {\n    est &lt;- data |&gt; \n      summarise(\n        mx = mean({{x}}),\n        my = mean({{y}})\n      )\n    ggplot(data, aes({{x}}, {{y}})) + \n      geom_point(size = .5, color = \"lightblue\") + \n      annotate(\"point\", x = true_x, y = true_y, color = \"red\", size = 4) +\n      annotate(\"point\", x = est$mx, y = est$my, color = \"darkblue\", size = 4) +\n      theme_bw()\n  }\n  \n  list(\n    plot_pair(data, emax, ec50, par$emax, par$ec50),\n    plot_pair(data, emax, e0, par$emax, par$e0),\n    plot_pair(data, ec50, e0, par$ec50, par$e0)\n  )\n}\n\nHere’s what we get:\nsmp0 |&gt; \n  plot_emax_pars(par0) |&gt; \n  walk(print)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis looks good to me. The estimated parameter values (dark blue dots) are pretty close to the true parameter values (red dots), and the true values are well within the high-density region spanned by the posterior samples (light blue dots). The posterior looks pretty close to a multivariate normal, there aren’t strong correlations among parameters, and there’s not even a lot of uncertainty (i.e., the axis scales aren’t very wide) either. For the record, here’s the posterior correlations:\n\nsmp0 |&gt; \n  select(emax, e0, ec50, sigma) |&gt; \n  cor()\n\n               emax           e0         ec50         sigma\nemax   1.0000000000 -0.276390637  0.465365131  0.0001689484\ne0    -0.2763906372  1.000000000  0.679605961 -0.0064145249\nec50   0.4653651313  0.679605961  1.000000000 -0.0027118046\nsigma  0.0001689484 -0.006414525 -0.002711805  1.0000000000\n\n\nYep, all good here. Mission accomplished, good job everyone, we can all go home now."
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#identifiability-issues",
    "href": "posts/2024-11-11_emax-parameters/index.html#identifiability-issues",
    "title": "Bayesian estimation for Emax regression",
    "section": "Identifiability issues",
    "text": "Identifiability issues\nWell, perhaps not. Earlier in the post I heavily foreshadowed the fact that I would take a look at what happens when the exposure values don’t include the placebo group or the phase 1 dose escalation samples. When those are removed from the data set we start to encounter problems associated with a restriction of range, and this really matters for an Emax regression model. There’s three versions of this scenario to consider, depending on whether the exposure range is restricted but happens to have the true value of ec50 nicely centred in the middle of the range (scenario 1), if ec50 lies at the lower end of the observed exposures (scenario 2), or if ec50 is at the upper end (scenario 3). In all three cases we end up with some identifiability issues, but they aren’t equally severe.\n\nScenario 1\nSo let’s start with the first scenario. This happens to correspond to the dat1 data set I constructed earlier, but as a little reminder, here’s the what we’re trying to fit:\nplot_emax_data(dat1, par1) \nplot_emax_data(dat1, par1) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nYou can see in the plot on the right (log-exposures) that we might have some difficulties, because the exposure-response curve looks rather linear here. But there are some hints of curvature at both ends, so perhaps it won’t be too bad?\nLet’s have a look. First, we fit the model:\n\nmod1 &lt;- stan_emax(\n  formula = response ~ exposure, \n  data = dat1,\n  priors = emax_priors(par1)\n)\n\nNext, we’ll see what the model predictions look like by calling the plot() method:\nbase &lt;- plot(mod1, show.ci = FALSE, show.pi = TRUE, pi = 0.95)\nbase + theme_bw()\nbase + theme_bw() + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nNow let’s visualise the posterior samples:\nsmp1 &lt;- extract_param(mod1)\nsmp1 |&gt; \n  plot_emax_pars(par1) |&gt; \n  walk(print)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we have a glaringly strong posterior correlation between e0 and ec50, which will cause us some difficulty, but apart from that obvious concern, this isn’t the worst outcome in the world. The posterior mean (dark blue dots) for the three parameters I’m most interested in sits pretty close to the true parameter values (red dots) that generated the data set, and the posterior distribution (light blue dots) is reeeeaaaassssonably close to being a multivariate normal distribution, and as such it’s not unreasonable to express the patterns by computing a variance-covariance matrix or a correlation matrix:\n\nsmp1 |&gt; \n  select(emax, e0, ec50, sigma) |&gt; \n  cor()\n\n             emax          e0       ec50      sigma\nemax   1.00000000 -0.01960671 0.39357381 0.02890908\ne0    -0.01960671  1.00000000 0.89987537 0.03791181\nec50   0.39357381  0.89987537 1.00000000 0.05428227\nsigma  0.02890908  0.03791181 0.05428227 1.00000000\n\n\nConfirming the visual pattern observed in the rightmost plot, we have a bit of an identifiability problem: a posterior correlation of about \\(r = 0.9\\) between ec50 and e0 isn’t great. Confirming the intuition earlier in the post, because we don’t have a placebo group to help us pin down the baseline response, the Emax regression ends up unable to distinguish these parameters. You could work with this, but a lot of care would be required in interpreting the results.\n\n\nScenario 2\nIn the first example it’s fairly clear that Emax regression can have model identifiability problems when we don’t have a wide enough range of exposures represented in the data. However, you might be inclined not to worry too much in the previous example because – high correlations notwithstanding – our point estimates for the parameters were actually pretty good. However, this turns out to be a cold comfort at best, as the next scenario illustrates.\nLast time around, we were a little fortunate: the true value of ec50 was set to 4500, which is is pretty close to the median value in the exposure vector. An alternative case to think about is one where the true value of ec50 is substantially below the median exposure in your data set. So let’s repeat the simulation exactly as before, but this time I’ll set ec50 = 2000 when generating the data set, placing it somewhere around the 10th percentile for exposures. Here’s what we get:\npar2 &lt;- emax_parameters(ec50 = 2000) # around 10th percentile\ndat2 &lt;- generate_emax_data(small_design$exposure, par2)\n\nplot_emax_data(dat2, par2) \nplot_emax_data(dat2, par2) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nWith the benefit of knowing that this is a simulation, and thus having the ability to draw the red lines that show us exactly what region of the exposure range is poorly represented in our data set, we can already see this is going to end badly. Even on the linear scale (left plot) we can see the problem: we don’t have a placebo group here, and the lowest response values in the data are nowhere near the actual value of e0 for the baseline response.\nOn the log-exposure scale the problem is even more painfully obvious: according to the Emax model the response should be a logistic function (3 parameters) of the log-exposure, but over the range we have data for the function looks linear (2 parameters). There is no chance whatsoever that any statistical method will be able to use these data to identify all our parameters. The signal we need is simply not there in the data.\nLet’s watch the trainwreck unfold, shall we? First, let’s fit the model:\n\nmod2 &lt;- stan_emax(\n  formula = response ~ exposure, \n  data = dat2,\n  priors = emax_priors(par2)\n)\n\nNext, let’s have a look at the posterior predictions. These actually look okay on the whole:\nbase &lt;- plot(mod2, show.ci = FALSE, show.pi = TRUE, pi = 0.95)\nbase + theme_bw()\nbase + theme_bw() + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nAh, but what about the parameter estimates, I hear you ask? Well, that’s when everything goes pear-shaped:\nmod2 |&gt; \n  extract_param() |&gt; \n  plot_emax_pars(par2) |&gt; \n  walk(print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod2 |&gt; \n  extract_param() |&gt; \n  select(emax, e0, ec50, sigma) |&gt; \n  cor()\n\n             emax          e0        ec50       sigma\nemax   1.00000000 -0.96880835 -0.82131556 -0.01762361\ne0    -0.96880835  1.00000000  0.92715674  0.03022421\nec50  -0.82131556  0.92715674  1.00000000  0.06361717\nsigma -0.01762361  0.03022421  0.06361717  1.00000000\n\n\nSo yeah, it’s bad. Not only are all three parameters correlated in an unpleasant way, the posterior distribution is starting to look rather non-normal so it’s not even obvious we should trust the correlation matrix to be telling us anything except the fact that we have severe model identifiability problems here. Sure, your model fits are actually pretty good as the plot below illustrates, but you have almost no idea what the values of emax, e0, or ec50 actually are. In the end, even though the true relationship is an Emax function, the impoverished nature of the data means that attempting to fit an Emax regression model to the data in this situation becomes a classic case of overfitting.\n\n\nScenario 3\nIn scenario 1, the data set had a limited range of exposures but as it happened the true value of ec50 is right in the middle of that range. We ended up with some identifiability problems, but not horribly bad ones. In scenario 2, however, the true value of ec50 was at the lower end of our observed exposures: that turned out to be a huge problem for us.\nSo what happens when the true value of ec50 is at the upper end of the measured exposure levels? To explore this, I’ll repeat the same exercise but this time set the true value of ec50 to 6500 units, which puts it at about the 80th percentile of the exposures represented in our exposure vector. Here’s what the data look like in this situation:\npar3 &lt;- emax_parameters(ec50 = 6500) # around 80th percentile\ndat3 &lt;- generate_emax_data(small_design$exposure, par3)\n\nplot_emax_data(dat3, par3) \nplot_emax_data(dat3, par3) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nThis is essentially the inverse of what we saw in scenario 2. The data don’t look terrible at the lower end of the observed exposure range, but at the upper end we have very few samples to work with. We might have problems again. Let’s fit the model:\n\nmod3 &lt;- stan_emax(\n  formula = response ~ exposure, \n  data = dat3,\n  priors = emax_priors(par3)\n)\n\nAs before, the posterior predictions aren’t too bad. The model does actually fit the data and make reasonable predictions about where future data might fall:\nbase &lt;- plot(mod3, show.ci = FALSE, show.pi = TRUE, pi = 0.95)\nbase + theme_bw()\nbase + theme_bw() + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nBut once again, when we look at the posterior samples to get a sense of what we can plausibly estimate for the parameters, we can see that we again have problems:\nmod3 |&gt; \n  extract_param() |&gt; \n  plot_emax_pars(par3) |&gt; \n  walk(print)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmod3 |&gt; \n  extract_param() |&gt; \n  select(emax, e0, ec50, sigma) |&gt; \n  cor()\n\n            emax         e0       ec50      sigma\nemax  1.00000000 0.40394014 0.76927912 0.05048414\ne0    0.40394014 1.00000000 0.87900916 0.02350481\nec50  0.76927912 0.87900916 1.00000000 0.04869102\nsigma 0.05048414 0.02350481 0.04869102 1.00000000\n\n\nOn the whole this isn’t quite as bad as scenario 2, but it’s not great either. You could probably extract something fairly sensible from this, but it would need to be caveatted very heavily in any write-up."
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#a-final-word-of-warning",
    "href": "posts/2024-11-11_emax-parameters/index.html#a-final-word-of-warning",
    "title": "Bayesian estimation for Emax regression",
    "section": "A final word of warning",
    "text": "A final word of warning\nNaturally, I have saved the worst for last.21 The last scenario to consider is one where the true effect size is very close to zero, and everything goes to hell. You would hope never to see this in a real exposure-response modelling situation, but I’ll show it here because it illustrates the absurdity of trying to fit a model to data that contain no signal at all. To that end, here’s a data set that is “technically” generated using the Emax model, but the signal is so weak that we might as well admit it’s pure random noise:\npar4 &lt;- emax_parameters(emax = .1) # very small effect\ndat4 &lt;- generate_emax_data(small_design$exposure, par4)\n\nplot_emax_data(dat4, par4) \nplot_emax_data(dat4, par4) + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nIn real life, what you should do here is immediately stop trying to do an exposure-response analysis at all. There’s nothing you can pull out of the data. But let’s imagine we were stupendously brave/unwise, and attempted to do so anyway:\n\nmod4 &lt;- stan_emax(\n  formula = response ~ exposure, \n  data = dat4,\n  priors = emax_priors(par4)\n)\n\nWhen you look at the output in this blog post, you’d think everything went fine. But that’s only because I’ve silenced all the warnings that stan is throwing and oh my there are a lot of them. The Emax model is not well-specified for this kind of data, and stan does not like it when you try to draw from a posterior density that is utter garbage. Not surprisingly, when we attempt to plot model predictions for this data we don’t get anything very helpful:\nbase &lt;- plot(mod4, show.ci = FALSE, show.pi = TRUE, pi = 0.95)\nbase + theme_bw()\nbase + theme_bw() + scale_x_log10()\n\n\n\n\n\n\n\n\n\n\nI mean, of course. There’s no signal of any kind in the data here, and the prediction intervals make it very obvious that there’s no signal. Duh.\nNow comes the “fun” part. All those stan warnings that I’ve silenced in the output here? Well, if you’re at all curious about why stan was screaming at us, let’s take a look at the distribution of “posterior samples” that it actually produced:22\nmod4 |&gt; \n  extract_param() |&gt; \n  plot_emax_pars(par4) |&gt; \n  walk(print)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is… utterly pathological. The model is attempting to estimate three parameters, from a data set that contains absolutely no information about any of them.23"
  },
  {
    "objectID": "posts/2024-11-11_emax-parameters/index.html#footnotes",
    "href": "posts/2024-11-11_emax-parameters/index.html#footnotes",
    "title": "Bayesian estimation for Emax regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI actually have a work-related project on my desk at the moment involving this package, so part of my motivation here was to familiarise myself with the package before diving into the real work.↩︎\nThis is… something of an understatement. The election brought some of the most hateful people in the world out in public, and they were gloating about the fact that other people were going to suffer. It was not simply an abstraction for me: I had people threatening to kill me (and everyone who belongs to my demographic) on election day. In Sydney. To my face. At my local pub. It’s very hard to just pick up and go on with everyday life as if everything were fine and normal under those circumstances.↩︎\nYou can run Emax models with binary outcomes, in which case the approach is to recast this within a logistic regression framework, but in the interest of simplicity I’m not going to cover that situation in this post.↩︎\nI have now been working in pharamacometrics long enough to be painfully aware that there are a lot of different measures used to formalise some notion of “drug exposure” that have different properties and different applicability to specific problems. This post is not for talking about such things: I will merely assume that some measure of exposure exists and has been chosen sensibly.↩︎\nThis function doesn’t really need the dots. I’m adding them here solely so that emax_fn() can silently ignore parameters that aren’t used in the model, which happens later in the document because the model also has a sigma parameter to describe residual variance. If I were a less lazy woman I’d write this in a tighter way but this is just a blog post, and the level of rigour I’m aiming for is not super high here.↩︎\nWhen \\(\\gamma=1\\) this is the usual two-parameter logistic function, but more generally Emax uses the three-parameter logistic curve.↩︎\nAs you can see in the code I’ve actually used a truncated lognormal, chopping off the lower and upper 1% of the distribution. That subtlety does matter a bit, actually, because extreme values of the exposures end up being high-leverage points when you fit the regression model (I think: to be honest I haven’t bothered to look at this in detail), and I don’t want to deal with outliers in this post.↩︎\nFor the pop-PK modeling stage that typically precedes an exposure-response analysis, the placebo samples aren’t of much interest because the drug concentration is zero. But placebo samples can be very useful in exposure-response, since they help us determine baseline response levels, so I’ve included a placebo group here.↩︎\nYes, I know this isn’t a particularly realistic representation of a phase 1 dose finding study. I’m really adding it just as a way of creating at least one version of the data set that doesn’t suffer from issues that arise when you have a truncated range of exposures. Hey, I have to make some simplifications here otherwise I’ll never make it to the regression section of the post.↩︎\nSigh. Yes, I know: the dose I’ve passed to generate_exposure() in the placebo condition is weird: you wouldn’t actually expect a .01 mg dose in a placebo condition, this is purely a hack I introduced so that the data from the placebo condition doesn’t look super weird↩︎\nDouble sigh. I am not considering the role played by BLQ censoring either. Look, this is a blog post: I am not trying to write an academic paper here. I’ve had quite enough of that for one lifetime already tyvm.↩︎\nA part of me wants to discuss kurtosis at this point but perhaps I should just bite my tongue instead? You don’t make friends with kurtosis.↩︎\nI should mention as an aside I’m being a little imprecise in my use of the term “design”. From a scientific perspective we would usually think of the design in terms of the number of subjects, the dosing regimes and so on, and treat the exposures as part of the stochastic outcome when the design is realised in data. In a regression context though the exposures are treated as fixed, so in that sense the \\(x_i\\) values are part of the design matrix even though they aren’t controlled by the researcher. In an earlier version of this post I actually did run a variation of all these simulations where I simulated many possible sets of exposures conditional on the experimental design, in order to see if the pattern of results I discuss here are representative of what happens in general. But then the post got even longer and I abandoned that effort. Such is life.↩︎\nWell, simulated data. Whatever. It’s a moot point for this post.↩︎\nAnd, to a lesser extent, the high-exposure samples from a phase 1 study.↩︎\nI will resist the temptation to dive into a philosophical discussion of what a Bayesian prior actually means statistically. There are many opinions. None are relevant to this post.↩︎\nThis is of course a convenient fiction, but so is everything else in science and statistics. As always, the question we actually care about is whether it is a useful and safe fiction to rely upon in our data analysis.↩︎\nThat’s true for gamma too, actually, but I’m not talking about that parameter in this post.↩︎\nYes I know there are already tools for doing this, hush.↩︎\nThe object returned is an S3 classed object of class “rstanemax”.↩︎\nI am the antithesis of Vanessa Redgrave↩︎\nI use scare quotes here because when you have numerical issues as severe as the ones that you get with this data set, the samples that you end up with can’t realistically be considered to be actual samples from the posterior. They’re diagnostic in the sense they can tell you what the hell went wrong, but it would be an error to think this is exactly what the posterior density looks like in this situation.↩︎\nYou might be wondering why I even bothered to think about this case. The answer, for whatever its worth, is that I had a client question a while back that related (rather loosely) to the question of how you would do formal model selection procedures for an Emax regression model, which led me to thinking about what you would do in an orthodox setting to handle the awkward case when the null model (e.g., no exposure-reponse relationship at all) is true. As this little simulation illustrates, this can be tricky because when the null is true, you can’t actually fit the Emax model to the data in a way that yields meaningful undertainty estimates. At best you could compute an AIC statistic or the like, because you can find the negative log-likelihood here, but if someone were to insist on computing p-values I wouldn’t really know how: imagine trying to construct a Wald statistic to test any specific parameter of this model? The standard errors wouldn’t make any sense at all. It seems to me that this would be entirely meaningless. But that’s a bit of a digression and I didn’t really want to dive deep on that in this blog post.↩︎"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html",
    "title": "Data types in Arrow and R",
    "section": "",
    "text": "Manuals for translating one language into another can be set up in divergent ways, all compatible with the totality of speech dispositions, yet incompatible with one another      – William Van Orman Quine, 1960, Word and Object\nAt the 2018 useR! conference in Brisbane, Roger Peng gave a fabulous keynote talk on teaching R to new users in which he provided an overview of the history of the language and how it is used in the broader community. One thing that stood out to me in his talk – and I’ve seen reflected in other data – is that R is unusual as a language because it’s not designed primarily for programmers. Software engineering practices have now become widespread in the R community, and that’s a good thing. Nevertheless, a very large proportion of the R community don’t have a traditional computer science background – and that’s okay! In fact, given the goals of the language that’s a good thing too.\nR is a language designed with a practical goal in mind: it is a tool for statistical programming and data analysis. Because of this design focus, R users tend to care most deeply about the tasks that make up their day to day jobs. Few of us care about the IEEE 754 standard for encoding floating point numbers. R users are not typically interested in the big-endian/little-endian distinction. The purpose of R as a high level statistical programming environment is to abstract away from these things, and to allow users to focus on data cleaning, wrangling, and visualisation. R tries to help you get to your data as easily as possible, build models for your data, report those models reliably, and so on. Because that’s the job.\nBut.\nThere’s always a “but”, isn’t there?\nArt\nOne of the huge changes in the data science ecosystem in recent years is the change in scale of our data sets. Data sets can now easily encompass billions of rows, and surpass the ability of your machine (and R) to hold in memory. Another huge change in the ecosystem is the proliferation of tools. Data sets have to be passed from one system to another, and when those data sets are large, problems follow. Apache Arrow solves these problems by providing a multi-language toolbox for data exchange and data analysis. It’s a toolbox designed for a big data environment, and a many-language environment. From the perspective of an R user, it supplies the arrow package that provides an interface to Apache Arrow, and through that package allows you to have access to all the other magic that Arrow exposes. It’s an extremely powerful toolbox… but to use it effectively you do need to learn more of those low-level concepts that we as R users like to skim over.\nThis post is an attempt to fill that gap for you! It’s a long form post, closer to a full length article than a typical blog. My goals in this post are to:\nThis post isn’t intended to be read in isolation. It’s the third part of a series I have been writing on Apache Arrow and R, and it probably works best if you’ve read the previous two. I’ve made every effort to make this post self-contained and self-explanatory, but it does assume you’re comfortable in R and have a little bit of knowledge about what the arrow package does. If you’re not at all familiar with arrow, you may find it valuable to read the first post in the series, which is a getting started post, and possibly the second one that talks about the arrow dplyr backend.\nStill keen to read? I haven’t scared you off?\nNo?\nFabulous! Then read on, my loves!\nlibrary(tibble)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(arrow)\n\n\nAttaching package: 'arrow'\n\n\nThe following object is masked from 'package:utils':\n\n    timestamp"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#regarding-magic",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#regarding-magic",
    "title": "Data types in Arrow and R",
    "section": "Regarding magic",
    "text": "Regarding magic\nConsider this piece of magic. I have a csv file storing a data set. I import the data set into R using whatever my favourite csv reader function happens to be:\n\nmagicians &lt;- read_csv_arrow(\"magicians.csv\")\nmagicians\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# ℹ 55 more rows\n\n\nThen I decide to “copy the data into Arrow”.1 I do that in a very predictable way using the arrow_table() function supplied by the arrow package:\n\narrowmagicks &lt;- arrow_table(magicians)\narrowmagicks\n\nTable\n65 rows x 6 columns\n$season &lt;int32&gt;\n$episode &lt;int32&gt;\n$title &lt;string&gt;\n$air_date &lt;date32[day]&gt;\n$rating &lt;double&gt;\n$viewers &lt;double&gt;\n\n\nThis is exactly the output I should expect, but the longer I think about it the more it seems to me that something quite remarkable is going on. Some magic is in play here, and I want to know how it works.\nTo understand why I’m so curious, consider the two objects I now have. The magicians data set is a data frame (a tibble, technically) stored in R. The arrowmagicks data set, however, is a pointer to a data structure stored in Arrow. That data structure is a Table object. Table objects in Arrow are roughly analogous to data frames – both represent tabular data with columns that may be of different types – but they are not the same. The columns of a Table are built from objects called ChunkedArrays that are in turn constructed from Arrays, and those Arrays can contain Scalar objects. In other words, to move data from one language to another an act of translation is required, illustrated below:\n\nIt’s not standard, but since this is a post about data types, I’ll italicise the names of data types in both R and Arrow (e.g., data.frame, Table). It gets a bit tiresome, but I think it’s helpful\n\n\n\n\n\n\nA miniature translation guide. On the left a data frame in R is shown: it is comprised of three columns. Each columns is an R vector. We use the term ‘element’ to refer to any length-1 constituent of a vector, even though it isn’t really a distinct object in its own right. On the right is a Table in Arrow: it too is comprised of three columns, encoded as ChunkedArrays. Each ChunkedArray is comprised of one or more Arrays, and each Array contains one or more Scalars, which (unlike elements of R vectors) are distinct objects. The data structure that translates one into the other is called a Schema.\n\n\n\n\nIn this post I’m not going to talk much about the difference between Arrays and ChunkedArrays, or why Arrow organises Tables this way (that will be the topic of a later post). For now it’s enough to recognise that Arrow does have this additional structure: the Table data type in Arrow is not equivalent to the data frame class in R, so a little work is required to map one to the other.\nA similar story applies when we look at the contents of the data set. The translation process doesn’t just apply to the “container” object (i.e., the data frame in R and the Table in Arrow), it also applies to the values that the object contains. If we look at the how the columns of magicians and arrowmagicks are labelled, we see evidence of this translation. The integer columns in R have been mapped to int32 columns in Arrow, Date columns in R become date32 columns in Arrow, and so on.\n\nVariable names like arrowmagicks and function calls like arrow_table() are shown in monospace typewriter font\n\nThere’s quite a lot of complexity to the translation process, yet it all seems to work seamlessly, and it works both ways. I can pull the arrowmagicks data back into R and recover the original data:\n\ncollect(arrowmagicks)\n\n# A tibble: 65 × 6\n   season episode title                                air_date   rating viewers\n    &lt;int&gt;   &lt;int&gt; &lt;chr&gt;                                &lt;date&gt;      &lt;dbl&gt;   &lt;dbl&gt;\n 1      1       1 Unauthorized Magic                   2015-12-16    0.2    0.92\n 2      1       2 The Source of Magic                  2016-01-25    0.4    1.11\n 3      1       3 Consequences of Advanced Spellcasti… 2016-02-01    0.4    0.9 \n 4      1       4 The World in the Walls               2016-02-08    0.3    0.75\n 5      1       5 Mendings, Major and Minor            2016-02-15    0.3    0.75\n 6      1       6 Impractical Applications             2016-02-22    0.3    0.65\n 7      1       7 The Mayakovsky Circumstance          2016-02-29    0.3    0.7 \n 8      1       8 The Strangled Heart                  2016-03-07    0.3    0.67\n 9      1       9 The Writing Room                     2016-03-14    0.3    0.71\n10      1      10 Homecoming                           2016-03-21    0.3    0.78\n# ℹ 55 more rows\n\n\nIn this example the translation back and forth “just works”. You really don’t have to think too much about the subtle differences in how Arrow and R “think about the world” and how their data structures are organised. And in general that’s what we want in a multi-language toolbox: we want the data analyst to be thinking about the data, not the cross-linguistic subtleties of the data structures!\nThat being said, it’s also valuable to give the data analyst flexibility. And that means we’re going to need to talk about Schemas. As shown in the “translation diagram” above, Schemas are the data structure arrow uses to govern the translation between R and Arrow, and since I’m going to be talking about data “on the R side” and data “on the Arrow side” a lot, it will be helpful to have some visual conventions to make it a little clearer. Throughout the post you’ll see diagrams showing the default mappings that the arrow package uses when converting data columns from R to Arrow and vice versa. In each case I’ll show R data types on the left hand side (against a blue background) and Arrow data types on the right hand side (against an orange background), like this:\n\n\n\n\n\nIllustration of the graphical convention used in the later diagrams, showing R on the left side (against a blue background) and Arrow on the right side (against an orange background)."
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#defining-schemas",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#defining-schemas",
    "title": "Data types in Arrow and R",
    "section": "Defining Schemas",
    "text": "Defining Schemas\nThe arrow package makes very sensible default choices about how to translate an R data structure into an Arrow data structure, but those choices can never be more than defaults because of the fundamental fact that the languages are inherently different. The quote about the indeterminacy of translation at the top of this post was originally written about natural languages, but I think it applies in programming too. There’s no single rulebook that tells you how to translate between R and Arrow: there can’t be.2\nSuppose that I knew that there would in fact be a “Season 5.1648” coming, consisting of a single episode that would air not only on a specific date, but at a specific time that would – for some bizarre reason3 – be important to encode in the data. Knowing that this new data point is coming, I’d perhaps want my Arrow data to encode season as a numeric variable, and I’d need to encode the air_date field using a date type that implicitly encodes time of day. I can do this with the schema() function:\n\ntranslation &lt;- schema(\n  season = float64(), # not the default\n  episode = int32(),\n  title = utf8(), \n  air_date = date64(), # not the default\n  rating = float64(),\n  viewers = float64()\n)\n\nNow I can use my schema to govern the translation:\n\narrowmagicks2 &lt;- arrow_table(magicians, schema = translation)\narrowmagicks2\n\nTable\n65 rows x 6 columns\n$season &lt;double&gt;\n$episode &lt;int32&gt;\n$title &lt;string&gt;\n$air_date &lt;date64[ms]&gt;\n$rating &lt;double&gt;\n$viewers &lt;double&gt;\n\n\nThe output may not make complete sense at this point, but hopefully the gist of what I’ve done should be clear. The season is no longer stored as an integer (it’s now a numeric type), and the air_date no longer uses “day” as the unit of encoding, it uses “ms” (i.e., millisecond). I’ve accomplished my goals. Yay!\nThis is of course a toy example, as are all the other examples you’ll encounter in this post. But the underlying issues are important ones!\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#why-mapping-languages-is-hard",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#why-mapping-languages-is-hard",
    "title": "Data types in Arrow and R",
    "section": "Why mapping languages is hard",
    "text": "Why mapping languages is hard\n\nOrganising the world into concepts (or data structures) is hard.4 We define ontologies that impose order on a chaotic world, but those structures are rarely adequate to describe the world as it is. While doing background research for this post I spent a little time reading various sections from An Essay Towards a Real Character, and a Philosophical Language, a monograph written by John Wilkins in 1668 that makes a valiant (but doomed… oh so doomed) attempt to organise all the categories of things and propose a mechanism by which we could describe them within a single universal language. The classification systems he came up with were… not great. For example, he divided BEASTS into two categories: VIVIPAROUS beasts are those that bear live young, whereas OVIPAROUS beasts are those that lay eggs. The viviparous ones could be subdivided into WHOLE-FOOTED ones and CLOVEN-FOOTED ones. The cloven-footed beasts could be subdivided into those that were RAPACIOUS and those that were not. RAPACIOUS types could be of the CAT-KIND or the DOG-KIND.\nSuffice it to say the poor man had never encountered a kangaroo.\nThe problem with trying to construct universal ontologies is that these things are made by humans, and humans have a perspective that is tied to their own experience and history. As a 17th century English gentleman, Wilkins saw the world in a particular way, and the structure of the language he tried to construct reflected that fact.\nI am of course hardly the first person to notice this. In 1952 the Argentinian author Jorge Luis Borges published a wonderful essay called The Analytical Language of John Wilkins that both praises Wilkins’ ambition and then carefully illustrates why it is necessarily doomed to fail. Borges’ essay describes a classification system from an fictitious “Celestial Emporium of Benevolent Knowledge” which carves up the beasts as follows:\n\nIn its remote pages it is written that the animals are divided into: (a) belonging to the emperor, (b) embalmed, (c) tame, (d) sucking pigs, (e) sirens, (f) fabulous, (g) stray dogs, (h) included in the present classification, (i) frenzied, (j) innumerable, (k) drawn with a very fine camelhair brush, (l) et cetera, (m) having just broken the water pitcher, (n) that from a long way off look like flies\n\nNow, it’s pretty unlikely that any human language would produce a classification system quite as chaotic as Borges’ fictional example, but the point is well made. Actual classification systems used in different languages and cultures are very different to one another and often feel very alien when translated. It’s a pretty fundamental point, and I think it applies to programming languages too.5 Every language carries with it a set of assumptions and structures that it considers “natural”, and translation across the boundaries between languages is necessarily a tricky business.6"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#a-little-bit-of-big-picture",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#a-little-bit-of-big-picture",
    "title": "Data types in Arrow and R",
    "section": "A little bit of big picture",
    "text": "A little bit of big picture\nBefore we get to “moving data around” part it’s helpful to step back a little and recognise that R and Arrow are designed quite differently. For starters, the libarrow library to which the arrow package provides bindings is written in C++, and C++ is itself a different kind of language than R. And in a sense, that’s actually the natural place to start because it influences a lot of things in the design of arrow.\n\n\nObject oriented programming in arrow\nOne of ways in which C++ and R differ is in how each language approaches object oriented programming (OOP). The approach taken in C++ is an encapsulated OOP model that is common to many programming languages: methods belong to objects. Anyone coming from outside R is probably most familiar with this style of OOP.\nThe approach taken in R is… chaotic. R has several different OOP systems that have different philosophies, and each system has its own strengths and weaknesses.7 The most commonly used system is S3, which is a functional OOP model: methods belong to generic functions like print(). Most R users will be comfortable with S3 because it’s what we see most often. That being said, there are several other systems out there, some of which adopt the more conventional encapsulated OOP paradigm. One of the most popular ones is R6, and it works more like the OOP systems seen in other languages.\nThe arrow package uses both S3 and R6, but it uses them for quite different things. Whenever arrow does something in an “R-native” way, S3 methods get used a lot. For example, in my earlier post on dplyr bindings for Arrow I talked about how arrow supplies a dplyr engine: this works in part by supplying S3 methods for various dplyr functions that are called whenever a suitable Arrow object gets passed to dplyr. The interface between arrow and dplyr uses S3 because this context is “R like”. However, this isn’t a post about that aspect of arrow, so we won’t need to talk about S3 again in this post.\nHowever, arrow has a second task, which is to interact with libarrow, the Arrow C++ library. Because the data structures there all use encapsulated OOP as is conventional in C++, it is convenient to adhere to those conventions within the arrow package. Whenever arrow has to interact with libarrow, it’s useful to be as “C++ like” as possible, and this in turn means that the interface between arrow and libarrow is accomplished using R6. So we will be seeing R6 objects appear quite often in this post.8\n\n\n\nTable, ChunkedArray, and Scalar\nYou may be wondering what I mean when I say that R6 objects are used to supply the interface between R and Arrow. I’ll try to give some concrete examples. Let’s think about the arrow_table() function. At the start of the post I used this function to translate an R data frame into an Arrow Table, like this:\n\narrow_table(magicians)\n\nThis is a natural way of thinking about things in R, but the arrow_table() function doesn’t actually do the work. It’s actually just a wrapper function. Within the arrow package is an R6 class generator object called Table,9 and its job is to create tables, modify tables, and so on. You can create a table by using the create() method for Table. In other words, instead of calling arrow_table() I could have done this:\n\nTable$create(magicians)\n\nand I would have ended up with the same result.\nThe same pattern appears throughout the arrow package. When I used the schema() function earlier, the same pattern was in play. There is an R6 class generator called Schema, and it too has a create() method. I could have accomplished the same thing by calling Schema$create().\nI could go on like this for some time. Though I won’t talk about all of them in this post, there are R6 objects for Dataset, RecordBatch, Array, ChunkedArray, Scalar, and more. Each of these provides an interface to a data structure in Arrow, and while you can often solve all your problems without ever interacting with these objects, it’s very handy to know about them and feel comfortable using them. As the post goes on, you’ll see me doing that from time to time.\nBut enough of that! It’s time to start moving data around…\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#logical-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#logical-types",
    "title": "Data types in Arrow and R",
    "section": "Logical types",
    "text": "Logical types\nAt long last we arrive at the point where I’m talking about the data values themselves, and the simplest kind of data to talk about are those used to represent truth values. In R, these are called logical data and can take on three possible values: TRUE and FALSE are the two truth values, and NA is used to denote missing data.10 In a moment I’ll show you how to directly pass individual values from R to Arrow, but for the moment let’s stick to what we know and pass the data across as part of a tabular data structure. Here’s a tiny tibble, with one column of logical values:\n\ndat &lt;- tibble(values = c(TRUE, FALSE, NA))\ndat\n\n# A tibble: 3 × 1\n  values\n  &lt;lgl&gt; \n1 TRUE  \n2 FALSE \n3 NA    \n\n\nWe’re going to pass this across to Arrow using arrow_table() but before we do let’s talk about what we expect to happen when the data arrive at the other side.\nIn this case, it’s quite straightforward. Arrow has a boolean type that has truth values true and false that behave the same way as their cousins in R. Just like R, Arrow allows missing values, though they’re called null values in Arrow. Unlike basically every other example we’re going to see in this post, this one is straightforward because the mapping is perfect. Unless you do something to override it, the arrow package will map an R logical to an Arrow boolean and vice versa. Here’s the diagram I use to describe it:\n\n\n\n\n\nDefault mappings for logical types\n\n\n\n\nSeems to make sense, right? So let’s stop talking about it and create the corresponding Table in Arrow:\n\ntbl &lt;- arrow_table(dat)\ntbl\n\nTable\n3 rows x 1 columns\n$values &lt;bool&gt;\n\n\nHm. Okay that’s a little underwhelming as output goes? I’d like to see the actual values please. Happily the arrow package supplies a $ operator for Table objects so we can extract an individual column from tbl the same way we can from the original R object dat. Let’s try that:\n\ntbl$values\n\nChunkedArray\n&lt;bool&gt;\n[\n  [\n    true,\n    false,\n    null\n  ]\n]\n\n\nThe output looks a little different to what we’d get when printing out a single column of a tibble (or data frame), but it’s pretty clear that we’ve extracted the right thing. A single column inside an Arrow Table is stored as a ChunkedArray, so this looks right.\nYay us!\nAt this point, it’s handy to remember that the arrow_table() function that I used to move the data into Arrow is really just a wrapper that allows you to access some of the Table functionality without having to think about R6 too much. I also mentioned there’s a class generator called ChunkedArray object and a chunked_array() wrapper function. In hindsight, I probably didn’t need to bother creating the tibble and porting that over as a Table. I could have created a logical vector in R and port that over as a ChunkedArray directly:\n\nvalues &lt;- c(TRUE, FALSE, NA)\nchunked_array(values)\n\nChunkedArray\n&lt;bool&gt;\n[\n  [\n    true,\n    false,\n    null\n  ]\n]\n\n\nThat’s a cleaner way of doing things. If you want a Table, use Table and its wrappers. If you want a ChunkedArray, use ChunkedArray and its wrappers. There’s no need to over-complicate things.\nSpeaking of which… later in the post, I’ll often want to send single values to Arrow. In those cases I don’t want to create a ChunkedArray, or even the simpler unchunked Array type. What I want to pass is a Scalar.\nIt’s worth unpacking this a little. Unlike some languages, R doesn’t really make a strong distinction between “vectors” and “scalars”: an R “scalar” is just a vector of length one. Arrow is stricter, however. A ChunkedArray is a container object with one or more Arrays, and an Array is also a container object with one or more Scalars. If it helps, you can think of it a little bit like working with lists in R: if I have a list lst, then lst[1] is still a list. It doesn’t return the contents of the list. If I want to extract the contents I have to use lst[[1]] to pull them out. Arrow Arrays contain Scalars in a fashion that we would call “list-like” in R.\nIn any case, the important thing to recognise is that arrow contains a class generator object called Scalar, and it works the same way as the other ones. The one difference is that there aren’t any wrapper functions for Scalar, so I’ll have to use Scalar$create() directly:\n\nScalar$create(TRUE, type = boolean())\n\nScalar\ntrue\n\n\nIn this example I didn’t really need to explicitly specify that I wanted to import the data as type = boolean(). The value TRUE is an R logical, and the arrow default is to map logicals onto booleans. I only included it here because I wanted to call attention to the type argument. Any time that you want to import data as a non-default type, you need to specify the type argument. If you look at the list of Apache Arrow data types on the arrow documentation page, you’ll see quite a lot of options. For now, the key thing to note is that the type argument expects you to call one of these functions.\nAnyway, that’s everything I had to say about logicals. Before moving on though, I’m going to write my own wrapper function, and define scalar() as an alias for Scalar$create():\n\nscalar &lt;- function(x, type = NULL) {\n  Scalar$create(x, type)\n}\n\nThe main reason I’m doing that is for convenience, because in this post I’m actually going to need this wrapper function a lot. So I should probably check… does it work?\n\nscalar(TRUE)\n\nScalar\ntrue\n\n\nAwesome!\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#integer-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#integer-types",
    "title": "Data types in Arrow and R",
    "section": "Integer types",
    "text": "Integer types\nWhen translating R logicals to Arrow booleans, there aren’t a lot of conceptual difficulties. R has one data structure and Arrow has one data structure, and they’re basically identical. This is easy. Integers, however, are a little trickier because there’s no longer an exact mapping between the two languages. Base R provides one integer type, but Arrow provides eight distinct integer types that it inherits from C++. As a consequence it will no longer be possible to provide one-to-one mappings between R and Arrow, and some choices have to be made. As we’ll see in this section, the arrow package tries very hard to set sensible default choices, and in most cases these will work seamlessly. It’s not something you actually have to think about much. But, as my dear friend Dan Simpson11 reminds me over and over with all things technical, “God is present in the sweeping gestures but the Devil is in the details”.\nIt is wise to look carefully at the details, so let’s do that.\n\n\n[Arrow] Eight types of integer\nTo make sense of the different types, it helps to take a moment to think about how integers are represented in a binary format. Let’s suppose we allocate 8 bits to specify an integer. If we do that, then there are \\(2^8 = 256\\) unique binary patterns we can create with these bits. Because of this, there is a fundamental constraint: no matter how we choose to set it up, 8-bit integers can only represent 256 distinct numbers. Technically, we could choose any 256 numbers we like, but in practice there are only two schemes used for 8-bit integers: “unsigned” 8-bit integers (uint8) use those bits to represent integers from 0 to 255, whereas “signed” 8-bit integers (int8) can represent integers from -128 to 127.\nMore generally, an unsigned n-bit integer can represent integers from 0 to \\(2^n - 1\\), whereas a signed n-bit integer can represent integers from \\(-2^{n-1}\\) to \\(2^{n-1} - 1\\). Here’s what that looks like for all the integer types supported by Arrow:\n\n\n\n\n\n\n\n\n\nDescription\nName\nSmallest Value\nLargest Value\n\n\n\n\n8 bit unsigned\nuint8\n0\n255\n\n\n16 bit unsigned\nuint16\n0\n65535\n\n\n32 bit unsigned\nuint32\n0\n4294967295\n\n\n64 bit unsigned\nuint64\n0\n18446744073709551615\n\n\n8 bit signed\nint8\n-128\n127\n\n\n16 bit signed\nint16\n-32768\n32767\n\n\n32 bit signed\nint32\n-2147483648\n2147483647\n\n\n64 bit signed\nint64\n-9223372036854775808\n9223372036854775807\n\n\n\n\n\n\n[R] One integer class\nOn the R side, the integer type supplied by base R is a 32 bit signed integer, and has a natural one-to-one mapping to the Arrow int32 type. Because of this, the arrow default is to convert an R integer to an Arrow int32 and vice versa. Here’s an example. I’ve been watching Snowpiercer lately, and the train is currently 1029 cars long so let’s pass the integer 1029L from R over to Arrow\n\nsnowpiercer &lt;- scalar(1029L)\nsnowpiercer\n\nScalar\n1029\n\n\nLet’s inspect the type field of the snowpiercer object in order to determine what type of object has arrived in Arrow:\n\nsnowpiercer$type\n\nInt32\nint32\n\n\nWe can apply the S3 generic function as.vector() to snowpiercer to pull the data back into R,12 and hopefully it comes as no surprise to see that we get the same number back:\n\nas.vector(snowpiercer)\n\n[1] 1029\n\n\nWe can take this one step further to check that the returned object is actually an R integer by checking its class(), and again there are no surprises:\n\nsnowpiercer %&gt;% \n  as.vector() %&gt;% \n  class()\n\n[1] \"integer\"\n\n\nAs you can see, the default behaviour in arrow is to translate an R integer into an Arrow int32, and vice versa. That part, at least, is not too complicated.\nThat being said, it’s worth unpacking some of the mechanics of what I’m doing with the code here. Everything I’ve shown above is R code, so it’s important to keep it firmly in mind that when I create the snowpiercer object there are two different things happening: a data object is created inside Arrow, and a pointer to that object is created inside R. The snowpiercer object is that pointer (it’s actually an R6 object). When I called snowpiercer$type in R, the output is telling me that the data object in Arrow has type int32. There’s a division of responsibility between R and Arrow that always needs to be kept in mind.\nNow, in this particular example there’s an element of silliness because my data object is so tiny. There was never a good reason to put the data in Arrow, and the only reason I’m doing it here is for explanatory purposes. But in real life (like in the TV shoe), snowpiercer might in fact be a gargantuan monstrosity over which you have perilously little control due to it’s staggering size. In that case it makes a big difference where the data object is stored. Placing the data object in Arrow is a little bit like powering your 1029-car long train using the fictitious perpetual motion engine from the show: it is a really, really good idea when you have gargantuan data.13\n\n\n\nWhen integer translation is easy\nWhat about the other seven C++ integer types? This is where it gets a little trickier. The table above illustrates that some integer types are fully contained within others: unsurprisingly, every number representable by int16 can also be represented by int32, so we can say that the int16 numbers are fully “contained” by (i.e. are a proper subset of) the int32 numbers. Similarly, uint16 is contained by uint32. There are many cases where an unsigned type is contained by a signed type: for instance, int32 contains all the uint16 numbers. However, because the unsigned integers cannot represent negative numbers, the reverse is never true. So we can map out the relationships between the different types like this:\n\n\n\n\n\nContainment relationships between the integer types.\n\n\n\n\nWhenever type A contains type B, it’s possible to transform an object of type B into an object of type A without losing information or requiring any special handling. R integers are 32 bit signed integers, which means it’s possible to convert Arrow data of types int32, int16, int8, uint16, and uint8 to R integers completely painlessly. So for these data types the arrow defaults give us this relationship:\n\n\n\n\n\nDefault mappings for some integer types\n\n\n\n\nThese are the cases where it is easy.\n\n\n\nWhen integer translation is hard\nOther integer types are messier. To keep things nice and simple, what we’d like to do is to map the Arrow uint32, uint64, and int64 types onto the R integer type. Sometimes that’s possible: if all the stored values fall within the range of values representable by R integers (i.e., are between -2147483648 and 2147483647) then we can do this, and that’s what arrow does by default. However, if there are values that “overflow” this range, then arrow will import the data as a different type. That leads to a rather messy diagram, I’m afraid:\n\n\n\n\n\nDefault mappings for other integer types. The asterisk notation here is intended to indicate that the path arrow follows can depend on the data values and other settings.\n\n\n\n\nTranslations become messy when the boxes in one language don’t quite match up to the content expressed in another. Sometimes it’s just easier to see the system in action, so let’s write a little helper function:\n\ntranslate_integer &lt;- function(value, type) {\n  fn &lt;- function(value, type) {\n    tibble(\n      value = value,\n      arrow_type = scalar(value, type)$type$name,\n      r_class = scalar(value, type) %&gt;% as.vector() %&gt;% class()\n    )\n  }\n  purrr::map2_dfr(value, type, fn)\n}\n\nThe translate_integer() function takes a value vector and a type list as input, and it returns a tibble that tells you what Arrow type was created from each input, and what R class gets returned when we import that Arrow object back into R. I’ll pass the inputs in as doubles originally, but as you’ll see they always get imported to Arrow as integer types because that’s what I’m telling arrow to do. So let’s start with an easy case. The number 10 is unproblematic because it’s very small, and arrow never encounters any problem trying to pull it back as an R integer:\n\ntranslate_integer(\n  value = c(10, 10, 10, 10), \n  type = list(uint8(), uint32(), uint64(), int64())\n)\n\n# A tibble: 4 × 3\n  value arrow_type r_class\n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;  \n1    10 uint8      integer\n2    10 uint32     integer\n3    10 uint64     integer\n4    10 int64      integer\n\n\nOkay, that makes sense. If the numbers can be represented using the R integer class then that’s what arrow will do. Why make life unnecessarily difficult for the user?\nNow let’s increase the number to a value that is too big to store as a signed 32-bit integer. This is a value that R cannot represent as an integer, but Arrow can store as a uint32, uint64 or int64. What happens when we try to pull that object back into R?\n\ntranslate_integer(\n  value = c(3000000000, 3000000000, 3000000000), \n  type = list(uint32(), uint64(), int64())\n)\n\n# A tibble: 3 × 3\n       value arrow_type r_class  \n       &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    \n1 3000000000 uint32     numeric  \n2 3000000000 uint64     numeric  \n3 3000000000 int64      integer64\n\n\nThe first two rows seem intuitive. In base R, whenever an integer overflows and becomes too large to store, R will coerce it to a double. This is exactly the same behaviour we’d observe if the data had never left R at all. The third row, however, might come as a bit of a surprise. It certainly surprised me the first time I encountered it. Until very recently I did not know that R even had an integer64 class. This class is supplied by the bit64 package, and although I’m not going to talk about it in any detail here, it provides a mechanism to represent signed 64-bit integers in R. However, the one thing I will mention is the fact that the existence of the integer64 class opens up the possibility of forcing arrow to always map the integer64 class to the int64 type and vice versa. If you set\n\noptions(arrow.int64_downcast = FALSE)\n\nit will change the arrow default so that int64 types are always returned as integer64 classes, even when the values are small enough that the data could have been mapped to a regular R integer. This can be helpful in situations where you need to guarantee type stability when working with int64 data. Now that I’ve altered the global options, I can repeat my earlier command with the number 10.\n\ntranslate_integer(\n  value = c(10, 10, 10, 10), \n  type = list(uint8(), uint32(), uint64(), int64())\n)\n\n# A tibble: 4 × 3\n  value arrow_type r_class  \n  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;    \n1    10 uint8      integer  \n2    10 uint32     integer  \n3    10 uint64     integer  \n4    10 int64      integer64\n\n\nNotice that the results change for the int64 type only. The “int64_downcast” option pertains only to the int64 type, and does not affect the other integer types.\nAnd that’s it for integers. Next up we’ll talk about numeric types, but first I’ll be a good girl and restore my options to their previous state:\n\noptions(arrow.int64_downcast = NULL)\n\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#numeric-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#numeric-types",
    "title": "Data types in Arrow and R",
    "section": "Numeric types",
    "text": "Numeric types\nIn the last section I talked about the rather extensive range of data types that Arrow has to represent integers. Sure, there’s a practical benefit to having all these different data types, but at the same time its wild that we even need so many different data structures to represent something so simple. Integers aren’t complicated things. We learn them as kids even before we go to school, and we get taught the arithmetic rules to operate on them very early in childhood.\nThe problem, though, is that there are A LOT of integers. It’s a tad inconvenient sometimes, but the set of integers is infinite in size,14 so it doesn’t matter how many bits you allocate to your “int” type, there will always be integers that your machine cannot represent. But this is obvious, so why am I saying it? Mostly to foreshadow that things get worse when we encounter…\n\n\nFloating point numbers and the desert of the reals\n\nTo dissimulate is to pretend not to have what one has. To simulate is to feign to have what one doesn’t have. One implies a presence, the other an absence. But it is more complicated than that because simulating is not pretending: “Whoever fakes an illness can simply stay in bed and make everyone believe he is ill. Whoever simulates an illness produces in himself some of the symptoms” (Littré). Therefore, pretending, or dissimulating, leaves the principle of reality intact: the difference is always clear, it is simply masked, whereas simulation threatens the difference between the “true” and the “false,” the “real” and the “imaginary.”      – Jean Baudrillard, 1981, Simulacra and Simulation15\n\nThe real numbers correspond to our intuitive concept of the continuous number line. Just like the integers, the real line extends infinitely far in both directions, but unlike the integers the reals are continuous: for any two real numbers – no matter how close they are to each other – there is always another real number in between. This, quite frankly, sucks. Because the moment you accept that this is true, something ugly happens. If I accept that there must exist a number between 1.01 and 1.02, which I’ll call 1.015, then I have to accept that there is a number between 1.01 and 1.015, which I’ll call 1.0075, and then I have to accept that… oh shit this is going to go on forever. In other words, the reals have the obnoxious property that there between any two real numbers there are an infinity of other real numbers.16\nTry shoving all that into your finite-precision machine.\nStepping away from the mathematics for a moment, most of us already know how programming languages attempt to solve the problem. They use floating point numbers as a crude tool to approximate the real numbers using a finite-precision machine, and it… sort of works, as long as you never forget that floating point numbers don’t always obey the normal rules of arithmetic. I imagine most people reading this post already know this but for those that don’t, I’ll show you the most famous example:\n\n0.1 + 0.2 == 0.3\n\n[1] FALSE\n\n\nThis is not a bug in R. It happens because 0.1, 0.2, and 0.3 are not real numbers in the mathematical sense. Rather, they are encoded in R as objects of type double, and a double is a 64-bit floating point number that adheres to the IEEE 754 standard. It’s a bit beyond the scope of this post to dig all the way into the IEEE standard, but it does help a lot to have a general sense of how a floating point number (approximately) encodes a real number, so in the next section I’m going to take a look under the hood of R doubles. I’ll show you how they’re represented as binary objects, and why they misbehave sometimes. I’m doing this for two reasons: firstly it’s just a handy thing to know, but secondly, understanding the misbehaviour of the “standard” binary floating point number representation used in R helps motivate why Arrow and some other platforms expose other options to the user.\n\n\n\n[R] The numeric class\nTo give you a better feel for what a double looks like when represented as a set of bits, I’ve written a little extractor function called unpack_double() that decomposes the object into its constituent bits and prints it out in a visually helpful way (source code here). In truth, it’s just a wrapper around the numTobits() function provided by base R, but one that gives slightly prettier output. Armed with this, let’s take a look at the format. To start out, I’ll do the most boring thing possible and show you the binary representation of 0 as a floating point number. You will, I imagine, be entirely unshocked to discover that it is in fact a sequence of 64 zeros:\n\nunpack_double(0)\n\n0 00000000000 0000000000000000000000000000000000000000000000000000 \n\n\nTruly amazing.\nReally, the only thing that matters here is to notice the spacing. The sequence of 64 bits are divided into three meaningful chunks. The “first” bit17 represents the “sign”: is this a positive number (first bit equals 0) or a negative number (first bit equals 1), where zero is treated as if it were a positive number. The next 11 bits are used to specify an “exponent”: you can think of these bits as if they describe a signed “int11” type, and can be used to store any number between -1022 and 1023.18 The remaining 53 bits are used to represent the “mantissa”.19\nThese three components carve up a real number by using this this decomposition:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 2 ^ {\\mbox{(exponent)}}\n\\] Any real number can be decomposed in this way, so long as you have enough digits to express your mantissa and your exponent. Of course, on a finite precision machine we won’t always have enough digits, and this representation doesn’t allow us to fit “more” numbers into the machine: there’s a fundamental limit on what you can accomplish with 64 bits. What it can do for you, however, is let you use your limited resources wisely. The neat thing about adopting the decomposed format that floating-point relies on is that we can describe very large magnitudes and very small magnitudes with a fixed-length mantissa.\nTo give a concrete example of how floating point works, let’s take a look at the internal representation of -9.832, which I am told is the approximate rate of acceleration experienced by a falling object in the Earth’s polar regions:\n\npolar_g &lt;- unpack_double(-9.832)\npolar_g\n\n1 10000000010 0011101010011111101111100111011011001000101101000100 \n\n\nI wrote some extractor functions that convert those binary components to the sign, exponent, and mantissa values that they represent, so let’s take a look at those:\n\nextract_sign(polar_g)\nextract_exponent(polar_g)\nextract_mantissa(polar_g)\n\n[1] -1\n[1] 3\n[1] 1.229\n\n\nNotice that the sign is always represented exactly: it can only be -1 or 1. The exponent is also represented exactly, as long as it’s not too large or too small: the number is always an integer value between -1022 and 1023. The mantissa, however, is a fractional value. When you encounter floating point errors it’s generally going to be because the stored mantissa doesn’t represent the true mantissa with sufficient precision.20 In any case, let’s check that the formula works:\n\nsign &lt;- extract_sign(polar_g)\nexponent &lt;- extract_exponent(polar_g)\nmantissa &lt;- extract_mantissa(polar_g)\n\nsign * mantissa * 2 ^ exponent\n\n[1] -9.832\n\n\nYay!\nJust to prove to you that this isn’t a fluke, I also included a repack_double() function that automates this calculation. It takes the deconstructed representation of an R double and packs it up again, so repack_double(unpack_double(x)) should return x. Here are a few examples:\n\nsanity_check &lt;- function(x) {\n  x == repack_double(unpack_double(x))\n}\nsanity_check(12)\nsanity_check(1345234623462342)\nsanity_check(0.000000002345345234523)\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nNow that we have some deeper knowledge of how R doubles are represented internally, let’s return to the numbers in the famous example of floating point numbers misbehaving:\n\nunpack_double(.1)\nunpack_double(.2)\nunpack_double(.3)\n\n0 01111111011 1001100110011001100110011001100110011001100110011010 \n0 01111111100 1001100110011001100110011001100110011001100110011010 \n0 01111111101 0011001100110011001100110011001100110011001100110011 \n\n\nAlthough these are clean numbers with a very simple decimal expansion, they are not at all simple when written in a binary floating point representation. In particular, notice that 0.1 and 0.2 share the same mantissa but 0.3 has a different mantissa, and that’s where the truncation errors occur. Let’s take a peek at 0.6 and 0.9:\n\nunpack_double(.6)\nunpack_double(.8)\nunpack_double(.9)\n\n0 01111111110 0011001100110011001100110011001100110011001100110011 \n0 01111111110 1001100110011001100110011001100110011001100110011010 \n0 01111111110 1100110011001100110011001100110011001100110011001101 \n\n\nSo it turns out that 0.6 has the same mantissa as 0.3, and 0.8 has the same mantissa as 0.1 and 0.2, but 0.9 has a different mantissa from all of them. So what we might expect is that floating point errors can happen for these cases:21\n\n0.1 + 0.2 == 0.3\n0.3 + 0.6 == 0.9\n\n[1] FALSE\n[1] FALSE\n\n\nbut not these ones:\n\n0.1 + 0.1 == 0.2\n0.3 + 0.3 == 0.6\n0.2 + 0.6 == 0.8\n\n[1] TRUE\n[1] TRUE\n[1] TRUE\n\n\nOkay that checks out! Now, it’s important to recognise that these errors are very small. So when I say that floating point arithmetic doesn’t actually “work”, a little care is needed. It does am impressively good job of approximating something very complicated using a quite limited tool:\n\n0.1 + 0.2 - 0.3\n\n[1] 5.551115e-17\n\n\nUltimately, floating point numbers are a simulation in the sense described by Baudrillard at the start of this section. They are a pretense, an attempt to act as if we can encode a thing (the reals) that we cannot encode. Floating point numbers are a fiction, but they are an extraordinarily useful one because they allow us to “cover” a very wide span of numbers across the real line, at a pretty high level of precision, without using too much memory.\nWe pretend that machines can do arithmetic on the reals. They can’t, but it’s a very powerful lie.\n\n\n\n[Arrow] The float64 type\nOkay. That was terribly long-winded, and I do apologise. Nevertheless, I promise there is a point to this story and it’s time we switched back over to the Arrow side of things to think about what happens there.\nBy now you’re probably getting used to the fact that Arrow tends to have more primitive types than R in most situations. Floating point numbers are no exception. R has only a single class, usually referred to as numeric but sometimes called double. In contrast, Arrow has three: float64, float32 and float16.22 It also has another numeric type called decimal that I’ll discuss later.\nThe easiest of these to discuss is float64, because it adopts the same conventions as the R double class. Just like R, it uses 64 bits to represent a floating point number.23 Because the data structures are so similar, the default behaviour in arrow is to translate an R double into an Arrow float64 and vice versa.\nAs always, I’ve got a little diagram summarising all the default mappings:\n\n\n\n\n\nDefault mappings for numeric types\n\n\n\n\nLet’s have a look at the Arrow float64 type. It’s a little anticlimactic in a sense, because it’s the same data structure as the R double type, so all we’re going to “learn” is that it behaves the same way! First, let’s create one:\n\nfloat_01 &lt;- scalar(0.1)\nfloat_01\n\nScalar\n0.1\n\n\nAs always, we’ll verify that the created object has the type we’re expecting…\n\nfloat_01$type\n\nFloat64\ndouble\n\n\n… and it does, but you might be slightly puzzled by the output this time. What’s going on with the top line and the bottom line? Why does one say “Float64” and the other say “double”?\nWe’ve seen the “two lines of output” pattern earlier in the post when printing out an int32, but last time the two lines both said the same thing so I didn’t bother to comment on it. This time, however, there’s something to unpack. The distinction here refers to the name of the object type at the R level and and the C++ level. The first line of the output reads “Float64” because that’s what this data structure is called at the R level (i.e., according to arrow). The second line reads “double” because that’s what this data structure is called at the C++ level (i.e., in libarrow). There are a few cases where the arrow package adopts a slightly different naming scheme to libarrow, and so you’ll see this happen from time to time later in the post. There are some good reasons for this difference in nomenclature, and it’s nothing to be concerned about!\nAnyway, getting back to the main thread… since we’ve created the value 0.1 as a float64 in Arrow, let’s go through the same exercise we did in R and show that Arrow floats produce the same floating point errors. We’ll create new variables for 0.2 and 0.3:\n\nfloat_02 &lt;- scalar(0.2)\nfloat_03 &lt;- scalar(0.3)\n\nJust like we saw in R, the logical test of equality gives a counterintuitive answer:\n\nfloat_01 + float_02 == float_03\n\nScalar\nfalse\n\n\n… and just like we saw in R, the reason for it is that there’s a very small rounding error:\n\nfloat_01 + float_02 - float_03\n\nScalar\n5.551115123125783e-17\n\n\nJust so you don’t have to scroll up to check, yes, the rounding error is the same as the one that R produces:\n\n0.1 + 0.2 - 0.3\n\n[1] 5.551115e-17\n\n\nR and Arrow implement the same standard for floating point arithmetic, and so they “fail” in the same way because the failure occurs at the level of the standard. But we don’t blame IEEE 754 for that, because it’s literally impossible to define any standard that will encode the real numbers in an error-free way on a finite-precision machine.\n\n\n\n[Arrow] The float32 and float16 types\nThe float64 type provides an excellent, high precision floating point representation of numeric data. As data types go it is a good type. However, it is a 64-bit type, and sometimes you don’t need to store your data at a high degree of precision. With that in mind, because Arrow places a strong emphasis on both scalability and efficiency, it also provides the float32 type and the float16 type (though float16 hasn’t really been implemented yet, as far as I know). Encoding numeric data in these formats will save space, but will come at a cost of precision. As always, the decision of what encoding works best for your application will depend on what your needs are.\nAs far as the arrow package is concerned, there are no difficulties in passing data back and forth between R doubles and Arrow float32 types, but at present it’s not really possible to do this with float16 because this isn’t implemented. Still, we can briefly take a look at how it works for float32. Here’s an example of me passing an R double to Arrow:\n\nfloat32_01 &lt;- scalar(.1, type = float32())\nfloat32_01\n\nScalar\n0.1\n\n\nLet’s quickly verify that it is in fact a 32-bit float:\n\nfloat32_01$type\n\nFloat32\nfloat\n\n\nAnd now let’s pull it back into R where it will be, once again, encoded as a double:\n\nas.vector(float32_01)\n\n[1] 0.1\n\n\nYay! It works!\n\n\n\nDecimal floating point numbers?\nIt’s time to talk about decimals. This is a fun topic, but I need to start with a warning: I mentioned that Arrow has a decimal type, and your first instinct as an R programmer might be to assume that this is another variation of floating point numbers. Fight this instinct: it’s not quite right.\nOkay, ready?\nEarlier in this section I promised that the Baudrillard quote from Simulacra and Simulation was going to be relevant? Well, that time has arrived. It’s also the moment at which the quote from Word and Object by Quine that opened this blog post becomes painfully relevant. Stripped of their fancy language, here’s what the two authors are telling us in these passages:\n\nThe Baudrillard quote emphasises that floating point numbers are a simulation. They are the mechanism by which we pretend to encode real numbers on computers. It’s a lie, but it’s a powerful lie that almost works.\nThe Quine quote emphasises that translation (and, I would argue, simulation also) is underdetermined. For any complicated thing there are many ways to simulate, or translate, or approximate it. These approximations can be extremely accurate and still be inconsistent with each other.\n\nQuine’s truism applies to floating point numbers, and it is the reason why “decimal floating point” numbers exist in addition “binary floating point” numbers. All floating point systems are simulations in the Baudrillard sense of the term: lies, strictly speaking, but close enough to true that the distinction between lies and truth gets a little blurry.\nLet’s see how that plays out with floating point numbers. When discussing doubles in R, I mentioned that they represent the real numbers using a decomposition that looks like this:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 2 ^ {\\mbox{(exponent)}}\n\\]\nThe number “2” pops out here, doesn’t it? Is there any reason to think that “2” is a pre-ordained necessity when approximating the real numbers on a finite-precision machine? Programmers have a tendency to like using “2” as the base unit for everything because it lines up nicely with binary representations, and that’s often a good instinct when dealing with machines.\nUnfortunately, life consists of more than machines.\nIn particular, binary representations create problems for floating point arithmetic because the world contains entities known as “humans”, who have a habit of writing numbers in decimal notation24. Numbers that look simple in decimal notation often look complicated in binary notation and vice versa. As we saw earlier, a “simple” decimal number like 0.1 doesn’t have a short binary expansion and so cannot be represented cleanly in a finite-precision binary floating point number system. Rounding errors are introduced every time a machine uses (base 2) floating point to encode data that were originally stored as a (base 10) number in human text.\nA natural solution to this is to design floating point data types that use other bases. It is entirely possible to adopt decimal floating point types that are essentially equivalent to the more familiar binary floating point numbers, but they rely on a base 10 decomposition:\n\\[\n(\\mbox{real number}) = (\\mbox{sign}) \\times (\\mbox{mantissa}) \\times 10 ^ {\\mbox{(exponent)}}\n\\]\nThe virtues of decimal floating point seem enticing, and it’s tempting to think that this must be what Arrow implements. However, as we’ll see in the next section, that’s not true.\nInstead of using floating-point decimals, it supplies “fixed-point” decimal types. In a floating-point representation, the exponent is chosen automatically, and it is a property of the value itself. The number -9.832 will always have an exponent of 3 when encoded as a binary floating-point number (as we saw in the polar_g example earlier), and that exponent will never be influenced by the values of other numbers stored in the same data set.\nA fixed-point representation is different. The exponent – and in a decimal representation, remember that the exponent is just “the location of the decimal point” – is chosen by the user. You have to specify where the decimal point is located manually, and this location will be applied to each value stored in the object. In other words, the exponent – which is now called the “scale”, and is parameterised slightly differently – becomes a property of the type, not the value.\nSigh. Nothing in life is simple, is it? It’ll become a little clearer in the next section, I promise!\n\n\n\n[Arrow] The decimal fixed-point types\nArrow has two decimal types, a decimal128 type that (shockingly) uses 128 bits to store a floating point decimal number, and a decimal256 type that uses 256 bits. As usual arrow package supplies type functions decimal128() and decimal256() that allow you to specify decimal types. Both functions have two arguments that you must supply:\n\nprecision specifies the number of significant digits to store, similar to setting the length of the mantissa in a floating-point representation.\nscale specifies the number of digits that should be stored after the decimal point. If you set scale = 2, exactly two digits will be stored after the decimal point. If you set scale = 0, values will be rounded to the nearest whole number. Negative scales are also permitted (handy when dealing with extremely large numbers), so scale = -2 stores the value to the nearest 100.\n\nOne convenience that exists both in the arrow R package and within libarrow itself is that it can automatically decide whether you need a decimal128 or a decimal256 simply by looking at the value of the precision argument. If the precision is 38 or less, you can encode the data with a decimal128 type. Larger values require a decimal256. If you would like to take advantage of this – as I will do in this post – you can use the decimal() type function which will automatically create the appropriate type based on the specified precision.\nOne inconvenience that I have in this post, however, is that R doesn’t have any analog of a fixed-point decimal, and consequently I don’t have any way to create an “R decimal” that I can then import into Arrow. What I’ll do instead is create a floating point array in Arrow, and then explicitly cast it to a decimal type. Step one, create the floating point numbers in Arrow:\n\nfloats &lt;- chunked_array(c(.01, .1, 1, 10, 100), type = float32())\nfloats\n\nChunkedArray\n&lt;float&gt;\n[\n  [\n    0.01,\n    0.1,\n    1,\n    10,\n    100\n  ]\n]\n\n\nStep two, cast the float32 numbers to decimals:\n\ndecimals &lt;- floats$cast(decimal(precision = 5, scale = 2))\ndecimals\n\nChunkedArray\n&lt;decimal128(5, 2)&gt;\n[\n  [\n    0.01,\n    0.10,\n    1.00,\n    10.00,\n    100.00\n  ]\n]\n\n\nThese two arrays look almost the same (especially because I chose the scale judiciously!), but the underlying encoding is different. The original floats array is a familiar float32 type, but if we have a look at the decimals object we see that it adopts a quite different encoding:\n\ndecimals$type\n\nDecimal128Type\ndecimal128(5, 2)\n\n\nTo illustrate that these do behave differently, let’s have fun making floating point numbers misbehave again:\n\nsad_floats &lt;- chunked_array(c(.1, .2, .3))\nsum(sad_floats)\n\nScalar\n0.6000000000000001\n\n\nOh noes. Okay, let’s take a sad float32 and turn it into a happy decimal. I’ll store it as a high precision decimal to make it a little easier to compare the results:\n\nhappy_decimals &lt;- sad_floats$cast(decimal(20, 16))\n\nNow let’s look at the two sums side by side:\n\nsum(sad_floats)\nsum(happy_decimals)\n\nScalar\n0.6000000000000001\nScalar\n0.6000000000000000\n\n\nYay!\nAs a final note before moving on, it is (of course!!!) the case that fixed-point decimals aren’t a universal solution to the problems of binary floating-point numbers. They have limitations of their own and there are good reasons why floats remain the default numeric type in most languages. But they have their uses: binary and decimal systems provide different ways to simulate the reals, as do fixed and floating point systems. Each such system is a lie, of course: the reals are too big to be captured in any finite system we create. They are, however, useful.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#character-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#character-types",
    "title": "Data types in Arrow and R",
    "section": "Character types",
    "text": "Character types\nOur journey continues. We now leave behind the world of number and enter the domain of text. Such times we shall have! What sights we shall see! (And what terrors lie within?)\nStrings are an interesting case. R uses a single data type to represent strings (character vectors) but Arrow has two types, known as strings and large_strings. When using the arrow package, Arrow strings are specified using the utf8() function, and large strings correspond to the large_utf8() type. The default mapping is to assume that an R character vector maps onto the Arrow utf8() type, as shown below:\n\n\n\n\n\nDefault mappings for character types\n\n\n\n\nThere’s a little more than meets the eye here though, and you might be wondering about the difference between strings and large_strings in Arrow, and when you might prefer one to the other. As you might expect, the large string type is suitable when you’re storing large amounts of text, but to understand it properly I need to talk in more depth about how R and Arrow store strings, and I’ll use this partial list of people that – according to the lyrics of Jung Talent Time by TISM – were perhaps granted slightly more fame than they had earned on merit:\nBert Newton\nBilly Ray Cyrus\nWarwick Capper\nUri Geller\nSamantha Fox\n\n\n[R] The character class\nSuppose I want to store this as a character vector in R, storing only the family names for the sake of brevity and visual clarity.\n\njung_talent &lt;- c(\"Newton\", \"Cyrus\", \"Capper\", \"Geller\", \"Fox\")\n\nEach element of the jung_talent vector is a variable-length string, and is stored internally by R as an array of individual characters25 So, to a first approximation, your mental model of how R stores the jung_talent variable might look something like this:\n\n\n\n\n\nSimplified representation of how character vectors are represented in R\n\n\n\n\nHere, the jung_talent variable is an object26 that contains five elements shown as the orange boxes. Internally, each of those orange boxes is itself an array of individual characters shown as the purple boxes. As a description of what R actually does this is a bit of an oversimplification because it ignores the global string pool, but it will be sufficient for the current purposes.\nThe key thing to understand conceptually is that R treats the elements of a character vector as the fundamental unit. The jung_talent vector is constructed from five distinct strings, \"Newton\", \"Cyrus\", etc. The \"Newton\" string is assigned to position 1, the \"Cyrus\" string is assigned to position 2, and so on.\n\n\n\n[Arrow] The utf8 type\nThe approach taken in Arrow is rather different. Instead of carving up the character vector into strings (and internally treating the strings as character arrays), it concatenates everything into one long buffer. The text itself is dumped into one long string, like this:\nNewtonCyrusCapperGellerFox\nThe first element of this buffer – the letter \"N\" – is stored at “offset 0” (indexing in Arrow starts at 0), the second element is stored at offset 1, and so on. This long array of text is referred to as the “data buffer”, and it does not specify where the boundaries between array elements are. Those are stored separately. If I were to create an Arrow string array called jung_talent_arrow, it would be comprised of a data buffer, and an “offset buffer” that specifies the positions at which each element of the string array begins. In other words, we’d have a mental model that looks a bit like this:\n\n\n\n\n\nSimplified representation of how character vectors are represented in Arrow\n\n\n\n\nHow are each of these buffers encoded?\n\nThe contents of the data buffer are stored as UTF-8 text, which is itself a variable length encoding: some characters are encoded using only 8 bits while others require 32 bits. This blog post on unicode is a nice explainer.\nThe contents of the offset buffer are stored as unsigned integers, either 32 bit or 64 bit, depending on which of the two Arrow string array types you’re using. I’ll unpack this in the next section.\n\nSheesh. That was long. Let’s give ourselves a small round of applause for surviving, and now actually DO something. We’ll port the jung_talent vector over to Arrow.\n\njung_talent_arrow &lt;- chunked_array(jung_talent)\njung_talent_arrow\n\nChunkedArray\n&lt;string&gt;\n[\n  [\n    \"Newton\",\n    \"Cyrus\",\n    \"Capper\",\n    \"Geller\",\n    \"Fox\"\n  ]\n]\n\n\nThat certainly looks like text to me! Let’s take a look at the data type:\n\njung_talent_arrow$type\n\nUtf8\nstring\n\n\nYep. Definitely text!\n\n\n\n[Arrow] The large_utf8 type\nOkay, so as I mentioned, Arrow has two different string types: it has strings (also called utf8) and large_strings (also called large_utf8).27 The default in arrow is to translate character data in R to the utf8 data type in Arrow, but we can override this if we want to. In order to help you make an informed choice, I’ll dig a little deeper into the difference between the two types.\nThe first thing to recognise is that the nature of the data buffer is the same for utf8 and large_utf8: the difference between the two lies in how the offset buffers are encoded. When character data are encoded as utf8 type, every offset value is stored as an unsigned 32-bit integer. That means that – as shown in the table of integer types earlier in the post – you cannot store an offset value larger than 4294967295. This constrain places a practical cap on the total length of the data buffer: if total amount of text stored in the data buffer is greater than about 2GiB, the offset buffer can’t encode the locations within it! Switching to large_utf8 means that the offset buffer will store every offset value as an unsigned 64-bit integer. This means that the offset buffer now takes up twice as much space, but it allows you to encode offset values up to… um… 18446744073709551615. And if you’ve got so much text that your data buffer is going to exceed that limit, well, frankly you have bigger problems.\nIn short, if you’re not going to exceed 2GiB of text in your array, you don’t need large_utf8. Once you start getting near that limit, you might want to think about switching:\n\njung_talent_arrow_big &lt;- chunked_array(jung_talent, type = large_utf8())\njung_talent_arrow_big$type\n\nLargeUtf8\nlarge_string\n\n\nBefore moving on, I’ll mention one additional complexity. This is a situation where the distinction between Arrays and ChunkedArrays begins to matter. Strictly speaking, I lied earlier when I said there’s only one data buffer. A more precise statement would be to say that there is one data buffer per chunk (where each chunk in a ChunkedArray is an Array). ChunkedArrays are designed to allow a block (or “chunk”) of contiguous rows in a table to be stored together in a single location (or file). There are good reasons for doing that28, but they aren’t immediately relevant. What matters is to recognise that in a ChunkedArray, the 2GiB limit on utf8 type data applies on a per-chunk basis. The net result of this is that you probably don’t need large_utf8 except in very specific cases.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#datetime-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#datetime-types",
    "title": "Data types in Arrow and R",
    "section": "Date/time types",
    "text": "Date/time types\nNext up on our tour of data types are dates and times. Internally, R and Arrow both adopt the convention of measuring time in terms of the time elapsed since a specific moment in time known as the unix epoch. The unix epoch is the time 00:00:00 UTC on 1 January 1970. It was a Thursday.\nDespite agreeing on fundamentals, there are some oddities in the particulars. Base R has three date/time classes (Date, POSIXct, and POSIXlt), and while Arrow also has three date/time classes (date32, date64, and timestamp), the default mappings between them are a little puzzling unless you are deeply familiar with what all these data types are and what they represent. I’ll do the deep dive in a moment, but to give you the big picture here’s how the mapping works:\n\n\n\n\n\nDefault mappings for date/time types\n\n\n\n\n\n\n[R] The Date class\nOn the R side of things, a Date object is represented internally as a numeric value, counting the number of days since the unix epoch. Here is today as a Date:\n\ntoday &lt;- Sys.Date()\ntoday\n\n[1] \"2023-05-27\"\n\n\nIf I use unclass() to see what it looks like under the hood:\n\nunclass(today)\n\n[1] 19504\n\n\nFundamentally, a Date object is a number:29 it counts the number of days that have elapsed since a fixed date. It does not care what the year is, what the month is, or what day of the month it is. It does not care how the date is displayed to the user. All those things are supplied by the print() method, and are not part of the Date itself.\n\n\n\n[R] The POSIXct class\nA date is a comparatively simple thing. When we want to represent dates and time together, we need to know the time of day, and we might need to store information about the timezone as well (more on that later). Base R has two different classes for representing this, POSIXct and POSIXlt. These names used to confuse me a lot. POSIX stands for “portable operating system interface”, and it’s a set of standards used to help operating systems remain compatible with each other. In this context though, it’s not very meaningful: all it says “yup we use unix time.”\nThe more important part of the name is actually the “ct” versus “lt” part. Let’s start with POSIXct. The “ct” in POSIXct stands for “calendar time”: internally, R stores the number of seconds30 that have elapsed since 1970-01-01 00:00 UTC.\n\nnow &lt;- Sys.time()\nnow\n\n[1] \"2023-05-27 18:24:15 AEST\"\n\n\nIf I peek under the hood using unclass() here’s what I see:\n\nunclass(now)\n\n[1] 1685175855\n\n\nThere are no attributes attached to this object, it is simply a count of the number of seconds since that particular moment in time. However, it doesn’t necessarily have to be this way: a POSIXct object is permitted to have a “tzone” attribute, a character string that specifies the timezone that is used when printing the object will be preserved when it is converted to a POSIXlt.\nNevertheless, when I created the now object by calling Sys.time(), no timezone information was stored in the object. The fact that it appears when I print out now occurs because the print() method for POSIXct objects prints the time with respect to a particular timezone. The default is to use the system timezone, which you can check by calling Sys.timezone(), but you can override this behaviour by specifying the timezone explicitly (for a list of timezone names, see OlsonNames()). So if I wanted to print the time in Berlin, I could do this:\n\nprint(now, tz = \"Europe/Berlin\")\n\n[1] \"2023-05-27 10:24:15 CEST\"\n\n\nIf you want to record the timezone as part of your POSIXct object rather than relying on the print method to do the work, you can do so by setting the tzone attribute. To illustrate this, let’s pretend I’m in Tokyo:\n\nattr(now, \"tzone\") &lt;- \"Asia/Tokyo\"\nnow\n\n[1] \"2023-05-27 17:24:15 JST\"\n\n\nThe important thing here is that the timezone is metadata used to change the how the time is displayed. Changing the timezone does not alter the number of seconds stored in the now object:\n\nunclass(now)\n\n[1] 1685175855\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\"\n\n\n\n\n\n[R] The POSIXlt class\nWhat about POSIXlt? It turns out that this is a quite different kind of data structure, and it “thinks” about time in a very different way. The “lt” in POSIXlt stands for “local time”, and internally a POSIXlt object is a list that stores information about the time in a way that more closely mirrors how humans think about it. Here’s what now looks like when I coerce it to a POSIXlt object:\n\nnow_lt &lt;- as.POSIXlt(now)\nnow_lt\n\n[1] \"2023-05-27 17:24:15 JST\"\n\n\nIt looks exactly the same, but this is an illusion produced by the print() method. Internally, the now_lt object is a very different kind of thing. To see this, let’s see what happens if we print it as if it were a regular list:\n\nunclass(now_lt)\n\n$sec\n[1] 15.1697\n\n$min\n[1] 24\n\n$hour\n[1] 17\n\n$mday\n[1] 27\n\n$mon\n[1] 4\n\n$year\n[1] 123\n\n$wday\n[1] 6\n\n$yday\n[1] 146\n\n$isdst\n[1] 0\n\n$zone\n[1] \"JST\"\n\n$gmtoff\n[1] 32400\n\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\" \"JST\"        \"JDT\"       \nattr(,\"balanced\")\n[1] TRUE\n\n\nAs you can see, this object separately stores the year (counted from 1900), the month (where January is month 0 and December is month 11), the day of the month (starting at day 1), etc.31 The timezone is stored, as is the day of the week (Sunday is day 0), it specifies whether daylight savings time is in effect, and so on. Time, as represented in the POSIXlt class, uses a collection of categories that are approximately the same as those that humans use when we talk about time.\nIt is not a compact representation, and it’s useful for quite different things than POSIXct. What matters for the current purposes is that POSIXlt is, fundamentally, a list structure, and is not in any sense a “timestamp”.\n\n\n\n[Arrow] The date32 type\nOkay, now let’s pivot over to the Arrow side and see what we have to work with. The date32 type is similar – but not identical – to the R Date class. Just like the R Date class, it counts the number of days since 1970-01-01. To see this, let’s create an analog of the today Date object inside Arrow, and represent it as a date32 type:\n\ntoday_date32 &lt;- scalar(today, type = date32())\ntoday_date32\n\nScalar\n2023-05-27\n\n\nWe can expose the internal structure of this object by casting it to an int32:\n\ntoday_date32$cast(int32())\n\nScalar\n19504\n\n\nThis is the same answer we got earlier when I used unclass() to take a peek at the internals of the today object. That being said, there is a subtle difference: in Arrow, the date32 type is explicitly a 32-bit integer. If you read through the help documentation for date/time classes in R you’ll see that R has something a little more complicated going on. The details don’t matter for this post, but you should be aware that Dates (and POSIXct objects) are stored as doubles. They aren’t stored as integers:\n\ntypeof(today)\ntypeof(now)\n\n[1] \"double\"\n[1] \"double\"\n\n\nIn any case, given that the Arrow date32 type and the R Date class are so similar to each other in structure and intended usage, it is natural to map R Dates to Arrow date32 types and vice versa, and that’s what the arrow package does by default.\n\n\n\n[Arrow] The date64 type\nThe date64 type is similar to the date32 type, but instead of storing the number of days since 1970-01-01 as a 32-bit integer, it stores the number of milliseconds since 1970-01-01 00:00:00 UTC as a 64-bit integer. It’s similar to the POSIXct class in R, except that (1) it uses milliseconds instead of seconds; (2) the internal storage is an int64, not a double; and (3) it does not have metadata and cannot represent timezones.\nAs you might have guessed, the date64 type in Arrow isn’t very similar to the Date class in R. Because it represents time at the millisecond level, the intended use of the date64 class is in situations where you want to keep track of units of time smaller than one day. Sure, I CAN create date64 objects from R Date objects if I want to…\n\nscalar(today, date64())\n\nScalar\n2023-05-27\n\n\n…but this is quite wasteful. Why use a 64-bit representation that tracks time at the millisecond level when all I’m doing is storing the date? Although POSIXct and date64 aren’t exact matches, they’re more closely related to each other than Date and date64. So let’s create an Arrow analog of now as a date64 object:\n\nnow_date64 &lt;- scalar(now, date64())\nnow_date64\n\nScalar\n2023-05-27\n\n\nThe output is printed as a date, but this is a little bit misleading because it doesn’t give you a good sense of the level of precision in the data. Again we can peek under the hood by explicitly casting this to a 64-bit integer:\n\nnow_date64$cast(int64())\n\nScalar\n1685175855169\n\n\nThis isn’t a count of the number of days since the unix epoch, it’s a count of the number of milliseconds. It is essentially the same number, divided by 1000, as the one we obtained when I typed unclass(now).\nHowever, there’s a puzzle here that we need to solve. Let’s take another look at unclass(now):\n\nunclass(now)\n\n[1] 1685175855\nattr(,\"tzone\")\n[1] \"Asia/Tokyo\"\n\n\nThis might strike you as very weird. On the face of it, what has happened is that I have taken now (which ostensibly represents time at “second-level” precision), ported it over to Arrow, and created an object now_date64 that apparently knows what millisecond it is???? How is that possible? Does Arrow have magic powers?\nNot really. R is playing tricks here. Remember how I said that POSIXct objects are secretly doubles and not integers? Well, this is where that becomes relevant. It’s quite hard to get R to confess that a POSIXct object actually knows the time at a more precise level than “to the nearest second” but you can get it do to so by coercing it to a POSIXlt object and then taking a peek at the sec variable:\n\nas.POSIXlt(now)$sec\n\n[1] 15.1697\n\n\nAha! The first few digits of the decimal expansion are the same ones stored as the least significant digits in now_date64. The data was there all along. Even though unclass(now) produces an output that has been rounded to the nearest second, the original now variable is indeed a double, and it does store the time a higher precision! Ultimately, the accuracy of the time depends on the system clock itself, but the key thing to know here is that even though POSIXct times are almost always displayed to the nearest second, they do have the ability to represent more precise times.\nBecause of this, the default behaviour in arrow is to convert date64 types (64-bit integers interpreted as counts of milliseconds) to POSIXct classes (which are secretly 64-bit doubles interpreted as counts of seconds).\nRight. Moving on.\n\n\n\n[Arrow] The timestamp type\nThe last of the Arrow date/time types is the timestamp. The core data structure is a 64-bit integer used to count the number of time units that have passed since the unix epoch, and this is associated with two additional pieces of metadata: the time unit used (e.g., “seconds”, “milliseconds,”microseconds”, “nanoseconds”), and the timezone. As with the POSIXct class in R, the timezone metadata is optional, but the time unit is necessary. The default is to use microseconds (i.e., unit = \"us\"):\n\nscalar(now)\nscalar(now, timestamp(unit = \"us\"))\n\nScalar\n2023-05-27 08:24:15.169700\nScalar\n2023-05-27 08:24:15.169700\n\n\nAlternatively, we could use seconds:\n\nscalar(now, timestamp(unit = \"s\"))\n\nScalar\n2023-05-27 08:24:15\n\n\nIt’s important to recognise that changing the unit does more than change the precision at which the timestamp is printed. It changes “the thing that is counted”, so the numbers that get stored in the timestamp are quite different depending on the unit. Compare the numbers that are stored when the units are seconds versus when the units are nanoseconds:\n\nscalar(now, timestamp(unit = \"s\"))$cast(int64())\nscalar(now, timestamp(unit = \"ns\"))$cast(int64())\n\nScalar\n1685175855\nScalar\n1685175855169700864\n\n\nOkay, what about timezone?\nRecall that now has a timezone attached to it, because I explicitly recorded the tzone attribute earlier. Admittedly I lied and I said I was in Tokyo and not in Sydney, but still, that information is in the now object:\n\nnow\n\n[1] \"2023-05-27 17:24:15 JST\"\n\n\nWhen I print the R object it displays the time in the relevant time zone. The output for the Arrow object doesn’t do that: the time as displayed is shown in UTC. However, that doesn’t mean that the metadata isn’t there:\n\nnow_timestamp &lt;- scalar(now)\nnow_timestamp$type\n\nTimestamp\ntimestamp[us, tz=Asia/Tokyo]\n\n\nI mention this because this caused me a considerable amount of panic at one point when I thought my timezone information had been lost when importing data from POSIXct into Arrow. Nothing was lost, it is simply that the arrow R package prints all timestamps in the corresponding UTC time regardless of what timezone is specified in the metadata.\nThere is, however, a catch. This worked last time because I was diligent and ensured that my now variable encoded the timezone. By default, a POSIXct object created by Sys.time() will not include the timezone. It’s easy to forget this because the print() method for POSIXct objects will inspect the system timezone if the POSIXct object doesn’t contain any timezone information, so it can often look like you have a timezone stored in your POSIXct object when actually you don’t. When that happens, Arrow can’t help you. Because the POSIXct object does not have a timezone (all appearances to the contrary), the object that arrives in Arrow won’t have a timezone either. Here’s what I mean:\n\n# a POSIXct object with no timezone\nnew_now &lt;- Sys.time() # has no time zone...\nnew_now               # ... but appears to!\n\n[1] \"2023-05-27 18:24:15 AEST\"\n\n# an Arrow timestamp with no timezone\nnew_now_timestamp &lt;- scalar(new_now)\nnew_now_timestamp$type\n\nTimestamp\ntimestamp[us, tz=Australia/Sydney]\n\n\nThe take-home message in all this is that if you’re going to be working in both Arrow and R, and using the arrow package for data interchange, you’d be well advised to be careful with your POSIXct objects and timezones. They are trickier than they look, and can lead to subtle translation errors if you are not careful!\n\n\n\nUm, but what about POSIXlt?\nAt long last we come to POSIXlt, which has no clear analog in Arrow. The key idea behind POSIXlt is to represent temporal information in terms of multiple different units: days, weeks, years, seconds, timezones, and so on. It’s a very different kind of thing to a POSIXct object in R or a timestamp in Arrow. In R terms, it’s essentially a list, and as a consequence the default behaviour in arrow is to import it as a struct (which serves essentially the same purpose). Here’s how that looks:\n\nscalar(now_lt)\n\nScalar\n2023-05-27 17:24:15\n\n\nThe struct object contains named fields that are identical to their POSIXlt equivalents, and whose types have been translated according to the default mappings: sec is a double, min is an int32, hour is an int32, zone is a string, and so on.\nThis arrangement, where POSIXct maps to timestamp and POSIXlt maps to struct, makes perfect sense when you think about the underlying data structures that POSIXct and POSIXlt encode. Where things can be tricky for the R user is in the “mental account keeping”. In order to be helpful, R displays POSIXct and POSIXlt objects in exactly the same way:\n\nnow\nnow_lt\n\n[1] \"2023-05-27 17:24:15 JST\"\n[1] \"2023-05-27 17:24:15 JST\"\n\n\nNot only that, because POSIXct and POSIXlt are both subclasses of the POSIXt class, R allows you to perform temporal arithmetic on objects of different types:\n\nnow - now_lt\n\nTime difference of 0 secs\n\n\nThis is very convenient from a data analysis perspective, since calculations performed with date/time classes “just work” even though POSIXct objects are secretly doubles and POSIXlt objects are secretly lists. However, all this happens invisibly. In much the same way that it’s easy to forget that POSIXct objects may not encode a timezone even though they look like they do, it can be easy to forget that POSIXct and POSIXlt are fundamentally different objects, and they map onto quite different data structures in Arrow.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#duration-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#duration-types",
    "title": "Data types in Arrow and R",
    "section": "Duration types",
    "text": "Duration types\nAny discussion of temporal data is incomplete without a discussion of duration types, which are used to describe a length of time without reference to any fixed origin. The figure below shows the default mappings used by arrow:\n\n\n\n\n\nDefault mappings for duration types\n\n\n\n\n\n\n[R] The difftime class\nIn base R, the difference between two date/time objects is stored as a difftime object. To give a better illustration of a difftime object, let’s create diff, a variable that stores the amount of time elapsed between executing the R markdown chunk that first computed the now variable, and executing the R markdown chunk below:\n\nnew_now &lt;- Sys.time()\nrmd_time &lt;- new_now - now\nrmd_time\n\nTime difference of 0.3497994 secs\n\n\nNow let’s take a look at how it’s actually stored:\n\nunclass(rmd_time)\n\n[1] 0.3497994\nattr(,\"units\")\n[1] \"secs\"\n\n\nThe duration is represented as a double variable, and the \"units\" attribute is used to specify the time unit that it represents: “secs”, “mins”, “hours”, “days” or “weeks”. Unless the user specifies exactly which unit is to be used, R will attempt to make a sensible choice. For instance, if I were to do this,\n\nhedy_lamarr &lt;- as.POSIXct(\"1914-11-09 19:30:00\", tz = \"Europe/Vienna\")\nhedy_age &lt;- now - hedy_lamarr\nhedy_age\n\nTime difference of 39645.58 days\n\n\nI would learn that it has been 39646 days since Hedy Lamarr was born.32 More to the point, R has guessed that the length of time is sufficiently long that “seconds” aren’t the appropriate encoding unit:\n\nunclass(hedy_age)\n\n[1] 39645.58\nattr(,\"units\")\n[1] \"days\"\n\n\n\n\n\n[Arrow] The duration type\nThe difftime class in R has a natural analog in Arrow, the duration type. As usual though, they are not exactly equivalent to one another. An R difftime object stores the value as a double, so it has no problems storing 0.35 as the value and setting the units to be seconds. This doesn’t work very well in Arrow because the value is stored as a signed 64 bit integer (int64), and a value of 0.35 seconds will simply round down to a duration of zero seconds. When importing my duration data into Arrow, then, I should be careful to ensure I choose a higher precision unit. If I don’t, things can go a little awry:\n\nrmd_time_arrow &lt;- scalar(rmd_time)\nrmd_time_arrow\n\nScalar\n0\n\n\nHm. Zero seconds was not exactly the answer I was looking for. It helps a little to take a peek at the data type and see what precisely it is that I have just created:\n\nrmd_time_arrow$type\n\nDurationType\nduration[s]\n\n\nThis reveals my mistake. I’ve encoded the time rounded to the nearest second, which is not very useful in this instance. What I really should have done is specify a higher level of precision. To import a duration into Arrow rounded to the nearest microsecond, I can do this:\n\nrmd_time_arrow &lt;- scalar(rmd_time, duration(unit = \"us\"))\nrmd_time_arrow\n\nScalar\n349799\n\n\nThat’s a little better! Again, I can inspect the data type and see that the unit of encoding is now set to microseconds:\n\nrmd_time_arrow$type\n\nDurationType\nduration[us]\n\n\n\n\n\n[R] The hms::hms class\nSo where are we up to in our voyage through the world of dates, times, and durations in the R world? We’ve talked about situations where you can specify a fixed date (with the Date class) and situations where you can specify a fixed moment in time (with POSIXct and POSIXlt classes). We’ve also talked about situations where you can specify an amount of time without fixing it to a specific date or time (with the difftime class). What we haven’t talked about is how to store the time of day. In base R you can talk about a date without needing to specify a time, or you can talk about times and dates together, but what you can’t do is specify a time on its own without a date.\nThe hms package fixes this by supplying the hms class. Internally, it’s just a difftime object that counts the number of seconds elapsed since midnight. As I type these words the current time is 14:05:25, and I could create an hms object representing this like so:\n\nhms_time &lt;- hms::hms(seconds = 25, minutes = 5, hours = 14)\nhms_time\n\n14:05:25\n\n\nThe nice thing about hms times is that they inherit from difftime, which we can see by checking the class vector for our hms_time object\n\nclass(hms_time)\n\n[1] \"hms\"      \"difftime\"\n\n\nJust to show that there really isn’t anything fancy going on, let’s strip the class attribute away and let R print out the raw object. As the output here shows, an hms object has the same underlying structure as a regular difftime object:\n\nunclass(hms_time)\n\n[1] 50725\nattr(,\"units\")\n[1] \"secs\"\n\n\n\n\n\n[Arrow] The time32 and time64 types\nWhat about Arrow?\nAt a technical level, it would be perfectly possible to translate an hms object in R into an Arrow duration object, but that feels slightly unnatural. The entire reason why the hms class exists in R is that we – the human users – attach special meaning to the “duration of time that has elapsed since midnight on an arbitrary day”. We call it the time of day, and while technically it is possible to represent the time of day as a duration (or an hms as a difftime), human beings like to treat special things as special for a reason.\nBecause of this, Arrow supplies two data types that are roughly analogous to the hms class in R, called time32 and time64. The time32 type stores the time of day as a signed 32-bit integer, which represents the number of seconds (or alternatively, milliseconds) since midnight. By default, the arrow package will translate an hms object to a time32 type, using seconds as the unit:\n\nhms_time32_s &lt;- scalar(hms_time)\nhms_time32_s\n\nScalar\n14:05:25\n\n\nAs usual, let’s just verify that the encoding unit is as expected:\n\nhms_time32_s$type\n\nTime32\ntime32[s]\n\n\nYep, we’re all good! To switch to milliseconds, I would use a command like this:\n\nhms_time32_ms &lt;- scalar(hms_time, time32(unit = \"ms\"))\nhms_time32_ms\n\nScalar\n14:05:25.000\n\n\nNotice that the output shows the time in a little more resolution. I find this a helpful touch, since it provides a visual cue letting me know what the unit is. But just to confirm, let’s inspect the type explicitly:\n\nhms_time32_ms$type\n\nTime32\ntime32[ms]\n\n\nIf you need to represent the time of day at a higher degree of precision, you’ll want to use the time64 type, which (shockingly!) represents the time of day as a signed 64-bit integer. When using the time64 class you can choose microseconds (unit = \"us\") or nanoseconds (the default, unit = \"ns\") as the unit:\n\nhms_time64_us &lt;- scalar(hms_time, time64())\nhms_time64_us\n\nScalar\n14:05:25.000000000\n\n\nThe display is showing more trailing zeros, so you can already be sure that the encoding unit has changed. So the only real question you might have pertains to the author. Will she be tediously predictable and check the data type yet again to verify that the encoding unit is nanoseconds, just like she has done every time before? Yes. Yes she will:\n\nhms_time64_us$type\n\nTime64\ntime64[ns]\n\n\nShe is quite tiresome at times.\nIn essence, the arrow defaults are set up such that if you choose time32() when going from R to Arrow without specifying a unit, you will end up with the lowest precision representation of time (rounded to the nearest second), whereas if you do the same with time64() you end up with the highest precision (nanosecond level) representation. When going the other way, arrow will map time32 types and time64 types to hms objects, and the end result is that the time of day will be stored as a double.\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#other-types",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#other-types",
    "title": "Data types in Arrow and R",
    "section": "Other types",
    "text": "Other types\nAs with any story told by humans, this one is incomplete. When I started writing this post I had the ambition to cover every single line in the Table of R/Arrow mappings shown in the arrow documentation. I didn’t quite get there in the end, and there are a few missing cases. I’ll briefly mention them here:\n\nArrow possesses a “null” value used to represent missing data, and behaves similarly to NA in R. In base R there are several different NA values, corresponding to the different atomic types: NA_logical, NA_character. The way this is handled in arrow is to rely on the vctrs package. Specifically, in vctrs there is a vctrs_unspecified class that works very well here, so Arrow nulls map to vctrs_unspecified and vice versa. In practice, this is where NA values enter into the picture.\nIn R there is the concept of the raw type used to represent bytes. Arrow doesn’t have a natural equivalent of this, but the closest is an unsigned 8-bit integer, so the default is to map raw to uint8.\nI haven’t talked about factors at all, and frankly I probably should have. My only excuse is exhaustion. The post was getting very long and I ran out of energy. The analog of factors in Arrow is the dictionary type. They’re not exact mirrors of each other so it’s worth reading the documentation, but it’s close enough that factors are mapped to dictionaries and vice versa.\nR and Arrow both allow more complicated data structures to be included as columns within a data frame (or Table). For example, in R each element of a column can itself be a data frame. In such cases, the default in arrow is to map each R data frame onto an Arrow struct. Again, this is one where it’s worth reading the documentation, because there are some subtleties with how things like list columns are handled.\n\n\n\n\n\n\n\nArt"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#the-magic-goes-away",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#the-magic-goes-away",
    "title": "Data types in Arrow and R",
    "section": "The magic goes away",
    "text": "The magic goes away\n\n“Being in love ruins my judgement. It takes my mind off what I’m doing, and I ruin spells”     – Mirandee, from The Magic Goes Away by Larry Niven\n\nWhen I first started using arrow, it was the magic I loved most. Everything just worked. I could move data between R and Arrow without having to think, I could manipulate enormous data sets using dplyr syntax that I’d never even be able to load into R, and I never had to look under the hood. Magic is always compelling. It is delightful. It makes the user feel joy, and it’s the experience the developer wants to provide.\nBut as any teacher will tell you, the magic always goes away. There comes a time when you have to sit down and read the manuals. You have to understand how the magic works, and when you do understand you realise there is no magic. At best, there is design. A system can work using all the default settings because it has been thoughtfully designed, but you will eventually encounter situations when the defaults don’t apply to you. It’s taken me some time to piece all this together, and at the end of the process I feel a lot more confident in my judgment. Having a deeper understanding of data types in Arrow and R is useful to me, even if I’m only using the default schemas.\nI hope the post is helpful for anyone else following a similar path.33"
  },
  {
    "objectID": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#footnotes",
    "href": "posts/2022-03-04_data-types-in-arrow-and-r/index.html#footnotes",
    "title": "Data types in Arrow and R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be honest, if my goal had been to read the data into Arrow, there was never a need to read it into R in the first place. The code read_csv_arrow(\"magicians.csv\", as_data_frame = FALSE) would create the Table object directly with no need for an intermediary in R. However, for the purposes of this post I’m pretending that I have data in R that I want to transfer to Arrow in order to talk about the translation process, so I’m doing things in a rather inefficient way. On top of that, there’s a whole extra layer of complexity I’m hiding here that relates to ALTREP vectors backed by Arrow. However, that’s a topic for a future post. We can only unravel the magical cloak one thread at a time!↩︎\nEarly in the writing process for this post I talked to the very lovely Jon Keane about this, and they pointed out that there’s a big difference in the “read” functions in arrow and the “translate” functions. The code in the read_*() functions is complicated: it handles everything for you because “reading the data” is a constrained task and the developers can optimise almost everything for you, including the strange edge cases. Translation is harder, and there’s less the developers can do to support it. As a consequence, the code underpinning schema() is much simpler. It takes care of choices for you when those choices are “obvious”, but it leaves all the edge cases for you to deal with. There are a lot of weird edge cases because “translating between languages” is under-determined. Edge cases are left to the user because only the user knows the context.↩︎\nIf you have never watched The Magicians I really need to emphasise that it was a really, really weird show and this is nowhere near as implausible as it might sound. I have literally scared off men who had previously been quite keen on me by making them watch this show. Not kidding.↩︎\nBack my former life as an mathematical psychologist I studied this stuff for a living, and wrote an absurd number of academic papers on how people learn and represent categories. What I wanted to understand was the relationship between human and machine learning, between natural and artificial intelligence, and so on. What I actually learned is that humans and machines are both super weird.↩︎\nAt this point I feel compelled to point out that while I may appear to be a bright young thing with little experience of the world, I am in fact a 44 year old woman and former academic who has read quite a few papers on algorithmic information theory and related fields. So, if you happen to be thinking thoughts along the lines of “aha, she doesn’t know about Turing equivalence!” and are considering explaining it to me… please don’t. This is one of those situations where that pesky little “finite length” prefix code required to allow machine P to simulate machine Q actually matters, and sometimes “finite” includes values like “staggeringly, painfully large”. As I used to wearily tell my students, in real life you can’t ignore the \\(O(1)\\) terms.↩︎\nAs an aside, Michel Foucault actually refers to this Borges passage in the preface to his famous work The Order of Things: An Archaeology of the Human Sciences on how different cultures and historical periods viewed the world from fundamentally different perspectives. According to Foucault, Borges essay “shattered … all the familiar landmarks of thought — our thought, the thought that bears the stamp of our age and our geography — breaking up all the ordered surfaces and all the planes with which we are accustomed to tame the wild profusion of existing things”. Seems fair to say that Borges’ essay was not without its admirers.↩︎\nMuch like the Honourable Member for Kennedy, this “let a thousand OOP systems bloom” philosophy has made me cry many times: sometimes from joy, often from frustration.↩︎\nThe typographical notation for R6 varies because the term can be used to refer to the OOP system itself (which I denote as R6), but could also refer to the R6 package or the R6 object type.↩︎\nNote that the Table object is not a Table: it is the class prototype for Table objects, and is therefore classed R6ClassGenerator. The same is true for Scalar, Schema, ChunkedArray, etc.↩︎\nShould I take this opportunity to discuss the fact that this means that what R actually implements is a three valued logic? No. No I should not. I am going to think it very loudly though.↩︎\nLest anyone think I am too wanton in my use of footnotes in tech blogging, I will simply mention that Dan has a 77-footnote blog post on Gaussian Processes. I am the nice queer in the room.↩︎\nThere is also an R6 method you could use here. snowpiercer$as_vector() would produce exactly the same answer.↩︎\nOkay that was a tortured metaphor and one that I probably don’t want to push too far given that Snowpiercer is a terrifyingly dark show filled with horrible, horrible people. But I’m nice though. Honest! Would I lie to you?↩︎\nRest assured, dear reader, while I am entirely aware of the distinction between countably and uncountably infinite sets, and have been forced to learn more about the cardinality of transfinite numbers than any human deserves to endure, I will not be discussing any of that in this post. The word “infinite” is perfectly serviceable for our purposes, and if anyone even TRIES to discuss this with me further on twitter I will be forced to engage the services of a very unpleasant lawyer…↩︎\nThe “desert of the real” phrase in the title of the section refers to the real numbers, but it’s also a quote from Simulacra and Simulation and The Matrix. Obviously I encountered it in The Matrix first because, all appearances to the contrary, I am uncultured swine.↩︎\nAt this point it is traditional to make a joke about Zeno’s paradox of Achilles and the Tortoise and honestly I did try, but to come up with a good joke I first had to get half an idea, and then I had to refine it half way to being a better idea, and then I had to refine that half way and… I guess I never quite got there in the end. Sorry.↩︎\nSigh. Technically, this is the last bit. R uses a little-endian representation here which I have flipped to a big-endian format so that it can be read from left to right, but for the love of all that is good and precious in this world please let me simplify a few things okay?↩︎\nOkay, if you were reading closely earlier you might be thinking this is wrong and the range should be -1023 to 1024. The reason it’s not is that those to values are reserved for “special” numbers.↩︎\nI still think “Man Tissa” would make a good drag king name, though I’ve been unreliably informed that it may sound odd to Norwegians.↩︎\nIn fact, one of the obnoxious things about the reals (which are uncountably infinite) is that almost all reals have infinitely long mantissas. Even if you had a infinite number of digits to work with (a countably infinite set) you’re still in trouble. Everything sucks, real numbers are almost surely uncomputable (yes that’s an actual result), and I don’t want to think about it any more and I need a lie down.↩︎\nOkay, so contrary to my stated intentions we’ve actually ended up quite a long way down into the IEEE 754 standard, so I might as well make an extra observation while we’re down here. Tests of floating point equality aren’t tests of mathematical equality. They’re really just checks that the absolute difference between two numbers is smaller than the machine precision. The value of the machine precision is stored in R as .Machine$double.eps.↩︎\nSome precision is needed here: in this post I am using the names that appear in the documentation to the arrow R package. I’m doing this because the intended audience is an R user who wants to use the arrow package to interact with Arrow. However, you should be aware that these types are given slightly different names in the C++ documentation to libarrow to which arrow provides bindings. In that documentation the terminology is as follows: float64 is called double, float32 is called float, and float16 is called half-float.↩︎\nThey are both little-endian too.↩︎\nThat’s assuming we’re writing numbers in Arabic number system. I suppose it would be different if our orthographic representations adopted the Sumerian sexagesimal notation. Admittedly, neither R nor Arrow existed at the time, so it’s a moot point. All I’m really saying is that decimal systems are no less arbitrary than binary ones. The problem arises because of the mismatch, not because one encoding is inherently better than the other.↩︎\nAs noted in the R internals manual, the specific data structure is referred to as a CHARSXP. For the purposes of the current post I’m pretending that character strings are always encoded as UTF-8 because there’s no need to complicate matters by talking about things like Latin-1, but be aware that R does support those things. If you’re looking for a good overview of what UTF-8 encoding is all about, this blog post on how unicode works is helpful.↩︎\nMore strictly, it is a symbol that points to an object. R makes a distinction between the symbol and the object to which it links, and allows multiple labels to point at the same object. There’s an excellent discussion of this in Chapter 3 of Advanced R↩︎\nOnce again, there’s a little terminology to explain because there’s some inconsistency in how the types are referred to in the Arrow C++ documentation and the arrow R package documentation. In the list of data types for libarrow you’ll find references to string types and large_string types. However, in list of data types documentation for the arrow R package you’ll see the same data types referred to as utf8 and large_utf8.↩︎\nNo I am not going to talk about them in this post. Masochist though I may be, nobody loves that much pain. If you are curious you will simply have to wait for a later post!↩︎\nPerhaps surprisingly, this number is encoded as a double and not an integer.↩︎\nAgain, this is stored as a double and not an integer. You can verify this with the command typeof(Sys.time())↩︎\nA little counterintuitively, the value of sec ranges from 0 to 61, presumably because leap seconds are a thing. I am NOT going to torture myself with that one today. My life has quite enough torture in it already.↩︎\nYes, I looked up her time and location of birth on an astrology website. I am, after all, a queer. I feel that I have been quite clear on this, and being weird about astrology it is one of the fine traditions of our people.↩︎\nA thank you to Jon Keane, Steph Hazlitt, and Neal Richardson for helpful conversations, encouragement, and feedback that have greatly improved the post.↩︎"
  }
]