---
title: "gamlss"
description: "This is a subtitle"
date: "2025-08-16"
knitr:
  opts_chunk: 
    dev.args:
      bg: "#00000000"
--- 

<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)

theme_custom <- function() {

  transparent_rect <- ggplot2::element_rect(fill = NA, color = NA)
  grey_text <- ggplot2::element_text(color = "#888")
  rotated_grey_text <- ggplot2::element_text(color = "#888", angle = 90)
  light_grey_rect <- ggplot2::element_rect(fill = "#ccc", color = "#ccc")

  plot_margin <- ggplot2::margin(18, 18, 18, 18)

  strip_margin <- ggplot2::margin(4.4, 4.4, 4.4, 4.4)
  strip_background <- ggplot2::element_rect(fill = "#333", color = "#333")
  strip_text <- ggplot2::element_text(colour = "#ddd", margin = strip_margin)

  ggplot2::`%+replace%`(
    ggplot2::theme_bw(),
    ggplot2::theme(

      # plot exterior regions
      plot.margin = plot_margin,
      plot.background   = transparent_rect,
      legend.background = transparent_rect,
      legend.key        = light_grey_rect,
      plot.title    = grey_text,
      plot.subtitle = grey_text,
      plot.caption  = grey_text,
      axis.text.x   = grey_text,
      axis.text.y   = grey_text,
      axis.title.x  = grey_text,
      axis.title.y  = rotated_grey_text, 
      legend.text   = grey_text,
      legend.title  = grey_text,

      # plot strip region
      strip.background = strip_background,
      strip.text = strip_text,

      # plot interior background
      panel.background = light_grey_rect
    ) 
  )
}
ggplot2::theme_set(theme_custom())

post_dir <- rprojroot::find_root_file(criterion = rprojroot::has_file(".here"))
```

<!--------------- post begins here ----------------->

Okay, so in my [last post](/posts/2025-08-02_box-cox-power-exponential/) I was whining about the Box-Cox power exponential distribution, and promising that it would be followed by a new post whining about the [generalised additive model for location, shape and scale](https://en.wikipedia.org/wiki/Generalized_additive_model_for_location,_scale_and_shape) (GAMLSS). This is that post.

My introduction to GAMLSS models came from a problem that is easy to state, and strangely difficult to solve. If you know a person's age $a$ and their sex $s$, describe (and sample from) the joint distribution over height $h$ and weight $w$ for people of that age and sex. That is, estimate the following distribution:

$$
p(h, w | a, s)
$$

If you don't think very hard this doesn't seem like it should be very difficult. Data sets containing age, sex, height and weight are not too difficult to find. The question doesn't ask for any fancy longitudinal modelling: all we're being asked to do is estimate a probability distribution. Not difficult at all. It's a problem I've encountered several times in my pharmacometric work -- very commonly, we have to run simulations to explore the expected distribution of drug exposures among a population of patients that varies in weight, height, age, and sex -- so it's one I've had to think about a few times now, and it's a trickier than it looks. Typically we have to rely on fairly large population survey data sets (e.g. NHANES) and sophisticated regression models (e.g. GAMLSS), and I will confess that neither of these two things were covered in my undergraduate classes. 

It's been a rather steep learning curve, and in keeping with my usual habit, I decided it might be wise to write up some notes. I know with a dread certainty that I'll have to use this knowledge again in a few months, and given how awful my memory is these days I'd prefer not to have to relearn the whole thing from the beginning next time. 

```{r}
#| label: load-packages
#| message: false
library(fs)
library(dplyr)
library(purrr)
library(haven)
library(readr)
library(tidyr)
library(gamlss)
library(tibble)
library(ggplot2)
library(quartose)
```

## The NHANES data

The data set I'll look at in this post comes from the [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/) (NHANES), a large ongoing study conducted under the auspices of the United States [Center for Disease Control](https://www.cdc.gov/nchs/nhanes/) (CDC). The program started in the 1960s, but the data set that is conventionally called "the NHANES data set" refers to the *continuous* data set that has been collected since 1999, with updates released every couple of years with approximately 5000 participants added each year. The data collection process is quite extensive. As described on the website, it includes:

> - Interviews about health, diet, and personal, social, and economic characteristics
> - Visits to our mobile exam center for dental exams and health and body measurements
> - Laboratory tests by highly trained medical professionals

For any given participant, records are split across several data files. For instance, the [body measurements](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination) table contains very typical measurements such as height and weight, but also includes variables like waist circumference, thigh circumference, and so on. The [demographics](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics) table data includes information like age, sex, and so forth. These are the only two tables I'll use in this post, but there are a great many others also: you can find blood pressure data, audiometry data, various clinical measurements, and so on. It's a very handy data set used for a variety of purposes, not least of which is the fact that NHANES data are used to build the [CDC growth charts](https://www.cdc.gov/growthcharts) that are themselves used for a wide range of scientific and medical purposes. In my own work as a newly-minted pharmacometrician I've needed to rely on the NHANES data and/or CDC growth charts in several projects over the last couple of years, and for that reason I've found it useful to dive into the data set quite a few times. 

As of the most recent release, the summaries below list the file names the body measurement (BMX) and demographics (DEMO) tables:

```{r}
#| results: asis
#| message: false
#| label: nhanes-metadata
metadata <- list( 
  BMX  = read_csv(path(post_dir, "nhanes", "bmx-summary.csv"), show_col_types = FALSE),
  DEMO = read_csv(path(post_dir, "nhanes", "demo-summary.csv"), show_col_types = FALSE) 
)

quarto_tabset(metadata, level = 3)
```

As you can see from the summaries, the data sets are released as SAS transport (i.e., XPT) files, with letters used to represent each data cut. In principle these should go from "A" (data from the 1999-2000 cohort) through to "L" (data from the 2021-2023 cohort), but that's not precisely what happens. The original release (which ostensibly should be the "A" data cut) doesn't have a label, and the "K" data cut is missing entirely due to the COVID-19 pandemic. In its place there is a "P" version of data set that breaks the file naming scheme, presumably short for "pandemic", and labelled differently to highlight that data from that release might be a little strange I suppose.  

### Importing the data

To explore the NHANES data, we can download all these files and load them into R:

```{r}
#| label: nhanes-import
# all demographics and body measurement files
demo_files <- dir_ls(path(post_dir, "nhanes", "data", "demo"))
bmx_files  <- dir_ls(path(post_dir, "nhanes", "data", "bmx"))

# read demographics file (selected variables only)
demos <- demo_files |> 
  map(\(xx) {
    dd <- read_xpt(xx) 
    if (!exists("RIDEXAGM", where = dd)) dd$RIDEXAGM <- NA_real_
    dd <- select(dd, SEQN, RIAGENDR, RIDAGEYR, RIDAGEMN, RIDEXAGM)
    dd
  }) |> 
  bind_rows(.id = "file_demo") |> 
  mutate(file_demo = path_file(file_demo)) |>
  left_join(metadata$DEMO, by = "file_demo") |>
  select(-description)

# read body measurements file (selected variables only)
bmxes <- bmx_files |> 
  map(\(xx) {
    dd <- read_xpt(xx) 
    dd <- select(dd, SEQN, BMXWT, BMXHT, BMXRECUM)
    dd
}) |> 
  bind_rows(.id = "file_bmx") |> 
  mutate(file_bmx = path_file(file_bmx)) |>
  left_join(metadata$BMX, by = "file_bmx") |>
  select(-description)
```

Looking at the code above, you can see that I haven't included *all* the columns from the BMX and DEMO tables, only the ones that are most relevant to my purposes. My `demos` and `bmxes` data frames are considerably smaller than they would be if I included everything. Importantly for our purposes the `SEQN` column serves as an id variable, and we can use it to join the tables. In the code below I'm using `left_join(bmxes, demos)` to do the work because I'm only really interested in those cases where the body measurement data exists, and I'm tidying the column names a little for the sake of my sanity: 

```{r}
#| label: nhanes-join
nhanes <- bmxes |>
  left_join(demos, by = c("SEQN", "cohort")) |>
  select(
    id          = SEQN,
    sex_s       = RIAGENDR, # sex/gender at screen (1 = M, 2 = F, . = NA)
    weight_kg_e = BMXWT,    # weight at exam
    height_cm_e = BMXHT,    # standing height at exam
    length_cm_e = BMXRECUM, # recumbent length at exam (0-47 months only)
    age_yr_s    = RIDAGEYR, # natal age at screening (years)
    age_mn_s    = RIDAGEMN, # natal age at screening (months; 0-24 mos only)
    age_mn_e    = RIDEXAGM, # natal age at exam (months; 0-19 years only)
    cohort
  )

 nhanes
```

Very nice indeed. 

### Preprocessing choices

Even with the small snippet shown above you can see hints of nuances that arise when working with the NHANES data, especially if pediatric age ranges are of interest (as they very often are for me). Suppose one of the variables of interest in your analysis is height. On row 4 we have data from a 22 month old boy: there is no height measurement for him. That's very often the case for young infants, most obviously because very young infants can't stand up so you can't measure standing height. What you *can* do for those infants is take a related measurement: recumbent length. Okay that makes sense: you measure height for adults, and length for babies. Except... there are some kids in the age range where it makes sense to take *both* measurements. Row 1 contains data from a 29 month old girl, who was measured as 91.6cm in height and... 93.2cm in length. 

Ah.

Right. The height of a standing human and the length of the same human lying down don't have to be the same, and the differences between the two won't be random measurement error, because gravity and posture are real things that exist! The NHANES data itself illustrates that:

```{r}
#| label: height-vs-length
ok <- function(x) !is.na(x)
nhanes |> 
  filter(ok(height_cm_e), ok(length_cm_e)) |> 
  summarise(
    mean_diff = mean(length_cm_e - height_cm_e),
    sd_diff = sd(length_cm_e - height_cm_e)
  )
```

So, when considering what *height* to use for the boy in row 4, it's not necessarily a good idea to substitute his *length*, because those probably aren't the same. An imputation procedure of some kind is needed. I won't go into it in this post, but I played around with a few possible models for predicting length from height in the process of writing it, and in the end concluded that nothing fancy is needed here. A simple adjustment will suffice: 

```{r}
#| label: height-vs-length-2
length_to_height <- function(length_cm) {
  length_cm - 1.06  # adjust based on average difference in NHANES
}
```

It feels a bit silly writing a function for this conversion, but it's helpful from a documentation purpose: creating a dummy function to use (rather than just subtracting 1.06 later in the code) makes it clear to the reader that I *am* using some kind of explicit conversion when I transform length to height, and the comment provides a little extra detail on how I chose the conversion rule.

Similar nuances for other measurements exist. There are three different measurements that record age, and they don't have to be in agreement: the *survey* takes place at a different point in time to the *exam*, so it's possible that the (now relabelled) `age_mn_e` and `age_mn_s` variables are different. Age in years at the time of the survey `age_yr_s` should presumably be consistent with the age in months at the time of the survey, but data aren't always straightforward. Given all this, the preprocessing steps used to create measurements of age and height look like this:

```{r}
#| label: nhanes-preprocessing
nhanes <- nhanes |>
  mutate(
    sex_num = sex_s - 1, # rescale to 0 = M, 1 = F
    sex_fct = factor(sex_s, levels = 1:2, labels = c("male", "female")),
    age_mn = case_when(
      !is.na(age_mn_e) ~ age_mn_e, # use exam months if present
      !is.na(age_mn_s) ~ age_mn_s, # else use survey months
      TRUE ~ (age_yr_s * 12)       # else use age in years
    ),
    age_yr    = age_mn / 12,
    weight_kg = weight_kg_e,
    height_cm = case_when(
      !is.na(height_cm_e) ~ height_cm_e, # use height if it was measured
      !is.na(length_cm_e) ~ length_to_height(length_cm_e), # or convert length
      TRUE ~ NA_real_, # else missing
    )
  )
```

The NHANES data set doesn't include information about transgender status or intersex status (at least not in the demographics and body measurements tables), and so for the purposes of NHANES-based analyses sex is a treated as binary variable, and doesn't require any extra steps beyond tidying and attaching nice labels. A similar story holds for weight: we have one measurement, the weight at time of exam, and that's what we'll use. 

At this point the unwary might fall into the trap of thinking that we're done, but -- alas -- we aren't quite there yet. There's one more detail to consider regarding the age data. In the NHANES data set the age values stop at 80, but that value doesn't literally mean "80 years" it means "80 years or older". Because of that, you need to take some care in how you work with data for older participants. As it happens, I've never had to deal with this in the wild because my projects have used NHANES data in the pediatric setting. Given that, I'll keep it simple here and just drop all cases with age recorded as "80+" years. Not a problem if your application is pediatric, but you wouldn't want to do this in situations where you're interested in older populations. 

In any case, having noted this I can finally complete the preprocessing and arrive at my `nhanes` data frame:

```{r}
nhanes <- nhanes |>
  select(id, sex_num, sex_fct, weight_kg, height_cm, age_mn, age_yr, cohort) |>
  filter(ok(sex_num), ok(weight_kg), ok(height_cm), ok(age_mn)) |>
  filter(age_yr < 80)

nhanes
```

A small slice through a very large data set, but one that contains the four variables that I need almost every time I have to use NHANES data in a pharmacometric analysis: `age_mn` ($a$), `sex_fct` ($s$), `height_cm` ($h$), and `weight_kg` ($w$). I can now turn to the substantive statistical problem: estimating the joint conditional density $p(h, w | a, s)$.


## The GAMLSS framework

### Structural model

Let $\tilde{y}_i = E[y_i|x_{i1}, \ldots, x_{ip}]$ denote the expected value of (the $i$-th observation of) the outcome variable $y$, conditioned on knowing the values of $p$ predictor variables $x_{1}, \ldots, x_{p}$. In linear regression we propose that

$$
\tilde{y}_i = \beta_0 + \sum_{k=1}^p \beta_k \ x_{ik}
$$

Often we would rewrite this in matrix notation and express it as $\tilde{\mathbf{y}} = \mathbf{X} \mathbf{\beta}$ to make it look pretty, but I honestly don't think it adds much in this context. There are two different paths you could pursue when extending this framework: you could change something on the left hand side, or you could change something on the right hand side. For example, the [generalised linear model](https://en.wikipedia.org/wiki/Generalized_linear_model) introduced by [Nelder and Wedderburn (1972)](https://www.jstor.org/stable/2344614) modifies the linear model by supplying a "link function" $g()$ that transforms $\tilde{y}$, 

$$
g(\tilde{y}_i) = \beta_0 + \sum_{k=1}^p \beta_k \ x_{ik}
$$

Crudely put, the GLM applies a linear model to a transformed **outcome** variable (i.e., $y$). On the other hand, in the [additive model](https://en.wikipedia.org/wiki/Additive_model) proposed by [Friedman and Stuetzle (1981)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477729), transformation functions $f()$ -- sometimes called "smoothing functions" -- are applied to the **predictor** variables (i.e., $x$):

$$
\tilde{y}_i = \beta_0 + \sum_{k=1}^p f_k(x_{ik})
$$

I'll talk more about the choice of smoothing function $f()$ later in the post, but for now the key thing to note is that the functions used for this purpose usually have free parameters that need to be estimated from the data. Because of this, fitting an additive model is a little more complicated than a simple "transform the predictors first and then fit a linear regression later" procedure. 

In any case, having noted that **generalised** linear models introduce the link function $g()$ and **additive** models introduce the smoothing functions $f()$, it seems natural to consider a modelling framework that uses both of these things. That framework exists: it is called the [generalised additive model](https://en.wikipedia.org/wiki/Generalized_additive_model) (GAM), introduced by [Hastie and Tibshirani (1990)](https://www.jstor.org/stable/2245459):

$$
g(\tilde{y}_i) = \beta_0 + \sum_{k=1}^p f_k(x_{ik})
$$

### Measurement model

In linear model with homogeneity of variance, we normally write this:

$$
y_i = \tilde{y}_i + \epsilon_i
$$

where the measurement error terms $\epsilon_i$ are assumed to be normally distributed with mean fixed at 0 and standard deviation $\sigma$ estimated from the data. Another way to express the same idea would be to say that the outcome variable $y_i$ is normally distributed with mean $\mu_i = \tilde{y}_i$, and a constant standard deviation $\sigma_i = \sigma$. Taking this line of reasoning a little further, we could rewrite the model in terms of *two* regression models: a linear model for the mean $\mu$, and another linear model for the standard deviation $\sigma$:

$$
\begin{array}{rcl}
\mu_i &=& \beta_{\mu 0} + \sum_k \beta_{\mu k} \ x_{ik} \\
\sigma_i &=& \beta_{\sigma 0} \\ \\
y_i &\sim& \mbox{Normal}(\mu_i, \sigma_i)
\end{array}
$$

The only way in which this framing of the model differs from the usual form is that I've used $\beta_{\sigma 0}$ to refer to the to-be-estimated $\sigma$ parameter. The difference is purely notational, but it reflects a slight shift in mindset: it signals that we're now considering the possibility that we *could* have chosen to develop a richer regression model that allows each observation to have its $\sigma_i$, rather than use an "intercept only" model for the variance. Indeed when expressed this way, the "homogeneity of variance" assumption in linear regression now corresponds to the special case in which no covariates (predictors) are included in the model for $\sigma$. Moreover, it makes clear that there's no inherent reason to limit ourselves in this way. Relaxing homogeneity of variance allows us to specify regression models for both $\mu$ and $\sigma$, giving us the following framework for linear regression:

$$
\begin{array}{rcl}
\mu_i &=& \beta_{\mu 0} + \sum_k \beta_{\mu k} \ x_{ik} \\
\sigma_i &=& \beta_{\sigma 0}  + \sum_k \beta_{\sigma k} \ x_{ik} \\ \\
y_i &\sim& \mbox{Normal}(\mu_i, \sigma_i)
\end{array}
$$

Having made this conceptual shift, it's not too hard to see that we can repeat the line of reasoning from the previous section that took us from linear models to generalised additive models. To wit, if we replace the regression coefficients $\beta_{\mu k}$ and $\beta_{\sigma k}$ with smooth functions $f_{\mu k}()$ and $f_{\sigma k}()$ and use these to transform the predictors, we have an additive regression model for the mean and the standard deviation...

$$
\begin{array}{rcl}
\mu_i &=& \beta_{\mu 0} + \sum_k f_{\mu k}(x_{ik}) \\
\sigma_i &=& \beta_{\sigma 0} + \sum_k f_{\sigma k}(x_{ik}) \\ \\
y_i &\sim& \mbox{Normal}(\mu_i, \sigma_i)
\end{array}
$$

Adding link functions $g()$ to connect the additive predictor to the parameters gives a generalised additive regression on the mean and the standard deviation...

$$
\begin{array}{rcl}
g_\mu(\mu_i) &=& \beta_{\mu 0} + \sum_k f_{\mu k}(x_{ik}) \\
g_\sigma(\sigma_i) &=& \beta_{\sigma 0} + \sum_k f_{\sigma k}(x_{ik}) \\ \\
y_i &\sim& \mbox{Normal}(\mu_i, \sigma_i)
\end{array}
$$

Finally, we recognise that the normal distribution is not the only choice of measurement model. Like many other distributions it can be described as a distribution that contains one parameter for the **location** (i.e., $\mu$) and **scale** (i.e., $\sigma$). More generally though, we might want to use distributions described by one parameter for location, one parameter for scale, and one or more parameters for the **shape**. If we instead choose the [Box-Cox power exponential](/posts/2025-08-02_box-cox-power-exponential/) (BCPE) -- in which $\mu$ is a location parameter, $\sigma$ is a scale parameter, and $\nu$ and $\tau$ together control the shape -- the statistical model now looks like this...

$$
\begin{array}{rcl}
g_\mu(\mu_i) &=& \beta_{\mu 0} + \sum_k f_{\mu k}(x_{ik}) \\
g_\sigma(\sigma_i) &=& \beta_{\sigma 0} + \sum_k f_{\sigma k}(x_{ik}) \\
g_\nu(\nu_i) &=& \beta_{\nu 0} + \sum_k f_{\nu k}(x_{ik}) \\
g_\tau(\tau_i) &=& \beta_{\tau 0} + \sum_k f_{\tau k}(x_{ik}) \\ \\
y_i &\sim& \mbox{BCPE}(\mu_i, \sigma_i, \nu_i, \tau_i)
\end{array}
$$

and we now have an example of a **generalised additive model for location, scale, and shape** (GAMLSS), noting of course you don't have to use the BCPE specifically. The key requirement here is that the distributional family be flexible enough that you can separately model location, scale, and shape. There are a lot of distrubutions that satisfy this property, though for the purposes of this post I am just going to stick with BCPE.^[One thing I will note as an aside, however, is that this stipulation is one of the key ways in which GAMLSS models extend GAM regressions and GLM analyses. In the GAM and GLM frameworks, the probabilistic component is assumed to be a distribution in the [exponential family](https://en.wikipedia.org/wiki/Exponential_family). The GAMLSS framework relaxes that considerably, insofar as location/scale/shape encompasses many distributions that fall outside the exponential family.]


### Model fitting

```{r}
# maximum age used in gamlss modelling; this restriction is
# to ensure that the gamlss models are well-behaved over
# the age range of most interest (i.e. pediatric)
age_max_yr <- 40

# subsets used for gamlss training
nhanes_m <- nhanes |> filter(sex_fct == "male", age_yr <= age_max_yr)
nhanes_f <- nhanes |> filter(sex_fct == "female", age_yr <= age_max_yr)

# gamlss settings that turned out not to matter much in this case
opt_control <- gamlss.control(c.crit = .001, n.cyc = 250)
```


```{r}
#| label: gamlss-height-models
#| eval: false
ht_m <- gamlss(
  formula       = height_cm ~ ps(age_mn),
  sigma.formula = ~ps(age_mn),
  nu.formula    = ~1,
  tau.formula   = ~1,
  data    = nhanes_m,
  family  = BCPE,
  control = opt_control
)

ht_f <- gamlss(
  formula       = height_cm ~ ps(age_mn),
  sigma.formula = ~ps(age_mn),
  nu.formula    = ~1,
  tau.formula   = ~1,
  data    = nhanes_f,
  family  = BCPE,
  control = opt_control
)
```

```{r}
#| label: gamlss-weight-models
#| eval: false
wt_htm <- gamlss(
  formula       = weight_kg ~ ps(age_mn) + height_cm + ps(age_mn):height_cm,
  sigma.formula = ~ps(age_mn),
  nu.formula    = ~1,
  tau.formula   = ~1,
  data    = nhanes_m,
  family  = BCPE,
  control = opt_control
)

wt_htf <- gamlss(
  formula       = weight_kg ~ ps(age_mn) + height_cm + ps(age_mn):height_cm,
  sigma.formula = ~ps(age_mn),
  nu.formula    = ~1,
  tau.formula   = ~1,
  data    = nhanes_f,
  family  = BCPE,
  control = opt_control
)
```

```{r}
#| label: gamlss-objects
#| results: asis
ht_m   <- readRDS(path(post_dir, "nhanes", "output", "ht-m-v03.rds"))
ht_f   <- readRDS(path(post_dir, "nhanes", "output", "ht-f-v03.rds"))
wt_htm <- readRDS(path(post_dir, "nhanes", "output", "wt-htm-v03.rds"))
wt_htf <- readRDS(path(post_dir, "nhanes", "output", "wt-htf-v03.rds"))

mod <- list(
  ht_m = ht_m,
  ht_f = ht_f,
  wt_htm = wt_htm,
  wt_htf = wt_htf
)

quarto_tabset(mod, level = 3)
```

## Using the GAMLSS models

### Quantiles

```{r}
#| label: height-quantiles
get_pars <- function(data, model) {
  pars <- tibble(
    mu    = predict(model, newdata = data, type = "response", what = "mu"),
    sigma = predict(model, newdata = data, type = "response", what = "sigma"),
    nu    = predict(model, newdata = data, type = "response", what = "nu"),
    tau   = predict(model, newdata = data, type = "response", what = "tau"),
  )
  bind_cols(data, pars)
}

predict_cases_ht <- expand_grid(
  age_mn  = 1:(age_max_yr * 12),
  sex_fct = factor(c("male", "female"))
)

predict_pars_ht <- bind_rows(
  predict_cases_ht |> filter(sex_fct == "male") |> get_pars(ht_m),
  predict_cases_ht |> filter(sex_fct == "female") |> get_pars(ht_f)
)

predict_quantiles_ht <- predict_pars_ht |>
  mutate(
    q05 = qBCPE(.05, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q25 = qBCPE(.25, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q50 = qBCPE(.50, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q75 = qBCPE(.75, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q95 = qBCPE(.95, mu = mu, sigma = sigma, nu = nu, tau = tau)
  )

predict_quantiles_ht
```

```{r}
#| label: height-quantiles-plot
#| fig-width: 8
#| fig-height: 6
ggplot() + 
  geom_point(
    data = nhanes |> filter(age_yr < age_max_yr), 
    mapping = aes(age_mn, height_cm),
    size = .25
  ) +
  geom_path(
    data = predict_quantiles_ht |> 
      pivot_longer(
        cols = starts_with("q"),
        names_to = "quantile",
        values_to = "height_cm"
      ),
    mapping = aes(age_mn, height_cm, color = quantile)
  ) +
  facet_wrap(~sex_fct) +
  theme(legend.position = "bottom")
```

Okay, that's nice but you might argue that the GAMLSS modelling is overkill here. The distribution of heights conditional on age and sex is fairly close to normal, and in any case we have about 100k rows in the data set spanning the full range of ages. However, it's not too hard to construct a case where you really do need the model.

We can take the logic further and predict the distribution of weights for very tall (99th height percentile) and short (1st height percentile) people, stratified by age and sex. It's a nice illustration of where the GAMLSS framework is useful even when you have a lot of data: as good as it is, the NHANES data become very sparse when you start focusing on very thin slices through the tails of the distributions. 

```{r}
predict_cases_wt <- predict_pars_ht |>
  mutate(
    very_tall  = qBCPE(.99, mu = mu, sigma = sigma, nu = nu, tau = tau),
    very_short = qBCPE(.01, mu = mu, sigma = sigma, nu = nu, tau = tau)
  ) |> 
  pivot_longer(
    cols = c(very_tall, very_short),
    names_to = "height_fct", 
    values_to = "height_cm"
  ) |> 
  mutate(height_fct = factor(height_fct)) |> 
  select(age_mn, sex_fct, height_fct, height_cm)

predict_cases_wt

predict_pars_wt <- bind_rows(
  predict_cases_wt |> filter(sex_fct == "male") |> select(-height_fct) |> get_pars(wt_htm),
  predict_cases_wt |> filter(sex_fct == "female") |> select(-height_fct) |> get_pars(wt_htf)
) |> 
  left_join(predict_cases_wt, by = join_by(age_mn, sex_fct, height_cm)) |> 
  relocate(height_fct, .before = height_cm)

predict_quantiles_wt <- predict_pars_wt |>
  mutate(
    q05 = qBCPE(.05, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q25 = qBCPE(.25, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q50 = qBCPE(.50, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q75 = qBCPE(.75, mu = mu, sigma = sigma, nu = nu, tau = tau),
    q95 = qBCPE(.95, mu = mu, sigma = sigma, nu = nu, tau = tau)
  )

predict_quantiles_wt
```

```{r}
permitted <- predict_cases_wt |> 
  mutate(
    height_cm_lo = height_cm * 0.99, 
    height_cm_hi = height_cm * 1.01
  ) |> 
  select(age_mn, sex_fct, height_fct, height_cm_lo, height_cm_hi)

match_rules <- join_by(
  x$sex_fct == y$sex_fct, 
  x$age_mn == y$age_mn, 
  x$height_cm > y$height_cm_lo, 
  x$height_cm < y$height_cm_hi
)

nhanes_very_short <- semi_join(
  x = nhanes, 
  y = permitted |> filter(height_fct == "very_short"),
  by = match_rules
)

nhanes_very_tall <- semi_join(
  x = nhanes, 
  y = permitted |> filter(height_fct == "very_tall"),
  by = match_rules
)

nhanes_partial <- bind_rows(
  very_short = nhanes_very_short,
  very_tall = nhanes_very_tall,
  .id = "height_fct"
) |> mutate(height_fct = factor(height_fct))

nhanes_partial
```

```{r}
#| label: weight-quantiles-plot
#| fig-width: 8
#| fig-height: 10
ggplot() + 
  geom_point(
    data = nhanes_partial, 
    mapping = aes(age_mn, weight_kg),
    size = .5
  ) +
  geom_path(
    data = predict_quantiles_wt |> 
      pivot_longer(
        cols = starts_with("q"),
        names_to = "quantile",
        values_to = "weight_kg"
      ),
    mapping = aes(age_mn, weight_kg, color = quantile),
    linewidth = 1
  ) +
  facet_grid(height_fct ~ sex_fct) +
  theme(legend.position = "bottom")

```

Works better than I would have expected. Note that the slight "clumpiness" of the NHANES data is partially tied to the slight misfit in the model at ~12 years. The data flatten out slightly faster than the model predictions, which has the effect that the curves are slightly below the data at that specific age. So if we slice through the data based on model-predicted 99th percentile, there will be more actual 12 year olds selected than other age groups.

### Sampling 

Sample height conditional on `age`, using an appropriate (sec-specific) gamlss model `ht_mod`. Similarly, we can sample weight conditional on `age` and `height` using a model. Note the use of a trimmed Box-Cox power-exponential: even with nu/tau parameters, the BCPE has support on non-biological values in the tails creating physically impossible outliers in large simulations. The default trim is 0.25% on either side, as compared to 3% for `nhanesgamlss::simwtage()`. 
  

```{r}
sample_hw <- function(age_mn, sex_fct, mod) {

  rTBCPE <- function(n, mu, sigma, nu, tau, trim = .0025) {
    p <- runif(n, min = trim, max = 1 - trim)
    if (any(mu <= 0)) mu[mu <= 0] <- 1E-6
    if (any(sigma <= 0)) sigma[sigma <= 0] <- 1E-6
    r <- qBCPE(p, mu = mu, sigma = sigma, nu = nu, tau = tau)
    r
  }
  sample_h <- function(age_mn, ht_mod) {
    tibble(age_mn) |> 
      get_pars(model = ht_mod) |> 
      select(mu, sigma, nu, tau) |> 
      pmap_dbl(\(mu, sigma, nu, tau) rTBCPE(n = 1, mu, sigma, nu, tau))
  }
  sample_w <- function(age_mn, height_cm, wt_mod) {
    tibble(age_mn, height_cm) |>
      get_pars(model = wt_mod) |> 
      select(mu, sigma, nu, tau) |> 
      pmap_dbl(\(mu, sigma, nu, tau) rTBCPE(n = 1, mu, sigma, nu, tau))
  }

  height_cm <- weight_kg <- numeric(length(age_mn))
  mm <- sex_fct == "male"
  ff <- sex_fct == "female"

  if (any(mm)) height_cm[mm] <- sample_h(age_mn[mm], mod$ht_m)
  if (any(ff)) height_cm[ff] <- sample_h(age_mn[ff], mod$ht_f)  
  if (any(mm)) weight_kg[mm] <- sample_w(age_mn[mm], height_cm[mm], mod$wt_htm)
  if (any(ff)) weight_kg[ff] <- sample_w(age_mn[ff], height_cm[ff], mod$wt_htf)
  
  tibble(age_mn, sex_fct, height_cm, weight_kg)
}
```


```{r}
nhanes_fit <- bind_rows(nhanes_m, nhanes_f) |> arrange(age_mn)

age_band <- function(age_mn) {
  factor(
    x = case_when(
      age_mn <= 12                 ~ 1,
      age_mn >  12 & age_mn <=  24 ~ 2,
      age_mn >  24 & age_mn <=  72 ~ 3,
      age_mn >  72 & age_mn <= 144 ~ 4,
      age_mn > 144 & age_mn <= 216 ~ 5,
      age_mn > 216                 ~ 6
    ),
    levels = 1:6,
    labels = c(
      "<1 year",
      "1-2 years",
      "2-6 years",
      "6-12 years",
      "12-18 years",
      ">18 years"
    )
  )
}

pop <- bind_rows(
  gamlss = sample_hw(nhanes_fit$age_mn, nhanes_fit$sex_fct, mod = mod),
  nhanes = nhanes_fit |> select(age_mn, sex_fct, height_cm, weight_kg),
  .id = "source"
) |> mutate(age_band_fct = age_band(age_mn))

pop
```

```{r}
#| label: height-weight-age-band-scatterplot
#| fig-width: 8
#| fig-height: 10
pop |>
  ggplot(aes(height_cm, weight_kg, color = sex_fct)) + 
  geom_point(size = .25, alpha = .5) +
  facet_grid(age_band_fct ~ source, scales = "free_y") +
  theme(legend.position = "bottom")
```

Once again this is a neat example, but it's mostly a validation of the GAMLSS model: it shows us the model does allow us to sample from the joint conditional density $p(h, w | a, s)$. But by design the GAMLSS samples in `pop` are matched to the NHANES data on age and sex. Anything that we could do with the GAMLSS samples is something that we could have done with the NHANES samples directly. 


```{r}
#| label: bsa-computation
compute_bsa <- function(height, weight, method = "dubois") {

  w <- weight # numeric (kg)
  h <- height # numeric (cm)

  # Du Bois D, Du Bois EF (Jun 1916). "A formula to estimate the approximate
  # surface area if height and weight be known". Archives of Internal Medicine
  # 17 (6): 863-71. PMID 2520314.
  if (method == "dubois") return(0.007184 * w^0.425 * h^0.725)

  # Mosteller RD. "Simplified calculation of body-surface area". N Engl J Med
  # 1987; 317:1098. PMID 3657876.
  if (method == "mosteller") return(0.016667 * w^0.5 * h^0.5)

  # Haycock GB, Schwartz GJ, Wisotsky DH "Geometric method for measuring body
  # surface area: A height-weight formula validated in infants, children and
  # adults" J Pediatr 1978, 93:62-66.
  if (method == "haycock") return(0.024265 * w^0.5378 * h^0.3964)

  # Gehan EA, George SL, Cancer Chemother Rep 1970, 54:225-235
  if (method == "gehan") return(0.0235 * w^0.51456 * h^0.42246)

  # Boyd, Edith (1935). The Growth of the Surface Area of the Human Body.
  # University of Minnesota. The Institute of Child Welfare, Monograph Series,
  # No. x. London: Oxford University Press
  if (method == "boyd") return(0.03330 * w^(0.6157 - 0.0188 * log10(w)) * h^0.3)

  # Fujimoto S, Watanabe T, Sakamoto A, Yukawa K, Morimoto K. Studies on the
  # physical surface area of Japanese. 18. Calculation formulae in three stages
  # over all ages. Nippon Eiseigaku Zasshi 1968;5:443-50.
  if (method == "fujimoto") return(0.008883 * w^0.444 * h^0.663)

  rlang::abort("unknown BSA method")
}
```

```{r}
#| label: height-weight-bsa-scatterplot
#| fig-width: 8
#| fig-height: 10
bsa_cutoff <- tibble(
  height_cm = 60:110,
  dubois  = (0.5 / 0.007184 / (height_cm^0.725)) ^ (1/0.425),
  haycock = (0.5 / 0.024265 / (height_cm^0.3964)) ^(1/0.5378)
) |> 
  pivot_longer(
    cols = c(dubois, haycock),
    names_to = "method",
    values_to = "weight_kg"
  )

bsa_pop <- pop |> 
  select(age_mn, age_band_fct, height_cm, weight_kg, sex_fct, source) |>
  filter(age_mn <= 72) |>
  mutate(
    bsa_m2_dubois  = compute_bsa(height_cm, weight_kg, method = "dubois"),
    bsa_m2_haycock = compute_bsa(height_cm, weight_kg, method = "haycock")
  ) |>
  filter(bsa_m2_dubois > 0.5) 
 
ggplot() + 
  geom_point(
    data = bsa_pop,
    mapping = aes(height_cm, weight_kg), 
    size = .25
  ) +
  geom_path(
    data = bsa_cutoff,
    mapping = aes(height_cm, weight_kg, linetype = method)
  ) +
  facet_grid(age_band_fct ~ source, scales = "free_y") +
  theme(legend.position = "bottom")
``` 

```{r}
cases <- expand_grid(
  age_mn  = 1:24,
  sex_fct = factor(c("male", "female"))
)
bsa_pop_2 <- sample_hw(
  age_mn  = rep(cases$age_mn, 10000),
  sex_fct = rep(cases$sex_fct, 10000),
  mod = mod 
) |> 
  mutate(
    id = row_number(),
    bsa_m2_dubois  = compute_bsa(height_cm, weight_kg, method = "dubois"),
    bsa_m2_haycock = compute_bsa(height_cm, weight_kg, method = "haycock"),
    threshold = case_when(
      bsa_m2_dubois >  .5 & bsa_m2_haycock >  .5 ~ "above both",
      bsa_m2_dubois >  .5 & bsa_m2_haycock <= .5 ~ "above dubois only",
      bsa_m2_dubois <= .5 & bsa_m2_haycock >  .5 ~ "above haycock only",
      bsa_m2_dubois <= .5 & bsa_m2_haycock <= .5 ~ "below both",
    )
  ) |> 
  relocate(id, 1)
bsa_pop_2
```


```{r}
#| label: height-weight-bsa-scatterplot-detailed
#| fig-width: 8
#| fig-height: 12
ggplot() + 
  geom_point(
    data = bsa_pop_2,
    mapping = aes(height_cm, weight_kg, color = threshold), 
    alpha = .25, 
    shape = ".",
    show.legend = FALSE
  ) +
  geom_path(
    data = bsa_cutoff,
    mapping = aes(height_cm, weight_kg, linetype = method)
  ) +
  facet_wrap(~ age_mn, ncol = 4) +
  theme(legend.position = "bottom")
```


```{r}
#| label: bsa-method-comparison
#| fig-width: 8
#| fig-height: 6
bsa_pop_2 |> 
  summarise(
    dubois_0.4  = mean(bsa_m2_dubois > 0.4),
    dubois_0.5  = mean(bsa_m2_dubois > 0.5),
    dubois_0.6  = mean(bsa_m2_dubois > 0.6),
    haycock_0.4 = mean(bsa_m2_haycock > 0.4),
    haycock_0.5 = mean(bsa_m2_haycock > 0.5),
    haycock_0.6 = mean(bsa_m2_haycock > 0.6),
    .by = age_mn
  ) |> 
  pivot_longer(
    cols = c(
      starts_with("dubois"), 
      starts_with("haycock")
    ),
    names_to = "group",
    values_to = "percent_above_threshold"
  ) |> 
  separate(
    col = group, 
    into = c("method", "threshold"), 
    sep = "_"
  ) |> 
  mutate(threshold = paste("BSA >", threshold, "m^2")) |> 
  ggplot(aes(
    x = age_mn, 
    y = percent_above_threshold, 
    linetype = method, 
    shape = method
  )) +
  scale_y_continuous(label = scales::label_percent()) +
  scale_x_continuous(breaks = seq(0, 24, 6)) +
  geom_path() +
  geom_point() +
  facet_wrap(~threshold) +
  theme(legend.position = "bottom")
```


## Epilogue

For example, [Hayes et al (2014)](https://pmc.ncbi.nlm.nih.gov/articles/PMC4339962/pdf/BLT.14.139113.pdf) provide detailed regional weight-for-age charts based on GAMLSS models
