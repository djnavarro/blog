---
title: "When good pseudorandom numbers go bad"
description: "Multivariate normal sampling can be wildly irreproducible if you're not careful. Sometimes more than others. There are eldritch horrors, ill-conditioned matrices, and floating point nightmares in here. Teaching sand to do linear algebra was a mistake"
date: "2025-05-04"
image: "unravel_17_1769.jpg"
image-alt: "A pair of squares being affine transformed so often that wildness ensues"
categories: ["R", "Reproducibility", "Statistics"]
--- 
<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

> Computing the eigendecomposition of a matrix is subject to errors on a real-world computer: the definitive analysis is Wilkinson (1965). All you can hope for is a solution to a problem suitably close to x. So even though a real asymmetric x may have an algebraic solution with repeated real eigenvalues, the computed solution may be of a similar matrix with complex conjugate pairs of eigenvalues. <br> &nbsp;&nbsp; -- `help("eigen")`

At work last week some colleagues^[For the pharmacometricians: Yes, Steve Duffull was involved in this landing on my desk. If I have learned nothing else in my brief tenure in this field it is that every one of these "Danielle gets dragged into the pits of hell" style R questions is *always* Steve's fault. Somehow.] mentioned a reproducibility issue they'd been having with some R code. They'd been running simulations that rely on generating samples from a multivariate normal distribution, and despite doing the prudent thing and using `set.seed()` to control the state of the random number generator (RNG), the results were not computationally reproducible. The same code, executed on different machines, would produce *different* random numbers. 

That's not supposed to happen, and in most situations it doesn't happen. Usually, the `set.seed()` method works just fine:

```{r}
# original 
set.seed(1L)
sample(letters)

# replication
set.seed(1L)
sample(letters)
```

Because the original code and the replication code both use `set.seed(1)` before calling `sample()` to shuffle the letters of the alphabet into a random order, we get the *same* random permutation in both cases. And although I'm exectuing this code twice on the same machine, there's no reason to expect that it would make a difference if I ran the original code on my ubuntu laptop running R 4.4.3 or on a window machine running R 4.3.1. It should be the same result either way. 

Yet that's not what my colleagues experienced with their code. Different machines produced different random samples even though the code used `set.seed()` to prevent that from happening. Their first thought was they must have done something wrong with their code that caused the simulations to break. It wasn't that. Their second thought was that something was wrong with R, or more precisely with `MASS::mvrnorm()`, since this is the function that was causing all the difficulty. They couldn't see anything wrong with the function though, so they asked me to look into it. It turns out that wasn't the problem either, not really. Well, maybe it is the problem. 

Look, it's complicated okay?

So let me get back to you on that because when you track down far enough, the root cause of the problem my colleagues ran into turned out to be the inconvenient fact that [floating point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic) does not behave like real arithmetic, which has the teeny tiny side effect that computers very often don't do what people expect them to do. I should have known. In hindsight it was so obviously going to be floating point issues^[I only say this because the context tells me in advance that it was probably not going to be a date/time computation problem, a map projection, or a fucking geodetic datum issue. There are exactly 24 hours in a day, timezones do not exist, and the Earth is a perfect sphere. Possibly a cube. I will hear no further discussions.] that I shouldn't have spent hours looking into it. As I said on Mastodon at the time:

> In future, whenever I'm asked "why is [thing] not reproducible even though I set the RNG seed?" I'm not even going to bother looking into the specifics: I'm just going to reply "floating point arithmetic" and wait until I am inevitably proven correct.

People will be amazed at my precognitive powers and my apparently-encyclopedic knowledge of all things computational. 

## The puzzle

It's not easy to replicate the precise issue that my colleagues encountered using a single machine, but I can approximate the issue. Consider these two covariance matrices, `cov1` and `cov2`. Here's the code that generates them:

```{r}
cov1 <- matrix(
  data = c(
    4.58, -1.07,  2.53,  0.14, 
    -1.07, 5.83,  1.15, -1.45, 
    2.53,  1.15,  2.26, -0.79, 
    0.14, -1.45, -0.79,  4.93
  ), 
  nrow = 4L, 
  ncol = 4L
)

set.seed(1L)
tol <- 10e-13
eps <- matrix(rnorm(16L) * tol, 4L, 4L)
eps <- eps + t(eps)

cov2 <- cov1 + eps
```

When printed out, they look identical:

```{r}
cov1
cov2
```

But of course, since you've already seen the code you will be entirely unsurprised to discover that `cov2` is in fact a very slightly perturbed version of `cov1`:

```{r}
cov1 - cov2
```

Tiny differences like this are what we encounter when floating point truncation errors occur.^[Well, I should be a bit careful here. These differences are a few orders of magnitude higher than the rounding error you'd expect by truncating one real number to a floating point number on this machine, since `.Machine$double.eps` is approximately $2.2 \times 10^{-16}$, but rounding errors have a nasty tendency to propagate, and anyway I haven't gotten to the bit of the post where I talk about numerical stability so... hush.] To use the classic example, the result of this sum should be zero, but it's not because the binary representation of 0.1 is infinitely long and cannot be exactly represented by a double precision floating point number (which is of course how R represents numeric values)

```{r}
0.1 + 0.2 - 0.3
```

Even worse, the *precise* result of a computation to which floating point truncation error applies is not necessarily invariant across systems. Operating system differences, compiler settings, and a host of other factors can influence the outcome. The details don't matter for this post, and frankly I don't understand all of them myself. For all I know the colour of the laptop case might be relevant, or the name of the programmer's cat. Weirdness abounds once your calculations start to run up against the limits of floating point precision. 

Just for the sake of argument then, let's imagine that during the course of some fancy simulation, you and I compute a covariance matrix on different machines. It's supposed to be the same covariance matrix, but thanks to the weirdness of floating point your machine computes `cov1` and mine computes `cov2`. The differences are very small, but they're large enough that this happens:

```{r}
set.seed(1L)
mvr1 <- MASS::mvrnorm(n = 1L, mu = rep(0L, 4L), Sigma = cov1)
mvr1

set.seed(1L)
mvr2 <- MASS::mvrnorm(n = 1L, mu = rep(0L, 4L), Sigma = cov2)
mvr2
```

At this point in the post, you will probably have one of two reactions depending on your background. If you have had the traumatising experience of reading a numerical linear algebra textbook and have somehow survived, you will be sighing wearily and going "yes Danielle, that's what happens when you try to do mathematics with sand". But if you live in the somewhat kinder lands of everyday applied science where the sun still shines and your gods were not brutally murdered by [IEEE-754](https://en.wikipedia.org/wiki/IEEE_754), you are probably thinking something more along the lines of **"WHAT THE FUCK IS THIS INSANITY DANIELLE????????"** 

The moment we attempt to generate random vectors with a multivariate normal distribution, very small differences between `cov1` and `cov2` lead to big differences in the numbers that get generated even though the RNG seed is the same. 

That's a little puzzling. Other kinds of calculation aren't affected the same way. Intuitively, we expect that tiny differences in the parameters should lead to tiny differences in sampled values. In fact, that's exactly what happens if we generate random samples from a *univariate* normal distribution with slightly different standard deviations:

```{r}
s1 <- sqrt(cov1[1L, 1L])
s2 <- sqrt(cov2[1L, 1L])

set.seed(1L)
r1 <- rnorm(n = 1L, mean = 0L, sd = s1)
r1

set.seed(1L)
r2 <- rnorm(n = 1L, mean = 0L, sd = s2)
r2
```

These numbers look identical, but since `s1` and `s2` are slightly different, there are slight differences between `r1` and `r2`:

```{r}
r2 - r1
```

The intuition that you might have, then, is that if `rnorm()` works and `mvrnorm()` doesn't, there must be something broken within the implementation of `mvrnorm()`. It's a reasonable intuition, but like so many reasonable intuitions, it is wrong. The problem is not `mvrnorm()`. The problem is that floating point numbers suck.

## How are multivariate normal samples generated?

To understand why this problem arises, it's important to understand that [sampling from a multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Drawing_values_from_the_distribution) is a somewhat different kettle of fish to drawing from a univariate normal distribution, and computationally trickier. In the univariate case, let's say we're using the polar form of the [Box-Muller method](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform). To transform two uniform variates into two normal variates requires three multiplications, one logarithm, one square root, and one division. Yes, floating point truncation error can creep in there^[I mean, I'm not really a specialist in this so my default is to assume that truncation errors insert themselves into every calculation that requires real numbers but is nevertheless performed with floating point numbers.] but there just aren't that many computations involved. With so few computations involved you aren't very likely^[Not a guarantee.] to encounter the "everything goes to shit" problem when runaway truncation error takes hold.

Sampling from a multivariate normal, on the other hand, requires a matrix decomposition. There are many different ways you can choose to do this decomposition and still end up with suitable samples, but no matter which method you choose you will be on the hook for a *lot* more computations than in the univariate case, and to put it crudely, more computations means more opportunities for floating point arithmetic to fuck you over. To set the stage for how this can all go horribly wrong, let's do a quick refresher on the multivariate normal distribution, because who doesn't love the opportunity to break out a mathematical statistics textbook?

Let $\mathbf{x} = (x_1, \ldots, x_k)$ be a $k$-dimensional random vector that is distributed according to a multivariate normal with mean vector $\mathbf{\mu} = (\mu_1, \ldots, \mu_k)$ and positive definite^[Okay fine you can get away with positive semidefinite covariance matrices but in such cases the density is undefined and anyway is not the point of any of this.] covariance matrix $\mathbf{\Sigma} = [\sigma_{ij}]$. The probability density function looks like this:

$$
p(\mathbf{x} | \mathbf{\mu}, \mathbf{\Sigma}) = (2\pi)^{-k/2} \det(\mathbf{\Sigma})^{-1/2} \exp \left(-\frac{1}{2} (\mathbf{x} - \mathbf{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \mathbf{\mu}) \right) 
$$


A key property of the multivariate normal is this: a linear transformation of a multivariate normal random vector is itself distributed according to a multivariate normal. More precisely, if $\mathbf{z} \sim \mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$ and $\mathbf{x} = \mathbf{Az} + \mathbf{b}$, then $\mathbf{x} \sim \mathcal{N}(\mathbf{A\mu} + \mathbf{b}, \mathbf{A \Sigma A^T})$. It's something I recite to myself as an axiom on a weekly basis, but for the purposes of this post I decided to dig out one of my old mathematical statistics textbooks and revisited the proof. It wasn't very interesting. As a corollary, we can assert that if $\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ and $\mathbf{x} = \mathbf{Az} + \mathbf{b}$, then $\mathbf{x} \sim \mathcal{N}(\mathbf{b}, \mathbf{AA^T})$. This gives us a convenient way to sample from a multivariate normal distribution. Without loss of generality^[I love pretending to be a real mathematician and using those words. Like, it's true here: the mean vector isn't really relevant to the discussion here, it's all about the covariance matrix, so I can just fix it at the origin and nothing changes. But it's more fun to be a pretentious twat, so I'll use the conventional language here.] I'll fix $\mathbf{b} = \mathbf{0}$ and note that if we have numbers $\mathbf{z}$ that follow independent standard normal distributions, and some matrix $\mathbf{A}$ such that $\mathbf{\Sigma} = \mathbf{A A}^T$, then the transformed variates $\mathbf{x} = \mathbf{Az}$ are multivariate normally distributed with covariance matrix $\mathbf{\Sigma}$. Thrilling stuff, I think we can all agree.

The key thing in here is that the linear transformation property gives us a simple algorithm for sampling multivariate normal variates:

1. Sample a vector of (pseudo-)random numbers $\mathbf{z} = (z_1, \ldots, z_k)$ independently from a standard normal distribution with mean 0 and standard deviation 1. In R that's usually as simple as calling `rnorm()`, but if all you have is uniformly distributed random numbers you can use the Box-Muller method to transform them appropriately.
2. Using whatever fancy matrix decomposition trick is still capable of bringing love into in your withered heart, find yourself a matrix $\mathbf{A}$ such that $\mathbf{\Sigma} = \mathbf{A A}^T$.
3. Calculate the vector $\mathbf{x} = (x_1, \ldots, x_k)$ where $\mathbf{x} = \mathbf{Az}$. The resulting values are multivariate normal distributed with mean $\mathbf{0}$ and covariance matrix $\mathbf{\Sigma}$. 
4. Celebrate. Eat cake.

## Hell is matrix decomposition

At this point you might be thinking, "but Danielle, matrix decomposition has never brought love into my life no matter how much I talk up the size of my eigenvalues on grindr, what do I do????" and okay yeah fair point.

To understand exactly where things went wrong, let's demystify what `MASS::mvrnorm()` does by implementing a slightly simplified version of what was going on under the hood. But before we start, let's make a copy of the hidden variable `.Random.seed` so that we're able to detect whenever something advances the state of the random number generator. 

```{r}
seed_state <- .Random.seed
```

Now let's get started on the random sampling itself. To mirror what `MASS::mvrnorm()` does, I'll call the `eigen()` function from base R to compute eigenvalues and eigenvectors. This actually the point at which everything goes awry for us, but to some extent you can't blame R for this, because what's actually happening here is that R passes all the work off to [LAPACK](https://en.wikipedia.org/wiki/LAPACK), and it's at that level that our problem arises:

```{r}
eig1 <- eigen(cov1)
eig2 <- eigen(cov2)
```

Just in case you happen to have "forgotten", the eigendecomposition of a real symmetric matrix $\mathbf{\Sigma}$ can be expressed as 

$$
\mathbf{\Sigma} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\prime
$$

where $\mathbf{\Lambda}$ is a diagonal matrix containing the eigenvalues of $\mathbf{\Sigma}$ and $\mathbf{Q}$ is an orthogonal matrix whose columns are the real, orthonormal eigenvectors of $\mathbf{\Sigma}$. It says so in the [wikipedia entry](https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices) so it must be true.^[I mean, it is true of course, but also this is a special case of the more general eigendecomposition for a square matrix $\mathbf{M} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^{-1}$. But whatever.] When calling `eigen()` in R, the return value is a list that contains a vector of eigenvalues, and a matrix of eigenvectors. To keep things consistent with the notation in the equation above, let's pull those out:

```{r}
# matrices of eigenvectors
Q1 <- eig1$vectors
Q2 <- eig2$vectors

# diagonal matrices of eigenvalues
L1 <- diag(eig1$values, 4L)
L2 <- diag(eig2$values, 4L)
```


Now that we have these two matrices, we can construct a matrix $\mathbf{A} = \mathbf{Q} \mathbf{\Lambda}^{1/2}$ where $\mathbf{\Lambda}^{1/2}$ is a diagonal matrix that contains the square root of the eigenvalues as its diagonal elements. This matrix has the desired property $\mathbf{A} \mathbf{A}^\prime = \mathbf{\Sigma}$, so we can use it as the transformation matrix to sample multivariate normal variates with the desired correlational structure. So let's do that:

```{r}
# compute the linear transformation matrices
A1 <- Q1 %*% sqrt(L1)
A2 <- Q2 %*% sqrt(L2)
```

At this point let's check the state of the random number generator. Has it changed as a result of any of these procedures?

```{r}
identical(seed_state, .Random.seed)
```

No. No it has not. Nothing that we have done so far has invoked the random number generator in R. Nor should it: constructing the matrix $\mathbf{A}$ isn't supposed to be a stochastic process. We should not expect R to have invoked the random number generator up to this point, and indeed it has not. However, we're now at the point where we *do* need to produce some random numbers, because we need a vector $\mathbf{z}$ of independent normally distributed variates with mean zero and standard deviation one. When I called `MASS::mvrnorm()` earlier, I used `set.seed(1L)` to fix the state of the random number generator beforehand, and I'll do so again:

```{r}
set.seed(1L)
z <- matrix(rnorm(4L), 4L)
```

For simplicity I've explicitly formatted the output as a matrix so that R will treat it as a column vector, and now all I have to do to construct my correlated random variates is to compute $\mathbf{Az}$:

```{r}
r1 <- as.vector(A1 %*% z)
r2 <- as.vector(A2 %*% z)
```

Et voila, we are done. We have now generated random vectors that are *identical* to those produced by `MASS::mvrnorm()`. Let's just confirm this:

```{r}
#| results: hold
identical(r1, mvr1)
identical(r2, mvr2)
```

Nevertheless as we've already seen, `mvr1` and `mvr2` are massively different to each other. They're not a teeny tiny bit different in the same way that `cov1` and `cov2` are a tiny bit different, the differences here are huge. 

So... where did things go wrong? Okay that's a silly question because I already said that the call to `eigen()` is what created the problem. A better question would be to ask *what* exactly when wrong when I called `eigen()`, and since I've dragged this out long enough already let's just jump to the correct answer and take a look at the matrix of eigenvectors $\mathbf{Q}$ that is computed in both cases, paying particular attention to the last column:

```{r}
Q1
Q2
```

Okay yeah... the sign on the last eigenvector has been reversed. We can see this more clearly by doing a scalar division of these two matrices:

```{r}
Q1 / Q2
```

As a consequence, here's what the transformation matrix $\mathbf{A}$ looks like in both cases:

```{r}
A1
A2
```

Yeah... those are not the same at all. They're both perfectly acceptable decompositions, in the sense that $\mathbf{A} \mathbf{A}^\prime = \mathbf{\Sigma}$ for both `A1` and `A2`, and will therefore produce multivariate normal distributed variates with the appropriate covariance structure when used, but they are not the *same* decomposition. 

Ultimately, it is this phenomenon that breaks reproducibility with `MASS::mvrnorm()`. Tiny quantitative changes in the covariance matrix that we pass as input can sometimes produce large *qualitative* changes in the eigendecomposition returned by LAPACK. `MASS::mvrnorm()` makes no attempt to protect the user from these effects, so when LAPACK creates this problem MASS does not try to fix it.^[Sometimes I wonder what comments this would receive from CRAN maintainers if MASS were a new package submitted by a new developer, but perhaps its better not to speculate.] 

## Safe passage through hell is notoriously expensive

The first thought you might have is "well, Danielle, could we maybe do something about these indeterminacies? Does floating point arithmetic have to be this unpredictable?" It's an enticing thought, right? I mean, if we could guarantee that every machine produced the same answer whenever asked to perform a simple arithmetic calculation, we wouldn't be in this mess. Problem solved. Rainbows. Unicorns. Sunshine. Fully automated gay space luxury communism.

Yeah, well. About that. Look, I am not an expert in this area at all, but just take a look at this page on [conditional numerical reproducibility](https://www.intel.com/content/www/us/en/developer/articles/technical/introduction-to-the-conditional-numerical-reproducibility-cnr.html) on Intel CPUs and GPUs. This is *hard*. If you want to make absolutely certain that two machines perform the exact same arithmetic operations in the exact same order so that you can guarantee that the exact collection of bits in the output is fully reproducible, you are going to have to make a lot of sacrifices and your computer will slow to a crawl trying to make it happen. We are almost never willing to pay the real costs that computational reproducibility imposes, if only because most of us would like to have our matrix decomposition complete sometime within the same century that it started. As Dan Simpson phrased it on [bluesky](https://bsky.app/profile/danpsimpson.bsky.social/post/3lp2a7on3ps2y):

> It is possible to make code [bit] reproducible. Waiting for it to run becomes reminiscent of the tar drop experiment. But it is possible.

It is an uncomfortable truth for a scientist to accept, but it is a truth nonetheless: the actual reason we don't have reproducible code is that we don't want it enough, and we never will. Life is short, and computational reproducibility is slow.

When faced with the cruel truths of material reality, one has a choice to make. We can either choose to live in a pure world of conceptual abstractions, floating above the mess and chaos of the world as it exists on the ground, or we can accept that -- in this instance -- we are engaged in the absurd exercise of trying to make a block of sand do linear algebra and of course that is going to be ugly. We are trying to force reality to bend to our mathematical whims and unfortunately reality is only partially willing to comply. 

So let us accept a core truth: for any realistic level of time and money that a human is willing to spend performing an automated computation, there is always some probability that the machine will fuck it up. We cannot eradicate this risk in real life, and we must always act on the assumption that the computational machinery underneath our code might occasionally do some batshit things.

## Living in a material world, and I am a material girl

There are a few ways of dealing with this problem. If you want to stay within the world of eigendecompositions (which actually you probably don't want to do, but we'll get to that later...) there is a "simple" trick that is adopted by `mvtnorm::rmvnorm()`. In `MASS::mvrnorm()` the transformation matrix computed is defined as $\mathbf{A} := \mathbf{Q} \mathbf{\Lambda}^{1/2}$, but that's not the only way to use the eigendecomposition to find an admissable matrix $\mathbf{A}$. I haven't been able to find an explicit statement of this in the documentation, but if you look at the source code it's not too hard to see that if you're calling `mvtnorm::rmvnorm()` with `method = "eigen"`, the actual transformation matrix it uses is this one:

$$
\mathbf{A} := \mathbf{Q} \mathbf{\Lambda}^{1/2}\mathbf{Q}^\prime 
$$ 
For the purpose of multivariate normal sampling this is a perfectly admissable choice of transformation matrix, which we can demonstrate with a few incredibly boring lines of matrix algebra:

$$
\begin{array}{rcl}
\mathbf{A} \mathbf{A}^\prime 
&=& (\mathbf{Q} \mathbf{\Lambda}^{1/2}\mathbf{Q}^\prime) (\mathbf{Q} \mathbf{\Lambda}^{1/2}\mathbf{Q}^\prime)^\prime \\ 
&=& \mathbf{Q} \mathbf{\Lambda}^{1/2}\mathbf{Q}^\prime \mathbf{Q} \mathbf{\Lambda}^{1/2}\mathbf{Q}^\prime \\
&=& \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\prime \\
&=& \mathbf{\Sigma}
\end{array}
$$

Somewhat to my horror, this trick actually fixes the problem. Remember, the nightmarish thing that we are trying to protect against is not "trivial" floating point errors where a few of the numeric values are perturbed by some small amount: we are applied scientists and we simply do not care about what is happening in the 16th digit of the decimal expansion of blah blah blah. That's not our problem. In the real world our problem is the catastrophic failure case in which those tiny perturbations cause LAPACK to flip the sign of an eigenvector. *That's* the thing that fucks us.^[In the bad way. Normally a girl doesn't mind that kind of thing.]

Formally, we can describe this "eigenvalue flip" operation by considering the possibility that LAPACK -- for whatever reason -- decides to return the matrix $\mathbf{QF}$ instead of $\mathbf{Q}$, where the "flip matrix" $\mathbf{F}$ is a diagonal matrix whose diagonal elements are either 1 or -1. We need a definition for our transformation matrix $\mathbf{A}$ that is robust in the face of this kind of floating point nonsense. It is very clear that the MASS method is not invariant when this happens, since in the flipped case case it will use the transformation matrix

$$
\mathbf{A} = \mathbf{QF} \mathbf{\Lambda}^{1/2}
$$

which is clearly not the same matrix it would have returned if that pesky flip matrix $\mathbf{F}$ had not been inserted. In contrast, take a look at what happens to the transformation matrix used by mvtnorm when a flip matrix is inserted by LAPACK and/or the capricious gods of floating point numbers. Turns out the effect is...

$$
\begin{array}{rcl}
\mathbf{A} &=& \mathbf{QF} \mathbf{\Lambda}^{1/2} \mathbf{(QF)}^\prime\\
&=& \mathbf{Q} \mathbf{F} \mathbf{\Lambda}^{1/2} \mathbf{F}^\prime \mathbf{Q}^\prime \\
&=& \mathbf{Q} \mathbf{\Lambda}^{1/2} \mathbf{Q}^\prime \\
\end{array}
$$

...absolutely nothing. Using `mvtnorm::rmvnorm()` instead of `MASS::rmvnorm()` won't do a damn thing to protect you from floating point errors, but it will protect you against a particular kind of catastrophic reproducibility failure caused by those floating point errors, and which MASS has no defence against. So that's nice. 

Some code:

```{r}
set.seed(1L)
mvtnorm::rmvnorm(n = 2L, sigma = cov1)
```

The internal code to `mvtnorm::rmvnorm()` is slightly more elaborate, and it's important to note that when sampling more than one vector it populates a matrix of random numbers rowwise rather than columnwise, but in essence what it does is this: 

```{r}
set.seed(1L)
z2 <- matrix(rnorm(8L), nrow = 2L, byrow = TRUE)

A <- Q1 %*% sqrt(L1) %*% t(Q1)
z2 %*% A
```

Yup, same thing.

## I compute his Cholesky till he decompose

In one sense, the problem that my colleagues brought to me is already solved. A switch from `MASS::mvrnorm()` to `mvtnorm::rmvnorm()` will be sufficient to guard against the kind of catastrophic irreproducibility they encountered. So yeah I could stop here. But I've spent so many hours already wrapping my head around the problem that it almost seems a shame not to see it through to the bitter end. 

Let's take a step back. Yes, it's nice that we have a safer way to sample from a multivariate normal using the eigendecomposition of the covariance matrix but... why are we doing an eigenanything here? In this particular context we have no inherent interest in the eigenvectors or the eigenvalues: our primary goal is to construct *some* matrix $\mathbf{A}$ that has the desired property $\mathbf{AA}^\prime = \mathbf{\Sigma}$, and our secondary goal is to do this using a method that is robust in the face of floating point madness. Characteristic polynomials be damned. 

With that in mind, it's useful to remember that the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) is a thing that exists. For symmetric positive definite covariance matrix $\mathbf{\Sigma}$, the Cholesky decomposition gives us a lower triangular matrix $\mathbf{L}$ with positive valued diagonal elements such that $\mathbf{LL}^\prime = \mathbf{\Sigma}$, and therefore we can use the Cholesky factorisation $\mathbf{L}$ as the transformation matrix (i.e., the vector $\mathbf{x} = \mathbf{Lz}$ will be multivariate normal distributed with covariance matrix $\mathbf{\Sigma}$). In fact, there are some good reasons to prefer the Cholesky approach over the eigendecomposition. 

- The first reason is speed, since as a general rule, the Cholesky decomposition is somewhat faster. Indeed, some algorithms for solving eigendecompositions will start by computing the Cholesky factorisation first anyway. In high performance situations this likely matters, but in my line of work it's not usually critical. 

- The second reason, and the one that appeals to me in this context is that the Cholesky decomposition gives us a simpler method to avoid the "catastrophic" irreproducibility issue that we encountered with `MASS::mvrnorm()`. 

To understand why Cholesky makes this a little easier to fix, notice that the issue with `MASS::mvrnorm()` arises because the eigenvectors that comprise $\mathbf{Q}$ are determined only up to scalar multiplication (i.e., if $\mathbf{v}$ is an eigenvector associated with eigenvalue $\lambda$ then so too is $u\mathbf{v}$ for scalar $u$)^[This is the footnote where the author reminds herself that in the *general* case eigenvectors need not be orthogonal, but for a real symmetric positive definite matrix then the eigenvectors will be orthogonal.] By convention the eigenvectors returned by `eigen()` are normalised (i.e., have length 1),^[I think that's imposed by the LAPACK routine.] but that still does not give us a unique solution, and as we have seen it's possible for the sign of an eigenvector to change with small perturbations to $\mathbf{\Sigma}$, and that is sufficient to break reproducibility of `MASS::mvrnorm()` because the transformation matrix it computes is not invariant to these flips. The solution adopted by `mvtnorm::rmvnorm()` with `method = "eigen"` is to define the transformation matrix differently, and in a way that sign changes on an eigenvector have no effect on the samples. 

Yes, okay Danielle thank you, you have now restated the result, but how does Cholesky fix that? Essentially, it's easier because the Cholesky factorization is unique up to the sign of the columns, and if we impose the requirement that the main diagonals be positive valued (and that constraint appears to be imposed by `chol()`, which -- as far as I can tell -- is inherited directly from the LAPACK routine `dpotrf()`), the solution is actually unique. 

Or... to simplify it further... `eigen()` returns an orthonormal matrix of eigenvectors $\mathbf{Q}$ but not always the same one; `chol()` returns a triangular matrix with positive diagonal entries, and there's only one of those, so you don't have the same worry.

```{r}
U1 <- chol(cov1)
U2 <- chol(cov2)

U1
U2
```

So this is what you get...

```{r}
#| results: hold
r1 <- z2 %*% U1
r2 <- z2 %*% U2

r1
r2
```

Well, not quite because...

```{r}
#| results: hold
set.seed(1L)
mvtnorm::rmvnorm(2L, sigma = cov1, method = "chol")

set.seed(1L)
mvtnorm::rmvnorm(2L, sigma = cov2, method = "chol")
```

...these aren't the same as the last lot. What you actually have to do is closer to this:

```{r}
#| results: hold
R1 <- chol(cov1, pivot = TRUE) 
R1 <- R1[, order(attr(R1, "pivot"))]
z2 %*% R1

R2 <- chol(cov2, pivot = TRUE) 
R2 <- R2[, order(attr(R2, "pivot"))]
z2 %*% R2
```

Okay so that actually is the same.

## Filling rowwise is good

Notice this:

```{r}
set.seed(1L)
mvtnorm::rmvnorm(2L, sigma = cov1, method = "chol")

# intuitive behaviour: the first two rows are the same as last time
set.seed(1L)
mvtnorm::rmvnorm(3L, sigma = cov1, method = "chol")
```

That's also true when the eigendecomposition is used:

```{r}
set.seed(1L)
mvtnorm::rmvnorm(2L, sigma = cov1)

# intuitive behaviour: the first two rows are the same as last time
set.seed(1L)
mvtnorm::rmvnorm(3L, sigma = cov1)
```

Versus this:

```{r}
set.seed(1L)
MASS::mvrnorm(2L, mu = rep(0, 4), Sigma = cov1)

# counterintuitive: all rows change
set.seed(1L)
MASS::mvrnorm(3L, mu = rep(0, 4), Sigma = cov1)
```

The MASS approach is still technically correct, in the sense that the three random vectors are all distributed in the way we want them to be, but again this is a source of fragility in the code. If the user decides to change the number of random variates in the simulation, all the variates will change.^[Older versions of mvtnorm had the same issue that MASS has. It was fixed in version 0.9.9994. For backwards compatibility `mvtnorm::rmvnorm()` has an argument `pre0.9_9994` that allows you to reproduce the older behaviour, but there's almost never a good reason to use it.]

## Numerical stability

Since the dawn of time, humanity has yearned to destroy linear algebra. It was never necessary. Linear algebra was perfectly willing to destroy itself, and all we had to do was find an ill-conditioned problem.

The time has come to talk about [numerical stability](https://en.wikipedia.org/wiki/Numerical_stability). Nobody really wants to, but we're all trapped here in the same asylum and it passes the time. 

Do we have a well-conditioned matrix or not?

Yeah well first, what the fuck is this? Firstly, there's no such thing as a "well-conditioned matrix" or an "ill-conditioned matrix". That's not a thing. Conditioning is a property that attaches to a *computing problem*, and it refers to how sensitive the solution is to tiny changes in the problem specification. A matrix is a mathematical object, it's not a computing problem. So we will need to be a little more precise. To quote Wilkinson (1965, chapter 2 section 30), 

> It is convenient to have some number that defines the condition of a matrix with respect to a computing problem and to call such a number a 'condition number'. Ideally it should give some 'overall assessment' of the rate of change of the solution with respect to changes in the coefficients and should therefore be in some way proportional to this rate of change <br>
> <br>
> It is evident from what we have said [earlier in the book] that even if we restrict ourselves to the problem of computing eigenvalues alone, then such a single number would have severe limitations. If any one of the eigenvalues were very sensitive then the condition number would have to be very large, even if some other eigenvalues were very insensitive.

He goes on to note that to fully describe the sensitivities involved for computing the eigenvectors of an $n \times n$ matrix you'd need a total of $n^3$ quantities, the partial derivatives of all $n$ eigenvalues with respect to all $n^2$ elements in the matrix. That's also unhelpful, at least from the perspective of the human being who has to make sense of it all. In practice then, defining condition numbers for a specific computing problem is a bit of a trade-off, trying to find something that a human being can make sense of without throwing away so much information as to render the whole exercise pointless. 

Okay yeah so... we shall consider ourselves duly warned. Condition numbers aren't things laid down by the gods of linear algebra, they're just rough guides.

For the eigenvalue problem, our matrix is well-conditioned because... well, that's true for every [normal matrix](https://en.wikipedia.org/wiki/Normal_matrix). Let's say we have covariance matrix $\mathbf{\Sigma}$ and an orthonormal matrix of eigenvectors $\mathbf{Q}$. In this case we can write $\mathbf{Q}^\prime \mathbf{\Sigma Q} = \mathbf{\Lambda}$,^[Note that Wilkinson states this in a slightly different and somewhat more general way. I'm taking shortcuts here by framing it solely in terms of the specific case where the matrix of interest is a covariance matrix $\mathbf{\Sigma}$ which we may assume here to be real, symmetric, positive-definite, and we are also by convention constructing $\mathbf{Q}$ from eigenvectors that have norm 1. You know, that thing that `eigen()` does.] the condition number for $\mathbf{\Sigma}$ with respect to its eigenvalue problem is $\kappa(\mathbf{Q})$ and, well...

```{r}
kappa(Q1, exact = TRUE)
```

Big deal. Nobody cares. This is stupid. [delete this section: better to just jump from the wilkinson quote re eigenvalues, to noting that the matrix inversion problem is a little more relevant...]

A slightly more relevant quantity for our purposes is the condition number with respect to matrix inversion of $\mathbf{\Sigma}$.^[I mean, ideally we'd have the condition number for multivariate normal *sampling* with covariance $\mathbf{\Sigma}$ but whatever... for now we'll have to make do with matrix inversion.] For that, we jump forward to chapter 4 section 3, where Wilkinson notes that the condition number most commonly used for matrix inversion is $\kappa(\mathbf{\Sigma})$

```{r}
kappa(cov1, exact = TRUE)
```

Here's how it's computed:

```{r}
inv_cov1 <- solve(cov1) 
norm(cov1, "2") * norm(inv_cov1, "2")
```

Or, alternatively, we could note that for a symmetric matrix this quantity is identical to the ratio of the largest to smallest (absolute values of) eigenvalues:^[If it's not symmetric it's the ratio of largest to smallest singular values.]

```{r}
max(abs(eig1$values)) / min(abs(eig1$values))
```

Okay great. Whatever. What does it actually *mean* in practice?

[more stuff in the chapter... starts to go on about pivots etc, and I think that's probably relevant to understanding some of the choices in mvtnorm]


## Pivot 

There's one last feature in `mvtnorm::rmvnorm()` that I was uncertain about, and required a painful dive into the darkness of chapter 4 in Wilkinson (1965), and then a further skim through chapter 8. In chapter 4 of the book, he focuses solutions to systems of linear equations, which is why that's the part of the book where he ends up talking about condition numbers for matrix inversions. But there's a lot more in that chapter, and quite frankly I did not understand all of it. I cried a few times.

However.

One of the key things that the chapter dives into is what actually happens on a computer when you ask it to do a Gaussian elimination procedure (remember those from undergrad... yeah, unfortunately they're hard to forget). At each step in the elimination process you have to divide one thing by another thing and blah blah blah you end up needing to divide one row by some multiplier of another row and oh dear lord i am trapped in 1994 again at the back row of that cursed lecture theatre and I DO NOT CARE...

Um.

Okay, let's simplify. There's one key idea here: if the divisor on one of these steps turns out to be very very small, the computation error due to floating point nonsense can be very large. So there's a trick where you interchange some of the rows to avoid running into the trap. This interchange tends to stabilise the procedure.^[Actually, there's a long discussion in the book where he basically admits this is not guaranteed.] Then, when you get to the very end of this section and are wondering whether you have made terrible life choices, there's a bit where he "reminds" you that Gaussian elimination and triangular decomposition (a la Cholesky) are more or less the same thing so the same logic applies: speaking quite generally, and noting that it's not always true, allowing row interchanges gives you more stable decompositions. 

Aaaaanyway, when we drag ourselves back to what `mvtnorm::rmvnorm()` does when `method = "chol"`, we notice internally that when it calls `chol()`, it sets `pivot = TRUE`. That's what this is about: pivoting makes it possible for the Cholesky decomposition to be computed for a positive semi-definite matrix (not just positive definite matrices), and more generally provides the stability that Wilkinson is banging on about. That being said, it should be noted that this appears to be mostly in relation to ill-conditioned matrices, and/or postive semi-definite matrices. As noted later in the chapter by Wilkinson, Gaussian elimination for symmetric positive definite matrices is quite stable without any pivoting strategy applied. In reference to Cholesky decompositions, we use pivoting only to handle those cases where the matrix is not positive definite (e.g., due to floating point problem or whatevs). Thankfully for my sanity the book doesn't supply the proof of this, but... it's Wilkinson, and at this point in my descent into madness I'm willing to trust him.^[Honestly, by the time the book got into discussions of LR and QR algorithms in chapter XX, and in which there's a lengthy commentary on Cholesky decomposition, my brain was already a molten mess.]

However, as the documentation to `chol()` points out:

> If `pivot = TRUE`, then the Cholesky decomposition of a positive semi-definite `x` can be computed. The rank of `x` is returned as `attr(Q, "rank")`, subject to numerical errors. The pivot is returned as `attr(Q, "pivot")`. It is no longer the case that `t(Q) %*% Q` equals `x`. However, setting `pivot <- attr(Q, "pivot")` and `oo <- order(pivot)`, it is true that `t(Q[, oo]) %*% Q[, oo]` equals `x`, or, alternatively, `t(Q) %*% Q` equals `x[pivot, pivot]`. 

Ah. That's what's going on with `rmvnorm()` code: because for our purposes it is critical that $\mathbf{Q}^\prime \mathbf{Q} = \mathbf{\Sigma}$, that's the whole bloody point. So if we allow pivoting for numerical stability reasons, it is critical that we reorder the matrix appropriately. 

## Some simulations

At this point we've sort of covered everything, and we can write our own code to implement multivariate normal sampling in three different ways:

- Using `eigen()` MASS-style
- Using `eigen()` mvtnorm-style
- Using `chol()` mvtnorm-style

These are of course not the only ways to do it but this post is already giving me body horror nightmares so I refuse to implement the matrix square root method. Not going to happen. 

```{r}
sampler <- function(n, sigma, method, seed = NULL) {
  
  # size of the matrix
  k <- nrow(sigma)
  
  # extract necessary quantities from eigendecomposition 
  if (method %in% c("eigen-mass", "eigen-mvtnorm")) {
    eig <- eigen(sigma)
    Q <- eig$vectors               # matrix of eigenvectors
    L <- diag(sqrt(eig$values), k) # diagonal matrix of sqrt eigenvalues
  }
  
  # compute the transformation matrix A for eigendecomposition
  if (method == "eigen-mass") A <- Q %*% L              # MASS-style
  if (method == "eigen-mvtnorm") A <- Q %*% L %*% t(Q)  # mvtnorm-style
  
  # compute the transformation matrix A for cholesky with pivoting
  if (method == "chol-mvtnorm") {
    U <- chol(sigma, pivot = TRUE) # upper triangular matrix 
    ord <- order(attr(U, "pivot")) # reordering required due to pivot
    A <- U[, ord]
  }
  
  # set seed if requested
  if (!is.null(seed)) set.seed(seed)
  
  # construct samples mvtnorm-style
  if (method %in% c("eigen-mvtnorm", "chol-mvtnorm")) {
    Z <- matrix(rnorm(k * n), n, k, byrow = TRUE)
    X <- Z %*% A   
  }
  
  # construct samples mass-style
  if (method == "eigen-mass") {
    Z <- matrix(rnorm(k * n), n, k)
    X <- t(A %*% t(Z))
  }
  
  return(X)
}
```

Do a quick sanity check:

```{r}
#| error: true
library(testthat)
library(withr)

test_that("smol test", {
  
  for (n in 3:6) {
    for (s in 1:3) {
      
      obj <- sampler(n, sigma = cov1, method = "eigen-mass", seed = s)
      ref <- with_seed(s, MASS::mvrnorm(n, mu = rep(0, 4), Sigma = cov1))
      attr(ref, "dimnames") <- NULL
      expect_equal(obj, ref)
      
      obj <- sampler(n, sigma = cov1, method = "eigen-mvtnorm", seed = s)
      ref <- with_seed(s, mvtnorm::rmvnorm(n, sigma = cov1, method = "eigen"))
      expect_equal(obj, ref)
      
      obj <- sampler(n, sigma = cov1, method = "chol-mvtnorm", seed = s)
      ref <- with_seed(s, mvtnorm::rmvnorm(n, sigma = cov1, method = "chol"))
      expect_equal(obj, ref)
      
    }
  }
})
```

## Recommendations

Siiiiiiiiiiiiiiiigh. Once upon a time I was an academic, and a weirdly successful one at that. One of the things I used to see happen on a regular basis is people with expertise in Field X learning exactly one fact about Field Y, writing entire papers filled with sweeping claims about what constitutes best practice for Field Y, and getting things catastrophically, dangerously wrong.^[See for example, psychologists talking about statistics, or physicists talking about anything that isn't physics.] It is a *thing*. As such, I am wary of issuing recommendations. I'm basically an amateur in this area, and you should take my thoughts with a massive grain of salt. Nevertheless, I'm also aware that some R users with even less expertise than me would perhaps like some suggestions for how to avoid this particular nightmare in their own work. So, here goes. My recommendations. In their entirety:

- Call `set.seed()` immediately before the sampling^[Or alternatively, wrap the call to `mvtnorm::rmvnorm()` inside a call to `withr::with_seed()`. It does the same thing and in my experience can be a useful coding practice because it starts to "nudge" you into working out which sections of your code you *really* need to protect, and then writing your code inside smaller "protected blocks". That way, if one block fails to be reproducible because of some wild and unanticipated madness, the other blocks are still protected.]
- Use `mvtnorm::rmvnorm()` instead of `MASS::rmvnorm()` to do the sampling
- While you're there, you might as well also set `method = "chol"`. It doesn't hurt
- Don't put too much faith in condition numbers, they're rough guides at best
- Learn to stop worrying and love the bomb^[The bomb in this case being floating point arithmetic. It's pure madness down there, and you can't control everything. Accept it, and move on with your life.]

## Further reading

- *Introduction to mathematical statistics* by Hogg, McKean, and Craig. Or, alternatively, any decent mathematical statistics textbook that you have lying around. I happened to have the 6th edition of this one by my bed (it's a sex thing, don't ask) and they're up to the 8th edition now, but really it doesn't matter. My actual point here is that most mathematical statistics textbooks will spend a bit of time walking you through the multivariate normal distribution, and you quite quickly get a feel for why matrix decomposition lies at the heart of anything you do with correlated normal variates.

- [The algebraic eigenvalue problem](https://archive.org/details/algebraiceigenva0000wilk_c5b6). Written in 1965 by James Wilkinson, this book is the primary reference discussed in the R documentation to `eigen()` and is very much the definitive source on the topic. It's also, thanks to the magic of the Internet Archive, quite easy to borrow online if you're so inclined. 

- [What every computer scientist should know about floating-point arithmetic](https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html), by David Goldberg, published in the March 1991 edition of "Computing Surveys". Kind of a classic article, and one that I have found myself accidentally rediscovering over and over whenever I make the mistake of assuming that floating point numbers aren't going to break my code.

- [LAPACK users guide](https://www.netlib.org/lapack/lug/lapack_lug.html). As a general life rule I have tried to learn as little as possible about BLAS and LAPACK: I'm not that kind of masochist. However, I will concede that sometimes it's a necessary evil, and the LAPACK users guide and various other resources at [netlib.org/lapack](https://www.netlib.org/lapack/) can be helpful whenever you find yourself in that terrible situation. Oh and naturally it's on [github](https://github.com/Reference-LAPACK/lapack) also.

- [On sampling from the multivariate t distribution](https://journal.r-project.org/archive/2013-2/hofert.pdf), by Marius Hofert, published by the R Journal in December 2013. As the title suggests, the focus is on the multivariate t distribution rather than the multivariate normal, but a lot of the lessons are relevant to both, and the article doubles as documentation of some key features of the mvtnorm R package. I didn't really understand a lot of the nuances at first, but the further I got into the Wilkinson book the more I realised that mvtnorm tries pretty hard to do the right thing.

