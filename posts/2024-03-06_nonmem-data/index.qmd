---
title: "Thoughts on data formats in pharmacometric modelling"
description: "A half-considered ramble based on some peculiarities in NONMEM"
date: "2024-03-06"
categories: ["Pharmacometrics", "NONMEM"]
--- 

<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

This is another entry in my intermittent series of posts on [pharmacometrics](/#category=Pharmacometrics), albeit a rather different one to the previous installments. This one came about because I've been slowly working my way through *Introduction to Population Pharmacokinetic/Pharmacodynamic Analysis with Nonlinear Mixed Effects Models* by Joel Owen and Jill Fiedler-Kelly. It's an excellent book, though in some ways the title is a little misleading: though the book does talk a little about PopPK modelling generally, the bulk of it is really a tutorial on how to use NONMEM, and it serves that purpose very nicely. 

What I'm actually going to talk about in this post are some of the difficulties I've faced in getting up to speed in NONMEM from a user perspective, and how some of these "pain points" partially reflect design flaws in NONMEM. It's a difficult topic to talk about without seeming unfair or overly critical, because there are *many* respects in which NONMEM is excellent software, and I don't like the idea of presenting a lopsided perspective. With this in mind, as I go through I will try my very best to highlight the positives, and also try to be as sympathetic as possible to the developers. 

I should also, I suppose, be explicit about stating that any opinions stated here are some personal thoughts. They don't represent my employer, or anybody else. And, quite frankly, I might be very wrong about a lot of them. I'm not so arrogant as to think that my opinions are necessarily right on all things. I am wrong about a depressingly large number of things.

Okay, enough preliminaries. Let's get started.

## What is NONMEM?

For a pharmacometrician, there is very little point in having a section with this title, but since the majority of people who read this blog aren't pharmacometricians, I'll unpack this a little bit. 

NONMEM is proprietary software designed specifically to support nonlinear mixed-effects modelling in pharmacometrics, and released by the company ICON Development Solutions. Originally developed in the 1970s by Lewis Shiner and Stuart Beal (with Alison Boeckmann joining the team a little later, I believe), NONMEM is written in Fortran and is comprised of several distinct parts:

- The core of the platform is the NONMEM engine. Although NONMEM-the-software is used almost entirely for the purpose of working with compartmental models used in pharmacometrics, NONMEM-the-engine^[For the 1000th time, I reiterate my frustration with software tools that use the same word to mean different things: rstudio-the-ide vs (formerly) rstudio-the-company, observable-the-company vs observable-the-javascript-library vs observable-the-hosted-notebook-platform, etc etc. It's unhelpful.] is not specific to these models: it's simply a suite of tools for estimating nonlinear mixed effects models. Indeed, the name NONMEM is an abbreviation of **NON**linear **M**ixed **E**ffects **M**odels.

- On top of NONMEM, there is are a collection of Fortran subroutines collectively referred to as PREDPP, short for **PRED**iction of **P**opulation **P**harmacometric models. There are many different subroutines within PREDPP, and they are given names like ADVAN1, ADVAN2, etc.^[I am told that the name ADVAN is short for "advance", and refers specifically to the manner in which the various ADVAN routines "advance" the underlying system of ODEs forward in time in discrete time steps that correspond to the times at which measurements are made]. Each of these ADVAN routines implements a specific type of model architecture: for instance, ADVAN2 implements a one-compartment model with first-order aborption and first-order elimination, whereas ADVAN4 corresponds to a two-compartment model with first-order absorption and elimination. As I've mentioned previously, these models can be parameterised in different ways, so each of these ADVAN subroutines is associated with a set of TRANS subroutines^[Named because of the gender dysphoria diagnosis that th... oh never mind. I will never stop finding the name funny though.] that provide the appropriate parameter transformation. 

- Finally, there is NM-TRAN, short for **N**ON**M**EM **TRAN**slator. NM-TRAN provides a domain-specific language that allows users to write a "control file" that provides an abstract specification for the model, the dataset, the estimation procedure to be used, and the requested output. In effect, NM-TRAN is the user interface for NONMEM. 

Understanding this three-part design helped me a lot in orienting myself to NONMEM and making sense of its behaviour. For instance, one strength in having the PREDPP routines is that it allows NONMEM (referring to the software as a whole, not just the engine) to be very performant. While in *principle* all compartmental popPK models form part of a broad family of nonlinear mixed effects models, there are huge performance gains to be had by implementing special cases as distinct subroutines. For example, the two-compartment model implemented by ADVAN4 is precisely the model for which I derived analytic solutions in [this post](/posts/2023-12-19_solving-two-compartment-pk-models/). There is absolutely no need to invoke a numerical ODE solver when fitting this model because there are closed-form solutions available. No matter how good your ODE solver is, it's always going to be orders of magnitude slower than using the algebraic form for the model predictions. By carving up the space of possible compartmental popPK models into "computationally meaningful" special cases, the NONMEM software is able to optimise performance very effectively.

All these optimisations, combined with the fact that NONMEM was the first-to-market software tool for pharmacometric modelling, leads to a situation where it remains the dominant software platform in this space today.

## Some difficulties with NONMEM

Okay, so now that I've provided some background and -- I hope -- been sufficiently effusive in my praise for the things NONMEM does very well, I'm going to pivot a bit and start talking about some limitations to NONMEM from the end-user perspective. Many of these limitations are things that seem unavoidable, especially when you consider the time it was originally developed. But not everything can be explained that way. Some things are simply confusing, and in some cases feel like genuine design flaws.

Please bear with me, dear reader. This section of the post is going to be very whiny, and in truth it is nothing more than a laundry list of random complaints I've had during my learning process. I promise that by the end of this, though, things will turn around again and I'll start offering some more constructive thoughts that are a bit more general (and potentially more useful) than simply grumbling about the things I have found difficult in NONMEM. However, in blog posts as in life, you sometimes have to go through some rough patches before getting to the good parts...

### Error reporting

NONMEM doesn't have a good system for error reporting. Error messages are often terse, don't provide much information about where the error occurred, and so on. This makes debugging hard. I don't want to be too critical here though: I'm not a Fortran expert by any stretch of the imagination, but I don't think it's controversial to say that Fortran in the 1970s did not provide the same level of detailed compiler-error reporting that Rust does in the 2020s, and as a consequence if your model produces a compile-time error, you have to do trial-and-error guesswork to figure out where it went wrong.

In practice, this is a big limitation to NONMEM. Error reporting is super-important for usability, and it is endlessly frustrating to me that NONMEM errors are so hard to debug. I'm not trying to be mean, because I really am sensitive to how hard it is to do good error reporting and can only imagine how tricky it must be with a codebase as old as NONMEMs. From a developer perspective it must be a nightmarishly hard problem to fix, so I am deeply sympathetic. But I won't lie: from a user perspective the lack of good error reporting is also nightmarish. 

### CSV parsing

Next up in my list of "Danielle has some concerns..." is the matter of how NONMEM validates the data input, or, more precisely the fact that it doesn't actually do this. 

Data passed to NONMEM take the format of a delimited text file, most commonly using the comma-separated value format. With a few specific exceptions, all input fields to NONMEM must be numeric: you cannot include strings. This is a little frustrating sometimes, but I'm not being critical here. This limitation is not really that uncommon or unusual when you consider what the tool exists for. As a point of comparison, Stan has the same limitation: it supports a variety of numeric types, but does not support strings. 

The fact that data must be numeric doesn't seem all that surprising to me. What *does* seem surprising to me, a quarter of the way through the 21st century, is that NONMEM has absolutely no ability to validate the input file. It does not check that the rows contain the same number of columns, and it cannot even read the header row to a CSV file. The *only* way you can provide names for the columns in your data set is to supply them manually through the $INPUT statement in your NM-TRAN control file.

This part is genuinely and deeply shocking to me. It is 2024. There are plenty of open source tools for reading and validating a CSV file that come with completely free licenses and are suitable for inclusion within proprietary software. To lack even the most basic facilities for CSV parsing and validation is utterly mystifying, and I have no idea whatsoever why NONMEM can't actually read a CSV file properly.

I've tried on numerous occasions to point out to people how truly bizarre this omission is, and I find there are two distinct reactions depending on who I'm talking to: 

- Software developers stare at me in blank horror, utterly speechless
- Pharmacometricians shrug their shoulders, because "that's how it is"

Count me with the software devs on this one. I am so confused. But I don't actually have anything meaningful or constructive to say on this other than "why????" so perhaps it is best to move on.

### Date handling

Okay, let me be up front: this bit is me being confused as hell about something that actually doesn't matter much in real life. But it's so bizarre that I just have to talk about it. NONMEM has a system for encoding dates that... well, I can honestly say I've never seen anything like it in all my years in scientific computing. 

Here's how it works. DATE columns are one of the few places where NONMEM can accept non-numeric input, and that seems very sensible to me. However, if you include a DATE column in your input data, the default format is MM/DD/YY^[Or to be more precise, MM-DD-YY is also accepted. You're permitted to use either "/" or "-" as the delimiter character for dates]. There are quite a lot of problems here packed into one sentence, and I'll have to unpack it piece by piece because there are layers to this onion.

The first problem here is that almost no-one outside the US uses a month-day-year ("mdy") format to express dates. That's just not a thing in the rest of the world. Fortunately, you do have the option of asking NONMEM to parse a date as "dmy", "ymd", even "ydm" if you need to. However, the mechanism for doing this is a little peculiar. What I would have expected in this situation is that NM-TRAN would support an option in the $DATA block where the user would specify the date formatting convention. For instance, I could imagine a solution where the user specifies DATEFMT="yy/mm/dd" or DATEFMT="yyyy-mm-dd". That makes sense to me.

However, this is not what NONMEM does. Instead, what happens is that NONMEM actuallyy has *four* different reserved names for date columns: DATE is used to specify a "mdy" ordering, DAT1 means that a date is "dmy", DAT2 is "ymd", and DAT3 is "ydm". Not what I'd have done -- and it does mean that the user has to just memorise which of the date variables corresponds to which of the date orderings -- but it doesn't seem like a hill to die on.

A more severe limitation though is -- as was the custom at the time NONMEM was developed -- the fact that "year" is hardcoded as a two digit number. Now, I know what you're thinking, dear reader. *Obviously*, since NONMEM has this "reserved names" system for switching between "mdy", "ymd", etc (DATE, DAT1, etc), you'd expect that as the 21st century rolled around and with [ISO-8601](https://en.wikipedia.org/wiki/ISO_8601) having been around since 1988 specifying an international standard stating that YYYY-MM-DD is the accepted format for dates, what NONMEM would do is introduce something like ISODATE as a reserved column name that accepts dates in the ISO-8601 format? That, at least, would be consistent with the DATE, DAT1, DAT2, etc system. 

Right? 

Wrong. 

When it comes to the problem of four-digit years, the NONMEM solution is completely different to the way it handles date-ordering, and NONMEM doesn't actually support four-digit years at all.^[My suspicion is that internally there is a hard-coded restriction that a date must be *exactly* 8 characters in length: two each for MM, YY, and DD, and two more characters for the delimiter. If that's what NONMEM is doing, yeah, it's a much bigger refactor to support a four-digit year. Taking into account the fact that DATE isn't used very often and they have a very small developer team supporting an old codebase that probably carries quite a bit of technical debt, I suppose I am not surprised that even in 2024 NONMEM still cannot handle a four-digit date. Yes, I do find the status quo to be bizarre, but I can readily imagine why this is a can that the dev team decided to kick down the road. I'm genuinely sympathetic, actually.] Everything remains two-digit. What you do here is set the LAST20 option in the $DATA block in your control file, specifically by setting LAST20=n where n is an integer between 0 and 99. If YY>n the date is assigned to the 20th century and the year is 19YY. Otherwise the date is assigned to the 21st century and the year is 20YY. 

On top of all this, there is additional weirdness because the mere presence of a DATE column in NONMEM changes the semantics of the TIME column. If there is no DATE in the data, TIME is interpreted as numeric, and considered to be "time elapsed since first dose". But... if there is a DATE column, TIME is now given a new meaning and is interpreted as clock time: NONMEM now expects that TIME is hh:dd formatted and doesn't permit times greater than 24 hours. That is of course a recipe for chaos, so I'm not surprised that every data set I've seen in the wild makes the choice to omit date information entirely. Ultimately, NONMEM is so bad with dates that users don't ever feed it dates

I... don't quite know what to say.  

All I *can* say here is that, well, in practice I've never actually seen a NONMEM data set that uses the DATE field so this bizarre behaviour has never affected me. But at the same time, *design matters*. I think part of the reason that I've never seen a NONMEM data file that uses proper date/time information is simply that NONMEM doesn't support dates properly. 

All of that being said... as mystified I am at the design choices here, I find it hard to get too worked up. In my experience (which is admittedly limited) you just don't need date information in popPK modelling very often, so the fact that NONMEM is bad at this doesn't really affect me as a user.

So let's move onto something more substantial, shall we?

### Untidy data

Up to this point, every "critique" I have made -- with the possible exception of the CSV parsing issue -- falls into one of two categories: (1) not important, or (2) really, really hard to fix. I don't think my "date criticism" is substantive, and I don't think my "error reporting criticism" has an easy fix. As frustrating as some of those things are, it would be churlish and unfair of me to be too harsh when talking about these issues.

However, I have one more critique I want to talk about here, and this one feels like something that was (a) a bad design choice from the very beginning, and (b) something that has had nasty knock-on consequences for every other piece of software that has been developed since NONMEM. 

That issue is "untidy data". 

One of the *fundamental* properties of data sets used in popPK modelling is that they are necessarily comprised of (at least) two qualitatively different kinds of record: observation records and dosing records. These things are deeply different to one another, and entirely incommensurate in what they mean:

- A **dosing** record specifies an *intervention*: it is used to specify what dose of a drug was administered, how it was administered, when those doses were administered, and so on. It is, fundamentally, a part of the study design (albeit one where the time-of-dose information might be empirical rather than precisely identical to what is stated in the study protocol)

- An **observation** record specifies a *measurement*: it is used to record empirical measurements, usually (though not exclusively) the plasma concentration of a drug or its metabolite. It is fundamentally part of the study data, and is also the place where you might record values of a time-varying covariate (e.g., body weight might not simply be a baseline measure, it could change over the study time course). 

These two things are deeply different kinds of record, and at some level NONMEM recognises this: a dosing record cannot, for instance, include a value for DV (i.e., the reserved column name used to record values of the dependent variable). However, the enforcement is inconsistent at best: you *are* permitted to include values of covariates within a dosing record, for instance.

The thing that feels like a design flaw, and one that seems to have been inherited across every piece of popPK software I've come across, is that *dosing records and observation records are intermixed into the same "event schedule" data file*. 

It seems to me that this is incoherent: dosing records and observation records are so incommensurate with each other in their structure that they properly belong in separate tables. You see this all the time when doing exploratory data analyses, exposure-response analyses, and other analyses associated with model validation. In those analyses, what you almost always do is read the data file into R, and one of the *first* things you have to do in the script is split the input data into (at least) two data frames: one containing the dosing records, and another one containing the observation records. It's necessary to do this because... well, it's sort of obvious right? One of these tables stores design/intervention information, and the other stores observation/measurement information. They're so different that *anything* you do with the data set downstream necessarily requires different analysis code.

This is telling us something. The input data should have been structured as two separate tables from the beginning. Mixing qualitatively different data records within a single table is *untidy*, and untidy data sets are hard to write analysis code for. 

Now at this point, dear reader, I must confess to a small faux pas that I regret slightly. I posted on mastodon about some of my... um... frustrations with the untidiness of NONMEM data files, and I was a bit more hyperbolic about it than is reasonable (I may have had a beer or two at the time of posting). I was genuinely a bit uncharitable. However, what followed after my foolishness was a very sensible conversation among pharmacometricians and software developers that was far more interesting than my initial remarks. One thing that I realised from that discussion -- in which every participant had immeasurably more experience in this field than I do -- is that because NONMEM has been so influential in the field, and because everything that NONMEM does has a tendency to become a de facto standard, the inherent untidiness of the input data has been inherited by every other software platform that has followed in NONMEM's footsteps. 

Because of its status in the field, NONMEM is the thing that shapes user expectations, and -- unfortunately in this instance -- I think the pervasiveness of NONMEM has led to a situation where pharmacometricians *expect* that PK data sets should be untidy, and forced developers of other platforms to adopt data standards that are almost as untidy as what NONMEM does.

This seems like a problem, and so it seems incumbent on me to stop behaving like a petulant child and instead try to offer some constructive thoughts. 

## What would a tidy PK data set look like?

At long last we get to the part of the post that is somewhat substantive in nature. It's the part of the post I've had to think hardest about, and also -- because this part is hard -- it's also the part I'm least certain about. But I am going to try anyway. Because a lot of the discussion of what constitutes tidy data in practical settings has come out of the R community, I'll start by quoting [this section](https://tidyr.tidyverse.org/articles/tidy-data.html#tidy-data) from the documentation of the "tidyr" R package:

>Tidy data is a standard way of mapping the meaning of a dataset to its structure. A dataset is messy or tidy depending on how rows, columns and tables are matched up with observations, variables and types. In tidy data:
>
> - Each variable is a column; each column is a variable.
> - Each observation is a row; each row is an observation.
> - Each value is a cell; each cell is a single value.

In the PK context, we should be careful about our use of the term "observation", because our data consists of "empirical observations" and "dosing interventions". The "tidy data" definition is uses the term "observation" more broadly: though not observational in nature, a dosing record absolutely counts as an "observation" in the sense implied the tidy data definition above. 

### What are we talking about?

In order to talk about what would constitute a "tidy" structure for PK data, I think it's important to be clear about what *isn't* part of the discussion, and you can see a few examples of this by looking at my "random"^[Okay, I'll be honest, it wasn't random at all.] laundry list of complaints about NONMEM.

- The encoding format does not matter. It doesn't matter whether you're importing data from a CSV file, or a Parquet file,^[Don't get me wrong I *love* Parquet. It's an amazing format for serializing large data sets, and -- as I've discussed in some of my posts about Apache Arrow -- it makes a massive difference when you have big data. But the largest PK data set I've seen so far in my consulting work has about 10K rows. This is not large enough for anyone to care about the performance differences between CSV and Parquet.] or from an in-memory data structure like a data frame in R, Julia, or whatever. Encoding is a fundamentally different issue to "tidiness".

- Data types do not (inherently) matter. Earlier in the post I tangentially grumbled about the fact that NONMEM (and Stan) do not support strings, a complaint that you could probably generalise (in the in-memory context) to talk about whether R factors or Julia dictionaries should be supported. This too is irrelevant. A data structure that encodes race^[In the conventional US-centric sense of the term.] as a numeric variable where "1 = white", "2 = Black", "3 = Asian", etc is neither more nor less tidy than a data structure that explicitly represents the labels (either as strings or via dictionaries etc). This is not what I mean when I talk about tidiness. There will be moments later in the post where I talk about data types, certainly, but those remarks should be viewed as implementation details and/or user conveniences -- they're not inherently about "tidiness".

- Implementation details do not matter. Again referring back to my miscellaneous list of grumbles, the fact that NONMEM can't validate a CSV file and that it has a bizarre internal representation of dates... this too is irrelevant. NONMEM data sets are neither tidy or untidy because of these facts. 

In other words, the "tidiness" of a data set is something that relates to the logical organisation and semantics of the data: it's not related to low-level implementation or encoding. 

By way of contrast, semantics *do* matter. Recall earlier that, toward the end of what seemed like a silly rant about how dates are encoded in NONMEM, I mentioned the fact that the semantics of the TIME column are *different* in NONMEM depending on whether a DATE column exists. This actually is -- in my opinion, at least -- a form of untidiness. Every column and every row in a data set should mean one thing and *only* one thing. Its meaning and parsing should not be dependent on the existence (or lack thereof) of another column (or row). The fact that you cannot assign meaning to TIME in NONMEM without first checking to see if DATE exists is a form of untidiness, and is one of the things we want to avoid when constructing tidy data.

### Why is it untidy to mix dosing with observations?

Okay yes, it seems intuitively "obvious" that it's a bad idea to mix observational records into the same table as dosing records, but even so I want to unpack *why* we might have that intuition. There's probably a long list of reasons, but it probably suffices to note the a few salient examples.

Firstly, notice that mixed table introduces messy dependencies between columns. A dosing record *must* have an AMT column (using NONMEM terminology here) because you can't really talk about dosing without recording the amount of drug administered. Yet at the same time an *observation* record cannot have an AMT value because it doesn't describe a dosing event. Similarly, a dosing record *cannot* have a DV value because it doesn't describe a measurement (in NONMEM, DV must be specified as missing for a dosing record), whereas an observation record can *only* have a missing value for DV if -- for whatever reason -- the relevant measurement wasn't made or wasn't recorded. By mixing these two things together you have a strange situation where there are "mandatory missing values" that don't *actually* correspond to real missingness: it's purely an artifact of the data structure.

Secondly, notice that mixing disparate record types into the same table has the effect of subtly changing the semantics attached to the columns. Some of these semantic shifts are trivial: observation records and dosing records both have values for TIME, but one refers to time of dosing and the other refers to time of measurement. That's a minor difference, and perhaps not important. But, to extend the example from the last paragraph, the meaning of a missing DV value changes a *lot* depending on whether you have a dosing record or an observation record. One of those two things is purely artifactual, the other is meaningful. 

Thirdly, notice that I lied in the previous paragraph when I said that the changes in the semantics of TIME depending on whether a row refers to a dosing event or an observation event are trivial. That's only really true when we're talking about dosing events that occur at discrete time points (e.g., oral dosing or bolus IV dosing). For an infusion dose, TIME interacts with RATE in a fashion that is very meaningful for the dosing information, but has no meaning whatsoever in the observation context.

Of course, none of these points are very deep. They are "statements of the obvious", and nobody should mistake them for anything other than what they are. However, what I'm trying to accomplish by stating the obvious out loud is highlighting the fact that "separate tables for dosing and observation" is a better representation of the structure of the data than "one big table that mixes them together, messes up the semantics, and introduces massive dependencies between the columns".

Our data are comprised of two fundamentally different types of records: as such, they belong in separate tables. 

### What does a tidy dosing table look like?

Dosing events in NONMEM have a fairly specific structure, with some mandatory and some optional columns. In what follows I'm going to talk a little about each of these, attempting to adopt a broader perspective than considering only NONMEM, but I will use "NONMEM-style" names to refer to each of the variables.

#### Mandatory columns

For some variables, a "tidy" format doesn't involve a lot of departure from the  NONMEM-style de facto standard. For example, TIME and AMT would both be required variables in a dosing table, and their meaning is not going to be any different from what NONMEM expects:

- TIME specifies the time at which the dose was administered (or, in the case of an infusion, the time at which the infusion started). In the typical case this is a numeric variable, but (as discussed earlier) there is the possibility of specifying a DATE and a TIME column, in which case the semantics are slightly different. Looking more broadly than NONMEM (because lets face it, NONMEM is never going to change its data standards), I think it would be reasonable to adopt a standard in which TIME could *either* be a simple numeric value, or else could be a fully-specified datetime variable (e.g., a POSIXct variable in R, or a correctly-formatted ISO compliant field in a CSV file). When specified as a numeric variable, it is up to the user to make sure that the units are sensible; when specified as a datetime input, the units are already prescribed by the data. There doesn't seem to be much value in considering other formats for this field.  
- AMT specifies the quantity of drug administered, as a numeric variable. The semantics of this column will depend slightly on whether the dose is administered all at once (e.g., bolus IV, oral) or over time (e.g., infusion), but either way the intended meaning of AMT is that it refers to the total amount of drug (in whatever units the user intends) administered. Naturally, this field should be a numeric variable (e.g., integer, float, a number in a CSV file, etc)

These are both required fields, and a dosing table that does not have an AMT column or a TIME column should throw an error. Indeed, since -- in practice -- pharmacometric software is used primarily for popPK analyses where there are multiple participants receiving doses of the drug, I think we should go one step further and make ID a mandatory column:

- ID specifies a unique identifier for a subject. In NONMEM there is a constraint that the ID column be a numeric variable (like any other NONMEM variable). This constraint, however, is entirely unprincipled from a data perspective and exists only because of how NONMEM is designed. It should not be part of a data standard. An ID column could be a numeric value, certainly, but strings should be permitted too. That's particularly valuable in the drug development context because most of the time we have access to the "universal subject identifier" field (which, in my experience is usually encoded as a USUBJID variable), which is a string that uniquely specifies both the subject *and* the study in which they were enrolled. It is counterproductive to have a data standard that precludes the user from passing a USUBJID field as the ID column, and as such the ID column should be permitted to be a string.

There's something else to unpack about ID variables, which relates to reuse: in a tidy data format, the user should not be permitted to reuse ID values: an ID value should be *unique* for every subject in the data set. Surprisingly, NONMEM does not actually abide by this constraint, because of another peculiarity in how NONMEM data files are structured. NONMEM imposes a strong constraint -- which, in my view, is wrong -- that the ordering of rows matters. Specifically, NONMEM expects that all rows associated with a particular subject be *contiguous* in the input data, and moreover that these rows be ordered by increasing value of TIME. This constraint -- while convenient to the user, and generally desirable -- violates the usual assumptions of tidy data (and indeed of most database software). The *record itself* should contain all information relevant to its interpretation: its *position* in the data file should not be important. NONMEM does not abide by this rule, and because of this NONMEM produces very peculiar behaviour with regards to ID columns. In NONMEM, you are permitted to use the *same* ID value to refer to two different subjects, if those subject records are noncontiguous in the input file. This behaviour should not be permitted in a tidy data structure: a true ID column should uniquely identify a subject regardless of how the rows are permuted. This should not be permissible in a tidy dosing table: an ID should uniquely specify a subject, but (remarkably, in my view!) the NONMEM data standard doesn't actually do this. 

But we should move on, because dosing tables are much more complicated in structure than merely specifying ID, TIME, and AMT, and the discussion gets a little more nuanced once we consider the "optional" columns. 

#### Repeated dosing

There are lots of other fields that are typically used in a NONMEM-style dosing record, and these have a tendency to be "optional" in the sense that you could safely omit them from a data file and supply default values if they aren't present. The two that come up most often, in my experience, are the ADDL and II columns that are used to describe *repeated dosing*:

- ADDL is a numeric value used to specify the number of "repetitions" of the same dosing event (after the original one), and if ADDL is missing you would naturally supply a default value of zero. In my personal view, framing this variable as "additional" doses is the wrong way to do it: the natural format for this column should have been to name it NDOSES or something like that (i.e., NDOSES refers to the total number of doses specified by the row), and the default value would be NDOSES=1 rather than ADDL=0. But I'm a pragmatist. This doesn't make a big difference in practice, and by now most people in the field are used to seeing ADDL. 

- II refers to the "interdose interval", and exists only when ADDL>0 (or NDOSES>1). This field specifies the length of time that separates each dose, and *must* be commensurate with the TIME column. That is to say, if TIME is numeric, II must be numeric and use the same units as TIME; whereas if TIME is a datetime object II must be a duration object (e.g., if TIME is a POSIXct object in R, II should probably be a difftime or something unambiguously coercible to a difftime).

Dealing with these "repeated dose" columns is difficult from a tidy data perspective, because they violate the tidy data principle that each row corresponds to a *single* event. They are very convenient, in the sense that they allow the user to specify a complex sequence of events using compact syntax, but they create downstream problems: if you need to write code to determine the time at which a particular subject received their most recent dose (i.e., max TIME such that TIME < t), your code needs to:

(a) inspect the TIME, II, and AMT columns
(b) unpack *all* the dosing events to create a table in which every row corresponds to a single dosing event, creating new TIME values for each discrete dose, and only then
(c) searching to find max TIME < t

I have only been working in this field for six months and I have already run afoul of this problem about a dozen times. It is a *huge* pain when you are trying to write analysis code, because a "compact" dosing table is inherently untidy.

What does all this tell us? 

- Well, first off we should start with the elephant in the room... in practice, you actually cannot escape the necessity of having something like an ADDL/II syntax for describing pharmacometric data. Over and over you will find it written into the protocol for a study: X doses of Y amount, administered once daily at time Z. It's a *natural* way to describe a dosing schedule, and you cannot (and should not) take that "description language" away from the user. But also...

- ... the software should *unpack* the dosing table. NONMEM doesn't do anything like this because (as usual) NONMEM is not user friendly and does not abide by any kind of tidy data principles, but to the extent that a user needs to write code to work with a dosing table, that table should be unpacked: there should be exactly one row per dosing event in the dosing table, no more and no less. 

### What are we missing?

So far, so good. At this point we have an opinionated^[Siiiiiiiigh. As every data scientist knows, everyone describes their approach as "opinionated" as if that were always a positive thing. Look, I feel like there is some value to being opinionated in a blog post like this, because indeed what I'm doing here is talking about my opinions and trying to articulate something about what I think tidy PK data might look like, but I am truly on the fence about whether it's a good thing for every single tool under the sun to be an "opinionated framework".] framework for thinking about dosing tables in a tidy format... but one that only really works for a subset of study designs (albeit a very common one). It works fine if you have oral dosing or bolus IV dosing, and it works if your data set is "complete" in the sense that it specifies all doses that a subject has received. But there's a lot of designs that doesn't cover:

- We don't have a language to describe infusion dosing
- We don't have a language to describe situations where the system starts at steady state because of prior doses (nor indeed any language to describe disruptions to steady state)
- We don't have a language to describe "interruption" or "reset" events, such as in a crossover study where one dose is bolus IV and another is (say) oral dosing, and the only thing we want to assert about the time separating those two doses is that it's long enough that the drug concentration has returned to zero in all compartments in the time between those two dosing events

NONMEM supplies a data specification that allows the user to describe those events, and a realistic "tidy" standard needs to be able to support those events also. 






 

### What does a tidy observation table look like?


## What might we lose by adopting a tidy data standard?

Tidy data is generally a good thing from a scripting and data analysis perspective, but there are other things we need to take into account. 

A big one is "unified data view" during exploratory data analysis. While some pharmacometricians are experts at scripting in R (or other languages), not everyone in the field possesses that expertise, and this does matter. 

TODO: unpack the point about being able to visually detect "last dose" in a data view, and the necessity of having a "data view" that makes this easily accessible to the analyst.
