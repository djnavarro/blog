---
title: "Using targets"
description: "A build automation tool that works really well for R projects"
date: "2025-01-02"
categories: ["R", "Reproducibility"]
--- 

<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

About 18 months ago I wrote a post about [balrogs and makefiles](/posts/2023-06-30_makefiles/). The post was long, strange, but also cathartic. It had bothered me for years that I didn't really understand [make](https://www.gnu.org/software/make/) as well as I wanted to, and it was really helpful to write up some notes about it as a way of teaching myself how to use it more effectively than I had done in the past.^[In all fairness, this is the *primary* purpose of this blog. Yes, it makes me very happy that other people find my posts useful and/or enjoyable, but that is actually a secondary goal. I write these posts for myself, because the act of writing is also an act of learning for me.] Buried at the very end of the post, somewhat sheepishly, is a reference to the [targets](https://docs.ropensci.org/targets/) R package by Will Landau. Even then I knew that I was going to need to learn how to use targets, but... life gets in the way. I have been preoccupied by other tasks, sadly, and it has taken me until now to (a) sit down and read through the [targets user manual](https://books.ropensci.org/targets/), and (b) come up with some [fun side projects](https://github.com/djnavarro/tartoys) that would give me the opportunity to try it out. 

```{r}
library(targets)
```

## Preliminaries

One key thing about targets is that it's a project oriented workflow, and not surprisingly it's easiest to use from *within* the project. By default, code in a quarto blog post like this one executes with the working directory set to the folder containing the qmd file, but to make my life easier I'll sometimes change that directory so that when I'm discussing a specific targets project, the code will execute from the root directory of that project. However, using `setwd()` to do this is not ideal: the proper way to do this within any knitr-based tool (quarto, rmarkdown, etc) is to set the `root.dir` knitr option. To that end, I'll define a little convenience function to take care of this whenever I need to switch to a new project:

```{r}
post_dir <- here::here("posts", "2025-01-02_using-targets")
set_knitr_dir <- function(dir, base = post_dir) {
  knitr::opts_knit$set(root.dir = fs::path(base, dir))
}
```

Right then. Time to play around with targets...

## Project 1: D&D spells


The first project is the fun [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-12-17) project that I described in my [last post](/posts/2025-01-01_schools-of-magic/). In that post, I talked about two data visualisation I made using data about D&D spells. When I wrote that post, I didn't talk about targets at all, and none of the code presented in that blog post is written in a "targets-friendly" way. However, all though that form of the code was the simplest way to write it up, it's not how I originally wrote it. The actual code was written for targets (it's included as the `spells` folder in [this github repo](https://github.com/djnavarro/tartoys)). 

As it happens, I have a slightly-modified copy of the project in the `spells` directory within this blog post, so that I play with this project within this post, and see how targets operates. With that in mind, the very first thing I'll do is navigate into that project folder:

```{r}
set_knitr_dir("spells")
```

```{r}
#| label: targets-spells-destroy
#| echo: false
# complete clean, mimic fresh state
if (fs::dir_exists("_targets")) fs::dir_delete("_targets")
if (fs::dir_exists("output")) fs::dir_delete("output")
```

Next, let's take a look at the structure of this project. At the moment this is a clean project (i.e., no code has been run yet), so it contains only the source files. There are only three:

```{r}
#| label: spells-dir-tree-fresh
fs::dir_tree()
```

Here's what each file does:

- The `spells.csv` file is the data set I wish to analyse
- The `analysis.R` script defines a collection of functions which, when called, will perform the required analyses and generate the outputs
- The `_targets.R` script is (unsurprisingly) the build script. It's the targets analog of a makefile, more or less

Let's unpack this a little further. If you read the [schools of magic](/posts/2025-01-01_schools-of-magic/) post, you've already seen the code underpinning the analysis. The only difference between the version in that post and the version I've used here is that the `analysis.R` script wraps each step of the analysis into a function. For example, the pipeline that constructs the "spell dice" plot uses these three functions:

- `set_output_dir()` returns the path to the output folder to which the images will be written, creating that output folder if it doesn't already exist
- `dice_data()` takes the spells data as input, and performs all the data wrangling steps required to construct a tidied version of the data that is suitable for visualisation
- `dice_plot()` takes the tidied dice data as input, specifies the "spell dice" plot, and writes it to an output file

To produce the "schools of magic" plot I also need these three functions:

- `scholastic_data()` takes the spells data as input, and performs the data wrangling steps required to create tidy data suitable for constructing the heatmap
- `scholastic_clusters()` takes this tidy data as input, and the performs additional steps required to construct the hierarchical clustering used to draw dendrograms alongside the heatmap
- `scholastic_plot()` takes the data set and the clustering as input, and uses them to build the "schools of magic" plot that is written to an output file

The actual code for these functions isn't very important, but for what it's worth the code is included below the fold here:

```{r}
#| label: spells-analysis
#| eval: true
#| code-line-numbers: true
#| code-fold: true
#| code-summary: "Click to show/hide the analysis.R code"
#| filename: "analysis.R"
# helper ------------------------------------------------------------------

set_output_dir <- function() {
  root <- find_root(has_file("_targets.R"))
  output <- path(root, "output")
  dir_create(output)
  return(output)
}

# spell dice plot ---------------------------------------------------------

dice_data <- function(spells) {
  dice_dat <- spells |>
    select(name, level, description) |>
    mutate(
      dice_txt = str_extract_all(description, "\\b\\d+d\\d+\\b"),
      dice_txt = purrr::map(dice_txt, unique)
    ) |>
    unnest_longer(
      col = "dice_txt",
      values_to = "dice_txt",
      indices_to = "position"
    ) |>
    mutate(
      dice_num = dice_txt |> str_extract("\\d+(?=d)") |> as.numeric(),
      dice_die = dice_txt |> str_extract("(?<=d)\\d+") |> as.numeric(),
      dice_val = dice_num * (dice_die + 1)/2,
      dice_txt = factor(dice_txt) |> fct_reorder(dice_val)
    )
  return(dice_dat)
}

dice_plot <- function(dice_dat, output) {
  
  palette <- hcl.colors(n = 10, palette = "PuOr")
  
  labs <- dice_dat |>
    summarise(
      dice_txt = first(dice_txt),
      count = n(),
      .by = dice_txt
    )
  
  pic <- ggplot(
    data = dice_dat,
    mapping = aes(
      x = dice_txt,
      fill = factor(level)
    )
  ) +
    geom_bar(color = "#222") +
    geom_label_repel(
      data = labs,
      mapping = aes(
        x = dice_txt,
        y = count,
        label = dice_txt
      ),
      size = 3,
      direction = "y",
      seed = 1,
      nudge_y = 4,
      color = "#ccc",
      fill = "#222",
      arrow = NULL,
      inherit.aes = FALSE
    ) +
    scale_fill_manual(
      name = "Spell level",
      values = palette
    ) +
    scale_x_discrete(
      name = "Increasing average outcome \u27a1",
      breaks = NULL,
      expand = expansion(.05)
    ) +
    scale_y_continuous(name = NULL) +
    labs(title = "Dice rolls described in D&D spell descriptions") +
    theme_void() +
    theme(
      plot.background = element_rect(fill = "#222"),
      text = element_text(color = "#ccc"),
      axis.text = element_text(color = "#ccc"),
      axis.title = element_text(color = "#ccc"),
      plot.margin = unit(c(1, 1, 1, 1), units = "cm"),
      legend.position = "inside",
      legend.position.inside = c(.3, .825),
      legend.direction = "horizontal",
      legend.title.position = "top",
      legend.byrow = TRUE
    )
  
  out <- path(output, "dice_pic.png")
  
  ggsave(
    filename = out,
    plot = pic,
    width = 2000,
    height = 1000,
    units = "px",
    dpi = 150
  )
  
  return(out)
}


# schools of magic plot ---------------------------------------------------

# constructs the data frame used by geom_tile() later
scholastic_data <- function(spells) {
  spells |>
    select(name, school, bard:wizard) |>
    pivot_longer(
      cols = bard:wizard,
      names_to = "class",
      values_to = "castable"
    ) |>
    summarise(
      count = sum(castable),
      .by = c("school", "class")
    ) |>
    mutate(
      school = str_to_title(school),
      class  = str_to_title(class)
    )
}

# hierarchical clustering for the schools and classes
scholastic_clusters <- function(dat) {
  
  # matrix of counts for each school/class combination
  mat <- dat |>
    pivot_wider(
      names_from = "school",
      values_from = "count"
    ) |>
    as.data.frame()
  rownames(mat) <- mat$class
  mat$class <- NULL
  as.matrix(mat)
  
  # each school is a distribution over classes,
  # each class is a distribution over schools
  class_distribution  <- mat / replicate(ncol(mat), rowSums(mat))
  school_distribution <- t(mat) / (replicate(nrow(mat), colSums(mat)))
  
  # pairwise distances
  class_dissimilarity  <- dist(class_distribution)
  school_dissimilarity <- dist(school_distribution)
  
  # hierarchical clustering
  clusters <- list(
    class = hclust(class_dissimilarity, method = "average"),
    school = hclust(school_dissimilarity, method = "average")
  )
  
  return(clusters)
}

scholastic_plot <- function(dat, clusters, output) {
  
  pic <- ggplot(dat, aes(school, class, fill = count)) +
    geom_tile() +
    scale_x_dendro(
      clust = clusters$school,
      guide = guide_axis_dendro(n.dodge = 2),
      expand = expansion(0, 0),
      position = "top"
    ) +
    scale_y_dendro(
      clust = clusters$class,
      expand = expansion(0, 0)
    ) +
    scale_fill_distiller(palette = "RdPu") +
    labs(
      x = "The Schools of Magic",
      y = "The Classes of Character",
      fill = "Number of Learnable Spells"
    ) +
    coord_equal() +
    theme(
      plot.background = element_rect(fill = "#222", color = "#222"),
      plot.margin = unit(c(2, 2, 2, 2), units = "cm"),
      text = element_text(color = "#ccc"),
      axis.text = element_text(color = "#ccc"),
      axis.title = element_text(color = "#ccc"),
      axis.ticks = element_line(color = "#ccc"),
      legend.position = "bottom",
      legend.background = element_rect(fill = "#222", color = "#222")
    )
  
  out <- path(output, "scholastic_pic.png")
  
  ggsave(
    filename = out,
    plot = pic,
    width = 1000,
    height = 1000,
    units = "px",
    dpi = 150
  )
  
  return(out)
}
```

### Defining the pipeline

Now that you've read the verbal description of what each function does, it's intuitively pretty clear how the analysis pipeline is supposed to work. Roughly speaking, you'd expect the analysis to be executed using a script like this:

```{r}
#| eval: false
# setup
input  <- "spells.csv"
output <- set_output_dir()
spells <- readr::read_csv(input)

# make the spell dice plot & write to output
dice_dat <- dice_data(spells)
dice_pic <- dice_plot(dice_dat, output)

# make the schools of magic plot & write to output
scholastic_dat  <- scholastic_data(spells)
scholastic_clus <- scholastic_clusters(scholastic_dat)
scholastic_pic  <- scholastic_plot(scholastic_dat, scholastic_clus, output)
```

So, how does it work with targets? Well, if we take a look at the `_targets.R` script, we can see it looks suspiciously similar to the code above:

```{r}
#| label: spells-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)

tar_option_set(packages = c(
  "rprojroot", "fs", "tibble", "readr", "ggplot2", "dplyr",
  "stringr", "tidyr", "forcats", "ggrepel", "legendry"
))

tar_source("analysis.R")

list(
  # preprocessing targets
  tar_target(input, "spells.csv", format = "file"),
  tar_target(output, set_output_dir()),
  tar_target(spells, read_csv(input, show_col_types = FALSE)),
  
  # dice plot targets
  tar_target(dice_dat, dice_data(spells)),
  tar_target(dice_pic, dice_plot(dice_dat, output)),
  
  # scholastic plot targets
  tar_target(scholastic_dat, scholastic_data(spells)),
  tar_target(scholastic_clus, scholastic_clusters(scholastic_dat)),
  tar_target(
    scholastic_pic,
    scholastic_plot(scholastic_dat, scholastic_clus, output)
  )
)
```


### Inspecting the pipeline

The `tar_visnetwork()` function provides a handy way of visualising the structure of a pipeline as a little HTML widget. By default it will show the dependencies between all functions and all targets, which is often totally fine, but I'm going to simplify it here by setting `targets_only = TRUE` (i.e., don't show the user-defined functions), and also set `exclude = c("input", "output")` to ignore the two really boring pre-processing targets! 

Anyway, here it is:

```{r}
#| label: targets-spells-visnetwork
tar_visnetwork()
```

This makes the structure of the project clear:

- The `dice_pic` image depends only on the `dice_dat` data set, and the `dice_dat` data set is derived from the `spells` data frame
- The `scholastic_pic` is a little more complicated because it depends on two data sets: there's the `scholastic_dat` data set that is derived directly from `spells`, but there's also the `scholastic_clus` object that is constructed from the `scholastic_dat` data set. 

If you want a little more detail, you can use `tar_manifest()` to get a summary of all the targets and the command with which they are associated:

```{r}
#| label: targets-spells-manifest
tar_manifest()
```

The output isn't as pretty as the HTML widget, but you can use it to see exactly what the target objects are, as well as the the command used to construct them. 

### Executing the pipeline

In the HTML widget above, all the targets are coloured in blue, indicating that these are "outdated". You can get a listing of all outdated targets like this:

```{r}
#| label: targets-spells-outdated
tar_outdated()
```

These are the targets that need to be (re)run. We can do this by calling `tar_make()`

```{r}
#| label: execute-spells
tar_make()
```

```{r}
#| label: spells-dir-tree-built
fs::dir_tree()
```

## Project 2: A toy blog

```{r}
set_knitr_dir("liteblog")
```

```{r}
#| label: targets-liteblog-destroy
#| echo: false
# complete clean, mimic fresh state
tar_destroy("all")
if (fs::dir_exists("_targets")) fs::dir_delete("_targets")
if (fs::dir_exists("site")) fs::dir_delete("site")
if (fs::file_exists("_liteblog.rds")) fs::file_delete("_liteblog.rds")
output_liteblog <- here::here("_site", "posts", "2025-01-02_using-targets", "site")
if (fs::dir_exists(output_liteblog)) fs::dir_delete(output_liteblog)
post_003 <- fs::path("source", "_003_schools-of-magic.rmd")
post_004 <- fs::path("source", "_004_spell-dice.rmd")
data_dir <- fs::path("source", "data")
if (fs::file_exists(post_003)) fs::file_delete(post_003)
if (fs::file_exists(post_004)) fs::file_delete(post_004)
if (fs::dir_exists(data_dir)) fs::dir_delete(data_dir)
```

```{r}
#| label: liteblog-filetree-fresh
fs::dir_tree()
```

```{r}
tar_visnetwork()
```

```{r}
#| label: liteblog-make
tar_make()
```

```{r}
tar_visnetwork()
```

Why are some targets still showing as outdated? Because that's how I set them up with `tar_cue()`. By design, every time we rebuild, the blog will check the file paths to see if there are any new static files to be copied or new posts to be fused. Consequently, those two targets and everything downstream of those show up as outdated. But notice that when we call `tar_make()` again...

```{r}
tar_make()
```

...only the `static_paths` and `post_paths` targets are rerun: nothing has actually changed in the `source` folder, so the fuse and copy targets are skipped.

```{r}
#| label: liteblog-filetree-built
fs::dir_tree()
```

Now suppose I were to add two new posts into the the `source` folder:

```{r}
#| label: add-liteblog-posts
#| echo: false
fs::file_copy(
  path = fs::path("..", "liteblog-source", "_003_schools-of-magic.rmd"),
  new_path = fs::path("source", "_003_schools-of-magic.rmd")
)
fs::file_copy(
  path = fs::path("..", "liteblog-source", "_004_spell-dice.rmd"),
  new_path = fs::path("source", "_004_spell-dice.rmd")
)
fs::dir_copy(
  path = fs::path("..", "liteblog-source", "data"),
  new_path = fs::path("source", "data")
)
fs::dir_tree("source")
```

```{r}
tar_visnetwork()
```

Looks the same. But this time when we call `tar_make()`, there will actually be some changes to the `source` folder, so the downstream targets run:

```{r}
tar_make()
```

This is clearly an inefficient design: whenever the source folder changes, all of the posts get rendered again. In an ideal world we wouldn't do this, and only those posts (or static files) that have been modified would get run again. However, it would take a little more effort than I'm willing to expend on this side-project to make this work properly: in order for it to behave the way we want it to, the build targets need to be able to inspect the internal contents of each blog post to determine which static files (or other posts!) are hidden dependencies. Discovering those dependencies reliably seems like hard work, so for the purposes of this toy project it just renders everything again.


You can browse the built website [here](./site/index.html)


```{r}
#| label: copy-liteblog-to-site
#| echo: false
fs::dir_copy(
  path = "site",
  new_path = here::here("_site", "posts", "2025-01-02_using-targets", "site"),
  overwrite = TRUE
)
```

## Project 3: Parallel computing

### Minimal version

```{r}
set_knitr_dir("threading1")
```

```{r}
#| echo: false
tar_destroy(destroy = "all", ask = FALSE)
```

Honestly this one isn't even pretending to be a real project. I just wanted to try out the parallel computing support with an example so minimal that it would be painfully obvious that it's working. There's not even an analysis script for this, it's just this one `_targets.R` file:

```{r}
#| label: threading1-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)
library(crew)

tar_option_set(controller = crew_controller_local(workers = 3))

list(
  tar_target(wait1, Sys.sleep(1)),
  tar_target(wait2, Sys.sleep(2)),
  tar_target(wait3, Sys.sleep(3)),
  tar_target(wait4, Sys.sleep(4))
)
```

There are four targets here, and all they do is pause execution. If these were run serially, you would expect this to take about 10 seconds to complete. But that's not what happens because I'm using the [crew](https://wlandau.github.io/crew/) package to split the processing across three workers. Here's what actually happens:

```{r}
tar_make()
```

### Slightly less minimal version

```{r}
set_knitr_dir("threading2")
```

```{r}
#| echo: false
tar_destroy(destroy = "all", ask = FALSE)
```

```{r}
#| label: threading2-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)
library(crew)

tar_option_set(controller = crew_controller_local(workers = 3))

sleeper <- function(duration, pipeline_start, name) {
  sleep_start <- Sys.time()
  Sys.sleep(duration)
  sleep_stop <- Sys.time()
  tibble::tibble(
    name           = name,
    pipeline_start = pipeline_start,
    worker_pid     = Sys.getpid(),
    begins_at      = difftime(sleep_start, pipeline_start),
    finishes_at    = difftime(sleep_stop, pipeline_start)
  )
}

startup <- function() {
  tibble::tibble(
    name = "start",
    pipeline_start = Sys.time(),
    worker_pid     = Sys.getpid(),
    begins_at      = as.difftime(0, units = "secs"),
    finishes_at    = difftime(Sys.time(), pipeline_start)
  )
}

collate <- function(...) {
  start <- Sys.time()
  na_difftime <- as.difftime(NA_real_, units = "secs")
  out <- rbind(...)
  pipeline_start <- out$pipeline_start[1]
  out$pipeline_start <- NULL
  out <- rbind(
    out,
    tibble::tibble(
      name         = "trace",
      worker_pid   = Sys.getpid(),
      begins_at    = difftime(start, pipeline_start),
      finishes_at  = difftime(Sys.time(), pipeline_start)
    )
  )
  out$duration    <- out$finishes_at - out$begins_at
  out$begins_at   <- round(as.numeric(out$begins_at), digits = 3)
  out$finishes_at <- round(as.numeric(out$finishes_at), digits = 3)
  out$duration    <- round(as.numeric(out$duration), digits = 3)
  out
}

list(
  tar_target(start, startup(), cue = tar_cue("always")),
  tar_target(wait1, sleeper(1, start$pipeline_start, "wait1")),
  tar_target(wait2, sleeper(2, start$pipeline_start, "wait2")),
  tar_target(wait3, sleeper(3, start$pipeline_start, "wait3")),
  tar_target(wait4, sleeper(4, start$pipeline_start, "wait4")),
  tar_target(trace, collate(start, wait1, wait2, wait3, wait4))
)
```

Essentially the same as the last project, but with some better tracking. Here's the network:

```{r}
tar_visnetwork(targets_only = TRUE)
```

Build:

```{r}
tar_make()
```

Details:

```{r}
trace <- tar_read(trace)
trace
```

```{r}
crew  <- tar_crew()
crew
```

```{r}
trace_sum <- trace |> 
  dplyr::select(worker_pid, duration) |> 
  dplyr::summarise(
    seconds = sum(duration), 
    targets = dplyr::n(), 
    .by = worker_pid
  )
trace_sum
```

```{r}
worker_lookup <- crew |> 
  dplyr::left_join(
    trace_sum, 
    by = dplyr::join_by(
      targets, 
      dplyr::closest(x$seconds > y$seconds)
    )
  ) |> 
  dplyr::select(worker, worker_pid)
worker_lookup
```

```{r}
trace |> dplyr::left_join(worker_lookup, by = "worker_pid")
```
