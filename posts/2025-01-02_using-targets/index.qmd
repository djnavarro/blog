---
title: "Three short stories about targets"
description: "In which our intrepid adventurer turns a hacky data visualisation exercise into an analysis pipeline; builds an R blog with litedown and targets; and tries to wrap her head around the crews integration for multithreaded target building"
date: "2025-01-02"
categories: ["R", "Reproducibility", "Parallel Computing"]
--- 

<!--------------- my typical setup ----------------->

```{r}
#| label: setup
#| include: false
very_wide <- 500
wide <- 136
narrow <- 76
options(width = narrow)
cache_images <- TRUE
set.seed(1)
```

<!--------------- post begins here ----------------->

About 18 months ago I wrote a post about [balrogs and makefiles](/posts/2023-06-30_makefiles/). The post was long, strange, but also cathartic. It had bothered me for years that I didn't really understand [make](https://www.gnu.org/software/make/) as well as I wanted to, and it was really helpful to write up some notes about it as a way of teaching myself how to use it more effectively than I had done in the past.^[In all fairness, this is the *primary* purpose of this blog. Yes, it makes me very happy that other people find my posts useful and/or enjoyable, but that is actually a secondary goal. I write these posts for myself, because the act of writing is also an act of learning for me.] Buried at the very end of the post, somewhat sheepishly, is a reference to the [targets](https://docs.ropensci.org/targets/) R package by Will Landau. Even then I knew that I was going to need to learn how to use targets, but... life gets in the way. I have been preoccupied by other tasks, sadly, and it has taken me until now to (a) sit down and read through the [targets user manual](https://books.ropensci.org/targets/), and (b) come up with some [fun side projects](https://github.com/djnavarro/tartoys) that would give me the opportunity to try it out. 

```{r}
#| label: package-load
#| message: false
library(targets)
library(ggplot2)
library(legendry)
library(tidyselect)
library(dplyr)
```

## Preliminaries

One key thing about targets is that it's a project oriented workflow, and not surprisingly it's easiest to use from *within* the project. By default, code in a quarto blog post like this one executes with the working directory set to the folder containing the qmd file, but to make my life easier I'll sometimes change that directory so that when I'm discussing a specific targets project, the code will execute from the root directory of that project. However, using `setwd()` to do this is not ideal: the proper way to do this within any knitr-based tool (quarto, rmarkdown, etc) is to set the `root.dir` knitr option. To that end, I'll define a little convenience function to take care of this whenever I need to switch to a new project:

```{r}
#| label: set-knitr-dir
post_dir <- here::here("posts", "2025-01-02_using-targets")
set_knitr_dir <- function(dir, base = post_dir) {
  knitr::opts_knit$set(root.dir = fs::path(base, dir))
}
```

Right then. Time to play around with targets...

## Project 1: Analysis pipelines

The first project is the fun [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/tree/main/data/2024/2024-12-17) project that I described in my [last post](/posts/2025-01-01_schools-of-magic/). In that post, I talked about two data visualisation I made using data about D&D spells. When I wrote that post, I didn't talk about targets at all, and none of the code presented in that blog post is written in a "targets-friendly" way. However, all though that form of the code was the simplest way to write it up, it's not how I originally wrote it. The actual code was written for targets (it's included as the `spells` folder in [this github repo](https://github.com/djnavarro/tartoys)). 

As it happens, I have a slightly-modified copy of the project in the `spells` directory within this blog post, so that I play with this project within this post, and see how targets operates. With that in mind, the very first thing I'll do is navigate into that project folder:

```{r}
#| label: switch-to-spells
set_knitr_dir("spells")
```

```{r}
#| label: targets-spells-destroy
#| echo: false
# complete clean, mimic fresh state
if (fs::dir_exists("_targets")) fs::dir_delete("_targets")
if (fs::dir_exists("output")) fs::dir_delete("output")
```

Next, let's take a look at the structure of this project. At the moment this is a clean project (i.e., no code has been run yet), so it contains only the source files. There are only three:

```{r}
#| label: spells-dir-tree-fresh
fs::dir_tree()
```

Here's what each file does:

- The `spells.csv` file is the data set I wish to analyse
- The `analysis.R` script defines a collection of functions which, when called, will perform the required analyses and generate the outputs
- The `_targets.R` script is (unsurprisingly) the build script. It's the targets analog of a makefile, more or less

Let's unpack this a little further. If you read the [schools of magic](/posts/2025-01-01_schools-of-magic/) post, you've already seen the code underpinning the analysis. The only difference between the version in that post and the version I've used here is that the `analysis.R` script wraps each step of the analysis into a function. For example, the pipeline that constructs the "spell dice" plot uses these three functions:

- `set_output_dir()` returns the path to the output folder to which the images will be written, creating that output folder if it doesn't already exist
- `dice_data()` takes the spells data as input, and performs all the data wrangling steps required to construct a tidied version of the data that is suitable for visualisation
- `dice_plot()` takes the tidied dice data as input, specifies the "spell dice" plot, and writes it to an output file

To produce the "schools of magic" plot I also need these three functions:

- `scholastic_data()` takes the spells data as input, and performs the data wrangling steps required to create tidy data suitable for constructing the heatmap
- `scholastic_clusters()` takes this tidy data as input, and the performs additional steps required to construct the hierarchical clustering used to draw dendrograms alongside the heatmap
- `scholastic_plot()` takes the data set and the clustering as input, and uses them to build the "schools of magic" plot that is written to an output file

The actual code for these functions isn't very important, but for what it's worth the code is included below the fold here:

```{r}
#| label: spells-analysis
#| eval: true
#| code-line-numbers: true
#| code-fold: true
#| code-summary: "Click to show/hide the analysis.R code"
#| filename: "analysis.R"
# helper ------------------------------------------------------------------

set_output_dir <- function() {
  root <- find_root(has_file("_targets.R"))
  output <- path(root, "output")
  dir_create(output)
  return(output)
}

# spell dice plot ---------------------------------------------------------

dice_data <- function(spells) {
  dice_dat <- spells |>
    select(name, level, description) |>
    mutate(
      dice_txt = str_extract_all(description, "\\b\\d+d\\d+\\b"),
      dice_txt = purrr::map(dice_txt, unique)
    ) |>
    unnest_longer(
      col = "dice_txt",
      values_to = "dice_txt",
      indices_to = "position"
    ) |>
    mutate(
      dice_num = dice_txt |> str_extract("\\d+(?=d)") |> as.numeric(),
      dice_die = dice_txt |> str_extract("(?<=d)\\d+") |> as.numeric(),
      dice_val = dice_num * (dice_die + 1)/2,
      dice_txt = factor(dice_txt) |> fct_reorder(dice_val)
    )
  return(dice_dat)
}

dice_plot <- function(dice_dat, output) {
  
  palette <- hcl.colors(n = 10, palette = "PuOr")
  
  labs <- dice_dat |>
    summarise(
      dice_txt = first(dice_txt),
      count = n(),
      .by = dice_txt
    )
  
  pic <- ggplot(
    data = dice_dat,
    mapping = aes(
      x = dice_txt,
      fill = factor(level)
    )
  ) +
    geom_bar(color = "#222") +
    geom_label_repel(
      data = labs,
      mapping = aes(
        x = dice_txt,
        y = count,
        label = dice_txt
      ),
      size = 3,
      direction = "y",
      seed = 1,
      nudge_y = 4,
      color = "#ccc",
      fill = "#222",
      arrow = NULL,
      inherit.aes = FALSE
    ) +
    scale_fill_manual(
      name = "Spell level",
      values = palette
    ) +
    scale_x_discrete(
      name = "Increasing average outcome \u27a1",
      breaks = NULL,
      expand = expansion(.05)
    ) +
    scale_y_continuous(name = NULL) +
    labs(title = "Dice rolls described in D&D spell descriptions") +
    theme_void() +
    theme(
      plot.background = element_rect(fill = "#222"),
      text = element_text(color = "#ccc"),
      axis.text = element_text(color = "#ccc"),
      axis.title = element_text(color = "#ccc"),
      plot.margin = unit(c(1, 1, 1, 1), units = "cm"),
      legend.position = "inside",
      legend.position.inside = c(.3, .825),
      legend.direction = "horizontal",
      legend.title.position = "top",
      legend.byrow = TRUE
    )
  
  out <- path(output, "dice_pic.png")
  
  ggsave(
    filename = out,
    plot = pic,
    width = 2000,
    height = 1000,
    units = "px",
    dpi = 150
  )
  
  return(out)
}


# schools of magic plot ---------------------------------------------------

# constructs the data frame used by geom_tile() later
scholastic_data <- function(spells) {
  spells |>
    select(name, school, bard:wizard) |>
    pivot_longer(
      cols = bard:wizard,
      names_to = "class",
      values_to = "castable"
    ) |>
    summarise(
      count = sum(castable),
      .by = c("school", "class")
    ) |>
    mutate(
      school = str_to_title(school),
      class  = str_to_title(class)
    )
}

# hierarchical clustering for the schools and classes
scholastic_clusters <- function(dat) {
  
  # matrix of counts for each school/class combination
  mat <- dat |>
    pivot_wider(
      names_from = "school",
      values_from = "count"
    ) |>
    as.data.frame()
  rownames(mat) <- mat$class
  mat$class <- NULL
  as.matrix(mat)
  
  # each school is a distribution over classes,
  # each class is a distribution over schools
  class_distribution  <- mat / replicate(ncol(mat), rowSums(mat))
  school_distribution <- t(mat) / (replicate(nrow(mat), colSums(mat)))
  
  # pairwise distances
  class_dissimilarity  <- dist(class_distribution)
  school_dissimilarity <- dist(school_distribution)
  
  # hierarchical clustering
  clusters <- list(
    class = hclust(class_dissimilarity, method = "average"),
    school = hclust(school_dissimilarity, method = "average")
  )
  
  return(clusters)
}

scholastic_plot <- function(dat, clusters, output) {
  
  pic <- ggplot(dat, aes(school, class, fill = count)) +
    geom_tile() +
    scale_x_dendro(
      clust = clusters$school,
      guide = guide_axis_dendro(n.dodge = 2),
      expand = expansion(0, 0),
      position = "top"
    ) +
    scale_y_dendro(
      clust = clusters$class,
      expand = expansion(0, 0)
    ) +
    scale_fill_distiller(palette = "RdPu") +
    labs(
      x = "The Schools of Magic",
      y = "The Classes of Character",
      fill = "Number of Learnable Spells"
    ) +
    coord_equal() +
    theme(
      plot.background = element_rect(fill = "#222", color = "#222"),
      plot.margin = unit(c(2, 2, 2, 2), units = "cm"),
      text = element_text(color = "#ccc"),
      axis.text = element_text(color = "#ccc"),
      axis.title = element_text(color = "#ccc"),
      axis.ticks = element_line(color = "#ccc"),
      legend.position = "bottom",
      legend.background = element_rect(fill = "#222", color = "#222")
    )
  
  out <- path(output, "scholastic_pic.png")
  
  ggsave(
    filename = out,
    plot = pic,
    width = 1000,
    height = 1000,
    units = "px",
    dpi = 150
  )
  
  return(out)
}
```

### Defining the pipeline

Now that you've read the verbal description of what each function does, it's intuitively pretty clear how the analysis pipeline is supposed to work. Roughly speaking, you'd expect the analysis to be executed using a script like this:

```{r}
#| label: spells-without-targets
#| eval: false
#| code-line-numbers: true
#| filename: "run_analysis.R"
# packages
library(rprojroot)
library(fs) 
library(tibble) 
library(readr) 
library(ggplot2)
library(dplyr)
library(stringr)
library(tidyr)
library(forcats)
library(ggrepel)
library(legendry)

# setup
output <- set_output_dir()
input  <- "spells.csv"
spells <- read_csv(input, show_col_types = FALSE)

# make the spell dice plot & write to output
dice_dat <- dice_data(spells)
dice_pic <- dice_plot(dice_dat, output)

# make the schools of magic plot & write to output
scholastic_dat  <- scholastic_data(spells)
scholastic_clus <- scholastic_clusters(scholastic_dat)
scholastic_pic  <- scholastic_plot(scholastic_dat, scholastic_clus, output)
```

So, how does it work with targets? Well, if we take a look at the `_targets.R` script, we can see it looks suspiciously similar to the code above:

```{r}
#| label: spells-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)

tar_option_set(packages = c(
  "rprojroot", "fs", "tibble", "readr", "ggplot2", "dplyr",
  "stringr", "tidyr", "forcats", "ggrepel", "legendry"
))

tar_source("analysis.R")

list(
  # preprocessing targets
  tar_target(output, set_output_dir()),
  tar_target(input, "spells.csv", format = "file"),
  tar_target(spells, read_csv(input, show_col_types = FALSE)),

  # dice plot targets
  tar_target(dice_dat, dice_data(spells)),
  tar_target(dice_pic, dice_plot(dice_dat, output)),

  # scholastic plot targets
  tar_target(scholastic_dat, scholastic_data(spells)),
  tar_target(scholastic_clus, scholastic_clusters(scholastic_dat)),
  tar_target(
    scholastic_pic,
    scholastic_plot(scholastic_dat, scholastic_clus, output)
  )
)
```

In this pipeline, the set up involves two steps:

- I've used [`tar_option_set()`](https://docs.ropensci.org/targets/reference/tar_option_set.html) to declare the required R packages, thereby making those packages available to the pipeline
- I've used [`tar_source()`](https://docs.ropensci.org/targets/reference/tar_source.html) to read the analysis script, thereby making the functions in that file accessible to the pipeline

Having taken care of the preliminaries, the pipeline is specified via a list of targets, each of which is defined by a call to [`tar_target()`](https://docs.ropensci.org/targets/reference/tar_target.html). Each target has a `name`, and is associated with a `command` that is to be executed. If I'd named the arguments on line 12 in my `_targets.R` script, the code would look like this:

```{r}
#| label: spells-targets-argument-names
#| eval: false
tar_target(
  name = output, 
  command = set_output_dir()
)
```

Notice the similarity to line 15 of the fictitious `run_analysis.R` script:

```{r}
#| label: spells-set-output-dir
#| eval: false
output <- set_output_dir()
```

In essence, that's what I'm doing with the call to `tar_target()`. I'm specifying that the command `set_output_dir()` is to be executed, and the results should be stored as the variable `output`. However, instead of immediately executing this code in the current R environment, what `tar_target()` does is create the infrastructure so that this command can be incorporated into the pipeline when it actually gets built. With one exception, this is the recipe I followed for constructing all the targeta in my `_targets.R` script.

The one exception to this pattern occurs on line 13, where my target is defined by this call to `tar_target()`:

```{r}
#| label: spells-targets-for-files
#| eval: false
tar_target(
  name = input, 
  command = "spells.csv", 
  format = "file"
)
```

In one sense, this target is pretty much the same as the others: it defines a variable called `input` using the "command" `"spells.csv"`, and so in that respect it's much the same as line 16 of the `run_analysis.R` script:

```{r}
#| eval: false
input <- "spells.csv"
```

However, notice that I've also specified that `format = "file"`. This tells targets that `"spells.csv"` isn't *just* a string, it's also the name of a file that needs to be tracked. By declaring it as a file target, I'm ensuring that if the `spells.csv` file gets altered in some way, this target and any target that depends on it will need to be rebuilt.  

### Inspecting the pipeline

The [`tar_visnetwork()`](https://docs.ropensci.org/targets/reference/tar_visnetwork.html) function provides a handy way of visualising the structure of a pipeline as a little HTML widget.

Anyway, here it is:

```{r}
#| label: targets-spells-visnetwork
tar_visnetwork()
```

This makes the structure of the project clear:

- The `dice_pic` image depends only on the `dice_dat` data set, and the `dice_dat` data set is derived from the `spells` data frame
- The `scholastic_pic` is a little more complicated because it depends on two data sets: there's the `scholastic_dat` data set that is derived directly from `spells`, but there's also the `scholastic_clus` object that is constructed from the `scholastic_dat` data set. 

If you want a little more detail, you can use `tar_manifest()` to get a summary of all the targets and the command with which they are associated:

```{r}
#| label: targets-spells-manifest
tar_manifest()
```

The output isn't as pretty as the HTML widget, but you can use it to see exactly what the target objects are, as well as the the command used to construct them. 

### Executing the pipeline

In the HTML widget above, all the targets are coloured in blue, indicating that these are "outdated". You can get a listing of all outdated targets like this:

```{r}
#| label: targets-spells-outdated
tar_outdated()
```

These are the targets that need to be (re)run. We can do this by calling `tar_make()`

```{r}
#| label: execute-spells
tar_make()
```

```{r}
#| label: spells-dir-tree-built
fs::dir_tree()
```

### Postmortem

For a first attempt at using targets, I'm not unhappy with this. It did what I needed it to do, and I was able to understand the basic structure of the package.

But there are some limitations. One thing that really bothers me is the way I handled the ggplot code. My thinking at the time was based on the thinking that real life analysis pipelines often have some *very* slow ggplot2 code, but the slow part is not the construction of the object itself, but rather the build, render, and draw stages. It's not much of an issue in this specific example because everything is runs fast, but you can see why I had this worry by looking at what `benchplot()` has to say about the plots I created here:

```{r}
tar_load(c(scholastic_dat, scholastic_clus))

benchplot(
  ggplot(scholastic_dat, aes(school, class, fill = count)) +
    geom_tile() +
    scale_x_dendro(
      clust = scholastic_clus$school,
      guide = guide_axis_dendro(n.dodge = 2),
      expand = expansion(0, 0),
      position = "top"
    ) +
    scale_y_dendro(
      clust = scholastic_clus$class,
      expand = expansion(0, 0)
    ) +
    coord_equal()
)
```

Even in this example, where there isn't very much that needs to be drawn to the graphics device, constructing the plot isn't the step that takes the most time. So it makes very little sense to treat the plot specification (i.e., the `gg` plot object) as the terminal target of a plotting pipeline, because 90% of the compute time takes place *after* the `gg` object is specified (this is even more obvious when you have a scatterplot that needs to draw millions of dots to the canvas).

Having been burned by this in the past, I made the decision that my plotting target would encapsulate *all* stages in the plot rendering process. Only once the image has been written to a file would my plot target be deemed complete. As far as it goes, this is very sensible reasoning, but in retrospect I think it might have made a lot more sense to split the plotting target into stages. Saving the `gg` object as an intermediate target (and possibly other stages of the plot construction too) might have been sensible. It might seem like I'm being weirdly nitpicky, but my motivation here is very practical: I have a couple of projects at work where rerunning the analysis very time-consuming, and the biggest bottleneck (by far) is rendering some very unpleasant `gg` objects. I've been using a variety of tricks to work around this issue, none of which have been satisfying. Targets offers a much cleaner solution to my problem, but it's clear to me just from this toy exercise that I will need to be careful about how I set up targets for these analysis pipelines.

Still, it seems very clear to me that the problem can be solved with targets. My toy example is a kind of worst case solution... if all else fails, I can define the output image itself as the the target to be built. From that perspective, mission accomplished babes. 

## Project 2: Building a blog

```{r}
#| label: switch-to-liteblog
set_knitr_dir("liteblog")
```

```{r}
#| label: targets-liteblog-destroy
#| echo: false
# complete clean, mimic fresh state
tar_destroy("all")
if (fs::dir_exists("_targets")) fs::dir_delete("_targets")
if (fs::dir_exists("site")) fs::dir_delete("site")
if (fs::file_exists("_liteblog.rds")) fs::file_delete("_liteblog.rds")
output_liteblog <- here::here("_site", "posts", "2025-01-02_using-targets", "site")
if (fs::dir_exists(output_liteblog)) fs::dir_delete(output_liteblog)
post_003 <- fs::path("source", "_003_schools-of-magic.rmd")
post_004 <- fs::path("source", "_004_spell-dice.rmd")
data_dir <- fs::path("source", "data")
if (fs::file_exists(post_003)) fs::file_delete(post_003)
if (fs::file_exists(post_004)) fs::file_delete(post_004)
if (fs::dir_exists(data_dir)) fs::dir_delete(data_dir)
```

The second project I attempted was a little more ambitious: I decided to write a lightweight blogging tool using [litedown](https://yihui.org/litedown/) to convert R markdown documents to HTML pages, and using targets to manage the build process for the site as a whole.^[I seem to have developed a habit for rolling my own half-arsed blogging tools. About a year ago I got bored and built a very silly [blog based on knitr and eleventy](https://knitr-11ty.djnavarro.net/), for literally no reason except that I was bored. The [liteblog](https://liteblog.djnavarro.net/) site I put together for this side project is no less stupid.] 

### Designing the blog

To give you a sense of how the blog is organised, this is what a "clean" version of the project looks like:

```{r}
#| label: liteblog-filetree-fresh
fs::dir_tree()
```

Inside the `source` folder there are four R markdown documents that will need to be built. Of these four, two should be built into the site root directory:

- the `index.rmd` document becomes the blog homepage `index.html`
- the `404.rmd` document becomes the 404 page `404.html`

The other two documents both correspond to blog posts, and in both cases the underscore is intended to have syntactic meaning. However, unlike the "usual" behaviour that you might expect (e.g., from R markdown, blogdown, quarto, etc), the underscore doesn't mean "do not build".^[If this were a serious project, this would be a very bad idea: the underscore has an accepted meaning in the various R-based literate programming tools, and it is not good practice to violate user expectations the way I'm doing here. However, this is a silly side project that is not intended to be used for anything, so I feel no compunction at all to follow the usual conventions.] Instead, it is a signifier used to describe the file path expected for the built file:

- `_001_hello-cruel-world.rmd` becomes a post at `/001/hello-cruel-world/index.html`
- `_002_blog-object.rmd` becomes a post at `/002/blog-object/index.html`

The other five files in the project folder define the structure and visual appearance of the blog itself. Three of these are grossly typical for any blogging tool:

- `_liteblog-header.html` and `_liteblog-footer.html` are HTML document fragments that will be inserted above and below the content that is specified within the R markdown files
- `_liteblog.css`, unsurprisingly, provides the styling

The final two files are `_liteblog.R` and `_targets.R`. These do all the heavy lifting:

- `_liteblog.R` supplies an [R6](https://r6.r-lib.org/) class called `Liteblog`. The intended usage is that any specific blog is an instance of the class, such that user-specified options (e.g., the name of the site folder, the url for the blog, etc) are supplied as arguments, and the resulting blog object provides all the core functionality (e.g., building pages to the correct folder) for the blog. 
- `_targets.R` provides the build tools: monitoring the source directory, building outdated pages, etc. 

Because the `_liteblog.R` and `_targets.R` scripts are intended to work together, I'll discuss both of them. First, here's what the `_liteblog.R` script looks like:

```{r}
#| label: liteblog-object-file
#| eval: true
#| code-line-numbers: true
#| filename: "_liteblog.R"
Liteblog <- R6::R6Class(
  classname = "Liteblog",
  public = list(

    initialize = function(root, source, output, url) {
      self$root <- root
      self$source <- source
      self$output <- output
      self$url <- url
    },

    root = NULL,
    source = NULL,
    output = NULL,
    url = NULL,
    pattern = "[.][rR]?md$",

    find_posts = function() {
      files <- fs::dir_ls(
        path = fs::path(self$root, self$source),
        recurse = TRUE,
        regexp = self$pattern,
        type = "file"
      )
      unname(unclass(files))
    },

    find_static = function() {
      files <- fs::dir_ls(
        path = fs::path(self$root, self$source),
        recurse = TRUE,
        regexp = self$pattern,
        invert = TRUE,
        all = TRUE,
        type = "file"
      )
      unname(unclass(files))
    },

    fuse_post = function(file, ...) {
      output_path <- litedown::fuse(file)
      output_file <- fs::path_file(output_path)
      if (stringr::str_detect(output_file, "^_")) {
        destination <- output_file |>
          stringr::str_replace_all("_", "/") |>
          stringr::str_replace("\\.html$", "/index.html") |>
          stringr::str_replace("^", paste0(self$output, "/"))
      } else {
        destination <- paste0(self$output, "/", output_file)
      }
      destination <- fs::path(self$root, destination)
      fs::dir_create(fs::path_dir(destination))
      fs::file_move(output_path, destination)
    },

    copy_static = function(file) {
      destination <- file |>
        stringr::str_replace(
          pattern = paste0("/", self$source, "/"),
          replacement = paste0("/", self$output, "/")
        )
      fs::dir_create(fs::path_dir(destination))
      fs::file_copy(
        path = file,
        new_path = destination,
        overwrite = TRUE
      )
    }

  )
)
```

It's not a very complicated R6 class as these things go, and to be honest it could use a lot more love than I've given it. But it works for my purposes so I'll run with it. To initialise a blog I call `Liteblog$new()`, indicating that the current directory is the project root, the `source` folder is the `"source"` folder, the `output` folder is called `"site"`, and the `url` for the blog is `"liteblog.djnavarro.net"`:

```{r}
blog <- Liteblog$new(
  root = fs::path_abs("."),
  source = "source",
  output = "site",
  url = "liteblog.djnavarro.net"
)
```

Looking at the code above, you can see that there are four methods:

- `$find_posts()` looks in the source folder and detects the R markdown documents
- `$find_static()` looks in the source folder and detects other files
- `$fuse_post()` takes the path to one R markdown document as input, and will build it (called "fusing" in the litedown nomenclature) to an HTML file at the appropriate location
- `$copy_static()` takes the path to a static file as input, and copies it to the output folder

This provides the core blogging toolkit that I'll now use when writing my target pipeline. It's *extremely* bare bones, and not very customisable, but of course my intention here isn't to build a proper blogging platform. It's just a toy that I can play with when building a more complex build pipeline. Speaking of which...

### The build pipeline

So, without further tedious exposition, here's the `_targets.R` file:

```{r}
#| label: liteblog-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)
tar_source("_liteblog.R")

list(

  # define blog configuration
  tar_target(
    name = blog,
    command = Liteblog$new(
      root = rprojroot::find_root(
        rprojroot::has_file("_liteblog.R")
      ),
      source = "source",
      output = "site",
      url = "liteblog.djnavarro.net"
    )
  ),

  # track configuration files
  tar_target(
    name = blog_rds, 
    command = saveRDS(blog, file = "_liteblog.rds"), 
    format = "file"
  ),
  tar_target(blog_css, "_liteblog.css", format = "file"),
  tar_target(blog_hdr, "_liteblog-header.html", format = "file"),
  tar_target(blog_ftr, "_liteblog-footer.html", format = "file"),

  # detect file paths (always run)
  tar_target(
    name = post_paths, 
    command = blog$find_posts(), 
    cue = tar_cue("always")
  ),
  tar_target(
    name = static_paths, 
    command = blog$find_static(), 
    cue = tar_cue("always")
  ),

  # specify file targets
  tar_target(
    name = post_files, 
    command = post_paths, 
    pattern = map(post_paths), 
    format = "file"
  ),
  tar_target(
    name = static_files, 
    command = static_paths, 
    pattern = map(static_paths), 
    format = "file"
  ),

  # fuse targets depend on blog configuration files
  # copy targets don't need dependencies
  tar_target(
    name = post_fuse,
    command = blog$fuse_post(
      post_files,
      post_paths,
      blog_css,
      blog_hdr,
      blog_ftr
    ),
    pattern = map(post_files)
  ),
  tar_target(
    name = static_copy, 
    command = blog$copy_static(static_files), 
    pattern = map(static_files)
  )
)
```

This is a bit more elaborate than the pipeline I built in my data visualisation process, and uses some fancier tools to work. To get an overall sense of how the pipeline works, I'll call `tar_visnetwork()` again. First, here's a version that only shows the "core" of the build pipeline:

```{r}
#| label: liteblog-visnetwork-1
tar_visnetwork(
  allow = any_of(c(
    "static_paths", "post_paths", "post_files", 
    "static_files", "post_fuse", "static_copy"
  ))
)
```

Reduced to this core, the build process is actually pretty simple. There are two separate pipelines. To build the blog posts, we use targets to:

1. Search the source folder to find all the R markdown posts. This is a *single* build target, it consists of a character vector `post_paths` that lists the paths to the various R markdown files. If a post is deleted, or a new R markdown file is added, this vector will change.
2. Next, we create a target called `post_files` that keeps track of the R markdown files themselves. Or, to be more precise, we create a *pattern* that defines a collection of targets, one per file. If any of those files change, the target will be outdated. I'll talk more about patterns later.
3. Finally, we have a `post_fuse` target which is used primarily for the side-effect that occurs when the target is built. Specifically, this is the target that renders the blog posts to HTML files, and then copies them over to the blog output folder. 

The second pipeline is analogous, but it operates on everything *other* than the markdown files:

1. The `static_paths` target tracks the paths to the files
2. The `static_files` target tracks the content of the files
3. The `static_copy` target copies the static files to the output folder

Here's the full network:

```{r}
#| label: liteblog-visnetwork-2
tar_visnetwork()
```

In this version, you can see that most of the build pipeline depends on the `blog` object: that makes sense because this is the R6 object that defines methods such as `$find_posts()` and `$fuse_post()` that are called when the blog is built. Similarly, since `blog` is itself an instance of the `Liteblog` class, the class itself is an upstream dependency of `blog`.

The other four targets are less interesting. Three of them (`blog_css`, `blog_hdr`, and `blog_ftr`) are used to keep track of the style files (i.e. `_liteblog.css`, `_liteblog-header.html`, and `_liteblog-footer.html`), so that if any of those files are modified the it will trigger a rebuild of any target that uses those files. The fourth one, `blog_rds`, is a bit of an odd one and I'm not sure I like it: basically it's just a serialised version of the `blog` object itself written to an rds file. The only reason that one is there is to make `blog` visible *within* an R markdown post (i.e., a post can read the `_liteblog.rds` file and call any of the relevant methods if necessary). In general you wouldn't need that, but it's useful if you want to construct a listing page. 
 
### Building the blog

Having now walked through the structure of the pipeline, it's time to call `tar_make()` and build the blog:

```{r}
#| label: liteblog-make
tar_make()
```

```{r}
#| label: liteblog-visnetwork-3
tar_visnetwork()
```

Why are some targets still showing as outdated? Because that's how I set them up with `tar_cue()`. By design, every time we rebuild, the blog will check the file paths to see if there are any new static files to be copied or new posts to be fused. Consequently, those two targets and everything downstream of those show up as outdated. But notice that when we call `tar_make()` again...

```{r}
#| label: liteblog-make-1
tar_make()
```

...only the `static_paths` and `post_paths` targets are rerun: nothing has actually changed in the `source` folder, so the fuse and copy targets are skipped.

```{r}
#| label: liteblog-filetree-built
fs::dir_tree()
```

Now suppose I were to add two new posts into the the `source` folder:

```{r}
#| label: add-liteblog-posts
#| echo: false
fs::file_copy(
  path = fs::path("..", "liteblog-source", "_003_schools-of-magic.rmd"),
  new_path = fs::path("source", "_003_schools-of-magic.rmd")
)
fs::file_copy(
  path = fs::path("..", "liteblog-source", "_004_spell-dice.rmd"),
  new_path = fs::path("source", "_004_spell-dice.rmd")
)
fs::dir_copy(
  path = fs::path("..", "liteblog-source", "data"),
  new_path = fs::path("source", "data")
)
fs::dir_tree("source")
```

```{r}
#| label: liteblog-visnetwork-4
tar_visnetwork()
```

Looks the same. But this time when we call `tar_make()`, there will actually be some changes to the `source` folder, so the downstream targets run:

```{r}
#| label: liteblog-make-2
tar_make()
```

This is clearly an inefficient design: whenever the source folder changes, all of the posts get rendered again. In an ideal world we wouldn't do this, and only those posts (or static files) that have been modified would get run again. However, it would take a little more effort than I'm willing to expend on this side-project to make this work properly: in order for it to behave the way we want it to, the build targets need to be able to inspect the internal contents of each blog post to determine which static files (or other posts!) are hidden dependencies. Discovering those dependencies reliably seems like hard work, so for the purposes of this toy project it just renders everything again.

You can browse the built website [here](./site/index.html)


```{r}
#| label: copy-liteblog-to-site
#| echo: false
fs::dir_copy(
  path = "site",
  new_path = here::here("_site", "posts", "2025-01-02_using-targets", "site"),
  overwrite = TRUE
)
```

### Postmortem


## Project 3: Parallel computing

For my third foray into targets, I wanted to take a look at how a targets pipeline can be distributed across multiple parallel threads. Happily, parallel computing is supported out of the box in targets, using the [crew](https://wlandau.github.io/crew/) package to distribute the targets across multiple workers. There are a few slightly different variations on how I looked at this, and I'll go through them each in turn. 

### Minimal version

```{r}
#| label: switch-to-threading-1
set_knitr_dir("threading1")
```

```{r}
#| label: threading-1-mimic-clean-state
#| echo: false
tar_destroy(destroy = "all", ask = FALSE)
```

As a very minimal implementation, consider this pipeline:

```{r}
#| label: threading1-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)
library(crew)

tar_option_set(controller = crew_controller_local(workers = 3))

list(
  tar_target(wait1, Sys.sleep(1)),
  tar_target(wait2, Sys.sleep(2)),
  tar_target(wait3, Sys.sleep(3)),
  tar_target(wait4, Sys.sleep(4))
)
```

There are four targets here, and all they do is pause execution. If these were run serially, you would expect this to take about 10 seconds to complete. But that's not what happens because I'm using `crew_controller_local()` to define a controller that will split the processing across three parallel workers. Here's what actually happens:

```{r}
#| label: threading-1-make
tar_make()
```

Upon starting the job, the first three targets (`wait1`, `wait2`, and `wait3`) are dispatched to the three workers and they all start running concurrently. The fourth job (`wait4`) is placed on hold, and doesn't start until the first of the three jobs finishes (`wait1`). Only then does the fourth job start. As the remaining jobs complete, the user is notified, and once they are all finalised targets reports that the pipeline is complete. You can get a high-level summary of the allocation of tasks across workers by calling `tar_crew()`:

```{r}
#| label: threading-1-crew
tar_crew()
```

As expected based on the intuitive description above, this output confirms that there's one worker process that handled two of the targets, and one each handled by the other two. 

### Slightly less minimal version

```{r}
#| label: switch-to-threading-2
set_knitr_dir("threading2")
```

```{r}
#| label: mimic-clean-state-threading-2
#| echo: false
tar_destroy(destroy = "all", ask = FALSE)
```

The previous example gives a general sense of how parallel execution works in a crew/targets pipeline, but -- possibly because once upon a time I used [callr](https://callr.r-lib.org/) to write my own R6 implementation of a multi-threaded [queue](/posts/2022-12-22_queue/),^[I hasten to add that nobody should be using the [queue](https://queue.djnavarro.net/) package. I wrote it to prove to myself that I could do it, but it's little more than a toy.] which piqued my curiosity about how these things play out -- I found myself wanting a finer-grained description of what each of the workers is doing at each point in time. 

As far as I can tell, neither targets nor crew provides an easy way to do this (but if they do, I would love it if someone can point me in the right direction), so I wrote a slightly more elaborate version of the previous pipeline, in which the targets themselves keep track of the time at which execution starts and stops, as well as the pid of the R process in which they are being executed. In this version, there are three functions that do the work:

- `startup()` is called to build the first target (`start`), and its primary job is to capture the system time at which execution of the first target begins
- `sleeper()` is used to build the `wait1`, `wait2`, `wait3`, and `wait4` targets. These targets are analogous to the wait targets in the original version, but they also capture information about when execution of these targets started and stopped
- `collate()` is called at the very end, and is used to construct the `trace` target. This target aggregates all the information from the other targets

Here's the entire code:

```{r}
#| label: threading2-targets-file
#| eval: false
#| code-line-numbers: true
#| filename: "_targets.R"
library(targets)
library(crew)

tar_option_set(controller = crew_controller_local(workers = 3))

sleeper <- function(duration, pipeline_start, name) {
  sleep_start <- Sys.time()
  Sys.sleep(duration)
  sleep_stop <- Sys.time()
  tibble::tibble(
    name           = name,
    pipeline_start = pipeline_start,
    worker_pid     = Sys.getpid(),
    begins_at      = difftime(sleep_start, pipeline_start),
    finishes_at    = difftime(sleep_stop, pipeline_start)
  )
}

startup <- function() {
  tibble::tibble(
    name = "start",
    pipeline_start = Sys.time(),
    worker_pid     = Sys.getpid(),
    begins_at      = as.difftime(0, units = "secs"),
    finishes_at    = difftime(Sys.time(), pipeline_start)
  )
}

collate <- function(...) {
  start <- Sys.time()
  na_difftime <- as.difftime(NA_real_, units = "secs")
  out <- rbind(...)
  pipeline_start <- out$pipeline_start[1]
  out$pipeline_start <- NULL
  out <- rbind(
    out,
    tibble::tibble(
      name         = "trace",
      worker_pid   = Sys.getpid(),
      begins_at    = difftime(start, pipeline_start),
      finishes_at  = difftime(Sys.time(), pipeline_start)
    )
  )
  out$duration    <- out$finishes_at - out$begins_at
  out$begins_at   <- round(as.numeric(out$begins_at), digits = 3)
  out$finishes_at <- round(as.numeric(out$finishes_at), digits = 3)
  out$duration    <- round(as.numeric(out$duration), digits = 3)
  out
}

list(
  tar_target(start, startup(), cue = tar_cue("always")),
  tar_target(wait1, sleeper(1, start$pipeline_start, "wait1")),
  tar_target(wait2, sleeper(2, start$pipeline_start, "wait2")),
  tar_target(wait3, sleeper(3, start$pipeline_start, "wait3")),
  tar_target(wait4, sleeper(4, start$pipeline_start, "wait4")),
  tar_target(trace, collate(start, wait1, wait2, wait3, wait4))
)
```

To give you a sense of the dependencies, this is what the targets network looks like for this version of the pipeline. All four `wait` targets depend on the `start` target, and the `trace` target depends on everything:

```{r}
#| label: threading-2-visnetwork
tar_visnetwork(targets_only = TRUE)
```

In hindsight, I realised that this could have been done more efficiently, but efficiency is not the primary goal here. I just want a pipeline in which all the targets report some detailed information about their execution. So let's run it and see what we get:

```{r}
#| label: threading-2-make
tar_make()
```

Okay yes this makes sense. Every other target is dependent on `start`, and by design this target is *always* treated as outdated. So when the pipeline begins, the first thing that happens is that the `start` target is dispatched to a worker; the other two workers do nothing. Once `start` completes, all four wait targets are eligible for dispatch, but we only have three workers and so `wait1`, `wait2`, and `wait4` are farmed out to workers while `wait4` remains in the queue. When `wait1` completes, one of the workers is freed up, thereby allowing `wait4` to be dispatched. Once all four of the wait targets are completed, the final `trace` target is built.

If we call `tar_crew()`, we get the same high-level overview that we got last time:

```{r}
#| label: threading-2-crew
crew  <- tar_crew()
crew
```

However, this time around we also have access to the `trace` target that provides a more detailed summary of which R process excecuted each target, and at what time that execution started and stopped:

```{r}
#| label: threading-2-trace
trace <- tar_read(trace)
trace
```

In this data frame, the `begins_at` column records the amount of time that has passed between the time at which the pipeline started, and the time at which the current target begins execution. Similarly `finishes_at` records the time from pipeline start to the current target finishing. The `duration` column is the difference between the two. That being said, if you look at the code I used to calculate these, it's clearly an approximation. But it will suffice. 

The nice thing about the `trace` data is that the `worker_pid` column associates each target with a specific pid for the R process used to build that target. This is slightly finer-grained information than what we got by calling `tar_crew()`. For example, I now know that `start` and `wait1` were both executed by the same R process (i.e., pid `r trace$worker_pid[1]`). Admittedly this is not much of a revelation: I could have guessed that just by looking at the logs when I called `tar_make()` and a few reasonable assumptions about how the scheduler^[Scheduling in `crew` is handled with the [mirai](https://shikokuchuo.net/mirai/) package, which looks amazing and is now on my to-do list to learn] works. 

One minor irritation I have with the `trace` output is that it doesn't directly allow me to match the `worker_pid` column against the `worker` column produced by `tar_crew()`. You can see the issue most cleanly if I aggregate the `trace` data frame by `worker_pid` like so:

```{r}
#| label: threading-2-trace-sum
trace_sum <- trace |> 
  select(worker_pid, duration) |> 
  summarise(
    seconds = sum(duration), 
    targets = n(), 
    .by = worker_pid
  )
trace_sum
```

Looking at this it's immediately apparent that worker 1 must corresponded to the R process with pid `r trace_sum$worker_pid[1]`, since `trace` and `crew` both agree that this is the only worker that executed four distinct targets. However, workers 2 and 3 both executed a single target, so we'll have to resolve the ambiguity by looking at the `seconds` column. This is a little awkward because `trace_sum$seconds` will necessarily be slightly lower than `crew$seconds` because the time estimate in the `trace` data is constructed from *within* the R process that builds the target, but `tar_crew()` would (I assume) estimate the time from outside that process. The differences will be small, but noticeable. So I'll use a rolling join:

```{r}
#| label: threading-2-worker-lookup
worker_lookup <- crew |> 
  left_join(
    trace_sum, 
    by = join_by(targets, closest(x$seconds > y$seconds))
  ) |> 
  select(worker, worker_pid)
worker_lookup
```

It's not ideal but it works: we now have a mapping between the numeric `worker` value returned by `tar_crew()` and the `worker_pid` value returned when `Sys.getpid()` is called from within the target function. Joining this gives us the following table:

```{r}
#| label: threading-2-target-trace
target_trace <- trace |> 
  left_join(worker_lookup, by = "worker_pid") |> 
  mutate(id = row_number()) |> 
  relocate(worker, .before = "worker_pid")
target_trace
```

Or, to adopt a slightly nicer way of looking at it, we can draw a picture:

```{r}
#| label: threading-2-trace-plot
#| fig.height: 3
#| fig.width: 8
ggplot(target_trace, aes(begins_at, worker, color = name)) + 
  geom_segment(aes(xend = finishes_at, yend = worker)) +
  geom_point() +
  geom_point(aes(x = finishes_at)) +
  theme_bw() + 
  scale_x_continuous(breaks = 0:6) +
  labs(x = "time", y = "worker", color = "target")
```

Okay yes, this now gives a detailed sense of which targets are dispatched to which workers, and at what time, but it does leave one thing missing: the total elapsed time shown by `target_trace` is about 5-6 seconds, but when I called `tar_make()` the total elapsed time was reported to be a little over 7 seconds. The discrepancy between the two is that the timing information recorded by my code doesn't account for the time that the targets package spends setting up the pipeline, and recording metadata and target values in the `_targets` folder. 

### Postmortem

One thing that bothered me a *lot* about the second version, when I implemented it, was this nagging intuition that most of what I was doing felt unnecessary. Certainly, there's a limitation to the output of `tar_crew()` in the sense that it doesn't tell you the worker to which each target was assigned, nor does it report the pid for each worker. That seems a little odd to me, but I am very willing to believe there's a good reason for that.

The part that seemed utterly baffling to me was seeing that the on-screen output to `tar_make()` reports the execution time for each target, but (at the time) I couldn't work out how to extract that information programmatically. It beggars belief to think that a package as sophisticated as targets wouldn't record that information somewhere and...

...yeah, of course it does. I was just looking in the wrong place. If you call `tar_meta()` it returns a tibble that stores a detailed listing of all targets -- including the "secret"^[Not very secret] one that records the value of [`.Random.seed`](/posts/2023-12-27_seedcatcher/) -- that includes their execution time, any warnings or errors produced during their execution, and a great deal more besides:

```{r}
#| echo: false
options(width = very_wide)
```

```{r}
#| label: threading-2-metadata
#| column: page
tar_meta()
```

True, it doesn't contain information about the system pid that executes the target, so I'd still have some work to do, but at least now I have the correct timing information for each target.

```{r}
fs::dir_tree("_targets")
```

Oh well. I'll know better next time, and anyway the entire point of this exercise was to teach myself how to use the package, so it's all good really.


