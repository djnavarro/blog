{
  "hash": "6255d61350a52b096ba3175d4c9eaf13",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Art from code V: Iterated function systems\"\ndescription: \"This is a subtitle\"\ndate: \"2024-12-22\"\ncategories: [\"R\", \"Art\"]\nimage: \"pretty-boxes.png\"\n--- \n\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\n(Adapted from my [Art From Code](https://art-from-code.netlify.app/day-2/session-1/) workshop)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Rcpp)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(ggthemes)\nlibrary(tictoc)\n```\n:::\n\n\n\nSo... iterated function systems. What are they? \n\n## Some tiresome formalism\n\nOne of the joys of leaving academia is that I can stop pretending that I don't get all my mathematical knowledge from Wikipedia, and as the entry for [iterated function systems](https://en.wikipedia.org/wiki/Iterated_function_system) oh so helpfully informs us, an [iterated function](https://en.wikipedia.org/wiki/Iterated_function) system is defined as a finite set of [contractive maps](https://en.wikipedia.org/wiki/Contraction_mapping) on a [complete metric space](https://en.wikipedia.org/wiki/Complete_metric_space) $X = (M, d)$, formally denoted \n\n$$\n\\left\\{f_i : X \\rightarrow X \\mid i = 1, 2, \\ldots N \\right\\}, N \\in \\mathcal{N}\n$$\n\nwhere the function $f_i$ is a contraction on $X$ if there exists some real number $k$ such that $d(f_i(x), f_i(y)) \\leq k \\ d(x,y)$ for all $x \\in M$ and $y \\in M$. \n\nIf that weren't impenetrable enough, Wikipedia continues to explain that\n\n> Hutchinson (1981) showed that, for the metric space ${\\displaystyle \\mathbb {R} ^{n}}$, or more generally, for a complete metric space $X$, such a system of functions has a unique nonempty [compact](https://en.wikipedia.org/wiki/Compact_space) (closed and bounded) fixed set $S$. One way of constructing a fixed set is to start with an initial nonempty closed and bounded set $S_0$ and iterate the actions of the $f_i$, taking $S_{n+1}$ to be the union of the images of $S_n$ under the $f_i$; then taking $S$ to be the [closure](https://en.wikipedia.org/wiki/Closure_(topology)) of the union of the $S_n$. Symbolically, the unique fixed (nonempty compact) set $S\\subseteq X$ has the property\n>\n$$S = \\overline{\\bigcup_{i=1}^N f_i(S)}.$$\n>\nThe set $S$ is thus the fixed set of the [Hutchinson operator](https://en.wikipedia.org/wiki/Hutchinson_operator) $F:2^{X}\\to 2^{X}$ defined for $A\\subseteq X$ via\n>\n$$F(A)={\\overline {\\bigcup _{i=1}^{N}f_{i}(A)}}.$$\n>\nThe existence and uniqueness of $S$ is a consequence of the [contraction mapping principle](https://en.wikipedia.org/wiki/Contraction_mapping_principle), as is the fact that\n>\n$$\\lim _{n\\to \\infty }F^{\\circ n}(A)=S$$\n>\nfor any nonempty compact set $A \\in X$. (For contractive IFS this convergence takes place even for any nonempty closed bounded set $A$). Random elements arbitrarily close to $S$ may be obtained by the \"chaos game\"\n\nI am entirely certain that you do not care.\n\nAs impressive as I find all this notation, I don't find it helps me understand what an iterated function system actually *does*. What I do find helpful, however, is to play the [chaos game](https://en.wikipedia.org/wiki/Chaos_game), because that's a concrete method we can use to simulate the behaviour of an IFS, and in practice that's what our code will actually do!\n\n## Chaos game for the Barnsley fern\n\nWhen written as pseudocode, the chaos game is remarkably simple:\n\n1. Choose a set of starting values $(x_0, y_0)$\n2. Set iteration number $i = 1$\n3. Choose a transformation function $f$ to use on this iteration\n4. Get the next value by passing the current value to the function, i.e. $(x_i, y_i) = f(x_{i-1}, y_{i-1})$\n5. Update iteration number $i = i + 1$ and return to step 3; or finish\n\nI've written this on the assumption that the functions are defined over a two dimensional space with $x$ and $y$ coordinates, but it generalises naturally to any number of dimensions. When choosing a transformation function in step 3, you can sample uniformly at random, or impose a bias so that some transformation are applied more often than others.\n\nTo get a sense of how this works, let's start with a classic example: the [Barnsley fern](https://en.wikipedia.org/wiki/Barnsley_fern). The Barnsley fern, like many iterated function systems I use for my art, is constructed from functions $f(x, y)$ defined in two dimensons. Better yet, they're all [affine transformations](https://en.wikipedia.org/wiki/Affine_transformation) so we can write any such function down using good old fashioned linear algebra, and compute everything using matrix multiplication and addition:\n\n$$f(x,y)={\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}}{\\begin{bmatrix}x\\\\y\\end{bmatrix}}+{\\begin{bmatrix}e\\\\f\\end{bmatrix}}$$\n\nThere are four such functions used to build the Barnsley fern, with coefficients shown below:\n\n\n|             |  $a$   | $b$   | $c$   | $d$  | $e$ | $f$  | weight | interpretation                     |\n|:------------|-------:|------:|------:|-----:|----:|-----:|-------:|:-----------------------------------|\t    \n| $f_1(x, y)$ |\t0      |\t0    | 0     | 0.16 | 0   | 0    | 0.01   | makes the stem                     |\n| $ƒ_2(x, y)$ |\t0.85   |\t0.04 | −0.04 | 0.85 | 0   | 1.60 | 0.85\t  | makes ever-smaller leaflets        |\n| $ƒ_3(x, y)$ |\t0.20   | −0.26 | 0.23  | 0.22 | 0   | 1.60 | 0.07\t  | makes largest left-hand leaflet    |\n| $ƒ_4(x, y)$ |\t−0.15  |\t0.28 | 0.26  | 0.24 | 0   | 0.44 | 0.07\t  | makes largest right-hand leaflet   |\n\nOkay, so let's start by implementing the Barnsley fern transformation functions in R. The `fern_transform()` function below takes `coord` input as a two-element numeric vector, and an `ind` argument that specifies which of the four transformations to apply (this should be an integer between 1 and 4). The output is the next set of `coord` values to use in the chaos game:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfern_transform <- function(coord, ind) {\n  \n  # coefficients for the stem function f_1\n  if(ind == 1) {\n    mat <- matrix(c(0, 0, 0, .16), 2, 2) # matrix to multiply\n    off <- c(0, 0)                       # offset vector to add\n  }\n  \n  # coefficients for the small leaflet function f_2\n  if(ind == 2) {\n    mat <- matrix(c(.85, -.04, .04, .85), 2, 2)\n    off <- c(0, 1.6)                      \n  }\n  # coefficients for the right-side function f_3\n  if(ind == 3) {\n    mat <- matrix(c(.2, .23, -.26, .22), 2, 2)\n    off <- c(0, 1.6)                      \n  }\n  \n  # coefficients for the left-side function f_4\n  if(ind == 4) {\n    mat <- matrix(c(-.15, .26, .28, .24), 2, 2)\n    off <- c(0, .44)                     \n  }\n  \n  # return the affine transformed coords\n  coord <- mat %*% coord + off\n  return(coord)\n}\n```\n:::\n\n\n\nArmed with the `fern_transform()` function, we can write a `fern_chaos()` function that implements the chaos game for the Barnsley fern. The arguments to `fern_chaos()` specify the number of iterations over which the game should be played, and (optionally) a `seed` to control the state of the random number generator:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfern_chaos <- function(iterations = 10000, seed = NULL) {\n  if(!is.null(seed)) set.seed(seed)\n  \n  # which transformation to apply at each iteration\n  transform_index <- sample(\n    x = 1:4, \n    size = iterations, \n    replace= TRUE, \n    prob = c(.01, .85, .07, .07)\n  )\n  \n  # initialise chaos game at the origin\n  start <- matrix(c(0, 0))\n  \n  # helper function to collapse accumulated output\n  bind_to_column_matrix <- function(lst) {\n    do.call(cbind, lst)\n  }\n  \n  # iterate until done!\n  coord_matrix <- transform_index |>\n    accumulate(fern_transform, .init = start) |>\n    bind_to_column_matrix() \n  \n  # tidy the output, add extra columns, and return\n  coord_df <- t(coord_matrix) |> \n    as.data.frame() \n  names(coord_df) <- c(\"x\", \"y\")\n  coord_df <- coord_df |>\n    as_tibble() |>\n    mutate(\n      transform = c(0, transform_index),\n      iteration = row_number() - 1\n    )\n  return(coord_df)\n}\n```\n:::\n\n\n\nThis function is a little fussier than it really needs to be. For example, if you compare my code to the [base R version on Wikipedia](https://en.wikipedia.org/wiki/Barnsley_fern#R) you'll see I spend extra effort tidying the results at the end: rather than returning a matrix of points, I've coerced it to a tibble that includes the coordinates as columns `x` and `y`, but in addition contains a column `transform` specifying which of the transformation functions was used to generate each point, and the `iteration` number as a unique identifier for each row. In any case, here's the output:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfern_dat <- fern_chaos(seed = 1)\nfern_dat\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10,001 × 4\n        x     y transform iteration\n    <dbl> <dbl>     <dbl>     <dbl>\n 1  0     0             0         0\n 2  0     1.6           2         1\n 3  0.064 2.96          2         2\n 4  0.173 4.11          2         3\n 5 -1.03  2.54          3         4\n 6 -0.778 3.80          2         5\n 7 -1.14  2.26          3         6\n 8  0.804 0.684         4         7\n 9  0.711 2.15          2         8\n10  0.690 3.40          2         9\n# ℹ 9,991 more rows\n```\n\n\n:::\n:::\n\n\n\nIt looks nicer as a plot though :)\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fern_dat, aes(x, y)) +\n  geom_point(colour = \"white\", size = 1, stroke = 0) +\n  coord_equal() +\n  theme_void()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/show-barnsley-fern-1.png){fig-align='center' width=1800}\n:::\n:::\n\n\n\nThe reason I went to the extra trouble of storing the `transform` column was so I could map it to the colour aesthetic in my plot. When I do this, I get this as the result: there's a transformation function that defines the left leaf shape, another that defines the right leaf shape, and a third one that defines the stem shape. Finally, there's a function that copies, shifts-up, and rotates its input in a way that produces the vertical symmetry in the output. \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fern_dat, aes(x, y, colour = factor(transform))) +\n  geom_point(size = 1, stroke = 0) +\n  coord_equal() +\n  theme_void() + \n  theme(\n    legend.text = element_text(colour = \"white\"),\n    legend.title = element_text(colour = \"white\")\n  ) +\n  guides(colour = guide_legend(\n    title = \"transformation\", \n    override.aes = list(size = 5))\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/transform-shaded-barnsley-fern-1.png){fig-align='center' width=1800}\n:::\n:::\n\n\n\nIt's painfully obvious now what each of the transformation functions does! \n\nAs we'll see a little later, it can be very useful to plot your outputs this way sometimes: even if you're planning to do something fancier with colour later, the ability to visualise which parts of your output are associated with a particular function is useful for diagnosing what your system is doing. My experience has been that iterated function systems are difficult to reason about just by looking at the code: the relationship between the code and the output is pretty opaque, so you have to rely on diagnostics like this when tweaking the output of your system.\n\nFor no particular reason, here's our fern with the colour aesthetic mapped to the `iteration` number:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fern_dat, aes(x, y, colour = iteration)) +\n  geom_point(size = 1, stroke = 0, show.legend = FALSE) +\n  coord_equal() +\n  theme_void() \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/randomly-shaded-barnsley-fern-1.png){fig-align='center' width=1800}\n:::\n:::\n\n\n\n::: {.callout-important icon=false #exercise-barsnley-fern}\n## Exercise\n\nCode for this system is included in the `barnsley-fern.R` script.\n\n:::\n\n\n## Happy accidents\n\nIterated function systems can be a lot more elaborate than the Barnsley fern, often involving transformation functions that are constructed according to some fancypants compositional rules. For example, the [fractal flame algorithm](https://en.wikipedia.org/wiki/Fractal_flame) proposed by Scott Draves in 1992 (here's [the original article](https://flam3.com/flame_draves.pdf)) specifies transformation functions $f_i()$ -- called \"flame functions\" -- that are composed according to the rule:\n\n$$\nf_i(\\mathbf{x}) = \\sum_j w_{ij} \\ g_j(\\mathbf{A}_i \\mathbf{x})\n$$\n\nwhere\n\n- $\\mathbf{A}_i$ is a matrix that defines an affine transformation of the coordinates $\\mathbf{x}$ associated with this specific flame function (i.e., each flame function $f_i()$ has its own transformation $\\mathbf{A_i}$, and in the two dimensional case $\\mathbf{x}$ is just the points $(x, y)$);\n- the various $g_j()$ functions are called \"variant functions\", and these don't have to be linear: they can be sinusoidal, or discontinuous, or whatever you like really; and\n- each flame function is defined as a linear combination of the variant functions: the coefficient $w_{ij}$ specifies the weight assigned to the $j$-th variant function by the $i$-th flame function.\n\nAdditionally, just as we saw with the Barnsley fern, the flame functions themselves can be weighted with a probability vector: a system can be defined in a way that has a bias for some flame functions over others. \n\nThis probably sounds a bit... intense, right? \n\nSo yeah. Um. \n\nWhen I first decided to try implementing the fractal flame algorithm I decided I wasn't going to bother with fancypants weights $w_{ij}$, so I... ignored them. But then -- because I was tired and not really paying attention to the subscripts in Draves equations -- I decided that my system was going to have one flame function for every possible combination of transformation matrix $\\mathbf{A}_i$ and variant function $g_j()$. What this meant is that the thing I actually coded was this. Given a set of variant functions $g_1, g_2, \\ldots, g_n$ and some set of transformation matrices $\\mathbf{A}_1, \\mathbf{A}_2, \\ldots, \\mathbf{A}_m$, I included every transformation function $f_{ij}(\\mathbf{x})$ of the following form:\n\n$$\nf_{ij}(\\mathbf{x}) = g_j(\\mathbf{A}_i \\mathbf{x})\n$$\nWhen your transformation functions are composed in this way you can sample a random transformation $f_{ij}$ by sampling the two components independently: sample a transformation matrix $\\mathbf{A}_i$ and a variant function $g_j$, and then you're done. It ends up being a weird special case of the fractal flame algorithm, but it turns out you can make pretty things that way.\n\nOh well. Whatever.\n\nThe point of art isn't to mindlessly copy what someone else has done, and if I'm being honest with myself the truth is that some of the best art I've created started with a coding error or a misinterpretation like this one. As [Bob Ross](https://en.wikipedia.org/wiki/Bob_Ross) famously said,\n\n> There are no mistakes, just happy accidents.\n\n\n## Chaos game for unboxing\n\nEnough chitchat about my artistic process. Let's actually implement a version of my [Unboxing](https://art.djnavarro.net/gallery/unboxing/) system. In this example, the coefficients that define the affine transformations $\\mathbf{A_i}$ have been sampled uniformly at random, with values ranging from -1 to 1. There's a `layers` input argument that specifies how many of these affine transformations to include (no I don't know why I called it `layers` -- it's a bad name I think). Anyway, the code snippet below shows how this is implemented:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeffs <- array(\n  data = runif(9 * layers, min = -1, max = 1), \n  dim = c(3, 3, layers)\n)\n```\n:::\n\n\n\nThe coefficients are stored in an array: `coeffs[,,i]` is the matrix of coefficients $\\mathbf{A_i}$.\n\nThere are three variant functions $g_j$ in this system: two of them are sinusoidal functions: one of them computes `sin(x)` and `sin(y)`, and the other computes the same thing but multiplies the output by two. Both of these will produce wavy shapes. The other one is a rescaling function: it tends to shift points towards the top right corner. The code snippet below implements these variant functions:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfuns <- list(\n  function(point) point + (sum(point ^ 2)) ^ (1/3),\n  function(point) sin(point),\n  function(point) 2 * sin(point)\n)\n```\n:::\n\n\n\nThe `unboxer_base()` function below implements the whole thing:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunboxer_base <- function(iterations, layers, seed = NULL) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  \n  # coefficients defining affine layer transforms, A_i\n  coeffs <- array(\n    data = runif(9 * layers, min = -1, max = 1), \n    dim = c(3, 3, layers)\n  )\n  \n  # list of variant functions, g_j\n  funs <- list(\n    function(point) point + (sum(point ^ 2)) ^ (1/3),\n    function(point) sin(point),\n    function(point) 2 * sin(point)\n  )\n  \n  # updater function: apply the layer, then the function\n  # (the weirdness with point[3] is me treating colour as special)\n  update <- function(point, layer, transform) {\n    f <- funs[[transform]]\n    z <- point[3]\n    point[3] <- 1\n    point <- f(point %*% coeffs[,,layer])\n    point[3] <- (point[3] + z)/2\n    return(point)\n  }\n  \n  # initial point\n  point0 <- matrix(\n    data = runif(3, min = -1, max = 1), \n    nrow = 1,\n    ncol = 3\n  )\n  \n  # sample points\n  layer_ind <- sample(layers, iterations, replace = TRUE)  \n  trans_ind <- sample(length(funs), iterations, replace = TRUE)  \n  points <- accumulate2(layer_ind, trans_ind, update, .init = point0)\n  \n  # tidy up, add columns, and return\n  points <- matrix(unlist(points), ncol = 3, byrow = TRUE)\n  points <- cbind(\n    points,\n    c(0, layer_ind),\n    c(0, trans_ind)\n  )\n  return(points)\n}\n```\n:::\n\n\n\nLet's run this system for a few iterations, just so we can see what the output looks like:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nunboxer_base(10, layers = 5, seed = 333)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]        [,2]        [,3] [,4] [,5]\n [1,] -0.8825507 -1.67203174  0.36262158    0    0\n [2,]  1.0764159  0.96570274 -0.98567258    5    3\n [3,]  0.9459713 -0.02509324  1.21601399    3    1\n [4,]  1.2223664 -0.93808971 -0.04718621    1    1\n [5,]  1.8462734  1.58519247  0.44778402    1    3\n [6,]  0.3400557 -0.99766695  0.21844210    3    2\n [7,]  0.3551727  0.20495003  3.28357073    4    2\n [8,] -0.9168023  0.19692929 -0.79482471    4    2\n [9,] -0.4479714 -0.19201355 -0.82068698    1    3\n[10,] -0.7158925 -0.36968979 -0.27724644    2    1\n[11,] -0.9885079  1.29440457  0.58763161    5    2\n```\n\n\n:::\n:::\n\n\n\nAs you can see, this time around I've not gone to the effort of converting it to a tibble or making it pretty. This output is a matrix. The first column is the x-coordinate and the second column is the y-coordinate. The third column is a \"z-coordinate\" that we'll map to the colour aesthetic later. Column four specifies the layer number (i.e., the value $i$ specifying which affine matrix $\\mathbf{A}_i$ was used), and column five specifies the variant function number (i.e., the value $j$ specifying which variant function $g_j()$ was used). \n\nIf we want to turn these numbers into art and attach colours to the points, we are going to need a palette function, so as usual I'll insert my code to sample one of the canva palettes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_canva2 <- function(seed = NULL, n = 4) {\n  \n  if(!is.null(seed)) set.seed(seed)\n  sample(ggthemes::canva_palettes, 1)[[1]] |>\n    (\\(x) colorRampPalette(x)(n))()  \n}\n```\n:::\n\n\n\nHaving done all that work, the rendering function in not very fancy: it's just some ggplot2 code to create a scatter plot from the points and colour them using a canva palette:\n\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\nunbox_art <- function(data, seed = NULL, size = 1) {\n  \n  # convert to data frame and sample a palette\n  data <- data |> as.data.frame() |> as_tibble()\n  names(data) <- c(\"x\", \"y\", \"c\", \"l\", \"t\")[1:ncol(data)]\n  shades <- sample_canva2(seed)\n  \n  # render image as a scatter plot\n  ggplot(data, aes(x, y, colour = c)) +\n    geom_point(\n      size = size,\n      stroke = 0,\n      show.legend = FALSE\n    ) + \n    theme_void() + \n    coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) + \n    scale_colour_gradientn(colours = shades) + \n    scale_x_continuous(expand = c(0, 0)) +\n    scale_y_continuous(expand = c(0, 0)) +\n    theme(panel.background = element_rect(\n      fill = shades[1], colour = shades[1]\n    ))\n}\n```\n:::\n\n\n\nThe results can be very pretty, especially when you generate a large number of points (say, 3 million) and plot them with a very small marker size (say, .1). \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmil <- 1000000\ntic()\nunboxer_base(3 * mil, layers = 3, seed = 66) |> \n  unbox_art(seed = 66, size = .1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/an-unboxing-1.png){fig-align='center' width=1800}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n65.016 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nThe system is slow, but I'm usually willing to wait 30 seconds for something pretty. (Though in a moment I'll talk about how we can speed this up drastically) \n\n\n::: {.callout-important icon=false #exercise-unboxing-base}\n## Exercise\n\nCode for this system is included in the `unbox-base.R` script.\n\n:::\n\n\nThe outputs from this system have a fairly consistent look and feel: a pair of nested boxes, with something \"bursting\" from the top right corner. The fine grained details vary a lot from output to output, and there are some systematic differences as a function of the number of layers. Here's an example showing what happens when I ratchet up the number of layers from 2 to 9:\n\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\ntic()\nunboxer_base(mil, layers = 2, seed = 999) |> unbox_art(seed = 2, size = .2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-unboxing-art-1.png){width=1800}\n:::\n\n```{.r .cell-code}\nunboxer_base(mil, layers = 5, seed = 333) |> unbox_art(seed = 2, size = .2) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-unboxing-art-2.png){width=1800}\n:::\n\n```{.r .cell-code}\nunboxer_base(mil, layers = 9, seed = 420) |> unbox_art(seed = 2, size = .2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-unboxing-art-3.png){width=1800}\n:::\n\n```{.r .cell-code}\ntoc() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n69.069 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n\nTo understand what's going on in this system, I'll go through the same exercise I did with the Barnsley fern. I'll generate the data for a piece of art by calling `unboxer_base()`, and then plot it three ways. First I'll show it as a pure black and white image to show the overall configuration of points, then I'll break it down based on the components. Because each transformation function is defined in terms the affine component and the variant component, I'll show two different versions of this. \n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\ndat <- unboxer_base(mil, layers = 2, seed = 99) |> \n  as.data.frame() |> \n  as_tibble()\n\nnames(dat) <- c(\"x\", \"y\", \"c\", \"affine_layer\", \"variant_function\")\n\ndat <- dat |> \n  slice_tail(n = -1) |> \n  mutate(\n    affine_layer = factor(affine_layer),\n    variant_function = factor(variant_function)\n  ) \n\nggplot(dat, aes(x, y)) +\n  geom_point(size = .4, stroke = 0, show.legend = FALSE) + \n  theme_void() + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/show-unboxing-components-1.png){width=1800}\n:::\n\n```{.r .cell-code}\nggplot(dat, aes(x, y, colour = variant_function)) +\n  geom_point(size = .4, stroke = 0) + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) +\n  scale_colour_brewer(palette = \"Set2\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 5))) +\n  theme_void() + \n  theme(legend.position = c(0.2, 0.1))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/show-unboxing-components-2.png){width=1800}\n:::\n\n```{.r .cell-code}\nggplot(dat, aes(x, y, colour = affine_layer)) +\n  geom_point(size = .4, stroke = 0) + \n  coord_equal(xlim = c(-4, 4), ylim = c(-4, 4)) +\n  scale_colour_brewer(palette = \"Set1\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 5))) +\n  theme_void() + \n  theme(legend.position = c(0.2, 0.1))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/show-unboxing-components-3.png){width=1800}\n:::\n:::\n\n\n\nThis gives you a sense of what's going on here: in the middle panel you can see that the two \"sinusoidal\" components have the effect of creating the boxes, because `sin(x)` is constrained to lie between -1 and 1. The snaky, wavy patterns that you see in some the outputs are also related to these components, but I haven't plotted the data in a way that makes this obvious. \n\nIn contrast, on the right you can see the effect of the affine transformations. Notice that the blue pattern kind of looks like a \"squashed and rotated\" version of the red pattern? That's exactly what the affine transforms do. They create these distortions. \n\n\n## Faster chaos with Rcpp\n\nWaiting 30 seconds for something pretty is kind of annoying, especially when you're still developing the system and you just want to tinker with the settings to see what it does. It would be nice if we could speed this up, right? The easiest way to speed things up is to run fewer iterations and use larger plot sizes. I mean, this works perfectly fine...\n\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\ntic()\nunboxer_base(50000, layers = 2, seed = 999) |> unbox_art(seed = 2, size = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/coarse-unboxing-art-1.png){width=1800}\n:::\n\n```{.r .cell-code}\nunboxer_base(50000, layers = 5, seed = 333) |> unbox_art(seed = 2, size = 1) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/coarse-unboxing-art-2.png){width=1800}\n:::\n\n```{.r .cell-code}\nunboxer_base(50000, layers = 9, seed = 420) |> unbox_art(seed = 2, size = 1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/coarse-unboxing-art-3.png){width=1800}\n:::\n\n```{.r .cell-code}\ntoc() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4.729 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nIf you're okay with a coarser grained output (or flat out don't want to mess around with C++ code), your problems are solved! Read no further!\n\nIf speed is a consideration -- especially if the rendering times are interfering with the creative process -- one possibility would be to write the slow parts of your code in C++, and then call it from R using the Rcpp package. To be honest, I'm not the best C++ coder myself and am only moderately comfortable with Rcpp, so I'm not going to attempt a tutorial here. Instead, what I'll do is mention that [rcpp.org](https://www.rcpp.org/) has some excellent resources, and *Advanced R* also has a good chapter on [Rewriting R code in C++](https://adv-r.hadley.nz/rcpp.html) that you may find helpful. I'll also show you what I did for this system, because sometimes it's helpful to see C++ code that implements the same functions as the original R code. Let's imagine I have a file called `unbox-fast.cpp` that includes the following:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.cpp .cell-code}\n#include <Rcpp.h>\n#include <iostream>\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_rcpp(int iterations, int layers) {\n  \n  // variables\n  NumericMatrix pts(iterations, 3); \n  NumericMatrix cff(9, layers);\n  int r, f;\n  double x, y, z, s;\n  \n  // coefficients\n  for(int i = 0; i < 9; i++) {\n    for(int j = 0; j < layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // initial point\n  pts(0, 0) = R::runif(-1, 1);\n  pts(0, 1) = R::runif(-1, 1);\n  pts(0, 2) = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t < iterations; t++) {\n    r = rand() % layers; // which transform to use?\n    f = rand() % 3;      // which function to use?\n    \n    // apply transformation\n    x = cff(0, r) * pts(t-1, 0) + cff(1, r) * pts(t-1, 1) + cff(2, r);\n    y = cff(3, r) * pts(t-1, 0) + cff(4, r) * pts(t-1, 1) + cff(5, r);\n    z = cff(6, r) * pts(t-1, 0) + cff(7, r) * pts(t-1, 1) + cff(8, r);\n    \n    // apply function\n    if(f == 0) {\n      s = pow(x*x + y*y + z*z, 1/3);\n      x = x + s;\n      y = y + s;\n      z = z + s;\n    } else if(f == 1) {\n      x = sin(x);\n      y = sin(y);\n      z = sin(z);\n    } else {\n      x = 2 * sin(x);\n      y = 2 * sin(y);\n      z = 2 * sin(z);\n    }\n    \n    // store new point\n    pts(t, 0) = x;\n    pts(t, 1) = y;\n    pts(t, 2) = (z + pts(t-1, 2))/2;\n  }\n  return pts;\n}\n```\n:::\n\n\n\nWhen sourced from R in the \"right\" way, this will create a function `unboxer_rcpp()` that I can call from R. And when I say \"sourced\" from R what I really mean is if I did this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nRcpp::sourceCpp(file = \"unbox-fast.cpp\")\n```\n:::\n\n\n\nIf you've used Rcpp, this should seem familiar. \n\nIf you haven't used Rcpp and are trying to make up your mind if it is worth the effort to learn, well, I'll offer this comparison. Here's the difference in speed for generating a million data points in the original system `unbox_base()`, compared to the C++ implementation `unbox_rcpp()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic(); set.seed(999); dat <- unboxer_base(mil, layers = 2); toc()\ntic(); set.seed(999); dat <- unboxer_rcpp(mil, layers = 2); toc() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n16.054 sec elapsed\n0.034 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nNot too bad, really :)\n\nWhen written in C++ we can generate 10 million data points extremely quickly. So much so that it's outrageously fast to do it three times with different seeds and different numbers of layers:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nset.seed(123); dat1 <- unboxer_rcpp(10 * mil, layers = 2) \nset.seed(101); dat2 <- unboxer_rcpp(10 * mil, layers = 5) \nset.seed(420); dat3 <- unboxer_rcpp(10 * mil, layers = 9) \ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.573 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-important icon=false #exercise-unboxing-rcpp}\n## Exercise\n\nThe C++ code for this system is included in the `unbox-fast.cpp` script, and the code calling it from R to test the timing is included as the `unbox-fast-test.R` script.\n\n:::\n\n\nTransforming the data into plots, on the other hand, is a little slower. At this point the rendering code is the part that is causing slowness:\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\ntic()\ndat1 |> unbox_art(seed = 123, size = .2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-rcpp-art-1.png){width=1800}\n:::\n\n```{.r .cell-code}\ndat2 |> unbox_art(seed = 101, size = .2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-rcpp-art-2.png){width=1800}\n:::\n\n```{.r .cell-code}\ndat3 |> unbox_art(seed = 420, size = .2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/first-rcpp-art-3.png){width=1800}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n207.29 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nHm. About 1.5 seconds to generate the data, about 100 seconds to produce the plots. That's a little unfortunate. Perhaps we can speed that up too? After all, ggplot2 has a lot of bells and whistles that we aren't using in this plot. Maybe we can sidestep the issue...\n\n\n## Even faster chaos with raster representation\n\nBack in the era when I held an academic appointment, one of my research topics used to be human mental representation. When people have to make judgments, choices, or reason about something unfamiliar, we rely on our knowledge of the world to guide us. We have rich, structured knowledge from our past experience that we can bring to bear on new situations, which is super useful because in addition to being fabulous and insanely complicated things, neurons are slow and squishy things relative to machines. Honestly it's a bit of a surprise that we can compute anything with these things, and borderline miraculous that we manage to think clever thoughts using them. \n\nAll of this is in service of a really basic comment: if your computing machine doesn't store data in a sensible format, you're going to find it really hard to do anything useful. But the converse is also true... if you represent information in the right way, you'll be able to accomplish a lot. Over and over again, across a lot of different problems I used to study, I'd see a consistent pattern: people make sensible choices when we're given information structured in the \"right\" way. But if you present the same information a different and counterintuitive way, people don't know what to do with it and they make insane choices. As a psychological researcher, it's really easy to design studies that make people look stupid, and equally easy to design studies that make people look smart. It's almost criminal easy to \"rig\" the results of a study this way. \n\nAnyway.\n\nMy point here is that machines are kind of the same. If you want your image rendering to go faster, well, maybe you should store the data in a format that mirrors the output you want? I mean, at this point we're storing a data frame with 10 millions coordinates, and then plotting circles in an abstract canvas that ggplot2 constructs with the help of the grid graphics system, and then... aren't you tired already?\n\nIf you want a bitmap that stores pixel values at the *end* of your generative process, why not *start* with the data in exactly the same format at the beginning? Don't draw circles-as-polygons-around-a-coordinate. Just store the damned pixel values from the outset. \n\nOkay, so here's a slight reimagining of our Rcpp function that does exactly that. We store a matrix representing the bitmap from the very beginning. The output of this `unboxer_grid()` function is a square matrix with the number of rows and columns determined by the `pixels` input: \n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.cpp .cell-code}\n#include <Rcpp.h>\n#include <iostream>\nusing namespace Rcpp;\nusing namespace std;\n\n// [[Rcpp::export]]\nNumericMatrix unboxer_grid(int iterations, \n                           int layers,\n                           int pixels, \n                           double border) {\n  \n  // variables\n  NumericMatrix image(pixels, pixels); \n  NumericMatrix cff(9, layers);\n  int r, c, f, x_ind, y_ind;\n  double x, y, z, s;\n  \n  // set image matrix to zeros\n  for(int r = 0; r < pixels; r++) {\n    for(int c = 0; c < pixels; c++) {\n      image(c, r) = 0;\n    }\n  }\n  \n  // coefficients\n  for(int i = 0; i < 9; i++) {\n    for(int j = 0; j < layers; j++) {\n      cff(i,j) = R::runif(-1,1);\n    }\n  }\n  \n  // values for initial state\n  double x_old = R::runif(-1, 1);\n  double y_old = R::runif(-1, 1);\n  double z_old = R::runif(-1, 1);\n  \n  // accumulate\n  for(int t = 1; t < iterations; t++) {\n    r = rand() % layers; // which transform to use?\n    f = rand() % 3;      // which function to use?\n    \n    // apply transformation\n    x = cff(0, r) * x_old + cff(1, r) * y_old + cff(2, r);\n    y = cff(3, r) * x_old + cff(4, r) * y_old + cff(5, r);\n    z = cff(6, r) * x_old + cff(7, r) * y_old + cff(8, r);\n    \n    // apply function\n    if(f == 0) {\n      s = pow(x*x + y*y + z*z, 1/3);\n      x = x + s;\n      y = y + s;\n      z = abs(z + s);\n    } else if(f == 1) {\n      x = sin(x);\n      y = sin(y);\n      z = sin(z) + 1;\n    } else {\n      x = 2 * sin(x);\n      y = 2 * sin(y);\n      z = 2 * (sin(z) + 1);\n    }\n    \n    // compute indices to be updated\n    x_ind = int (x * pixels / (2 * border)) + pixels / 2;\n    y_ind = int (y * pixels / (2 * border)) + pixels / 2;\n    \n    // store results if they fall within the range\n    if(x_ind >= 0 & x_ind < pixels) {\n      if(y_ind >= 0 & y_ind < pixels) {\n        image(x_ind, y_ind) = z;\n      }\n    }\n    \n    // move new to old\n    x_old = x;\n    y_old = y;\n    z_old = (z + z_old) / 2; \n  }\n  return image;\n}\n```\n:::\n\n\n\nFrom a data generation perspective, there's really not much difference between this version and the last one. They're both fast. The C++ code to generate the image in a bitmap format isn't faster or slower than the C++ code we wrote last time:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntic()\nset.seed(123); mat1 <- unboxer_grid(10 * mil, 2, 1000, 4) \nset.seed(101); mat2 <- unboxer_grid(10 * mil, 5, 1000, 4) \nset.seed(420); mat3 <- unboxer_grid(10 * mil, 9, 1000, 4)\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1.207 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nAh, but now look what happens when we generate an image from the data. Originally we were working with ggplot2, and we were forcing it to convert a large data frame to an image in a very very painful way. This time around, the data is already in the right format. It's a bitmap that we can pass to `image()`. No heavy lifting required!\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\nraster_art <- function(mat, seed = NULL, trim = .001) {\n  \n  zlim <- quantile(mat, c(trim, 1 - trim))\n  mat[mat < zlim[1]] <- zlim[1]\n  mat[mat > zlim[2]] <- zlim[2]\n  \n  op <- par(mar = c(0, 0, 0, 0))\n  image(\n    z = mat, \n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = sample_canva2(seed, n = 256)\n  )\n  par(op)\n}\n\ntic()\nraster_art(mat1, seed = 123)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/image-plot-1.png){width=1800}\n:::\n\n```{.r .cell-code}\nraster_art(mat2, seed = 101)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/image-plot-2.png){width=1800}\n:::\n\n```{.r .cell-code}\nraster_art(mat3, seed = 420)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/image-plot-3.png){width=1800}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.921 sec elapsed\n```\n\n\n:::\n:::\n\n\n\nOkay fine, this new version doesn't handle shading in precisely the same way the original version did, but it's still very pretty -- and it's soooooo much faster! \n\nHow fast is it? Fast enough that I'm perfectly willing to generate an image by playing the chaos game for 100 million iterations. Hell, it's fast enough that I'll generate six of them:\n\n\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\npretty_boxes <- function(\n    seed,\n    iterations = 100000000, \n    layers = 5, \n    pixels = 4000, \n    background = \"black\",\n    border = 4,\n    trim = .001\n) {\n  \n  set.seed(seed)\n  \n  mat <- unboxer_grid(\n    iterations = iterations, \n    layers = layers, \n    pixels = pixels, \n    border = border\n  )\n  \n  shades <- c(background, sample_canva2(seed, n = 1023))\n  \n  zlim <- quantile(mat, c(trim, 1 - trim))\n  mat[mat < zlim[1]] <- zlim[1]\n  mat[mat > zlim[2]] <- zlim[2]\n  \n  op <- par(mar = c(0, 0, 0, 0))\n  image(\n    z = mat, \n    axes = FALSE, \n    asp = 1, \n    useRaster = TRUE, \n    col = shades\n  )\n  par(op)\n}\n\ntic()\npretty_boxes(286)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pretty-boxes-1.png){width=4000}\n:::\n\n```{.r .cell-code}\npretty_boxes(380)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pretty-boxes-2.png){width=4000}\n:::\n\n```{.r .cell-code}\npretty_boxes(100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pretty-boxes-3.png){width=4000}\n:::\n\n```{.r .cell-code}\ntoc()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n33.626 sec elapsed\n```\n\n\n:::\n:::\n\n::: {.cell .column-screen-inset layout-ncol=\"3\"}\n\n```{.r .cell-code}\ntic()\npretty_boxes(222)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/more-pretty-boxes-1.png){width=4000}\n:::\n\n```{.r .cell-code}\npretty_boxes(567)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/more-pretty-boxes-2.png){width=4000}\n:::\n\n```{.r .cell-code}\npretty_boxes(890)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/more-pretty-boxes-3.png){width=4000}\n:::\n\n```{.r .cell-code}\ntoc() \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n39.632 sec elapsed\n```\n\n\n:::\n:::\n\n\n\n\n\n::: {.callout-important icon=false #exercise-pretty-boxes}\n## Exercise\n\nThe C++ code to generate the data for this system is included in the `unbox-grid.cpp` script, and plotting code is in the `pretty-boxes.R` script.\n\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}