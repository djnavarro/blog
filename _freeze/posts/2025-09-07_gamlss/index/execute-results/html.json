{
  "hash": "1ec1cff7dfebf04b05e47d981e5907a3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"gamlss\"\ndescription: \"This is a subtitle\"\ndate: \"2025-09-07\"\ncategories: [\"statistics\", \"R\"]\nknitr:\n  opts_chunk: \n    dev.args:\n      bg: \"#00000000\"\n--- \n\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nOkay, so [two posts ago](/posts/2025-08-02_box-cox-power-exponential/) I was whining about the Box-Cox power exponential distribution, and promising that it would be followed by a new post whining about the [generalised additive model for location, shape and scale](https://en.wikipedia.org/wiki/Generalized_additive_model_for_location,_scale_and_shape) (GAMLSS). It turns out I was lying because I had to set time aside and write a separate whiny [post about penalized splines](/posts/2025-09-06_p-splines/) and promising it would be followed by a post whining about GAMLSS.\n\nThis is that post.\n\nMy introduction to GAMLSS models came from a problem that is easy to state, and strangely difficult to solve. If you know a person's age $a$ and their sex $s$, describe (and sample from) the joint distribution over height $h$ and weight $w$ for people of that age and sex. That is, estimate the following distribution:\n\n$$\np(h, w | a, s)\n$$\n\nIf you don't think very hard this doesn't seem like it should be very difficult. Data sets containing age, sex, height and weight are not too difficult to find. The question doesn't ask for any fancy longitudinal modelling: all we're being asked to do is estimate a probability distribution. Not difficult at all. It's a problem I've encountered several times in my pharmacometric work -- very commonly, we have to run simulations to explore the expected distribution of drug exposures among a population of patients that varies in weight, height, age, and sex -- so it's one I've had to think about a few times now, and it's a trickier than it looks. Typically we have to rely on fairly large population survey data sets (e.g. NHANES) and sophisticated regression models (e.g. GAMLSS), and I will confess that neither of these two things were covered in my undergraduate classes. \n\nIt's been a rather steep learning curve, and in keeping with my usual habit, I decided it might be wise to write up some notes. I know with a dread certainty that I'll have to use this knowledge again in a few months, and given how awful my memory is these days I'd prefer not to have to relearn the whole thing from the beginning next time. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fs)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(haven)\nlibrary(readr)\nlibrary(tidyr)\nlibrary(gamlss)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(quartose)\n```\n:::\n\n\n\n## The NHANES data\n\nThe data set I'll look at in this post comes from the [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/) (NHANES), a large ongoing study conducted under the auspices of the United States [Center for Disease Control](https://www.cdc.gov/nchs/nhanes/) (CDC). The program started in the 1960s, but the data set that is conventionally called \"the NHANES data set\" refers to the *continuous* data set that has been collected since 1999, with updates released every couple of years with approximately 5000 participants added each year. The data collection process is quite extensive. As described on the website, it includes:\n\n> - Interviews about health, diet, and personal, social, and economic characteristics\n> - Visits to our mobile exam center for dental exams and health and body measurements\n> - Laboratory tests by highly trained medical professionals\n\nFor any given participant, records are split across several data files. For instance, the [body measurements](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination) table contains very typical measurements such as height and weight, but also includes variables like waist circumference, thigh circumference, and so on. The [demographics](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics) table data includes information like age, sex, and so forth. These are the only two tables I'll use in this post, but there are a great many others also: you can find blood pressure data, audiometry data, various clinical measurements, and so on. It's a very handy data set used for a variety of purposes, not least of which is the fact that NHANES data are used to build the [CDC growth charts](https://www.cdc.gov/growthcharts) that are themselves used for a wide range of scientific and medical purposes. In my own work as a newly-minted pharmacometrician I've needed to rely on the NHANES data and/or CDC growth charts in several projects over the last couple of years, and for that reason I've found it useful to dive into the data set quite a few times. \n\nAs of the most recent release, the summaries below list the file names the body measurement (BMX) and demographics (DEMO) tables:\n\n\n\n\n```{.r .cell-code}\nmetadata <- list( \n  BMX  = read_csv(path(post_dir, \"nhanes\", \"bmx-summary.csv\"), show_col_types = FALSE),\n  DEMO = read_csv(path(post_dir, \"nhanes\", \"demo-summary.csv\"), show_col_types = FALSE) \n)\n\nquarto_tabset(metadata, level = 3)\n```\n\n\n\n::: {.panel-tabset}\n\n \n\n\n### BMX\n\n \n<pre> \n# A tibble: 12 × 3 \n   file_bmx  cohort    description   \n   <chr>     <chr>     <chr>         \n 1 BMX.xpt   1999-2000 body measures \n 2 BMX_B.xpt 2001-2002 body measures \n 3 BMX_C.xpt 2003-2004 body measures \n 4 BMX_D.xpt 2005-2006 body measures \n 5 BMX_E.xpt 2007-2008 body measures \n 6 BMX_F.xpt 2009-2010 body measures \n 7 BMX_G.xpt 2011-2012 body measures \n 8 BMX_H.xpt 2013-2014 body measures \n 9 BMX_I.xpt 2015-2016 body measures \n10 BMX_J.xpt 2017-2018 body measures \n11 BMX_L.xpt 2021-2023 body measures \n12 P_BMX.xpt 2017-2020 body measures \n</pre> \n\n\n### DEMO\n\n \n<pre> \n# A tibble: 12 × 3 \n   file_demo  cohort    description                     \n   <chr>      <chr>     <chr>                           \n 1 DEMO.xpt   1999-2000 demographics and sample weights \n 2 DEMO_B.xpt 2001-2002 demographics and sample weights \n 3 DEMO_C.xpt 2003-2004 demographics and sample weights \n 4 DEMO_D.xpt 2005-2006 demographics and sample weights \n 5 DEMO_E.xpt 2007-2008 demographics and sample weights \n 6 DEMO_F.xpt 2009-2010 demographics and sample weights \n 7 DEMO_G.xpt 2011-2012 demographics and sample weights \n 8 DEMO_H.xpt 2013-2014 demographics and sample weights \n 9 DEMO_I.xpt 2015-2016 demographics and sample weights \n10 DEMO_J.xpt 2017-2018 demographics and sample weights \n11 DEMO_L.xpt 2021-2023 demographics and sample weights \n12 P_DEMO.xpt 2017-2020 demographics and sample weights \n</pre> \n\n\n::: \n\n \n\n\n\nAs you can see from the summaries, the data sets are released as SAS transport (i.e., XPT) files, with letters used to represent each data cut. In principle these should go from \"A\" (data from the 1999-2000 cohort) through to \"L\" (data from the 2021-2023 cohort), but that's not precisely what happens. The original release (which ostensibly should be the \"A\" data cut) doesn't have a label, and the \"K\" data cut is missing entirely due to the COVID-19 pandemic. In its place there is a \"P\" version of data set that breaks the file naming scheme, presumably short for \"pandemic\", and labelled differently to highlight that data from that release might be a little strange I suppose.  \n\n### Importing the data\n\nTo explore the NHANES data, we can download all these files and load them into R:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# all demographics and body measurement files\ndemo_files <- dir_ls(path(post_dir, \"nhanes\", \"data\", \"demo\"))\nbmx_files  <- dir_ls(path(post_dir, \"nhanes\", \"data\", \"bmx\"))\n\n# read demographics file (selected variables only)\ndemos <- demo_files |> \n  map(\\(xx) {\n    dd <- read_xpt(xx) \n    if (!exists(\"RIDEXAGM\", where = dd)) dd$RIDEXAGM <- NA_real_\n    dd <- select(dd, SEQN, RIAGENDR, RIDAGEYR, RIDAGEMN, RIDEXAGM)\n    dd\n  }) |> \n  bind_rows(.id = \"file_demo\") |> \n  mutate(file_demo = path_file(file_demo)) |>\n  left_join(metadata$DEMO, by = \"file_demo\") |>\n  select(-description)\n\n# read body measurements file (selected variables only)\nbmxes <- bmx_files |> \n  map(\\(xx) {\n    dd <- read_xpt(xx) \n    dd <- select(dd, SEQN, BMXWT, BMXHT, BMXRECUM)\n    dd\n}) |> \n  bind_rows(.id = \"file_bmx\") |> \n  mutate(file_bmx = path_file(file_bmx)) |>\n  left_join(metadata$BMX, by = \"file_bmx\") |>\n  select(-description)\n```\n:::\n\n\n\nLooking at the code above, you can see that I haven't included *all* the columns from the BMX and DEMO tables, only the ones that are most relevant to my purposes. My `demos` and `bmxes` data frames are considerably smaller than they would be if I included everything. Importantly for our purposes the `SEQN` column serves as an id variable, and we can use it to join the tables. In the code below I'm using `left_join(bmxes, demos)` to do the work because I'm only really interested in those cases where the body measurement data exists, and I'm tidying the column names a little for the sake of my sanity: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes <- bmxes |>\n  left_join(demos, by = c(\"SEQN\", \"cohort\")) |>\n  select(\n    id          = SEQN,\n    sex_s       = RIAGENDR, # sex/gender at screen (1 = M, 2 = F, . = NA)\n    weight_kg_e = BMXWT,    # weight at exam\n    height_cm_e = BMXHT,    # standing height at exam\n    length_cm_e = BMXRECUM, # recumbent length at exam (0-47 months only)\n    age_yr_s    = RIDAGEYR, # natal age at screening (years)\n    age_mn_s    = RIDAGEMN, # natal age at screening (months; 0-24 mos only)\n    age_mn_e    = RIDEXAGM, # natal age at exam (months; 0-19 years only)\n    cohort\n  )\n\n nhanes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 119,926 × 9\n      id sex_s weight_kg_e height_cm_e length_cm_e age_yr_s age_mn_s\n   <dbl> <dbl>       <dbl>       <dbl>       <dbl>    <dbl>    <dbl>\n 1     1     2        12.5        91.6        93.2        2       29\n 2     2     1        75.4       174          NA         77      926\n 3     3     2        32.9       137.         NA         10      125\n 4     4     1        13.3        NA          87.1        1       22\n 5     5     1        92.5       178.         NA         49      597\n 6     6     2        59.2       162          NA         19      230\n 7     7     2        78         163.         NA         59      712\n 8     8     1        40.7       162          NA         13      159\n 9     9     2        45.5       157.         NA         11      133\n10    10     1       112.        190.         NA         43      518\n# ℹ 119,916 more rows\n# ℹ 2 more variables: age_mn_e <dbl>, cohort <chr>\n```\n\n\n:::\n:::\n\n\n\nVery nice indeed. \n\n### Preprocessing choices\n\nEven with the small snippet shown above you can see hints of nuances that arise when working with the NHANES data, especially if pediatric age ranges are of interest (as they very often are for me). Suppose one of the variables of interest in your analysis is height. On row 4 we have data from a 22 month old boy: there is no height measurement for him. That's very often the case for young infants, most obviously because very young infants can't stand up so you can't measure standing height. What you *can* do for those infants is take a related measurement: recumbent length. Okay that makes sense: you measure height for adults, and length for babies. Except... there are some kids in the age range where it makes sense to take *both* measurements. Row 1 contains data from a 29 month old girl, who was measured as 91.6cm in height and... 93.2cm in length. \n\nAh.\n\nRight. The height of a standing human and the length of the same human lying down don't have to be the same, and the differences between the two won't be random measurement error, because gravity and posture are real things that exist! The NHANES data itself illustrates that:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nok <- function(x) !is.na(x)\nnhanes |> \n  filter(ok(height_cm_e), ok(length_cm_e)) |> \n  summarise(\n    mean_diff = mean(length_cm_e - height_cm_e),\n    sd_diff = sd(length_cm_e - height_cm_e)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  mean_diff sd_diff\n      <dbl>   <dbl>\n1      1.06   0.998\n```\n\n\n:::\n:::\n\n\n\nSo, when considering what *height* to use for the boy in row 4, it's not necessarily a good idea to substitute his *length*, because those probably aren't the same. An imputation procedure of some kind is needed. I won't go into it in this post, but I played around with a few possible models for predicting length from height in the process of writing it, and in the end concluded that nothing fancy is needed here. A simple adjustment will suffice: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength_to_height <- function(length_cm) {\n  length_cm - 1.06  # adjust based on average difference in NHANES\n}\n```\n:::\n\n\n\nIt feels a bit silly writing a function for this conversion, but it's helpful from a documentation purpose: creating a dummy function to use (rather than just subtracting 1.06 later in the code) makes it clear to the reader that I *am* using some kind of explicit conversion when I transform length to height, and the comment provides a little extra detail on how I chose the conversion rule.\n\nSimilar nuances for other measurements exist. There are three different measurements that record age, and they don't have to be in agreement: the *survey* takes place at a different point in time to the *exam*, so it's possible that the (now relabelled) `age_mn_e` and `age_mn_s` variables are different. Age in years at the time of the survey `age_yr_s` should presumably be consistent with the age in months at the time of the survey, but data aren't always straightforward. Given all this, the preprocessing steps used to create measurements of age and height look like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes <- nhanes |>\n  mutate(\n    sex_num = sex_s - 1, # rescale to 0 = M, 1 = F\n    sex_fct = factor(sex_s, levels = 1:2, labels = c(\"male\", \"female\")),\n    age_mn = case_when(\n      !is.na(age_mn_e) ~ age_mn_e, # use exam months if present\n      !is.na(age_mn_s) ~ age_mn_s, # else use survey months\n      TRUE ~ (age_yr_s * 12)       # else use age in years\n    ),\n    age_yr    = age_mn / 12,\n    weight_kg = weight_kg_e,\n    height_cm = case_when(\n      !is.na(height_cm_e) ~ height_cm_e, # use height if it was measured\n      !is.na(length_cm_e) ~ length_to_height(length_cm_e), # or convert length\n      TRUE ~ NA_real_, # else missing\n    )\n  )\n```\n:::\n\n\n\nThe NHANES data set doesn't include information about transgender status or intersex status (at least not in the demographics and body measurements tables), and so for the purposes of NHANES-based analyses sex is a treated as binary variable, and doesn't require any extra steps beyond tidying and attaching nice labels. A similar story holds for weight: we have one measurement, the weight at time of exam, and that's what we'll use. \n\nAt this point the unwary might fall into the trap of thinking that we're done, but -- alas -- we aren't quite there yet. There's one more detail to consider regarding the age data. In the NHANES data set the age values stop at 80, but that value doesn't literally mean \"80 years\" it means \"80 years or older\". Because of that, you need to take some care in how you work with data for older participants. As it happens, I've never had to deal with this in the wild because my projects have used NHANES data in the pediatric setting. Given that, I'll keep it simple here and just drop all cases with age recorded as \"80+\" years. Not a problem if your application is pediatric, but you wouldn't want to do this in situations where you're interested in older populations. \n\nIn any case, having noted this I can finally complete the preprocessing and arrive at my `nhanes` data frame:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes <- nhanes |>\n  select(id, sex_num, sex_fct, weight_kg, height_cm, age_mn, age_yr, cohort) |>\n  filter(ok(sex_num), ok(weight_kg), ok(height_cm), ok(age_mn)) |>\n  filter(age_yr < 80)\n\nnhanes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 113,319 × 8\n      id sex_num sex_fct weight_kg height_cm age_mn age_yr cohort   \n   <dbl>   <dbl> <fct>       <dbl>     <dbl>  <dbl>  <dbl> <chr>    \n 1     1       1 female       12.5      91.6     29   2.42 1999-2000\n 2     2       0 male         75.4     174      926  77.2  1999-2000\n 3     3       1 female       32.9     137.     125  10.4  1999-2000\n 4     4       0 male         13.3      86.0     22   1.83 1999-2000\n 5     5       0 male         92.5     178.     597  49.8  1999-2000\n 6     6       1 female       59.2     162      230  19.2  1999-2000\n 7     7       1 female       78       163.     712  59.3  1999-2000\n 8     8       0 male         40.7     162      159  13.2  1999-2000\n 9     9       1 female       45.5     157.     133  11.1  1999-2000\n10    10       0 male        112.      190.     518  43.2  1999-2000\n# ℹ 113,309 more rows\n```\n\n\n:::\n:::\n\n\n\nA small slice through a very large data set, but one that contains the four variables that I need almost every time I have to use NHANES data in a pharmacometric analysis: `age_mn` ($a$), `sex_fct` ($s$), `height_cm` ($h$), and `weight_kg` ($w$). I can now turn to the substantive statistical problem: estimating the joint conditional density $p(h, w | a, s)$.\n\n## The GAMLSS framework\n\n### Structural model\n\nLet $\\tilde{y}_i = E[y_i|x_{i1}, \\ldots, x_{ip}]$ denote the expected value of (the $i$-th observation of) the outcome variable $y$, conditioned on knowing the values of $p$ predictor variables $x_{1}, \\ldots, x_{p}$. In linear regression we propose that\n\n$$\n\\tilde{y}_i = \\beta_0 + \\sum_{k=1}^p \\beta_k \\ x_{ik}\n$$\n\nOften we would rewrite this in matrix notation and express it as $\\tilde{\\mathbf{y}} = \\mathbf{X} \\mathbf{\\beta}$ to make it look pretty, but I honestly don't think it adds much in this context. There are two different paths you could pursue when extending this framework: you could change something on the left hand side, or you could change something on the right hand side. For example, the [generalised linear model](https://en.wikipedia.org/wiki/Generalized_linear_model) introduced by [Nelder and Wedderburn (1972)](https://www.jstor.org/stable/2344614) modifies the linear model by supplying a \"link function\" $g()$ that transforms $\\tilde{y}$, \n\n$$\ng(\\tilde{y}_i) = \\beta_0 + \\sum_{k=1}^p \\beta_k \\ x_{ik}\n$$\n\nCrudely put, the GLM applies a linear model to a transformed **outcome** variable (i.e., $y$). On the other hand, in the [additive model](https://en.wikipedia.org/wiki/Additive_model) proposed by [Friedman and Stuetzle (1981)](https://www.tandfonline.com/doi/abs/10.1080/01621459.1981.10477729), transformation functions $f()$ -- sometimes called \"smoothing functions\" -- are applied to the **predictor** variables (i.e., $x$):\n\n$$\n\\tilde{y}_i = \\beta_0 + \\sum_{k=1}^p f_k(x_{ik})\n$$\n\nI'll talk more about the choice of smoothing function $f()$ later in the post, but for now the key thing to note is that the functions used for this purpose usually have free parameters that need to be estimated from the data. Because of this, fitting an additive model is a little more complicated than a simple \"transform the predictors first and then fit a linear regression later\" procedure. \n\nIn any case, having noted that **generalised** linear models introduce the link function $g()$ and **additive** models introduce the smoothing functions $f()$, it seems natural to consider a modelling framework that uses both of these things. That framework exists: it is called the [generalised additive model](https://en.wikipedia.org/wiki/Generalized_additive_model) (GAM), introduced by [Hastie and Tibshirani (1990)](https://www.jstor.org/stable/2245459):\n\n$$\ng(\\tilde{y}_i) = \\beta_0 + \\sum_{k=1}^p f_k(x_{ik})\n$$\n\n### Measurement model\n\nIn linear model with homogeneity of variance, we normally write this:\n\n$$\ny_i = \\tilde{y}_i + \\epsilon_i\n$$\n\nwhere the measurement error terms $\\epsilon_i$ are assumed to be normally distributed with mean fixed at 0 and standard deviation $\\sigma$ estimated from the data. Another way to express the same idea would be to say that the outcome variable $y_i$ is normally distributed with mean $\\mu_i = \\tilde{y}_i$, and a constant standard deviation $\\sigma_i = \\sigma$. Taking this line of reasoning a little further, we could rewrite the model in terms of *two* regression models: a linear model for the mean $\\mu$, and another linear model for the standard deviation $\\sigma$:\n\n$$\n\\begin{array}{rcl}\n\\mu_i &=& \\beta_{\\mu 0} + \\sum_k \\beta_{\\mu k} \\ x_{ik} \\\\\n\\sigma_i &=& \\beta_{\\sigma 0} \\\\ \\\\\ny_i &\\sim& \\mbox{Normal}(\\mu_i, \\sigma_i)\n\\end{array}\n$$\n\nThe only way in which this framing of the model differs from the usual form is that I've used $\\beta_{\\sigma 0}$ to refer to the to-be-estimated $\\sigma$ parameter. The difference is purely notational, but it reflects a slight shift in mindset: it signals that we're now considering the possibility that we *could* have chosen to develop a richer regression model that allows each observation to have its $\\sigma_i$, rather than use an \"intercept only\" model for the variance. Indeed when expressed this way, the \"homogeneity of variance\" assumption in linear regression now corresponds to the special case in which no covariates (predictors) are included in the model for $\\sigma$. Moreover, it makes clear that there's no inherent reason to limit ourselves in this way. Relaxing homogeneity of variance allows us to specify regression models for both $\\mu$ and $\\sigma$, giving us the following framework for linear regression:\n\n$$\n\\begin{array}{rcl}\n\\mu_i &=& \\beta_{\\mu 0} + \\sum_k \\beta_{\\mu k} \\ x_{ik} \\\\\n\\sigma_i &=& \\beta_{\\sigma 0}  + \\sum_k \\beta_{\\sigma k} \\ x_{ik} \\\\ \\\\\ny_i &\\sim& \\mbox{Normal}(\\mu_i, \\sigma_i)\n\\end{array}\n$$\n\nHaving made this conceptual shift, it's not too hard to see that we can repeat the line of reasoning from the previous section that took us from linear models to generalised additive models. To wit, if we replace the regression coefficients $\\beta_{\\mu k}$ and $\\beta_{\\sigma k}$ with smooth functions $f_{\\mu k}()$ and $f_{\\sigma k}()$ and use these to transform the predictors, we have an additive regression model for the mean and the standard deviation...\n\n$$\n\\begin{array}{rcl}\n\\mu_i &=& \\beta_{\\mu 0} + \\sum_k f_{\\mu k}(x_{ik}) \\\\\n\\sigma_i &=& \\beta_{\\sigma 0} + \\sum_k f_{\\sigma k}(x_{ik}) \\\\ \\\\\ny_i &\\sim& \\mbox{Normal}(\\mu_i, \\sigma_i)\n\\end{array}\n$$\n\nAdding link functions $g()$ to connect the additive predictor to the parameters gives a generalised additive regression on the mean and the standard deviation...\n\n$$\n\\begin{array}{rcl}\ng_\\mu(\\mu_i) &=& \\beta_{\\mu 0} + \\sum_k f_{\\mu k}(x_{ik}) \\\\\ng_\\sigma(\\sigma_i) &=& \\beta_{\\sigma 0} + \\sum_k f_{\\sigma k}(x_{ik}) \\\\ \\\\\ny_i &\\sim& \\mbox{Normal}(\\mu_i, \\sigma_i)\n\\end{array}\n$$\n\nFinally, we recognise that the normal distribution is not the only choice of measurement model. Like many other distributions it can be described as a distribution that contains one parameter for the **location** (i.e., $\\mu$) and **scale** (i.e., $\\sigma$). More generally though, we might want to use distributions described by one parameter for location, one parameter for scale, and one or more parameters for the **shape**. If we instead choose the [Box-Cox power exponential](/posts/2025-08-02_box-cox-power-exponential/) (BCPE) -- in which $\\mu$ is a location parameter, $\\sigma$ is a scale parameter, and $\\nu$ and $\\tau$ together control the shape -- the statistical model now looks like this...\n\n$$\n\\begin{array}{rcl}\ng_\\mu(\\mu_i) &=& \\beta_{\\mu 0} + \\sum_k f_{\\mu k}(x_{ik}) \\\\\ng_\\sigma(\\sigma_i) &=& \\beta_{\\sigma 0} + \\sum_k f_{\\sigma k}(x_{ik}) \\\\\ng_\\nu(\\nu_i) &=& \\beta_{\\nu 0} + \\sum_k f_{\\nu k}(x_{ik}) \\\\\ng_\\tau(\\tau_i) &=& \\beta_{\\tau 0} + \\sum_k f_{\\tau k}(x_{ik}) \\\\ \\\\\ny_i &\\sim& \\mbox{BCPE}(\\mu_i, \\sigma_i, \\nu_i, \\tau_i)\n\\end{array}\n$$\n\nand we now have an example of a **generalised additive model for location, scale, and shape** (GAMLSS), noting of course you don't have to use the BCPE specifically. The key requirement here is that the distributional family be flexible enough that you can separately model location, scale, and shape. There are a lot of distributions that satisfy this property, though for the purposes of this post I am just going to stick with BCPE.^[One thing I will note as an aside, however, is that this stipulation is one of the key ways in which GAMLSS models extend GAM regressions and GLM analyses. In the GAM and GLM frameworks, the probabilistic component is assumed to be a distribution in the [exponential family](https://en.wikipedia.org/wiki/Exponential_family). The GAMLSS framework relaxes that considerably, insofar as location/scale/shape encompasses many distributions that fall outside the exponential family.]\n\n### Model fitting\n\nOkay, so that's the basic idea. How do we go about building a model? It's not entirely straightforward: because the GAMLSS framework is so flexible, there are a lot of different possibilities. My approach to the problem was heavily influenced by the [nhanesgamlss](https://github.com/smouksassi/nhanesgamlss) R package developed by my former colleague, the always-wonderful Samer Mouksassi. In fact, the first few times I fit a GAMLSS model all I did was use the `nhanesgamlss::simwtage()` function his package provides, and to be perfectly honest I probably wouldn't have bothered to dive any deeper than this except for the fact that twice this year I've collided with a real world modelling problem that needed something that Samer hadn't already solved for me! Specifically, the **nhanesgamlss** package is concerned with estimating and sampling from the distribution of weight conditional on age and sex, $p(w | a, s)$ and unfortunately for me, the problems I've worked on lately have needed me to sample height *and* weight. The density I need is $p(h, w | a, s)$. The NHANES data set and the GAMLSS modelling framework are both well suited to estimating this density, of course, but it's a different density to the one that `nhanesgamlss::simwtage()` is designed for. And so it was with a great deal of sadness I discovered that this time I would have to do my own work and not rely on Samer to have already done it for me. Curses. \n\nThe approach I've taken when tackling this problem is split it into two distinct parts. Rather than try to build a multivariate model with $(h, w)$ as the outcome vector, I use exchangeability to factorise the joint density as follows:\n\n$$\np(h, w | a, s) = p(w | h, a, s) p(h | a, s)\n$$\n\nMore precisely, the goal is to fit four GAMLSS models one for each of the following densities:\n\n$$\n\\begin{array}{l}\np(h | a, s = \\mbox{male}) \\\\\np(h | a, s = \\mbox{female}) \\\\\np(w | h, a, s = \\mbox{male}) \\\\\np(w | h, a, s = \\mbox{female})\n\\end{array}\n$$\n\nTo get started, there are a few tiresome preliminaries to take care of. First, noting that the age range of most interest in the problems I've worked on is the pediatric range, I made a pragmatic decision to restrict the training data to participants aged 40 years or less, and split the NHANES data into two data sets: `nhanes_m` contains data for male participants, and `nhanes_f` contains data from female participants.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage_max_yr <- 40\nnhanes_m <- nhanes |> filter(sex_fct == \"male\", age_yr <= age_max_yr)\nnhanes_f <- nhanes |> filter(sex_fct == \"female\", age_yr <= age_max_yr)\n```\n:::\n\n\n\nSecond, noting that I'm about to use the [gamlss](https://gamlss.org/) R package to do the estimation, I'll define some settings for the optimisation routine using `gamlss::gamlss.control()`. As it happens, these settings turned out not to matter much for the models I ended up building, but during the process I'd tried out a few different settings and decided to stick with these:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt_control <- gamlss.control(c.crit = .001, n.cyc = 250)\n```\n:::\n\n\n\nLet's start with the simpler models. When estimating $p(h | a, s = \\mbox{male})$ and $p(h | a, s = \\mbox{female})$ there is only a single predictor that varies within the data set (i,e., age), so we don't have to think about headache inducing questions like what interaction terms to include. Here's the code I used to fit these two models:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nht_m <- gamlss(\n  formula       = height_cm ~ pb(age_mn),\n  sigma.formula = ~pb(age_mn),\n  nu.formula    = ~1,\n  tau.formula   = ~1,\n  data    = nhanes_m,\n  family  = BCPE,\n  control = opt_control\n)\n\nht_f <- gamlss(\n  formula       = height_cm ~ pb(age_mn),\n  sigma.formula = ~pb(age_mn),\n  nu.formula    = ~1,\n  tau.formula   = ~1,\n  data    = nhanes_f,\n  family  = BCPE,\n  control = opt_control\n)\n```\n:::\n\n\n\nFor the post part, the code here is fairly straightforward. I've chosen to use the Box-Cox power exponential distribution as my measurement model, so I've set `family = BCPE`. Calling `BCPE()` returns a \"gamlss.family\" object that does two things: it indicates that the Box-Cox power exponential distribution should be used, and it specifies the link functions to be used. Under the default settings for `BCPE()`, the link functions $g_{\\mu}()$ and $g_{\\nu}()$ are the identity-link, whereas $g_{\\sigma}()$ and $g_{\\tau}()$ are both log-link:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nBCPE()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nGAMLSS Family: BCPE Box-Cox Power Exponential \nLink function for mu   : identity \nLink function for sigma: log \nLink function for nu   : identity \nLink function for tau  : log \n```\n\n\n:::\n:::\n\n\n\nSince we're using the BCPE distribution, we need to pass four formula arguments, one for each parameter. After trying out a few different possibilities, I decided to keep things simple for the shape parameters. For $\\nu$ and $\\tau$ I specified an intercept-only model. The `nu.formula` and `tau.formula` arguments both accept a one-sided formula, and in this case I've gone with the simplest possibility:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnu.formula   = ~1\ntau.formula  = ~1\n```\n:::\n\n\n\nIn other words, while the location and scale of the height distribution can change over the lifespan, the model assumes the shape of the distribution remains the same. Straightforward enough. The formulas for the location parameter $\\mu$ and the scale parameter $\\sigma$ are a little more complex. As with `nu.formula` and `tau.formula`, the `sigma.formula` argument accepts a one-sided formula. Because we're expecting the variability of height^[More precisely, for the BCPE distribution the scale parameter $\\sigma$ roughly maps onto the coefficient of variation.] to change as age increases, `age_mn` is included as a predictor here. Similarly, it's pretty obvious that we'd expect the average height to change with age, I'll also include `age_mn` as a predictor for $\\mu$. The code looks like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformula       = height_cm ~ pb(age_mn),\nsigma.formula = ~pb(age_mn),\n```\n:::\n\n\n\nNote that the `formula` argument takes a two-sided formula as its argument: the outcome variable `height_cm` is passed on the left hand side of the formula. \n\nOkay, yes Danielle that's all well and good, but what's the story with the `pb()` function? As you've probably guessed, this is where we specify the smoothing functions $f()$ in the additive model. The `pb()` function is used specify P-spline smoothing ([Eilers & Marx, 1996](https://www.jstor.org/stable/2246049), [Eilers, Marx & Durbán 2016](https://www.researchgate.net/publication/290086196_Twenty_years_of_P-splines))\n\n\nWhen this code is executed, the output looks something like this:\n\n```\nGAMLSS-RS iteration 1: Global Deviance = 251100.2 \nGAMLSS-RS iteration 2: Global Deviance = 245961 \nGAMLSS-RS iteration 3: Global Deviance = 245664.8 \nGAMLSS-RS iteration 4: Global Deviance = 245645 \nGAMLSS-RS iteration 5: Global Deviance = 245643.9 \nGAMLSS-RS iteration 6: Global Deviance = 245643.9 \nGAMLSS-RS iteration 7: Global Deviance = 245643.9 \nGAMLSS-RS iteration 8: Global Deviance = 245643.8 \nGAMLSS-RS iteration 9: Global Deviance = 245643.8 \n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwt_htm <- gamlss(\n  formula       = weight_kg ~ pb(age_mn) + height_cm + pb(age_mn):height_cm,\n  sigma.formula = ~pb(age_mn),\n  nu.formula    = ~1,\n  tau.formula   = ~1,\n  data    = nhanes_m,\n  family  = BCPE,\n  control = opt_control\n)\n\nwt_htf <- gamlss(\n  formula       = weight_kg ~ pb(age_mn) + height_cm + pb(age_mn):height_cm,\n  sigma.formula = ~pb(age_mn),\n  nu.formula    = ~1,\n  tau.formula   = ~1,\n  data    = nhanes_f,\n  family  = BCPE,\n  control = opt_control\n)\n```\n:::\n\n\n```{.r .cell-code}\nht_m   <- readRDS(path(post_dir, \"nhanes\", \"output\", \"ht-m.rds\"))\nht_f   <- readRDS(path(post_dir, \"nhanes\", \"output\", \"ht-f.rds\"))\nwt_htm <- readRDS(path(post_dir, \"nhanes\", \"output\", \"wt-htm.rds\"))\nwt_htf <- readRDS(path(post_dir, \"nhanes\", \"output\", \"wt-htf.rds\"))\n\nmod <- list(\n  ht_m = ht_m,\n  ht_f = ht_f,\n  wt_htm = wt_htm,\n  wt_htf = wt_htf\n)\n\nquarto_tabset(mod, level = 3)\n```\n\n\n\n::: {.panel-tabset}\n\n \n\n\n### ht_m\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:   \ngamlss(formula = height_cm ~ pb(age_mn), sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = BCPE,   \n    data = nhanes_m, control = opt_control)  \n \nMu Coefficients: \n(Intercept)   pb(age_mn)   \n    91.8664       0.2703   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n -3.087e+00   -1.098e-05   \nNu Coefficients: \n(Intercept)   \n     0.7608   \nTau Coefficients: \n(Intercept)   \n      0.587   \n \n Degrees of Freedom for the fit: 40.41 Residual Deg. of Freedom   36764  \nGlobal Deviance:     239756  \n            AIC:     239837  \n            SBC:     240181  \n</pre> \n\n\n### ht_f\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:   \ngamlss(formula = height_cm ~ pb(age_mn), sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = BCPE,   \n    data = nhanes_f, control = opt_control)  \n \nMu Coefficients: \n(Intercept)   pb(age_mn)   \n    93.4863       0.2205   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n -3.0928135   -0.0001298   \nNu Coefficients: \n(Intercept)   \n     0.5832   \nTau Coefficients: \n(Intercept)   \n      0.608   \n \n Degrees of Freedom for the fit: 39.65 Residual Deg. of Freedom   37603  \nGlobal Deviance:     240867  \n            AIC:     240946  \n            SBC:     241285  \n</pre> \n\n\n### wt_htm\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:  gamlss(formula = weight_kg ~ pb(age_mn) + height_cm +   \n    pb(age_mn):height_cm, sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = gamlss.dist::BCPE,   \n    data = nhanes_m, control = opt_control)  \n \nMu Coefficients: \n         (Intercept)            pb(age_mn)             height_cm   \n          -10.526575             -0.326806              0.268436   \npb(age_mn):height_cm   \n            0.002829   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n  -2.156377     0.001858   \nNu Coefficients: \n(Intercept)   \n     -1.217   \nTau Coefficients: \n(Intercept)   \n     0.5824   \n \n Degrees of Freedom for the fit: 37.53 Residual Deg. of Freedom   36766  \nGlobal Deviance:     245644  \n            AIC:     245719  \n            SBC:     246038  \n</pre> \n\n\n### wt_htf\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:  gamlss(formula = weight_kg ~ pb(age_mn) + height_cm +   \n    pb(age_mn):height_cm, sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = BCPE,   \n    data = nhanes_f, control = opt_control)  \n \nMu Coefficients: \n         (Intercept)            pb(age_mn)             height_cm   \n          -10.642828             -0.271211              0.257454   \npb(age_mn):height_cm   \n            0.002598   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n  -2.129442     0.002198   \nNu Coefficients: \n(Intercept)   \n     -1.009   \nTau Coefficients: \n(Intercept)   \n     0.7359   \n \n Degrees of Freedom for the fit: 37.29 Residual Deg. of Freedom   37606  \nGlobal Deviance:     256606  \n            AIC:     256680  \n            SBC:     256998  \n</pre> \n\n\n::: \n\n \n\n\n\n## Using the GAMLSS models\n\n### Quantiles\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_pars <- function(data, model) {\n  pars <- tibble(\n    mu    = predict(model, newdata = data, type = \"response\", what = \"mu\"),\n    sigma = predict(model, newdata = data, type = \"response\", what = \"sigma\"),\n    nu    = predict(model, newdata = data, type = \"response\", what = \"nu\"),\n    tau   = predict(model, newdata = data, type = \"response\", what = \"tau\"),\n  )\n  bind_cols(data, pars)\n}\n\npredict_cases_ht <- expand_grid(\n  age_mn  = 1:(age_max_yr * 12),\n  sex_fct = factor(c(\"male\", \"female\"))\n)\n\npredict_pars_ht <- bind_rows(\n  predict_cases_ht |> filter(sex_fct == \"male\") |> get_pars(ht_m),\n  predict_cases_ht |> filter(sex_fct == \"female\") |> get_pars(ht_f)\n)\n\npredict_quantiles_ht <- predict_pars_ht |>\n  mutate(\n    q05 = qBCPE(.05, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q25 = qBCPE(.25, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q50 = qBCPE(.50, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q75 = qBCPE(.75, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q95 = qBCPE(.95, mu = mu, sigma = sigma, nu = nu, tau = tau)\n  )\n\npredict_quantiles_ht\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 960 × 11\n   age_mn sex_fct    mu  sigma    nu   tau   q05   q25   q50   q75   q95\n    <int> <fct>   <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1      1 male     56.8 0.0470 0.761  1.80  52.5  55.1  56.8  58.6  61.3\n 2      2 male     59.1 0.0465 0.761  1.80  54.6  57.3  59.1  60.9  63.7\n 3      3 male     61.3 0.0459 0.761  1.80  56.7  59.5  61.3  63.1  66.0\n 4      4 male     63.3 0.0454 0.761  1.80  58.6  61.5  63.3  65.2  68.1\n 5      5 male     65.3 0.0449 0.761  1.80  60.5  63.4  65.3  67.2  70.1\n 6      6 male     67.1 0.0445 0.761  1.80  62.2  65.1  67.1  69.0  72.0\n 7      7 male     68.8 0.0441 0.761  1.80  63.8  66.8  68.8  70.7  73.8\n 8      8 male     70.3 0.0437 0.761  1.80  65.3  68.4  70.3  72.3  75.4\n 9      9 male     71.8 0.0433 0.761  1.80  66.8  69.8  71.8  73.9  77.0\n10     10 male     73.2 0.0429 0.761  1.80  68.1  71.2  73.2  75.3  78.5\n# ℹ 950 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_point(\n    data = nhanes |> filter(age_yr < age_max_yr), \n    mapping = aes(age_mn, height_cm),\n    size = .25\n  ) +\n  geom_path(\n    data = predict_quantiles_ht |> \n      pivot_longer(\n        cols = starts_with(\"q\"),\n        names_to = \"quantile\",\n        values_to = \"height_cm\"\n      ),\n    mapping = aes(age_mn, height_cm, color = quantile)\n  ) +\n  facet_wrap(~sex_fct) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/height-quantiles-plot-1.png){width=768}\n:::\n:::\n\n\n\nOkay, that's nice but you might argue that the GAMLSS modelling is overkill here. The distribution of heights conditional on age and sex is fairly close to normal, and in any case we have about 100k rows in the data set spanning the full range of ages. However, it's not too hard to construct a case where you really do need the model.\n\nWe can take the logic further and predict the distribution of weights for very tall (99th height percentile) and short (1st height percentile) people, stratified by age and sex. It's a nice illustration of where the GAMLSS framework is useful even when you have a lot of data: as good as it is, the NHANES data become very sparse when you start focusing on very thin slices through the tails of the distributions. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict_cases_wt <- predict_pars_ht |>\n  mutate(\n    very_tall  = qBCPE(.99, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    very_short = qBCPE(.01, mu = mu, sigma = sigma, nu = nu, tau = tau)\n  ) |> \n  pivot_longer(\n    cols = c(very_tall, very_short),\n    names_to = \"height_fct\", \n    values_to = \"height_cm\"\n  ) |> \n  mutate(height_fct = factor(height_fct)) |> \n  select(age_mn, sex_fct, height_fct, height_cm)\n\npredict_cases_wt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,920 × 4\n   age_mn sex_fct height_fct height_cm\n    <int> <fct>   <fct>          <dbl>\n 1      1 male    very_tall       63.3\n 2      1 male    very_short      50.5\n 3      2 male    very_tall       65.8\n 4      2 male    very_short      52.7\n 5      3 male    very_tall       68.1\n 6      3 male    very_short      54.7\n 7      4 male    very_tall       70.3\n 8      4 male    very_short      56.6\n 9      5 male    very_tall       72.3\n10      5 male    very_short      58.4\n# ℹ 1,910 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\npredict_pars_wt <- bind_rows(\n  predict_cases_wt |> filter(sex_fct == \"male\") |> select(-height_fct) |> get_pars(wt_htm),\n  predict_cases_wt |> filter(sex_fct == \"female\") |> select(-height_fct) |> get_pars(wt_htf)\n) |> \n  left_join(predict_cases_wt, by = join_by(age_mn, sex_fct, height_cm)) |> \n  relocate(height_fct, .before = height_cm)\n\npredict_quantiles_wt <- predict_pars_wt |>\n  mutate(\n    q05 = qBCPE(.05, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q25 = qBCPE(.25, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q50 = qBCPE(.50, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q75 = qBCPE(.75, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q95 = qBCPE(.95, mu = mu, sigma = sigma, nu = nu, tau = tau)\n  )\n\npredict_quantiles_wt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,920 × 13\n   age_mn sex_fct height_fct height_cm    mu  sigma    nu   tau   q05   q25\n    <int> <fct>   <fct>          <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n 1      1 male    very_tall       63.3  7.29 0.101  -1.22  1.79  6.26  6.84\n 2      1 male    very_short      50.5  3.83 0.101  -1.22  1.79  3.29  3.59\n 3      2 male    very_tall       65.8  7.99 0.100  -1.22  1.79  6.87  7.51\n 4      2 male    very_short      52.7  4.39 0.100  -1.22  1.79  3.78  4.13\n 5      3 male    very_tall       68.1  8.64 0.0987 -1.22  1.79  7.45  8.13\n 6      3 male    very_short      54.7  4.92 0.0987 -1.22  1.79  4.24  4.63\n 7      4 male    very_tall       70.3  9.25 0.0975 -1.22  1.79  7.99  8.70\n 8      4 male    very_short      56.6  5.41 0.0975 -1.22  1.79  4.67  5.09\n 9      5 male    very_tall       72.3  9.82 0.0963 -1.22  1.79  8.49  9.24\n10      5 male    very_short      58.4  5.86 0.0963 -1.22  1.79  5.07  5.52\n# ℹ 1,910 more rows\n# ℹ 3 more variables: q50 <dbl>, q75 <dbl>, q95 <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npermitted <- predict_cases_wt |> \n  mutate(\n    height_cm_lo = height_cm * 0.99, \n    height_cm_hi = height_cm * 1.01\n  ) |> \n  select(age_mn, sex_fct, height_fct, height_cm_lo, height_cm_hi)\n\nmatch_rules <- join_by(\n  x$sex_fct == y$sex_fct, \n  x$age_mn == y$age_mn, \n  x$height_cm > y$height_cm_lo, \n  x$height_cm < y$height_cm_hi\n)\n\nnhanes_very_short <- semi_join(\n  x = nhanes, \n  y = permitted |> filter(height_fct == \"very_short\"),\n  by = match_rules\n)\n\nnhanes_very_tall <- semi_join(\n  x = nhanes, \n  y = permitted |> filter(height_fct == \"very_tall\"),\n  by = match_rules\n)\n\nnhanes_partial <- bind_rows(\n  very_short = nhanes_very_short,\n  very_tall = nhanes_very_tall,\n  .id = \"height_fct\"\n) |> mutate(height_fct = factor(height_fct))\n\nnhanes_partial\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1,513 × 9\n   height_fct    id sex_num sex_fct weight_kg height_cm age_mn age_yr cohort\n   <fct>      <dbl>   <dbl> <fct>       <dbl>     <dbl>  <dbl>  <dbl> <chr> \n 1 very_short   106       0 male         71.8      157.    479   39.9 1999-…\n 2 very_short   181       0 male         50.8      153.    177   14.8 1999-…\n 3 very_short   186       0 male         57.7      158.    220   18.3 1999-…\n 4 very_short   590       1 female       46.5      146.    186   15.5 1999-…\n 5 very_short   599       0 male         42.2      143.    162   13.5 1999-…\n 6 very_short   708       1 female       73.8      146.    313   26.1 1999-…\n 7 very_short   849       0 male         35.7      150.    172   14.3 1999-…\n 8 very_short   941       1 female       57.1      146.    262   21.8 1999-…\n 9 very_short  1012       0 male         38        141.    158   13.2 1999-…\n10 very_short  1075       0 male         54.6      156.    237   19.8 1999-…\n# ℹ 1,503 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_point(\n    data = nhanes_partial, \n    mapping = aes(age_mn, weight_kg),\n    size = .5\n  ) +\n  geom_path(\n    data = predict_quantiles_wt |> \n      pivot_longer(\n        cols = starts_with(\"q\"),\n        names_to = \"quantile\",\n        values_to = \"weight_kg\"\n      ),\n    mapping = aes(age_mn, weight_kg, color = quantile),\n    linewidth = 1\n  ) +\n  facet_grid(height_fct ~ sex_fct) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/weight-quantiles-plot-1.png){width=768}\n:::\n:::\n\n\n\nWorks better than I would have expected. Note that the slight \"clumpiness\" of the NHANES data is partially tied to the slight misfit in the model at ~12 years. The data flatten out slightly faster than the model predictions, which has the effect that the curves are slightly below the data at that specific age. So if we slice through the data based on model-predicted 99th percentile, there will be more actual 12 year olds selected than other age groups.\n\n### Sampling \n\nSample height conditional on `age`, using an appropriate (sec-specific) gamlss model `ht_mod`. Similarly, we can sample weight conditional on `age` and `height` using a model. Note the use of a trimmed Box-Cox power-exponential: even with nu/tau parameters, the BCPE has support on non-biological values in the tails creating physically impossible outliers in large simulations. The default trim is 0.25% on either side, as compared to 3% for `nhanesgamlss::simwtage()`. \n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_hw <- function(age_mn, sex_fct, mod) {\n\n  rTBCPE <- function(n, mu, sigma, nu, tau, trim = .0025) {\n    p <- runif(n, min = trim, max = 1 - trim)\n    if (any(mu <= 0)) mu[mu <= 0] <- 1E-6\n    if (any(sigma <= 0)) sigma[sigma <= 0] <- 1E-6\n    r <- qBCPE(p, mu = mu, sigma = sigma, nu = nu, tau = tau)\n    r\n  }\n  sample_h <- function(age_mn, ht_mod) {\n    tibble(age_mn) |> \n      get_pars(model = ht_mod) |> \n      select(mu, sigma, nu, tau) |> \n      pmap_dbl(\\(mu, sigma, nu, tau) rTBCPE(n = 1, mu, sigma, nu, tau))\n  }\n  sample_w <- function(age_mn, height_cm, wt_mod) {\n    tibble(age_mn, height_cm) |>\n      get_pars(model = wt_mod) |> \n      select(mu, sigma, nu, tau) |> \n      pmap_dbl(\\(mu, sigma, nu, tau) rTBCPE(n = 1, mu, sigma, nu, tau))\n  }\n\n  height_cm <- weight_kg <- numeric(length(age_mn))\n  mm <- sex_fct == \"male\"\n  ff <- sex_fct == \"female\"\n\n  if (any(mm)) height_cm[mm] <- sample_h(age_mn[mm], mod$ht_m)\n  if (any(ff)) height_cm[ff] <- sample_h(age_mn[ff], mod$ht_f)  \n  if (any(mm)) weight_kg[mm] <- sample_w(age_mn[mm], height_cm[mm], mod$wt_htm)\n  if (any(ff)) weight_kg[ff] <- sample_w(age_mn[ff], height_cm[ff], mod$wt_htf)\n  \n  tibble(age_mn, sex_fct, height_cm, weight_kg)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes_fit <- bind_rows(nhanes_m, nhanes_f) |> arrange(age_mn)\n\nage_band <- function(age_mn) {\n  factor(\n    x = case_when(\n      age_mn <= 12                 ~ 1,\n      age_mn >  12 & age_mn <=  24 ~ 2,\n      age_mn >  24 & age_mn <=  72 ~ 3,\n      age_mn >  72 & age_mn <= 144 ~ 4,\n      age_mn > 144 & age_mn <= 216 ~ 5,\n      age_mn > 216                 ~ 6\n    ),\n    levels = 1:6,\n    labels = c(\n      \"<1 year\",\n      \"1-2 years\",\n      \"2-6 years\",\n      \"6-12 years\",\n      \"12-18 years\",\n      \">18 years\"\n    )\n  )\n}\n\npop <- bind_rows(\n  gamlss = sample_hw(nhanes_fit$age_mn, nhanes_fit$sex_fct, mod = mod),\n  nhanes = nhanes_fit |> select(age_mn, sex_fct, height_cm, weight_kg),\n  .id = \"source\"\n) |> mutate(age_band_fct = age_band(age_mn))\n\npop\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 148,894 × 6\n   source age_mn sex_fct height_cm weight_kg age_band_fct\n   <chr>   <dbl> <fct>       <dbl>     <dbl> <fct>       \n 1 gamlss      0 male         55.3      5.87 <1 year     \n 2 gamlss      0 male         57.3      5.36 <1 year     \n 3 gamlss      0 male         56.3      4.86 <1 year     \n 4 gamlss      0 male         56.5      5.54 <1 year     \n 5 gamlss      0 male         54.1      5.03 <1 year     \n 6 gamlss      0 male         53.8      5.23 <1 year     \n 7 gamlss      0 male         56.6      4.89 <1 year     \n 8 gamlss      0 male         55.0      5.20 <1 year     \n 9 gamlss      0 male         55.4      4.98 <1 year     \n10 gamlss      0 male         53.4      4.38 <1 year     \n# ℹ 148,884 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npop |>\n  ggplot(aes(height_cm, weight_kg, color = sex_fct)) + \n  geom_point(size = .25, alpha = .5) +\n  facet_grid(age_band_fct ~ source, scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/height-weight-age-band-scatterplot-1.png){width=768}\n:::\n:::\n\n\n\nOnce again this is a neat example, but it's mostly a validation of the GAMLSS model: it shows us the model does allow us to sample from the joint conditional density $p(h, w | a, s)$. But by design the GAMLSS samples in `pop` are matched to the NHANES data on age and sex. Anything that we could do with the GAMLSS samples is something that we could have done with the NHANES samples directly. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncompute_bsa <- function(height, weight, method = \"dubois\") {\n\n  w <- weight # numeric (kg)\n  h <- height # numeric (cm)\n\n  # Du Bois D, Du Bois EF (Jun 1916). \"A formula to estimate the approximate\n  # surface area if height and weight be known\". Archives of Internal Medicine\n  # 17 (6): 863-71. PMID 2520314.\n  if (method == \"dubois\") return(0.007184 * w^0.425 * h^0.725)\n\n  # Mosteller RD. \"Simplified calculation of body-surface area\". N Engl J Med\n  # 1987; 317:1098. PMID 3657876.\n  if (method == \"mosteller\") return(0.016667 * w^0.5 * h^0.5)\n\n  # Haycock GB, Schwartz GJ, Wisotsky DH \"Geometric method for measuring body\n  # surface area: A height-weight formula validated in infants, children and\n  # adults\" J Pediatr 1978, 93:62-66.\n  if (method == \"haycock\") return(0.024265 * w^0.5378 * h^0.3964)\n\n  # Gehan EA, George SL, Cancer Chemother Rep 1970, 54:225-235\n  if (method == \"gehan\") return(0.0235 * w^0.51456 * h^0.42246)\n\n  # Boyd, Edith (1935). The Growth of the Surface Area of the Human Body.\n  # University of Minnesota. The Institute of Child Welfare, Monograph Series,\n  # No. x. London: Oxford University Press\n  if (method == \"boyd\") return(0.03330 * w^(0.6157 - 0.0188 * log10(w)) * h^0.3)\n\n  # Fujimoto S, Watanabe T, Sakamoto A, Yukawa K, Morimoto K. Studies on the\n  # physical surface area of Japanese. 18. Calculation formulae in three stages\n  # over all ages. Nippon Eiseigaku Zasshi 1968;5:443-50.\n  if (method == \"fujimoto\") return(0.008883 * w^0.444 * h^0.663)\n\n  rlang::abort(\"unknown BSA method\")\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbsa_cutoff <- tibble(\n  height_cm = 60:110,\n  dubois  = (0.5 / 0.007184 / (height_cm^0.725)) ^ (1/0.425),\n  haycock = (0.5 / 0.024265 / (height_cm^0.3964)) ^(1/0.5378)\n) |> \n  pivot_longer(\n    cols = c(dubois, haycock),\n    names_to = \"method\",\n    values_to = \"weight_kg\"\n  )\n\nbsa_pop <- pop |> \n  select(age_mn, age_band_fct, height_cm, weight_kg, sex_fct, source) |>\n  filter(age_mn <= 72) |>\n  mutate(\n    bsa_m2_dubois  = compute_bsa(height_cm, weight_kg, method = \"dubois\"),\n    bsa_m2_haycock = compute_bsa(height_cm, weight_kg, method = \"haycock\")\n  ) |>\n  filter(bsa_m2_dubois > 0.5) \n \nggplot() + \n  geom_point(\n    data = bsa_pop,\n    mapping = aes(height_cm, weight_kg), \n    size = .25\n  ) +\n  geom_path(\n    data = bsa_cutoff,\n    mapping = aes(height_cm, weight_kg, linetype = method)\n  ) +\n  facet_grid(age_band_fct ~ source, scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/height-weight-bsa-scatterplot-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncases <- expand_grid(\n  age_mn  = 1:24,\n  sex_fct = factor(c(\"male\", \"female\"))\n)\nbsa_pop_2 <- sample_hw(\n  age_mn  = rep(cases$age_mn, 10000),\n  sex_fct = rep(cases$sex_fct, 10000),\n  mod = mod \n) |> \n  mutate(\n    id = row_number(),\n    bsa_m2_dubois  = compute_bsa(height_cm, weight_kg, method = \"dubois\"),\n    bsa_m2_haycock = compute_bsa(height_cm, weight_kg, method = \"haycock\"),\n    threshold = case_when(\n      bsa_m2_dubois >  .5 & bsa_m2_haycock >  .5 ~ \"above both\",\n      bsa_m2_dubois >  .5 & bsa_m2_haycock <= .5 ~ \"above dubois only\",\n      bsa_m2_dubois <= .5 & bsa_m2_haycock >  .5 ~ \"above haycock only\",\n      bsa_m2_dubois <= .5 & bsa_m2_haycock <= .5 ~ \"below both\",\n    )\n  ) |> \n  relocate(id, 1)\nbsa_pop_2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 480,000 × 8\n      id age_mn sex_fct height_cm weight_kg bsa_m2_dubois bsa_m2_haycock\n   <int>  <int> <fct>       <dbl>     <dbl>         <dbl>          <dbl>\n 1     1      1 male         58.1      5.70         0.286          0.310\n 2     2      1 female       55.0      4.37         0.246          0.263\n 3     3      2 male         62.2      6.58         0.319          0.343\n 4     4      2 female       56.5      4.52         0.254          0.270\n 5     5      3 male         59.6      6.41         0.307          0.333\n 6     6      3 female       63.6      7.08         0.335          0.361\n 7     7      4 male         64.4      6.87         0.334          0.356\n 8     8      4 female       63.8      6.34         0.321          0.340\n 9     9      5 male         65.9      7.42         0.351          0.375\n10    10      5 female       64.5      8.19         0.360          0.392\n# ℹ 479,990 more rows\n# ℹ 1 more variable: threshold <chr>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() + \n  geom_point(\n    data = bsa_pop_2,\n    mapping = aes(height_cm, weight_kg, color = threshold), \n    alpha = .25, \n    shape = \".\",\n    show.legend = FALSE\n  ) +\n  geom_path(\n    data = bsa_cutoff,\n    mapping = aes(height_cm, weight_kg, linetype = method)\n  ) +\n  facet_wrap(~ age_mn, ncol = 4) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/height-weight-bsa-scatterplot-detailed-1.png){width=768}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbsa_pop_2 |> \n  summarise(\n    dubois_0.4  = mean(bsa_m2_dubois > 0.4),\n    dubois_0.5  = mean(bsa_m2_dubois > 0.5),\n    dubois_0.6  = mean(bsa_m2_dubois > 0.6),\n    haycock_0.4 = mean(bsa_m2_haycock > 0.4),\n    haycock_0.5 = mean(bsa_m2_haycock > 0.5),\n    haycock_0.6 = mean(bsa_m2_haycock > 0.6),\n    .by = age_mn\n  ) |> \n  pivot_longer(\n    cols = c(\n      starts_with(\"dubois\"), \n      starts_with(\"haycock\")\n    ),\n    names_to = \"group\",\n    values_to = \"percent_above_threshold\"\n  ) |> \n  separate(\n    col = group, \n    into = c(\"method\", \"threshold\"), \n    sep = \"_\"\n  ) |> \n  mutate(threshold = paste(\"BSA >\", threshold, \"m^2\")) |> \n  ggplot(aes(\n    x = age_mn, \n    y = percent_above_threshold, \n    linetype = method, \n    shape = method\n  )) +\n  scale_y_continuous(label = scales::label_percent()) +\n  scale_x_continuous(breaks = seq(0, 24, 6)) +\n  geom_path() +\n  geom_point() +\n  facet_wrap(~threshold) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/bsa-method-comparison-1.png){width=768}\n:::\n:::\n\n\n\n\n## Epilogue\n\nFor example, [Hayes et al (2014)](https://pmc.ncbi.nlm.nih.gov/articles/PMC4339962/pdf/BLT.14.139113.pdf) provide detailed regional weight-for-age charts based on GAMLSS models\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}