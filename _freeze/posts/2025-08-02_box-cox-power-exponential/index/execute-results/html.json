{
  "hash": "4a7f98389fba1df2d713fa18f24edbc8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The Box-Cox power exponential distribution\"\ndescription: \"This is so obviously a prequel post to something else that I want to write about GAMLSS models\"\ndate: \"2025-08-02\"\n--- \n\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nI have for the longest time been intending to write up some notes about [generalised additive model for location scale and shape (GAMLSS)](https://en.wikipedia.org/wiki/Generalized_additive_model_for_location,_scale_and_shape) regression and its application to modelling growth curves of various kinds. It's a problem that pops up in my pharmacometrics work every now and then, usually because we're interested in simulating the distribution of drug exposures across some target population, often alongside distributions of efficacy and safety endpoints under some proposed dosing regimen. If a proposed dosing regimen depends on measurements such as age, weight, height, or body surface area, then it becomes important to be able to be able to say something about how these measures are distributed across the population and sample from the appropriate distribution.\n\nI mean... duh. \n\nOne of the most commonly used approaches to this problem (see discussion in [Borghi et al 2005](https://doi.org/10.1002/sim.2227)) is to use the GAMLSS framework proposed by [Rigby and Stasinopoulos (2004)](https://doi.org/10.1002%2Fsim.1861), which in turn extends the [generalised additive model](https://en.wikipedia.org/wiki/Generalized_additive_model) and [generalised linear model](https://en.wikipedia.org/wiki/Generalized_linear_model) frameworks that I'm too lazy to reference properly in a blog post. This is the approach I've typically used, and I've found that it works rather well. Indeed, the use of GAMLSS regressions for growth curve modelling is the thing I *actually* want to write a blog post about, but every time I start thinking about what I want to say, I keep coming to the conclusion that the place I need to *start* isn't with the regression model per se, it's with the distributions used to describe variability in the GAMLSS framework. And so with a heavy sigh, I put aside the fun thing I want to write about, and instead decide that the thing my blog needs is...\n\n...a tutorial introduction to the Box-Cox power exponential distribution.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(gamlss.dist)\n```\n:::\n\n\n\n## Oh look, real world data are annoying\n\nI'll start with a data set I use very often when doing this kind of work: the [National Health and Nutrition Examination Survey](https://wwwn.cdc.gov/nchs/nhanes/),^[Yes, it's an American data set. You can tell by the fact that it uses the word \"national\" in the title, without perceiving that there might be a need to specify *which* of the literally hundreds of nations on this planet the word refers to. It takes a certain kind of insular psychology to do this, which very quickly narrows down the list of possible culprits. Suffice it to say, the citizens of every other nation besides the one that *always* does this find it extraordinarily annoying.] usually abbreviated to NHANES. I'll probably talk more about NHANES when I finally write the GAMLSS post I've been promising, but for now let's just load some data that I preprocessed earlier:^[If you must know, this is a subset constructed from the `DEMO-L.xpt` and `BMX-L.xpt` data cuts for demographics and body measurements in the post-pandemic 2021-2023 release.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes <- read_csv(\"nhanes-v01.csv\", show_col_types = FALSE) \nnhanes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8,403 × 6\n   sex_num sex_fct weight_kg height_cm age_mn age_yr\n     <dbl> <chr>       <dbl>     <dbl>  <dbl>  <dbl>\n 1       0 male         86.9      180.    516  43   \n 2       0 male        102.       174.    792  66   \n 3       1 female       69.4      153.    528  44   \n 4       1 female       34.3      120.     71   5.92\n 5       0 male         90.6      173.    408  34   \n 6       1 female      104.       156.    816  68   \n 7       1 female      124.       168.    324  27   \n 8       0 male         79.8      169.    708  59   \n 9       1 female      123.       163.    372  31   \n10       1 female      116.       173.    396  33   \n# ℹ 8,393 more rows\n```\n\n\n:::\n:::\n\n\n\nSo here we have the kind of measurements you might expect to encounter when doing growth curve modelling. It's not super fancy as such things go: I've got measurements for age, weight, height, and sex. To give you a sense of what the data look like here's a scatterplot showing the joint distirbution of age, weight, and sex in the data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes |>\n  filter(age_yr <= 50) |>\n  ggplot(aes(age_yr, weight_kg)) + \n  geom_point() +\n  facet_wrap(~sex_fct) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/age-weight-curves-1.png){width=672}\n:::\n:::\n\n\n\nThere's nothing partictularly suprising about this plot: human beings do have a tendency to get bigger and heavier until we reach adulthood, after which the changes in weight are much less dramatic. Again... duh. But the devil is in the details here. You can see just from looking at the chart that (again, obviously) the variability changes over time: adult weights are a lot more variable than infant weights. A model for this data set will necessarily need to capture that heterogeneity, otherwise it will make some hilariously wrong predictions about how heavy a baby can be.\n\nBut if we look a little more closely, it becomes clear that capturing the mean weight and the standard deviation of the weight distribution will not be sufficient for any serious modelling purposes. To show this, I'll simplify the data set by looking only at adults over the age range 25-49, and -- not entirely accurately, as my scales pointedly remind me every morning -- making the assumption that the weight distributions don't change much over those years. After doing that we can plot good old fashioned histograms for adult weights separately for males and females:^[As always, I shall add the relevant \"trans and intersex disclaimer\": this is not the time and place to discuss nuance around gender and sex characteristics. These things do matter, but they aren't in the NHANES data nor does the Box-Cox power exponential distribution have anything useful to say on the topic, and with this in mind we shall move forward unburdened by such complexities.]\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes |>\n  filter(age_yr > 25 & age_yr <= 50) |>\n  ggplot(aes(weight_kg)) + \n  geom_histogram(bins = 40) +\n  facet_wrap(~sex_fct) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/weight-distribution-1.png){width=672}\n:::\n:::\n\n\n\nAh. Yeah, there's no way we're going to have much luck fitting a normal distribution to that: the distribution of body weight is positively skewed in pretty much every data set we encounter.^[To a pharmacometrician (as I now apparently am), this is a \"well duh\" kind of thing to say, because a shockingly large number of the variables we typically deal with in this field are right-skewed and are usually fit with log-normal distributions. In psychology, the discipline from which I emerged like some kind of twisted transsexual moth, it might be more of a surprise because unless you have been cursed with the knowledge of how to model reaction time distributions (sadly, like most mathematical psychologists, I have been so cursed) you might be used to data that are typically normal, or approximately so.] So we will need a family of distributions that can capture the [skewness](https://en.wikipedia.org/wiki/Skewness) we typically see in this kind of data.\n\nLe sigh. \n\nThe fact that we need something more flexible seems immediately apparent, but what shall we use? In a statement of the most trite variety, let us begin by noting that if we want a family of probability distributions that is flexible enough to be able to independently describe the *location* (e.g., mean), *scale* (e.g., variance), and *skewness* of the data, your distributional family will require at least three parameters. There's no escaping this: it is as inevitable as it is obvious.^[This is assuming you're not doing something ridiculous with your parameters. There are malicious ways of making one parameter do the work of more (see [Piantidosi, 2018](https://doi.org/10.1063/1.5031956) if you absolutely must), but to do so you have to create parameters that don't mean anything. It's a silly exercise, and we will not be engaging in that kind of chicanery here. We want to capture three properties: we will use three parameters. The end.] However, it's sometimes worth stating the obvious because in doing so we can automatically rule out a great many possible candidates: for example, the [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution), much beloved by pharmacometricians in other contexts, will in this particular instance be unsuitable for our needs. The [gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution) is likewise ruled out. Not even the [Weibull distribution](https://en.wikipedia.org/wiki/Weibull_distribution) is spared. Something more general is required. \n\nEnter, stage left and hopefully not pursued by a bear, the Box-Cox transformation and its close relative, the Box-Cox normal distribution.  \n\n## The Box-Cox transformation\n\nThe story begins in a place simple enough that I used to discuss it in graduate statistics classes for social science students. Often when we encounter empirical data that are visibly and blantantly non-normal in their distribution and our toolkit is only designed to handle normally distributed data, it is common to *transform* the data in some fashion that makes the transformed values sufficiently close to normal that we can heave a sigh of relief, chuck the transformation into a footnote somewhere, and happily report a $p$-value in the paper. While such a cavalier approach to data transformation was probably not what they had in mind at the time, a widely-used approach to this problem was introduced by [Box and Cox (1964)](https://www.jstor.org/stable/2984418), who suggested that we could compute the transformed variable $f_\\lambda(y)$ for a suitable value of $\\lambda$ as follows:^[Actually, if you look at the original paper they also suggest a two-parameter version of the transformation, but since I've never seen anyone use that version in the wild I shall not concern myself with it.]\n\n$$\nf_\\lambda(y) = \\left\\{ \\begin{array}{rl} \\frac{y^\\lambda - 1}{\\lambda} & \\mbox{ if } \\lambda \\neq 0 \\\\ \\ln(y) & \\mbox{ if } \\lambda = 0 \\end{array} \\right.\n$$\n\nThis has since become known as the [Box-Cox transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation) and has been the saviour of many an otherwise doomed Ph.D. thesis.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboxcox <- function(y, lambda) {\n  if (lambda == 0) return(log(y))\n  (y^lambda - 1) / lambda\n}\n\nbc <- expand_grid(\n  y = seq(1, 5, .01),\n  lambda = seq(-2, 2, .5)\n) |>\n  mutate(\n    fy = boxcox(y, first(lambda)),\n    .by = lambda\n  )\n\nggplot(bc, aes(y, fy, colour = factor(lambda))) +\n  geom_path() + \n  theme_bw() + \n  labs(\n    x = \"Untransformed value\", \n    y = \"Box-Cox transformed value\", \n    colour = \"lambda\"\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/box-cox-1.png){width=672}\n:::\n:::\n\n\n\n## The Box-Cox normal distribution\n\nIntroduced^[At least to the growth curve modelling literature, I'm sure there's an obscure physics paper in 1937 that used it once and then vanished into the mists of history.] by [Cole and Green (1992)](https://doi.org/10.1002/sim.4780111005). As described by [Freeman and Modarres (2002)](https://files.udc.edu/docs/dc_water_resources/technical_reports/report_n_190.pdf), this is a distribution over positive-valued numbers $y$ that has probability density function\n\n$$\np(y | \\mu, \\sigma^2, \\lambda) \n= \n\\frac{1}{k} \\frac{1}{\\sqrt{2\\pi} \\sigma} \n\\exp\\left( \n  - \\frac{1}{2\\sigma^2} \\left(\n        \\frac{y^{\\lambda - 1}}{\\lambda - 1} - \\mu\n    \\right)^2 \n\\right)\n$$\n\nwhere\n\n$$\nk = \\left\\{ \n\\begin{array}{cl}\n\\Phi(z) & \\mbox{ if } \\lambda > 0 \\\\\n1 & \\mbox{ if } \\lambda = 0 \\\\\n\\Phi(-z) & \\mbox{ if } \\lambda < 0 \n\\end{array}\n\\right.\n$$\n\nAs usual, $\\Phi(\\cdot)$ denotes the cumulative distribution function for a normal distribution with mean 0 and standard deviation 1, and in this expression $z = (1/(\\lambda\\sigma) + \\mu/\\sigma)$. If for some reason we ever wanted to implement this in R ourselves, a function like this would work:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity_bcn <- function(y, mu, sigma, lambda) {\n  z <- 1/(lambda * sigma) + mu/sigma\n  if (lambda > 0)  k <- pnorm(z)\n  if (lambda == 0) k <- 1\n  if (lambda < 0)  k <- pnorm(-z)\n  const <- 1 / k / sqrt(2 * pi) / sigma\n  f <- lambda - 1\n  distr <- exp(-(1/2/sigma^2) * (y^f/f - mu)^2)\n  p <- const * distr\n  p\n}\n```\n:::\n\n\n\nWhich is very nice and fancy, but usually we think of the Box-Cox normal distribution in a simpler way: we imagine some variable $z$ that is normally distributed with mean $\\mu$ and variance $\\sigma^2$, and then apply the inverse Box-Cox transform with parameter $\\lambda$:\n\n$$\n\\begin{array}{rcl} \nz & \\sim & \\mbox{Normal}(0, 1) \\\\\ny & = & \\left\\{ \n  \\begin{array}{rl} \n  \\mu (\\lambda \\sigma z + 1)^{1/\\lambda} & \\mbox{ if } \\lambda \\neq 0 \\\\\n  \\mu \\exp(\\sigma z) & \\mbox{ if } \\lambda = 0\n  \\end{array}\n\\right.\n\\end{array}\n$$\n\nThe resulting variable $y$ follows the Box-Cox normal distribution with parameters $\\mu$, $\\sigma^2$, and $\\lambda$. If we were truly desperate, we could write our own function that would sample from the Box-Cox normal distribution: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_bcn <- function(n, mu, sigma, lambda) {\n  z <- rnorm(n, mean = 0, sd = 1)\n  if (lambda == 0) return(mu * exp(sigma * z))\n  mu * (lambda * sigma * z + 1) ^ (1/lambda)\n}\n```\n:::\n\n\n\nHowever, there is little need for us to do so because the **gamlss.dist** package in R already does it for us. In **gamlss.dist** this distribution is referred to as the \"Box-Cox Cole and Green\" distribution, in reference to the Cole and Green paper I referenced above, but the documentation notes that this is simply another name for the Box-Cox normal distribution. The two functions I wrote in the previous section are entirely unnecessary because `gamlss.dist::rBCCG()` and `gamlss.dist::dBCCG()` already do exactly that, and are much more carefully written than my lazy implementation above. To illustrate the point, here's an example showing that `sample_bcn()` does the same thing as `rBCCG()`,\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- tibble(\n  lazy   = sample_bcn(10000, mu = 1, sigma = .1, lambda = 2),\n  proper = rBCCG(10000, mu = 1, sigma = .1, nu = 2) \n) \ndat |>\n  pivot_longer(\n    cols = c(lazy, proper),\n    names_to = \"type\",\n    values_to = \"values\"\n  ) |> \n  ggplot(aes(values)) + \n  geom_histogram(bins = 100) +\n  facet_wrap(~type) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-to-rBCCG-1.png){width=672}\n:::\n:::\n\n\n\nand here's an example illustrating the connection between `density_bcn()` and `dBCCG()`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- tibble(\n  y      = seq(0.4, 1.4, by = 0.01),\n  lazy   = density_bcn(y, mu = 1, sigma = .1, lambda = 2),\n  proper = dBCCG(y, mu = 1, sigma = .1, nu = 2) \n) \ndat |>\n  pivot_longer(\n    cols = c(lazy, proper),\n    names_to = \"type\",\n    values_to = \"density\"\n  ) |> \n  ggplot(aes(y, density)) + \n  geom_path() +\n  facet_wrap(~type, scales = \"free\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-to-dBCCG-1.png){width=672}\n:::\n:::\n\n\n\nOkay. So now we have a family of distributions that has one parameter describing the location (i.e., $\\mu$), another parameter describing the scale (i.e., $\\sigma$), and a third parameter describing the skewness (i.e., $\\lambda$). This is good, and it will allow us to solve the problem at hand. Even better, this family of distributions has a natural connection to the Box-Cox method of data transformation that many (most?) of us encounter as graduate students, so there is a sense in which it feels quite natural. Regression models that use the Box-Cox normal distribution are often referred to as the \"LMS method\" for growth curve modelling. This terminology was introduced by Cole and Green (1992):\n\n- The **L** parameter refers to lambda ($\\lambda$), and captures skewness\n- The **M** parameter refers to mu ($\\mu$), and captures location\n- The **S** parameter refers to sigma ($\\sigma$), and captures scale\n\nIn the [CDC growth charts](https://www.cdc.gov/growthcharts/cdc-data-files.htm) for example, the \"LMS\" parameters refer to the parameters of the corresponding Box-Cox normal distribution associated with the relevant age group. \n\n## The Box-Cox power exponential distribution\n\nWe are now in a position to introduce the Box-Cox power exponential (BCPE) distribution. The goal when building the Box-Cox normal (BCN) distribution was to extend the normal distribution in a way that allows us to manipulate the skewness; by analogy, the goal in building the BCPE distribution is to extend the BCN distribution to capture kurtosis. As such, we'll now need four parameters to describe the distribution. \n\n$$\np(y | \\mu, \\sigma, \\nu, \\tau) \n= \\frac{y^{\\nu - 1}}{\\mu^\\nu \\sigma} p(z | \\tau)\n$$\n\nwhere $z$ is a power-exponential distributed variable related to $y$ via:\n\n$$\nz = \\left\\{ \n\\begin{array}{rl}\n1/(\\sigma\\nu) \\left( (y/\\mu)^\\nu - 1 \\right) & \\mbox{ if } \\nu \\neq 0 \\\\\n1/\\sigma \\log (y/\\mu) & \\mbox{ if } \\nu = 0\n\\end{array}\n\\right.\n$$\n\nThe probability density function $p(z|\\tau)$ is given by\n\n$$\np(z | \\tau) = \\frac{\\tau}{c2^{1 + 1/\\tau} \\Gamma(1/\\tau)} \\exp(-0.5 |z/c|^\\tau)\n$$\n\nwhere $c^2 = 2^{-2/\\tau} \\Gamma(1/\\tau) (\\Gamma(3/\\tau))^{-1}$, and $\\Gamma(\\cdot)$ is the usual gamma function.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndensity_bcpe <- function(y, mu, sigma, nu, tau) {\n  c <- sqrt(2^{-2/tau} * gamma(1/tau) / gamma(3/tau))\n  if (nu == 0) z <- (1/sigma) * log (y/mu)\n  if (nu != 0) z <- (1/(sigma*nu)) * ((y/mu)^nu - 1)\n  pz <- tau / (c * 2^(1 + 1/tau) * gamma(1/tau)) * exp(-0.5 * abs(z/c)^tau)\n  py <- pz * y^(nu - 1) / (sigma * mu^nu)\n  py\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndat <- tibble(\n  y      = seq(6, 14, by = 0.1),\n  lazy   = density_bcpe(y, mu = 10, sigma = .1, nu = 2, tau = 5),\n  proper = dBCPE(y, mu = 10, sigma = .1, nu = 2, tau = 5) \n) \ndat |>\n  pivot_longer(\n    cols = c(lazy, proper),\n    names_to = \"type\",\n    values_to = \"density\"\n  ) |> \n  ggplot(aes(y, density)) + \n  geom_path() +\n  facet_wrap(~type, scales = \"free\") + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare-to-dBCPE-1.png){width=672}\n:::\n:::\n\n\n\nOkay. Fabulous. It works. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_val   <- seq(6, 14, 0.01)\nnu_val  <- c(-9, -3, 0, 3, 9)\ntau_val <- c(1, 2, 4, 16)\n\ndat <- expand_grid(y = y_val, mu = 10, sigma = .1, nu = nu_val, tau = tau_val) |> \n  mutate(\n    density = dBCPE(y, mu, sigma, nu, tau), .by = c(\"mu\", \"sigma\", \"nu\", \"tau\"),\n    nu_lbl = factor(nu, levels = nu_val, labels = paste(\"nu =\", nu_val)),\n    tau_lbl = factor(tau, levels = tau_val, labels = paste(\"tau =\", tau_val))\n  )\n\ndat |>\n  ggplot(aes(y, density)) + \n  geom_path() +\n  facet_grid(tau_lbl ~ nu_lbl) + \n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/explore-bcpe-1.png){width=768}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}