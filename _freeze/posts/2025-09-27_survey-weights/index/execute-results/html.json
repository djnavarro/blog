{
  "hash": "e02038fbae653bb1b0762ab447185220",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Some notes on survey weights\"\ndescription: \"An area of statistics in which the author is not strong, and really needs to up her game\"\ndate: \"2025-09-27\"\ncategories: [\"Statistics\", \"Surveys\"]\nknitr:\n  opts_chunk: \n    dev.args:\n      bg: \"#00000000\"\n--- \n\n\n\n<!--------------- my typical setup ----------------->\n\n\n\n\n\n\n\n<!--------------- post begins here ----------------->\n\nðŸ«€ : Okay. So. Um. Hey babe. You know that [GAMLSS regression post](/posts/2025-09-07_gamlss/) we wrote? <br>\n\nðŸ§  : Do you mean the monstrosity that took an entire month out of our lives, spawned an unhinged prequel post about the [Box-Cox power exponential distribution](/posts/2025-08-02_box-cox-power-exponential/) and then *another* unhinged prequel post about [B-splines and P-splines](/posts/2025-09-06_p-splines/)? The one that brought us to tears several times and made us question our entire reason for being? The one that literally gave you nightmares? The accursed one, the post we swore we were finished with and would never ever revisit upon pain of death. Is **that** the post you mean? <br>\n\nðŸ«€ : Uh, yeah.<br>\n\nðŸ§  : Oh yes, I remember it. What about it? <br>\n\nðŸ«€ : I, um... look... I think it, uhhhhh... needs a sequel post. We should talk about survey weights. I mean, I know it's not our job to talk about all the things, but it does sort of matter right? And we've come so far with this thing, we should finish the job properly, right? I mean, I know we're not getting paid for this, but you *do* like to do a good job with things right? Just one more post? Please????<br>\n\nðŸ§  : ... <br>\n\nðŸ«€ : ... <br>\n\nðŸ§  : Girl. Seriously though. What the actual fuck is wrong with you?\n\n<br><br>\n\nThe worst thing about being a scientist^[Aside from, oh idk, just a lil hypothetical here, an authoritarian government gutting science funding, branding scientists as traitors for researching the wrong topics, threatening researchers, and gluing the cobblestones on the road to fascism in place using the blood and tears of those scientists that have the gall to care about human beings that aren't themselves.] is the fact that no matter how hard you work and no matter how diligent you are in learning skills that fall outside your core discipline, you will on a regular basis get slammed by [\"outside context problems\"](https://tvtropes.org/pmwiki/pmwiki.php/Main/OutsideContextProblem). You are trained to work within a particular framework, and you are exquisitely skilled at handling situations that fall within the scope of that framework. You are, almost by definition, a *specialist*. The days of the \"renaissance polymath\" are well and truly behind us, simply because science has advanced so far that no human mind can encompass the whole bloody thing at this point. It's impossible.\n\nThis is of course a great triumph for science, but a terrible tragedy for the scientist who will *always* find themselves getting fucked over badly the moment \"the thing I was trained for\" turns out to require knowledge from \"one of the million other things I was not trained for\". The cruel reality of science is that we feel like fucking morons every single day because there are so very many traps, tripwires, and landmines strewn across the golden fields of science. \n\nWe all fuck up. All the time. It's a core part of the job, actually. The trick, if you're a newly minted scientist hoping to survive in the badlands of real research, is to remain humble in the face complexity. When -- not if -- it happens to you, and you make a mistake, the thing you have to do own it and admit you were wrong.^[I am currently giving very serious side eye at [Uri Simonsohn](https://datacolada.org/129) in this regard. Stop being a dick, dude. You fucked up because you ventured outside your area of expertise, and you've been gently corrected in the literature by people who know this stuff better than you do. And that's okay, as long as you stop doubling down on the mistake. Just a thought from a girl who no longer has skin in that particular game.] Nobody with a shred of integrity will think less of you for admitting a mistake. \n\nAnd so it is in this spirit that I have to admit there's a mistake in my [GAMLSS post](/posts/2025-09-07_gamlss/). As of this exact moment of writing I don't know for certain how bad the mistake is because I haven't checked yet.^[I suppose if you really want to check you can take a look at the git log on the blog repo and confirm that yep, I'm writing this introductory section *before* writing any of the code to investigate the mistake. The anxiety is real.] I have some reason to think my mistake isn't *terrible*, but I don't know for sure, and I'm about to find out as soon as I write the rest of this post. \n\nNot gonna lie, I'm a bit nervous. \n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gamlss)\nlibrary(ggplot2)\nlibrary(quartose)\n```\n:::\n\n\n\n## Survey weights\n\nThe mistake I made in that post is one that experimentalists such as myself^[I mean, it's now been about 5 years since the last time I actually *ran* an experiment of my very own. I'm a data analyst by trade now, and unlike my old discipline of mathematical psychology where analysts tend to also run their own experiments, pharmacometricians tend not to conduct the studies they analyse themselves. Nevertheless, both disciplines are built atop a foundation of experimental science, and I notice that pharmacometricians tend to think in ways I find very familiar as a former math psych girl.] are particularly prone to. I took data from a structured, stratified survey -- in this case the [National Health and Nutrition Examination Survey](https://www.cdc.gov/nchs/nhanes/) (NHANES) -- and analysed it as if it were a simple random sample, which it most certainly is not. In short, I forgot to consider **survey weights.** The mistake is embarrassing to me because the NHANES website has an entire [study design tutorial](https://wwwn.cdc.gov/nchs/nhanes/tutorials/sampledesign.aspx) that talks specifically about this issue^[My sincere thanks to [Benjamin Rich](https://www.linkedin.com/in/benjaminrichphd/) and [Thomas Lumley](https://profiles.auckland.ac.nz/t-lumley) who, in different contexts, both found very gentle ways to point me in the right direction. I mean, I did sort of know I had to think about this but I just... didn't.] and like an idiot^[In my defence, I was not an idiot. I was an analyst operating under time pressure, and I missed a detail that in hindsight I should have paid more attention to. Sigh. But it happens to us all, and in an attempt to practice what I preach, I'm admitting it openly.] I didn't take it into account. Siiiiiiiiiiiiiiiigh.\n\nTo understand the nature of my mistake, all you need to do is read the first paragraph of the NHANES tutorial discussing the design of the study:\n\n> The NHANES samples are not simple random samples. Rather, a complex, multistage, probability sampling design is used to select participants representative of the civilian, non-institutionalized US population. Oversampling of certain population subgroups is also done to increase the reliability and precision of health status indicator estimates for these particular subgroups. Researchers need to take this into account in their analyses by appropriately specifying the sampling design parameters. \n\nOh. Right. Yeah. Guess which bitch forgot to do that? Guess which bitch is now all of a sudden remembering that *lots* of surveys use stratified sampling, *lot* of surveys use oversampling for populations of interest, and *lots* of surveys discuss survey weights quite prominently on their websites^[As another example, here's the Australian [Survey of Income and Housing](https://www.abs.gov.au/statistics/detailed-methodology-information/concepts-sources-methods/survey-income-and-housing-user-guide-australia/2019-20), and lo and behold it has a page explicitly discussing [survey weights](https://www.abs.gov.au/statistics/detailed-methodology-information/concepts-sources-methods/survey-income-and-housing-user-guide-australia/2019-20/weights) for the study.] in the hope that researchers don't make the exact mistake that she did, in fact, make? That's right, this bitch.\n\nOops.\n\n<br>\n\n## How bad was my mistake?\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for NHANES data import and preprocessing\"}\n# directories\npost_dir   <- rprojroot::find_root_file(\n  criterion = rprojroot::has_file(\".here\")\n)\nlocal_dir  <- fs::path(post_dir, \"nhanes\")\ndata_dir   <- fs::path(local_dir, \"data\")\noutput_dir <- fs::path(local_dir, \"output\")\n\n# conversion based on NHANES average\nlength_to_height <- function(length_cm) {\n  length_cm - 1.06\n}\n\n# all demographics and body measurement files\ndemo_files <- fs::dir_ls(fs::path(data_dir, \"demo\"))\nbmx_files <- fs::dir_ls(fs::path(data_dir, \"bmx\"))\n\n# read demographics file (selected variables only)\ndemos <- demo_files |> \n  purrr::map(\\(xx) {\n    dd <- haven::read_xpt(xx) \n    if (!exists(\"WTMEC2YR\", where = dd)) dd$WTMEC2YR <- NA_real_\n    if (!exists(\"RIDEXAGM\", where = dd)) dd$RIDEXAGM <- NA_real_\n    dd <- dplyr::select(dd, SEQN, RIAGENDR, RIDAGEYR, RIDAGEMN, RIDEXAGM, WTMEC2YR)\n    dd\n  }) |> \n  dplyr::bind_rows(.id = \"file_demo\") |> \n  dplyr::mutate(file_demo = fs::path_file(file_demo))\n\n# read body measurements file (selected variables only)\nbmxes <- bmx_files |> \n  purrr::map(\\(xx) {\n    dd <- haven::read_xpt(xx) \n    dd <- dplyr::select(dd, SEQN, BMXWT, BMXHT, BMXRECUM)\n    dd\n}) |> \n  dplyr::bind_rows(.id = \"file_bmx\") |> \n  dplyr::mutate(file_bmx = fs::path_file(file_bmx))\n\n# join data sets, retaining only those rows where the\n# required body measurements exist\nnhanes <- bmxes |>\n  dplyr::left_join(demos, by = \"SEQN\") |>\n  dplyr::select(\n    id          = SEQN,\n    sex_s       = RIAGENDR, # sex/gender at screen (1 = M, 2 = F, . = NA)\n    weight_kg_e = BMXWT,    # weight at exam\n    height_cm_e = BMXHT,    # standing height at exam\n    length_cm_e = BMXRECUM, # recumbent length at exam (0-47 months only)\n    age_yr_s    = RIDAGEYR, # natal age at screening (years)\n    age_mn_s    = RIDAGEMN, # natal age at screening (months; 0-24 mos only)\n    age_mn_e    = RIDEXAGM, # natal age at exam (months; 0-19 years only)\n    survey_wt   = WTMEC2YR, # 2-year weight for persons with exam data \n    file_demo,\n    file_bmx\n  ) |>\n  dplyr::mutate(\n    sex_num = sex_s - 1, # rescale to 0 = M, 1 = F\n    sex_fct = factor(sex_s, levels = 1:2, labels = c(\"male\", \"female\")),\n    age_mn = dplyr::case_when(\n      !is.na(age_mn_e) ~ age_mn_e, # use exam months if present\n      !is.na(age_mn_s) ~ age_mn_s, # else use survey months\n      TRUE ~ (age_yr_s * 12)       # else use age in years\n    ),\n    age_yr = age_mn / 12,\n    weight_kg = weight_kg_e,\n    height_cm = dplyr::case_when(\n      !is.na(height_cm_e) ~ height_cm_e, # use height if it was measured\n      !is.na(length_cm_e) ~ length_to_height(length_cm_e), # or convert length\n      TRUE ~ NA_real_, # else missing\n    ),\n    cohort = dplyr::case_when(\n      file_bmx == \"BMX.xpt\"   & file_demo == \"DEMO.xpt\"   ~ \"1999-2000\",\n      file_bmx == \"BMX_B.xpt\" & file_demo == \"DEMO_B.xpt\" ~ \"2001-2002\",\n      file_bmx == \"BMX_C.xpt\" & file_demo == \"DEMO_C.xpt\" ~ \"2003-2004\",\n      file_bmx == \"BMX_D.xpt\" & file_demo == \"DEMO_D.xpt\" ~ \"2005-2006\",\n      file_bmx == \"BMX_E.xpt\" & file_demo == \"DEMO_E.xpt\" ~ \"2007-2008\",\n      file_bmx == \"BMX_F.xpt\" & file_demo == \"DEMO_F.xpt\" ~ \"2009-2010\",\n      file_bmx == \"BMX_G.xpt\" & file_demo == \"DEMO_G.xpt\" ~ \"2011-2012\",\n      file_bmx == \"BMX_H.xpt\" & file_demo == \"DEMO_H.xpt\" ~ \"2013-2014\",\n      file_bmx == \"BMX_I.xpt\" & file_demo == \"DEMO_I.xpt\" ~ \"2015-2016\",\n      file_bmx == \"BMX_J.xpt\" & file_demo == \"DEMO_J.xpt\" ~ \"2017-2018\",\n      file_bmx == \"P_BMX.xpt\" & file_demo == \"P_DEMO.xpt\" ~ \"2017-2020\",\n      file_bmx == \"BMX_L.xpt\" & file_demo == \"DEMO_L.xpt\" ~ \"2021-2023\",\n      TRUE ~ NA\n    ),\n    is_pandemic = dplyr::case_when(\n      file_bmx == \"P_BMX.xpt\" & file_demo == \"P_DEMO.xpt\" ~ TRUE,\n      TRUE ~ FALSE\n    )\n  )\n\n# retain only the to-be-used columns, and only those cases for which\n# age, weight, height, and sex are all present; filter to age < 80\n# because NHANES uses \"80\" to mean \"80 and above\" so the actual age\n# is not known\nok <- function(x) !is.na(x)\nnhanes <- nhanes |>\n  dplyr::select(\n    id, sex_num, sex_fct, weight_kg, height_cm, \n    age_mn, age_yr, cohort, survey_wt\n  ) |>\n  dplyr::filter(cohort != \"1999-2000\") |>  \n  dplyr::filter(\n    ok(sex_num), ok(weight_kg), ok(height_cm), \n    ok(age_mn), ok(survey_wt)\n  ) |>\n  dplyr::filter(age_yr < 80)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnhanes\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 90,954 Ã— 9\n      id sex_num sex_fct weight_kg height_cm age_mn age_yr cohort  survey_wt\n   <dbl>   <dbl> <fct>       <dbl>     <dbl>  <dbl>  <dbl> <chr>       <dbl>\n 1  9966       0 male         91.7      174.    472   39.3 2001-2â€¦    91353.\n 2  9967       0 male         84        167.    283   23.6 2001-2â€¦    29457.\n 3  9969       1 female       58        161.    612   51   2001-2â€¦    78536.\n 4  9970       0 male        139.       188.    200   16.7 2001-2â€¦    34060.\n 5  9971       1 female       62.2      151     176   14.7 2001-2â€¦     6968.\n 6  9972       0 male        102.       173.    534   44.5 2001-2â€¦    93559.\n 7  9973       1 female       64.9      156.    762   63.5 2001-2â€¦     8634.\n 8  9974       0 male         74.5      172     156   13   2001-2â€¦     5822.\n 9  9976       0 male         72.3      179.    436   36.3 2001-2â€¦    38188.\n10  9978       0 male         74.8      188.    193   16.1 2001-2â€¦    30235.\n# â„¹ 90,944 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for GAMLSS model fitting\"}\nheight_unweighted_file <- fs::path(post_dir, \"nhanes\", \"output\", \"height_unweighted.rds\")\nheight_weighted_file   <- fs::path(post_dir, \"nhanes\", \"output\", \"height_weighted.rds\")\n\nrerun <- FALSE \nif (rerun) {\n\n  # unweighted model for height by age\n  height_unweighted <- gamlss(\n    formula       = height_cm ~ pb(age_mn),\n    sigma.formula = ~pb(age_mn),\n    nu.formula    = ~1,\n    tau.formula   = ~1,\n    data    = nhanes,\n    family  = BCPE\n  )\n\n  # unweighted model for height by age\n  height_weighted <- gamlss(\n    formula       = height_cm ~ pb(age_mn),\n    sigma.formula = ~pb(age_mn),\n    nu.formula    = ~1,\n    tau.formula   = ~1,\n    data    = nhanes,\n    weights = survey_wt,\n    family  = BCPE\n  )\n\n  saveRDS(height_unweighted, file = height_unweighted_file)\n  saveRDS(height_weighted, file = height_weighted_file)\n\n} else {\n\n  height_unweighted <- readRDS(file = height_unweighted_file)\n  height_weighted <- readRDS(file = height_weighted_file)\n\n}\n\nheight_mod <- list(\n  unweighted = height_unweighted,\n  weighted = height_weighted\n)\n```\n:::\n\n\n```{.r .cell-code}\nquarto_tabset(height_mod, level = 3)\n```\n\n\n\n::: {.panel-tabset}\n\n \n\n\n### unweighted\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:   \ngamlss(formula = height_cm ~ pb(age_mn), sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = BCPE,      data = nhanes)  \n \nMu Coefficients: \n(Intercept)   pb(age_mn)   \n  112.16062      0.09085   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n -3.0352766    0.0003445   \nNu Coefficients: \n(Intercept)   \n     0.3298   \nTau Coefficients: \n(Intercept)   \n     0.8092   \n \n Degrees of Freedom for the fit: 46.85 Residual Deg. of Freedom   90907  \nGlobal Deviance:     634809  \n            AIC:     634903  \n            SBC:     635344  \n</pre> \n\n\n### weighted\n\n \n<pre> \n \nFamily:  c(\"BCPE\", \"Box-Cox Power Exponential\")  \nFitting method: RS()  \n \nCall:   \ngamlss(formula = height_cm ~ pb(age_mn), sigma.formula = ~pb(age_mn),   \n    nu.formula = ~1, tau.formula = ~1, family = BCPE,   \n    data = nhanes, weights = survey_wt)  \n \nMu Coefficients: \n(Intercept)   pb(age_mn)   \n  126.20643      0.07016   \nSigma Coefficients: \n(Intercept)   pb(age_mn)   \n -3.0012425    0.0002662   \nNu Coefficients: \n(Intercept)   \n     0.4141   \nTau Coefficients: \n(Intercept)   \n     0.8859   \n \n Degrees of Freedom for the fit: 43.5 Residual Deg. of Freedom   90911  \nGlobal Deviance:     20784300000  \n            AIC:     20784300000  \n            SBC:     20784300000  \n</pre> \n\n\n::: \n\n \n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code to compute and plot quantiles\"}\nget_pars <- function(data, model) {\n  pars <- tibble::tibble(\n    mu    = predict(model, newdata = data, type = \"response\", what = \"mu\"),\n    sigma = predict(model, newdata = data, type = \"response\", what = \"sigma\"),\n    nu    = predict(model, newdata = data, type = \"response\", what = \"nu\"),\n    tau   = predict(model, newdata = data, type = \"response\", what = \"tau\"),\n  )\n  dplyr::bind_cols(data, pars)\n}\n\nage_max_yr <- 40 \n\npredict_cases <- tidyr::expand_grid(\n  age_mn  = 1:(age_max_yr * 12),\n  weighted = c(TRUE, FALSE)\n)\n\npredict_pars <- dplyr::bind_rows(\n  predict_cases |> \n    dplyr::filter(weighted == TRUE) |> \n    dplyr::select(-weighted) |> \n    get_pars(height_mod$weighted) |> \n    dplyr::mutate(model_type = \"weighted\"),\n  predict_cases |> \n    dplyr::filter(weighted == FALSE) |> \n    dplyr::select(-weighted) |> \n    get_pars(height_mod$unweighted) |> \n    dplyr::mutate(model_type = \"unweighted\")\n)\n\npredict_quantiles <- predict_pars |>\n  dplyr::mutate(\n    q05 = qBCPE(.05, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q25 = qBCPE(.25, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q50 = qBCPE(.50, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q75 = qBCPE(.75, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    q95 = qBCPE(.95, mu = mu, sigma = sigma, nu = nu, tau = tau),\n    age_yr = age_mn / 12\n  )\n\npredict_quantiles_long <- predict_quantiles |> \n  tidyr::pivot_longer(\n    cols = dplyr::starts_with(\"q\"),\n    names_to = \"quantile\",\n    values_to = \"height_cm\"\n  )\n\npredict_quantiles_compare <- predict_quantiles_long |> \n  dplyr::select(age_mn, age_yr, height_cm, quantile, model_type) |> \n  tidyr::pivot_wider(\n    names_from = model_type,\n    values_from = height_cm\n  )\n\nggplot2::ggplot() + \n  ggplot2::geom_point(\n    data = nhanes |> \n      dplyr::filter(age_yr < age_max_yr), \n    mapping = ggplot2::aes(age_mn, height_cm),\n    size = .25\n  ) +\n  ggplot2::geom_path(\n    data = predict_quantiles_long,\n    mapping = ggplot2::aes(age_mn, height_cm, color = quantile)\n  ) +\n  ggplot2::facet_wrap(~model_type) +\n  ggplot2::theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/predict-quantiles-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for the replotting\"}\npredict_quantiles_compare |> \n  ggplot2::ggplot(\n    mapping = ggplot2::aes(\n      x = unweighted,\n      y = weighted, \n      color = quantile\n    )\n  ) +\n  ggplot2::geom_abline(intercept = 0, slope = 1) + \n  ggplot2::geom_point(size = 3, alpha = .1, show.legend = FALSE) + \n  ggplot2::facet_wrap(~quantile)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/predict-quantiles-2-1.png){width=672}\n:::\n:::\n\n\n\nWell thank fuck. \n\nThere **are** systematic differences between the unweighted and weighted models, which show up the moment we plot the *difference* between the two versions of the model on the y-axis, but... note the scale of the y-axis. The differences between the two models are never larger than a 2cm discrepancy in height regardless of age:\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Code for the residuals plot\"}\npredict_quantiles_compare |> \n  ggplot2::ggplot(\n    mapping = ggplot2::aes(\n      x = unweighted,\n      y = unweighted - weighted, \n      color = quantile\n    )\n  ) +\n  ggplot2::geom_hline(yintercept = 0) + \n  ggplot2::geom_point(size = 1.5, alpha = .1, show.legend = FALSE) + \n  ggplot2::facet_wrap(~quantile)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/predict-quantiles-residuals-1.png){width=672}\n:::\n:::\n\n\n\nI did fuck up, yes. But thankfully the discrepancies between what the correctly weighted model predicts and what my original unweighted model predicts are fairly small. In the context of the analyses that I described in the original [GAMLSS post](/posts/2025-09-07_gamlss/) and -- much more importantly -- the real world analyses in which I've used an unweighted model when I should have used a weighted model, the scale of the discrepancy is so small that it won't affect any of the \"downstream\" analyses. I'm not going to have to send an emergency mea culpa to any regulatory agency. In the context at hand for most pharmacometric simulations of expected drug exposure, something like this amounts to a mere rounding error and will not have even the slightest effect on the conclusions anyone draws from the data.\n\nAgain, thank fuck.\n\nBut.\n\nThe truth is that I got lucky, and so did all the other pharmacometricans who have used NHANES data the way I did. I forgot to take account of something that does matter, and I was fortunate enough that the scale of the mistake is so trivial that nobody who works with this kind of data in the real world would care. But the nature of luck is that you aren't always lucky. It just doesn't work that way. It is not too difficult to come up with examples where the survey weights really do matter, and the rest of the post will be devoted to showing why. \n\n\n<br>\n\n## References\n\nFor a post like this one, where I don't have strong expertise of my own and am writing it with the primary goal of improving my skills, I'm a little wary about suggesting references for others. But for what it's worth, here's some open access resources I relied on for my reading:\n\n- There's a nice page on the Pew Research Center website containing notes by Andrew Mercer, Arnold Lau, and Courtney Kennedy on [how different weighting methods work](https://www.pewresearch.org/methods/2018/01/26/how-different-weighting-methods-work/). If you read it on its own it's brief and helpful, is a nice entry point if you're an inveterate experimentalist at heart and survey data isn't your strength. Better yet, it's not a standalone document, it's acutally part of a much more comprehensive report on [weighting online opt-in samples](https://www.pewresearch.org/methods/2018/01/26/for-weighting-online-opt-in-samples-what-matters-most/), so it gives you ample opportunity to branch out.\n\n- One field in which I know these issues arise often is epidemiology, so it came as little surprise to me to discover that there is a useful chapter on [survey analysis](https://epirhandbook.com/en/new_pages/survey_analysis.html) chapter contained within [The Epidemiologist R Handbook](https://epirhandbook.com/en/).\n\n- Another resource I found useful is this online book on [how to weight a survey](https://bookdown.org/jespasareig/Book_How_to_weight_a_survey/) by Josep Espasa Reig. One nice thing about this one is that it's an informal walkthrough using examples in R, and it's intended to be readable by social scientists without any much expertise in the area.\n\nOn the software side, here are a few tools I started investigating:\n\n- The [survey](https://cran.r-project.org/package=survey) R package by Thomas Lumley, and the [srvyr](http://gdfe.co/srvyr/) package that provides a [dplyr](https://dplyr.tidyverse.org/)-like syntax for it.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}